{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MCI | Industrial Engineering &amp; Management","text":""},{"location":"#welcome-to-the-world-of-python","title":"Welcome to the world of <code>Python</code> !","text":"<p>Here, you\u2019ll find a variety of hands-on resources designed to guide you step-by-step through the world of programming, data analysis, and beyond. All educational materials on this page can be followed without any prior knowledge -- All you need is a computer, an internet connection, and curiosity . Since our course materials are centered around the <code>Python</code>  programming language, all software used is free and open-source. </p> <p>Dive in and discover new skills at your own pace!</p> Info <p>This page hosts course materials for various Python  lectures.</p> <p>As we continously try to improve the materials and this page, content might be subject to change. If you have any questions or suggestions, feel free to open an issue on GitHub or contact the maintainers:</p> <ul> <li> <p>     Manuel</p> <p> - manuel.ferdik@mci.edu</p> </li> <li> <p>     Jakob</p> <p> - jakob.klotz@mci.edu</p> </li> </ul>"},{"location":"#topics","title":"Topics","text":""},{"location":"#python-crash-course","title":"Python Crash Course","text":"<ul> <li> <p> Get started with the basics of Python, including installation, variables, data types, and control structures. You will learn how to work with lists, dictionaries, and tuples, and discover the essentials of writing functions and using packages like pandas. By the end, you will be able to write simple Python programs and understand fundamental programming concepts.</p> <p> Get started</p> </li> </ul>"},{"location":"#python-extensive-course","title":"Python Extensive Course","text":"<ul> <li> <p> Delve deeper into Python\u2019s capabilities, covering advanced use of data types, control structures, and functions - plus practical development tools like Git. You will practice data handling with pandas, learn how to build basic graphical interfaces, and get hands-on experience with data acquisition and plotting. By the end, you will be confident in developing more robust Python applications and managing code in collaborative environments.</p> <p> Get started</p> </li> </ul>"},{"location":"#statistics","title":"Statistics","text":"<ul> <li> <p> Explore the foundations of descriptive and inferential statistics with Python. You will learn how to calculate univariate and bivariate measures, work with probability theory, and perform hypothesis testing and regression analysis. By the end, you will be able to apply statistical methods to real-world datasets and interpret the results with confidence.</p> <p> Get started</p> </li> </ul>"},{"location":"#data-science","title":"Data Science","text":"<ul> <li> <p> Learn how to prepare and preprocess data before diving into machine learning concepts such as supervised and unsupervised learning. You will explore evaluation metrics, build end-to-end pipelines, and discover best practices for model persistence. By the end, you will be ready to implement data science workflows and create data-driven solutions to complex problems.</p> <p> Get started</p> </li> </ul>"},{"location":"#computer-vision","title":"Computer Vision","text":"<ul> <li> <p> Gain an understanding of object detection and segmentation techniques, while exploring YOLO-based approaches for both images and videos. You will learn how to process visual data, develop detection solutions, and even train your own models. By the end, you will be equipped to tackle fundamental computer vision tasks and build custom vision applications.</p> <p> Get started</p> </li> </ul>"},{"location":"assets/python/pandas/spotify-top50/","title":"Spotify top50","text":"<p>Excerpt of the data set (snapshot date: 2024-09-25):</p> daily_rank name artists popularity is_explicit energy 1 The Emptiness Machine Linkin Park 93 True 0.872 2 Rote Flaggen Berq 76 True 0.336 3 Bauch Beine Po Shirin David 80 True 0.746 4 Die With A Smile Lady Gaga, Bruno Mars 100 False 0.592 5 BIRDS OF A FEATHER Billie Eilish 99 False 0.507"},{"location":"data-science/","title":"Home","text":"Under Construction <p>The <code>Data Science Course</code> is currently under construction.  </p> <p>Please check back later for updates.</p> <p></p>"},{"location":"data-science/algorithms/","title":"Introduction","text":"<p>With extensive data preparation knowledge, we can tackle the next big part of the course: algorithms. An algorithm is a</p> <p>a set of mathematical instructions or rules that, especially if given to a computer, will help to calculate an answer to a problem.</p> <p>Cambridge Dictionary</p> <p>In data science/machine learning, algorithms are used to solve problems,  such as modelling data to make predictions for unseen data, or clustering data  to find patterns.</p> <p>The consecutive chapters will introduce you to common algorithms, like  linear and logistic regression, decision trees, and k-means clustering. We  will explore the theory as well as practical examples. First, we establish two  main concepts in machine learning: supervised and unsupervised learning.</p>"},{"location":"data-science/algorithms/#supervised-learning","title":"Supervised Learning","text":"<p>Supervised learning is a type of machine learning where algorithms learn from  labeled training data to make predictions on new, unseen data. The term  \"supervised\" comes from the idea that the algorithm is guided by a  \"supervisor\" (the labeled data) that provides the correct answers during training.</p> <p>In supervised learning, each training example consists of:</p> <ul> <li>Input features (\\(X\\)): The characteristics or attributes we use to make      predictions</li> <li>Target variable (\\(y\\)): The correct output we want to predict</li> </ul> <p>The algorithm learns the relationship between inputs (\\(X\\)) and outputs  (\\(y\\)), creating a model that can then (hopefully!) generalize to new data.</p>"},{"location":"data-science/algorithms/#example","title":"Example","text":"<p>Assume we want to predict apartment prices (\\(y\\)) based on their size plus the number of rooms (\\(X\\)):</p> <pre><code>from sklearn.linear_model import LinearRegression\n\n# apartment data [m\u00b2, rooms]\nX = [\n    [75, 3],\n    [120, 4],\n    [50, 2],\n]\n# apartment prices\ny = [500_000, 675_000, 425_000]  # (1)!\n\n# use linear regression to predict apartment prices\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# predict price for a new apartment with 150m\u00b2 and 5 rooms\nnew_apartment = [[150, 5]]\npredicted_price = model.predict(new_apartment)\n</code></pre> <ol> <li>Underscores can be used as visual separators in numeric literals    to improve readability. They have no effect on the value of the number. For    example, <code>500_000</code> is the same as <code>500000</code>.</li> </ol> <p>For each new observation, we can use the trained model to predict the price. The apartment with 150m\u00b2 and 5 rooms has a predicted price of <code>775000</code>.</p> Info <p>Whether this estimate is actually close to reality depends on the quality of the model and its underlying data. Later, we will  discuss how to measure a model's quality.</p>"},{"location":"data-science/algorithms/#classification-vs-regression","title":"Classification vs. Regression","text":"<p>Supervised learning encapsulates both classification and regression tasks.</p> <pre><code>graph LR\n  A[Supervised Learning] --&gt; B[Classification];\n  A --&gt; C[Regression];</code></pre>"},{"location":"data-science/algorithms/#classification","title":"Classification","text":"<p>Classification problems involve predicting discrete categories or labels. The output is always one of a fixed set of classes. For instance, in binary classification, the model decides between two possibilities. </p> <p>For example, the Portuguese retail bank data can be used to predict  whether a customer would subscribe to a term deposit. The target variable is  binary: yes or no.</p> <p>On the other hand, multiclass classification handles three or more categories  (like classifying animals in photos  dog,  cat, dolphin, tiger, elephant, etc.).</p>"},{"location":"data-science/algorithms/#regression","title":"Regression","text":"<p>Regression problems, on the other hand, predict continuous numerical values. Instead of categorizing inputs into classes, regression models estimate a numerical value along a continuous spectrum. These models work by finding patterns in the data to estimate a mathematical function that best describes the relationship between inputs and the target variable.</p> <p>For instance the example, predicting the price of an apartment based on  its size and the number of rooms is a regression task.</p>"},{"location":"data-science/algorithms/#examples","title":"Examples","text":"<ul> <li> <p>Classification</p> <p>Predicting a categorical target variable:</p> <ul> <li>Spam or not spam</li> <li>Fraudulent or legitimate transaction</li> <li>Medical diagnosis (disease or no disease)</li> <li>Sentiment analysis of text (positive, negative, neutral)</li> <li>Image classification (cat, dog, dolphin, etc.)</li> <li>...</li> </ul> </li> <li> <p>Regression</p> <p>Predicting a continuous target variable:</p> <ul> <li>Apartment prices (like in the example above)</li> <li>Temperature</li> <li>Sales revenue</li> <li>...</li> </ul> </li> </ul> Info <p>No matter if you're dealing with a classification or regression task, the  key to successful supervised learning lies in having high-quality labeled data and selecting appropriate features (variables) that have predictive  power for the target variable.</p>"},{"location":"data-science/algorithms/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Contrary, unsupervised learning deals with unlabeled data to discover  hidden patterns and structures. Unlike supervised learning, there is no  \"supervisor\" providing correct answers - the algorithm must find meaningful patterns on its own.</p> <p>In unsupervised learning, we solely have:</p> <ul> <li>Input features (\\(X\\)): The characteristics or attributes of the data</li> </ul> <p>The algorithm's task is to find groupings, reduce complexity, or reveal underlying structures in the data.</p>"},{"location":"data-science/algorithms/#example_1","title":"Example","text":"<p>Let's say we want to segment customers based on their shopping behavior:</p> <pre><code>from sklearn.cluster import KMeans\n\n# customer data [annual_spending, avg_basket_size]\nX = [\n    [1200, 50],\n    [5000, 150],\n    [800, 30],\n    [4500, 140],\n    [1000, 45]\n]\n\n# use k-means to find customer segments\nmodel = KMeans(n_clusters=2)\nsegments = model.fit_predict(X)\n\nprint(segments)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1 0 1 0 1]\n</code></pre> <p>The variable <code>segments</code> contains the cluster assignments for each customer.  The cluster assignment is simply an <code>int</code> indicating which group the  customer belongs to. In this example, we have two clusters with the first  customer (<code>[1200, 50]</code>) belonging to cluster 1 and the second  customer (<code>[5000, 150]</code>) to cluster 0 and so on.</p> <p>The following plot visualizes the input data as scatter plot  colored by the cluster assignments:</p>          Similar data points are grouped to a cluster. Cluster 0 in the left          corner represents the first customer segment and cluster 1 in the right         corner the second.      <p>The algorithm will group similar customers together without being told what these groups should be - it discovers the patterns from the attributes itself.</p>"},{"location":"data-science/algorithms/#clustering-dimensionality-reduction","title":"Clustering &amp; Dimensionality Reduction","text":"<p>Unsupervised learning can be further divided into two main categories:</p> <pre><code>graph LR\n  A[Unsupervised Learning] --&gt; B[Clustering];\n  A --&gt; C[Dimensionality Reduction];</code></pre>"},{"location":"data-science/algorithms/#clustering","title":"Clustering","text":"<p>Clustering algorithms group similar data points together based on their features. The goal is to find cluster/groups in the data without any  prior knowledge of the groups just like in the previous customer segmentation example.</p>"},{"location":"data-science/algorithms/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction techniques aim to reduce the number of input features while preserving the most important information. This can help to simplify complex data, speed up algorithms, and improve model performance.</p>"},{"location":"data-science/algorithms/#examples_1","title":"Examples","text":"<ul> <li> <p>Clustering</p> <p>Clustering/grouping of similar data points:</p> <ul> <li>Customer segmentation in marketing (like in the example above)</li> <li>Anomaly detection</li> <li>Finding similar products in recommendations</li> <li>...</li> </ul> </li> <li> <p>Dimensionality Reduction</p> <p>Reducing the complexity of data:</p> <ul> <li>Feature extraction from high-dimensional data</li> <li>Visualization of complex datasets</li> <li>Noise reduction in signals</li> <li>...</li> </ul> </li> </ul> Info <p>While unsupervised learning offers powerful ways to explore and understand data, its results can be harder to evaluate since there are no \"correct\" answers to compare against. The value of the results often depends on how meaningful the discovered patterns are for the specific application.</p> Domain knowledge <p>No matter if you're dealing with supervised or unsupervised learning, domain knowledge is crucial. Understanding the data and the problem you're trying to solve will help you select the right algorithms, features,  and interpret the results.</p>"},{"location":"data-science/algorithms/#recap","title":"Recap","text":"<p>In this chapter, we introduced two fundamental concepts in machine learning: supervised and unsupervised learning. While supervised learning works with labeled data to make predictions, unsupervised learning is used with  unlabeled data to reduce complexity or find clusters.</p> <p>The following chapters will explore specific algorithms from both categories:</p> <p>Supervised Learning:</p> <pre><code>graph LR\n A[Supervised Learning] --&gt; B[Regression: *Linear Regression*];\n A --&gt; C[Classification: *Logistic Regression*];\n C --&gt; D[*Decision Tree, Random Forest, Neural Network*];\n B --&gt; D;</code></pre> <ul> <li>Logistic Regression for classification tasks</li> <li>Linear Regression for predicting continuous values</li> <li>Decision Tree, Random Forest and Neural Network for both regression and    classification tasks</li> </ul> <p>Unsupervised Learning:</p> <pre><code>graph LR\n A[Unsupervised Learning] --&gt; B[Clustering: *k-means*];\n A --&gt; C[Dimensionality Reduction: *Principal Component Analysis*];</code></pre> <ul> <li>k-means for clustering similar data points</li> <li>Principal Component Analysis (PCA) for dimensionality reduction</li> </ul> <p>We will cover the theory and illustrate each algorithm with a practical  example.</p>"},{"location":"data-science/algorithms/supervised/classification/","title":"Classification","text":""},{"location":"data-science/algorithms/supervised/classification/#logistic-regression","title":"Logistic Regression","text":"<p>While linear regression helps us predict continuous values, many real-world problems require predicting categorical outcomes: Will a customer subscribe to a term deposit? Is an email spam? Is a transaction fraudulent?  Logistic regression addresses these binary classification problems by extending the concepts we learned in linear regression to predict probabilities between  0 and 1.</p> <p>We will cover the theory and apply logistic regression to the breast cancer dataset to predict whether a tumor is malignant or benign.</p>"},{"location":"data-science/algorithms/supervised/classification/#theory","title":"Theory","text":"Info <p>The theoretical part is adapted from:</p> <p>Daniel Jurafsky and James H. Martin. 2025. Speech and Language  Processing: An Introduction to Natural Language Processing, Computational  Linguistics, and Speech Recognition with Language Models<sup>1</sup></p>"},{"location":"data-science/algorithms/supervised/classification/#deja-vu-linear-regression","title":"Deja vu: Linear regression","text":"<p>Just like in linear regression, we have a set of features \\(x_1, x_2, ..., x_n\\) describing an outcome \\(y\\). But instead of predicting a continuous value, \\(y\\) is binary: 0 or 1.</p> <p>Similar to linear regression, logistic regression uses a linear combination of the features to predict the outcome. I.e., each feature is assigned a  weight, and a bias term is added at the end.</p> Linear combination \\[ z = b_1 \\cdot x_1 + b_2 \\cdot x_2 + ... + b_n \\cdot x_n + a \\] <p>with \\(a\\) being the bias term and \\(b_1, b_2, ..., b_n\\) the weights.  \"The resulting single number \\(z\\) expresses the weighted sum of the evidence for the class.\" (Jurafsky &amp; Martin, 2025 p. 79) Bias, weights and the intercept are all real numbers.</p> <p>So far, logistic regression is the same as linear regression with the sole  difference that in linear regression we  referred to the bias \\(a\\) as the intercept, and the  weights \\(b_1, b_2, ..., b_n\\) as coefficients or slope.</p> <p>However, \\(z\\) is not the final prediction, since it can take real values  and in fact ranges from \\(-\\infty\\) to \\(+\\infty\\). Thus, \\(z\\) needs to be  transformed to a probability between 0 and 1. This is where the sigmoid function comes into play.</p>"},{"location":"data-science/algorithms/supervised/classification/#the-sigmoid-function","title":"The sigmoid function","text":"<p>Unlike linear regression, which outputs unbounded values, logistic regression uses the sigmoid (or logistic) function to transform \\(z\\) into a probability</p> Sigmoid function \\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\] <p>The sigmoid function takes the real number \\(z\\) and transforms it to the  range (0,1).</p>          An illustration of the sigmoid function often referred to as          logistic function. Thus, the name logistic regression.      <p>For given input features \\(x_1, x_2, ..., x_n\\), we can calculate the  linear combination \\(z\\) and then apply the sigmoid function to get the  probability of the outcome. To compute the probability of the outcome being 1   \\(P(y=1|x)\\), for example  if an email is spam, we have to set a decision boundary.</p> Decision boundary <p>If \\(\\sigma(z) \\gt 0.5\\), we predict \\(y=1\\), otherwise \\(y=0\\).</p> <p>For instance, if the probability of an email being spam is 0.7, we predict that the email is spam \\((0.7 \\gt 0.5)\\). With a probability of 0.4, we  predict that the email is not spam \\((0.4 \\le 0.5)\\).</p>"},{"location":"data-science/algorithms/supervised/classification/#the-optimization-problem","title":"The optimization problem","text":"<p>But how do we find the best parameter combination (weights and bias) for our  logistic regression model? Unlike linear regression, which uses ordinary least squares, logistic regression typically uses Maximum Likelihood Estimation (MLE), i.e., the best parameters (weights and bias) that maximize the likelihood of the observed data.</p> Lo and behold, even more math... <p>For optimization purposes we use the negative log-likelihood as our loss  function:</p> Negative log-likelihood \\[ J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m [y_i\\log(\\sigma(z_i)) + (1-y_i)\\log(1-\\sigma(z_i))] \\] <p>with:</p> <ul> <li>\\(m\\) as the number of training examples</li> <li>\\(y_i\\) being the the actual class (0 or 1)</li> <li>\\(\\sigma(z_i)\\) is the predicted probability using the sigmoid function</li> <li>\\(\\theta\\) represents all parameters (weights and bias)</li> </ul> Tip <p>Intuitively speaking, the loss function penalizes the model for making  wrong predictions. If the model predicts a probability of 0.9 for a  spam email, and the email is actually spam (\\(y=1\\)), the loss is small. On the other hand, if the model predicts a probability of 0.1 for a  spam email, and the email is spam (\\(y=1\\)), the loss will be high. </p> <p>The weights are gradually adjusted to minimize the loss. Think of it like turning knobs slowly until we get better predictions.</p> <p>Gradually adjusting these knobs to minimize the loss is referred to as gradient descent.</p> <p>Conveniently, <code>scikit-learn</code> provides a logistic regression implementation that takes care of the optimization for us. Finally, we look at a  practical example to see logistic regression in action.</p>"},{"location":"data-science/algorithms/supervised/classification/#example","title":"Example","text":"<p>Let's apply logistic regression to the breast cancer dataset, a classic binary classification problem where we need to predict whether a tumor is malignant  or benign based on various features.</p> <p>With class labels \\(y\\) being 0 (malignant) or 1 (benign), we can use logistic regression to predict the probability of a tumor being benign. The features  were calculated from digitized images of a breast mass.</p> Info <p>See the UCI Machine Learning Repository for more information on the data set.<sup>2</sup></p>"},{"location":"data-science/algorithms/supervised/classification/#load-the-data","title":"Load the dataHow many features (attributes) does the breast cancer dataset have?","text":"<p>Conveniently, <code>scikit-learn</code> provides a couple of data sets for both regression and classification tasks. One of them is the breast cancer dataset.</p> <pre><code>from sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\n\n# count the number of malignant and benign tumors\nprint(y.value_counts())\n</code></pre> &gt;&gt;&gt; Output<pre><code>target\n1    357\n0    212\n</code></pre> <p>In total, the data contains 569 samples with 357 benign and 212 malignant tumors.</p> Tip <p>You might be wondering why the data was divided into <code>X</code>, containing the  attributes and <code>y</code> holding the corresponding labels. Having attributes and  labels separated, makes life a bit easier when training and testing the  model.</p> Number of features <p>Investigate the <code>DataFrame</code> <code>X</code> to the below quiz question.</p> 302932Submit<p>Correct, for example <code>X.shape</code> reveals that we are dealing with 30 features. </p>"},{"location":"data-science/algorithms/supervised/classification/#split-the-data","title":"Split the data","text":"<p>Before training our model, we want to split our data into two parts:</p> <ul> <li>Training set: Used to teach our model (in our example we use 80%)</li> <li>Test set: Used to evaluate how well our model learned (the remaining 20%)</li> </ul> <p>This is like splitting a textbook into two parts: one for studying and one for taking a quiz. We learn from one part and test our knowledge with the other.</p> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, shuffle=True\n)\n</code></pre> <p>Let's break the code snippet down:</p> <ol> <li>The whole data set (<code>X</code> and <code>y</code>) is given to the <code>train_test_split()</code>     function.</li> <li>We specify that 20% should be used for testing (<code>test_size=0.2</code>).</li> <li>For reproducibility, a seed is set with <code>random_state=42</code>     which ensures the same outcome every time the code snippet is executed.</li> </ol> <p>After splitting, we put our test data (<code>X_test</code> and <code>y_test</code>) aside and only use it at the very end to evaluate our model's performance.</p>"},{"location":"data-science/algorithms/supervised/classification/#train-the-model","title":"Train the model","text":"<p>Now that we have our training data, we can train the logistic regression model.</p> <pre><code>from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(random_state=42, max_iter=3_000)  # (1)!\nmodel.fit(X_train, y_train)\n</code></pre> <ol> <li>The <code>random_state</code> parameter ensures reproducibility, while     <code>max_iter</code> specifies the maximum number of iterations taken for the solver      to converge (i.e., solving the optimization problem to find the best      parameter combination).</li> </ol> <p><code>model=LogisticRegression(...)</code> creates an instance of the logistic regression model. Only after calling the <code>fit()</code> method, the <code>model</code> is  actually trained. Since we separated attributes and labels into <code>X_train</code> and  <code>y_train</code> respectively, we can directly call the method without any  further data handling.</p>"},{"location":"data-science/algorithms/supervised/classification/#weights-and-bias","title":"Weights and bias","text":"<p>With a trained model at hand, we can look at the weights \\((b_1, b_2, ...,  b_n)\\) and bias \\((a)\\).</p> <pre><code>print(f\"Model weights: {model.coef_}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Model weights: [[ 0.98293997  0.22667548 -0.36956971  0.02637225 ... ]]\n</code></pre> <p>The <code>coef_</code> attribute contains the weight for each feature.  As discussed, the weights are real numbers.</p> <p>Now, it's your turn to look at the bias.</p> Model bias <ol> <li>Open the <code>scikit-learn</code> docs on the    <code>LogisticRegression</code>    class.</li> <li>Find out how to access the bias term of the model.</li> <li>Simply print the bias term of the model.</li> </ol> <p> Remember, the bias is often referred to as  intercept.</p>"},{"location":"data-science/algorithms/supervised/classification/#predictions","title":"Predictions","text":"<p>Since, the main purpose of a machine learning model is to make predictions, we will do just that.</p> <p>Predicting, is as simple as using the <code>predict()</code> method. We will use the  patient measurements of the test set - <code>X_test</code>.</p> <pre><code>y_pred = model.predict(X_test)\n\n# first 5 predictions\nprint(y_pred[:5])\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1 0 0 1 1]\n</code></pre> <p>Congratulations, you just build a machine learning model to predict breast  cancer. But how good is the model? To conclude the chapter, we will briefly  evaluate the model's performance.</p>"},{"location":"data-science/algorithms/supervised/classification/#evaluate-the-model","title":"Evaluate the model","text":"<p>Surely, we could just manually compare the predictions (<code>y_pred</code>) with the  actual labels (<code>y_test</code>) and evaluate how often the model was correct. Or  instead, we can leverage another method called <code>score()</code>.</p> <pre><code>score = model.score(X_test, y_test)\n</code></pre> <p>First, the <code>score()</code> method takes <code>X_test</code> and makes the corresponding  predictions and programmatically compares the predictions with the actual  labels <code>y_test</code>. <code>score()</code> returns the accuracy   the proportion of correctly classified instances. </p> <pre><code>print(f\"Model accuracy: {round(score, 4)}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Model accuracy: 0.9561\n</code></pre> <p>In our case, the model correctly classified 95.61% of the test set. In  other words, in 95.61% of instances, the model was able to correctly predict  if a tumor is malignant or benign.</p> Tip <p>As the test set (both attributes and labels) were never used to train the model, the accuracy is a good indicator of how well the model generalizes to unseen data.</p>"},{"location":"data-science/algorithms/supervised/classification/#recap","title":"Recap","text":"<p>We covered logistic regression, a popular algorithm for binary classification.</p> <p>Upon discussing the theory, we discovered similarities to linear regression  in regard to the linear combination of features. With the help of the  sigmoid function, we transformed the linear combination into probabilities between 0 and 1.</p> <p>Subsequently, we trained a logistic regression model on the breast cancer data to predict whether a tumor is malignant or benign. To evaluate the  model we split the data and finally calculated the accuracy.</p> Info <p>In subsequent chapters we will explore more sophisticated ways to split  data and evaluate models.</p> <p>Next up, we will dive into algorithms, like decision trees and random forest, that can handle both regression and classification problems.</p> <ol> <li> <p>3rd edition. Online manuscript released January 12, 2025. https://web.stanford.edu/~jurafsky/slp3 \u21a9</p> </li> <li> <p>Wolberg, W., Mangasarian, O., Street, N., &amp; Street, W. (1993).  Breast Cancer Wisconsin (Diagnostic) [Dataset]. UCI Machine  Learning Repository.  https://doi.org/10.24432/C5DW2B.\u00a0\u21a9</p> </li> </ol>"},{"location":"data-science/algorithms/supervised/flexible-learners/","title":"Flexible learners","text":""},{"location":"data-science/algorithms/supervised/regression/","title":"Regression","text":""},{"location":"data-science/algorithms/supervised/regression/#regression","title":"Regression","text":"<p>In machine learning, we often face the challenge of making predictions based on patterns in our data. Linear regression, a supervised method addresses  this by providing a straightforward approach to modeling the relationship  between variables, allowing us to both explain existing data and make  predictions with new observations.</p> <p>This chapter introduces linear regression through practical examples, starting with a simple mobile plan pricing model and progressing to more nuanced relationships between variables. We'll explore how to evaluate model performance using the coefficient of determination (\\( R^2 \\) ), and understand the mathematics behind finding the \"best fit\" line - concepts that form the foundation for many more sophisticated machine learning algorithms.</p>"},{"location":"data-science/algorithms/supervised/regression/#motivation","title":"Motivation","text":"<p>To understand the motivation behind linear regression we will start this chapter with an example. Consider a mobile plan that costs \u20ac26, including unlimited SMS, calls, and data within the country. Data roaming costs \u20ac0.84 per MB. The bills for the last year show monthly expenses based on roaming usage.</p> Month Roaming [MB] Bill [\u20ac] Month Roaming [MB] Bill [\u20ac] January 25 47.00 July 125 131.00 February 300 278.00 August 62 78.08 March 258 242.72 September 94 104.96 April 135 139.40 October 381 346.04 May 12 36.08 November 12 36.08 June 0 26.00 December 18 41.12 Code <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\n\n# Create a DataFrame\ndf = pd.DataFrame([(25, 47.00), (300, 278.00), (258, 242.72), (135, 139.40), (12, 36.08), \n                (0, 26.00), (125, 131.00), (62, 78.08), (94, 104.96), \n                (381, 346.04), (12, 36.08), (18, 41.12)], \n                columns=['Roaming', 'Price'])\n\n# Linear Regression\nmodel = LinearRegression()\nmodel.fit(df[['Roaming']], df['Price'])\n\nintercept = model.intercept_\nslope = model.coef_[0]\nr_sq = model.score(df[['Roaming']], df['Price'])\n\n# Generate regression line\ndf['Regression Line'] = intercept + slope * df['Roaming']\n\n# Create Plotly Express figure\nfig = px.scatter(df, x='Roaming', y='Price')\nfig['data'][0]['marker'] = {'color':'red', 'size':10}\n\n# Add regression line\nfig.add_traces(px.line(df, x='Roaming', y='Regression Line').data)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Roaming [MB]',\n    yaxis_title_text='Price [\u20ac]',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Smartphone Bill&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Variables: roaming, price&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> <p>A scatter plot of the data reveals a perfect linear relationship, allowing us to describe the relationship with a linear function:</p> \\[ y = 26 + 0.84 \\cdot x \\] <p>This has several advantages. For one, the bill amount can be explained through fixed and variable costs, specifically showing how the MB usage affects the total cost. Additionally, it allows for predictions of the bill amount for any unobserved amount of MB.</p> <p>However, in reality, most relationships are not perfectly linear. Let's consider two samples, each with variables \\(X\\) and \\(Y\\).</p> \\(X_1\\) \\(Y_1\\) \\(X_2\\) \\(Y_2\\) 0.00 0.23 0.14 2.00 0.12 0.31 0.25 2.41 0.18 0.49 0.18 2.69 0.26 1.11 0.27 3.41 0.40 1.03 0.42 3.43 0.51 1.32 0.50 3.82 0.60 1.58 0.62 4.18 0.68 1.66 0.70 4.36 0.80 1.65 0.79 4.45 0.80 1.85 0.85 4.75 0.99 1.69 1.00 4.69 <p>When analyzing these samples, we find:</p> <ul> <li>Sample 1 has a Pearson correlation coefficient of \\( \\rho_1 = 0.938 \\).</li> <li>Sample 2 has a Pearson correlation coefficient of \\( \\rho_2 = 0.942 \\).</li> </ul> <p>These values are very similar and suggest a strong correlation.</p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul> Code <pre><code>import pandas as pd\nimport plotly.express as px\n\nx1 = [0.00, 0.12, 0.18, 0.26, 0.40, 0.51, 0.60, 0.68, 0.80, 0.80, 0.99]\ny1 = [0.23, 0.31, 0.49, 1.11, 1.03, 1.32, 1.58, 1.66, 1.65, 1.85, 1.69]\n\nx2 = [0.14, 0.25, 0.18, 0.27, 0.42, 0.50, 0.62, 0.70, 0.79, 0.85, 1.00]\ny2 = [2.00, 2.41, 2.69, 3.41, 3.43, 3.82, 4.18, 4.36, 4.45, 4.75, 4.69]\n\ndf = pd.DataFrame({'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2})\n\n\n# Create Plotly Express figure\nfig = px.scatter(df, x='x1', y='y1')\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='x1',\n    yaxis_title_text='y1',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Dataset 1&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n\n\n# Create Plotly Express figure\nfig2 = px.scatter(df, x='x2', y='y2')\n\n# Adjust the plot\nfig2.update_layout(\n    xaxis_title_text='x2',\n    yaxis_title_text='y2',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Dataset 2&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig2.show()\n</code></pre> <p>At first glance, a scatter plot supports this conclusion, but the impression changes when the axes are normalized equally. </p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul> Code <pre><code>import pandas as pd\nimport plotly.express as px\n\nx1 = [0.00, 0.12, 0.18, 0.26, 0.40, 0.51, 0.60, 0.68, 0.80, 0.80, 0.99]\ny1 = [0.23, 0.31, 0.49, 1.11, 1.03, 1.32, 1.58, 1.66, 1.65, 1.85, 1.69]\n\nx2 = [0.14, 0.25, 0.18, 0.27, 0.42, 0.50, 0.62, 0.70, 0.79, 0.85, 1.00]\ny2 = [2.00, 2.41, 2.69, 3.41, 3.43, 3.82, 4.18, 4.36, 4.45, 4.75, 4.69]\n\ndf = pd.DataFrame({'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2})\n\n\n# Create Plotly Express figure\nfig = px.scatter(df, x='x1', y='y1')\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='x1',\n    yaxis_title_text='y1',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Dataset 1&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig.update_layout(yaxis_range=[0,6])\n\n# Show the plot\nfig.show()\nfig.write_html(\"outputpic/regression_scatter_unscale1.html\", full_html=False, include_plotlyjs='cdn')\n\n\n# Create Plotly Express figure\nfig2 = px.scatter(df, x='x2', y='y2')\n\n# Adjust the plot\nfig2.update_layout(\n    xaxis_title_text='x2',\n    yaxis_title_text='y2',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Dataset 2&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig2.update_layout(yaxis_range=[0,6])\n\n# Show the plot\nfig2.show()\n</code></pre> <p>Proper scaling reveals:</p> <ol> <li>For every \\(X\\) value, the corresponding \\(Y\\) value in Sample 2 is consistently larger than in Sample 1.</li> <li>The change in \\(Y\\) is more significant in Sample 2 compared to Sample 1 when \\(X\\) changes.</li> </ol> <p>This phenomenon occurs because we intuitively focus on the overall picture and draw a mental line through the points. The question then arises: how do we determine this line? This leads us into the core of linear regression, where we aim to model the relationship between variables and make informed predictions.</p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul> Code <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\n\nx1 = [0.00, 0.12, 0.18, 0.26, 0.40, 0.51, 0.60, 0.68, 0.80, 0.80, 0.99]\ny1 = [0.23, 0.31, 0.49, 1.11, 1.03, 1.32, 1.58, 1.66, 1.65, 1.85, 1.69]\n\nx2 = [0.14, 0.25, 0.18, 0.27, 0.42, 0.50, 0.62, 0.70, 0.79, 0.85, 1.00]\ny2 = [2.00, 2.41, 2.69, 3.41, 3.43, 3.82, 4.18, 4.36, 4.45, 4.75, 4.69]\n\ndf = pd.DataFrame({'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2})\n\n# Linear Regression Sample 1\nmodel1 = LinearRegression()\nmodel1.fit(df[['x1']], df['y1'])\nintercept1 = model1.intercept_\nslope1 = model1.coef_[0]\ndf['y1_hat'] = intercept1 + slope1 * df['x1']\n\n# Linear Regression Sample 2\nmodel2 = LinearRegression()\nmodel2.fit(df[['x2']], df['y2'])\nintercept2 = model2.intercept_\nslope2 = model2.coef_[0]\ndf['y2_hat'] = intercept2 + slope2 * df['x2']\n\n# Create Plotly Express figure\nfig = px.scatter(df, x='x1', y='y1')\nfig['data'][0]['marker'] = {'color':'red', 'size':10}\n\n# Add regression line\nfig.add_traces(px.line(df, x='x1', y='y1_hat').data)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='x1',\n    yaxis_title_text='y1',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample 1&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig.update_layout(yaxis_range=[0,6])\n\n# Show the plot\nfig.show()\n\n\n# Create Plotly Express figure\nfig2 = px.scatter(df, x='x2', y='y2')\nfig2['data'][0]['marker'] = {'color':'red', 'size':10}\n\n# Add regression line\nfig2.add_traces(px.line(df, x='x2', y='y2_hat').data)\n\n# Adjust the plot\nfig2.update_layout(\n    xaxis_title_text='x2',\n    yaxis_title_text='y2',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample 2&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig2.update_layout(yaxis_range=[0,6])\n\n# Show the plot\nfig2.show()\n</code></pre>"},{"location":"data-science/algorithms/supervised/regression/#linear-regression","title":"Linear Regression","text":"<p>In linear regression, the relationship between variables is not exactly described. To account for this, a random error \\( e_i \\) is added. </p> Definition <p>Approximation of the real values \\( y_i \\)</p> \\[ y_i = \\hat{y}_i + e_i \\] <p>With the linear regression</p> \\[ \\hat{y}_i = a + b \\cdot x_i \\] <p>The goal of linear regression is to find the best fit line by solving a minimization problem. This problem minimizes the sum of the squared residuals, expressed as</p> Definition <p>Minimization problem for fitting the linear regression</p> \\[ \\min_{a,b} \\sum_{i=1}^{n}e_i^2 = \\min_{a,b} \\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2= \\min_{a,b} \\sum_{i=1}^{n}(y_i-a-bx_i)^2 \\] <p>By solving this, the coefficients of the regression line can be determined, with</p> Definition <p>Intercept</p> \\[ a = \\bar{y} - b \\bar{x} \\] <p>Slope</p> \\[ b = \\frac{\\text{cov}(X, Y)}{\\sigma_x^2} \\] <p>The best fit line is the one that minimizes the sum of squared differences between observed and predicted values. These differences, known as residuals, represent the distance between the actual \\( Y \\)-values and the predicted \\( Y \\)-values from the model.</p>"},{"location":"data-science/algorithms/supervised/regression/#coefficient-of-determination","title":"Coefficient of Determination","text":"<p>To evaluate the goodness of fit of a model, the coefficient of determination \\( R^2 \\) can be used. It is calculated as:</p> Definition <p>Coefficient of Determination</p> \\[ R^2 = \\frac{\\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} = \\rho_{X,Y}^2 \\] <p>The coefficient of determination indicates the proportion of variance explained by the model. In essence, \\( R^2 \\) measures how well the regression model fits the observed data.</p> <p>\\( R^2 \\) can range from zero to one:</p> <ul> <li> <p>\\( R^2 = 0 \\): None of the variance is explained by the model, indicating a poor fit.</p> </li> <li> <p>\\( R^2 = 1 \\): All of the variance is explained by the regression, indicating a perfect fit, which can only occur when the original data points lie exactly on the regression line.</p> </li> </ul>"},{"location":"data-science/algorithms/supervised/regression/#recap","title":"Recap","text":"<ul> <li>Linear regression attempts to model the relationship between multiple variables.</li> <li>Using the model, further values can be predicted.</li> <li>In linear regression, the squared distance between the raw data points and the regression line is minimized.</li> <li>The coefficient of determination \\(R^2\\) is used to assess the model's goodness of fit.</li> <li>The closer the \\(R^2\\) value is to one, the better the model fits the data.</li> </ul>"},{"location":"data-science/algorithms/supervised/regression/#tasks","title":"Tasks","text":"Task <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ncars = fetch_ucirepo(id=9) \n# https://archive.ics.uci.edu/dataset/9/auto+mpg\n\n# data (as pandas dataframes) \ndata = cars.data.features\ndata = data.join(cars.data.ids)\n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following task: </p> <ol> <li>Perform a linear regression for the following attribute combinations: <ul> <li>Displacement vs. Weight</li> <li>Displacement vs. Acceleration</li> <li>Acceleration vs. Weight</li> </ul> </li> <li>For all performed regressions calculate the coefficient of determination</li> <li>Write down the formula for all performed regressions </li> </ol>"},{"location":"data-science/algorithms/unsupervised/unsupervised/","title":"Clustering &amp; Dimensionality Reduction","text":""},{"location":"data-science/algorithms/unsupervised/unsupervised/#k-means-clustering","title":"k-means clustering","text":""},{"location":"data-science/algorithms/unsupervised/unsupervised/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":""},{"location":"data-science/data/preparation/","title":"Data preparation","text":""},{"location":"data-science/data/preparation/#preface","title":"Preface","text":"Info <p>Starting with this chapter, we will work with adapted data from:</p> <p>S. Moro, P. Cortez and P. Rita (2014). A Data-Driven Approach to  Predict the Success of Bank Telemarketing<sup>1</sup></p> <p>The publicly available dataset is from a Portuguese retail bank  and houses information on direct marketing campaigns (phone calls). Bank  customers were contacted and asked to subscribe to a term deposit. Using  this practical example, we will explore the realms of:</p> <ul> <li>Data merging</li> <li>Data cleaning</li> <li>Data transformation</li> <li>Machine learning (with selected algorithms)</li> <li>Comparison of model performance</li> <li>Model persistence (practical guide on how to save and load machine    learning models)</li> </ul> <p>Eventually, you will end up with a model that predicts whether a customer   will subscribe to a term deposit or not.</p>"},{"location":"data-science/data/preparation/#obtaining-the-data","title":"Obtaining the data","text":"Set up a project <p>As always, we strongly recommend to set up a new project including a  virtual environment. We will perform all steps from data merging to  saving the model in this project.</p> <p>If you are having trouble setting up a virtual environment, please refer  to the virtual environment creation  guide.</p> <p>Let's dive right in and download both files:</p> <p>Bank Marketing  Bank Marketing Social Features </p> <p>Place the files in a new folder called <code>data</code>. Your project now should look  like this:</p> <pre><code>\ud83d\udcc1 bank_marketing/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 data/\n\u251c\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank.tsv\n\u2514\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank-social.csv\n</code></pre> Open the files <p>Before we start, simply open the files with a text editor. Scroll through both files and read a couple of rows to get acquainted with  the data.</p>"},{"location":"data-science/data/preparation/#read-the-files","title":"Read the files","text":"<p>Since we are obviously dealing with two rather large files, we opt to read  them with <code>Python</code> . At the end of this section we end up with a single (clean!) data set.</p> Info <p>Conveniently, in our case the data was already collected, saving us hours  and hours of work. Thus, we can focus on the data preparation step.  Since data is commonly obtained from different sources and in various  different formats, both data sets we have at hand (<code>bank.tsv</code> and <code>bank-social.csv</code>) will mimic theses scenarios.</p> <p>To start, we are using <code>pandas</code> for reading and manipulating data. If you  haven't already, install the package within your environment.  Assuming your Jupyter Notebook or script is located at the project's root, we start by reading the first file  <code>bank.tsv</code>.</p> <pre><code>import pandas as pd\n\ndata = pd.read_csv(\"data/bank.tsv\", sep=\"\\t\")\n</code></pre> <p>Although, we can use a simple single-liner to read the file, there are a  couple of things to break down:</p> <ol> <li>We are dealing with a tab-separated file, meaning values within the file     are separated by a tab character (<code>\\t</code>). The fact that we are dealing     with a tab-separated file is indicated by the file extension <code>.tsv</code> and     the space surrounding the values within the file.</li> <li>Although we do not have a <code>csv</code> file at hand, <code>pandas</code> is versatile enough     to handle different separators.     Thus, we can utilize the <code>pd.read_csv()</code> function to read the     file. Tip: All sorts of text files can be usually read with     <code>pd.read_csv()</code>.</li> <li>Lastly, the <code>sep</code> parameter is set to <code>\\t</code> to indicate the tab     separation.</li> </ol> <p>Let's read the second file  <code>bank-social.csv</code>.</p> Open the file <code>bank-social.csv</code> with your text editor. Which separator is used in the file?: (colon)None, it is not a valid csv file.; (semicolon), (comma)Submit<p>Exactly, values are separated by a semicolon.</p> Read the second file <p>Simply read the second file (<code>bank-social.csv</code>) with <code>pd.read_csv()</code>  and specify the appropriate separator. Store the <code>DataFrame</code> in a  variable called <code>data_social</code>.</p>"},{"location":"data-science/data/preparation/#duplicated-data","title":"Duplicated data","text":"<p>Now, with both files in memory, let's examine them closer in order to  perform a merge.</p> <code>print(data.head())</code><code>print(data_social.head())</code> id age default housing ... cons.conf.idx euribor3m nr.employed y 1 30 no yes ... -46.2 1.313 5099.1 no 2 39 no no ... -36.4 4.855 5191.0 no 3 25 no yes ... -41.8 4.962 5228.1 no 4 38 no unknown ... -41.8 4.959 5228.1 no 5 47 no yes ... -42.0 4.191 5195.8 no <p>The rows represent customers and the columns are features of the    customers. The column <code>y</code> indicates whether a customer subscribed to a    term deposit or not. Customers are uniquely identified by the <code>id</code>    column. Later on, we will have a closer look at the attributes when    modelling the data.</p> id job marital education 2178 technician married professional.course 861 blue-collar single professional.course 3020 technician married professional.course 2129 self-employed married basic.9y 3201 blue-collar married basic.9y <p>Again, each row represents a customer (uniquely identified with <code>id</code>).   The remaining columns <code>job</code>, <code>marital</code>, and <code>education</code> are social   attributes.</p> <p>Let's examine the shape of both <code>DataFrame</code>s as well.</p> <pre><code>print(f\"Shape of data: {data.shape}; Shape of data_social: {data_social.shape}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Shape of data: (4530, 18); Shape of data_social: (4304, 4)\n</code></pre> <p>The output indicates that <code>data</code> contains more observations (customers) than <code>data_social</code>. However, first and foremost it is good practice to check  for duplicated data.</p> <pre><code># check for duplicated rows\nprint(data.duplicated().sum())\n</code></pre> &gt;&gt;&gt; Output<pre><code>np.int64(411)\n</code></pre> <p><code>data</code> contains <code>411</code> duplicated rows. These can be removed easily:</p> <pre><code>data = data.drop_duplicates()\n</code></pre> Check for duplicates <p>Check for duplicates in <code>data_social</code> and remove them if necessary.</p> How many duplicates were present in <code>data_social</code>?None3760411376Submit<p><code>data_social</code> had 376 duplicated rows.</p> A note on <code>pd.DataFrame.drop_duplicates()</code> <p>By default, the method <code>pd.DataFrame.drop_duplicates()</code> removes all duplicated rows. However, you can pass an argument to <code>subset</code> in  order to remove duplicates based on specific columns. For example, if we  want to drop duplicates based on the <code>id</code> column, we can do so by:</p> <pre><code>data_social = data_social.drop_duplicates(subset=[\"id\"])\n</code></pre> <p><code>subset</code> can also be all list of multiple columns.</p>"},{"location":"data-science/data/preparation/#merge-methods","title":"Merge methods","text":"<p>To combine both data sets we will use the <code>pd.DataFrame.merge()</code>  method to</p> <p>Merge DataFrame or named Series objects with a database-style join</p> <p>-- pandas <code>merge()</code>docs</p> <p>Looking at the <code>how</code> parameter we are presented with 5 (!) different options  to perform a merge. The most common ones are:</p> <ul> <li><code>\"left\"</code></li> <li><code>\"right\"</code></li> <li><code>\"inner\"</code></li> <li><code>\"outer\"</code></li> </ul> <p></p>        And now what?!   <p>In order to be able to choose the appropriate method, we need to break them  down:</p> <p></p> <ul> <li>Left join: The resulting <code>DataFrame</code> will contain all rows from the    left <code>DataFrame</code> (data 1) and the matched rows from the right <code>DataFrame</code>    (data 2).</li> <li>Right join: The resulting <code>DataFrame</code> will contain all rows from the    right <code>DataFrame</code> (data 2) and the matched rows from the left <code>DataFrame</code>    (data 1).</li> <li>Inner join: The resulting <code>DataFrame</code> will contain only the rows that    have matching values in both <code>DataFrame</code>s.</li> <li>Outer join: The resulting <code>DataFrame</code> will contain all rows from both    <code>DataFrame</code>s.</li> </ul>"},{"location":"data-science/data/preparation/#perform-merges","title":"Perform merges","text":"<p>To get further acquainted with merge methods, we simply perform them all. But first, we need to pick a column which uniquely identifies a row (customer) in both data sets. Conveniently, we have the <code>id</code> column. Regardless of the merge we perform, the parameter <code>on</code> requires a column to match the  rows (in our case we set <code>on=\"id\"</code>).</p> <pre><code>left_join = data.merge(data_social, on=\"id\", how=\"left\")\nright_join = data.merge(data_social, on=\"id\", how=\"right\")\ninner_join = data.merge(data_social, on=\"id\", how=\"inner\")\nouter_join = data.merge(data_social, on=\"id\", how=\"outer\")\n</code></pre>"},{"location":"data-science/data/preparation/#a-closer-look","title":"A closer look","text":"<p>To comprehend on of these merges, have a look at the resulting shape and the  identifiers the <code>DataFrame</code> contains. Let's examine the <code>right_join</code>:</p> <pre><code>equal_nrows = len(data_social) == len(right_join)\nprint(f\"Merged data has the same number of rows as data_social: {equal_nrows}\")\n\nall_ids_present = data_social[\"id\"].isin(right_join[\"id\"]).all()\nprint(f\"All identifiers from data_social are present? {all_ids_present}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Merged data has the same number of rows as data_social: True\nAll identifiers from data_social are present? True\n</code></pre> <p>A breakdown of the code snippet:</p> <ol> <li> <p><code>equal_nrows</code> indicates that all rows from <code>data_social</code> are present in     <code>right_join</code>.</p> Info <p><code>len(data_social)</code> is equivalent to  <code>data_social.shape[0]</code>.</p> </li> <li> <p>To verify that <code>right_join</code> contains all identifiers of <code>data_social</code>,     we make use of the <code>pd.Series.isin()</code> method. This method checks     whether each element of a <code>Series</code> is contained in another <code>Series</code>.</p> Info <p><code>pd.Series.all()</code> returns <code>True</code> if all elements in  the <code>Series</code> are <code>True</code>.</p> </li> </ol> Counter check <p>Extend, the previous code snippet:</p> <ol> <li>Do the number of rows from <code>data</code> and <code>right_join</code> match?</li> <li>Are all identifiers from <code>data</code> present in <code>right_join</code>?</li> </ol> <p>Our examinations should conclude that <code>right_join</code> contains all  rows/customer data from <code>data_social</code> and solely the matching records  from <code>data</code>.</p> Examine remaining merges <p>Look at the shapes of the remaining merges (<code>left_join</code>,  <code>inner_join</code>, <code>outer_join</code>) to get a better understanding of the different merge methods and its results.</p> <p>Compare the shape of each merge with the shapes of <code>data</code> and  <code>data_social</code>.</p>"},{"location":"data-science/data/preparation/#final-merge-application","title":"Final merge (application)","text":"<p>With a solid understanding of different merge methods, we revisit our  initial task to join both data sets. But how can we choose the appropriate  method? The short answer; it depends on your data and task at hand.  There is no method that fits all scenarios.</p> SQL Explained byu/UnorthodoxPrimitive inProgrammerHumor <p>Since we are eventually fitting a machine learning model on the data, we are  interested in customer data that is present in both data sets. I.e., we  want to prevent the introduction of additional missing values that would  result from an outer join. Furthermore, a left join would leave us with missing attributes from <code>social_data</code>. The same applies to a right join, just vice  versa.</p> <p>Long story short, we opt for an <code>\"inner\"</code> merge (or join) which  leaves us with only the customers that are present in both data sets. The  final merge is as simple as:</p> <pre><code>data_merged = data.merge(data_social, on=\"id\", how=\"inner\")\n</code></pre> <p>Let's examine the shape of the merged data set.</p> <pre><code>print(f\"Shape of data_merged: {data_merged.shape}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Shape of data_merged: (3928, 21)\n</code></pre> <p>We end up with <code>3928</code> customers that are present in both data sets. Lastly, we can write the merged data set to a new file. Let's use a common  format  <code>csv</code> with the default <code>,</code> as separator.</p> <pre><code>data_merged.to_csv(\"data/bank-merged.csv\", index=False)\n</code></pre> <p>With <code>index=False</code>, we do not</p> <p>Write row names (index).</p> <p>-- pandas <code>to_csv()</code> docs</p> <p></p>          Congratulations! \ud83c\udf89 You have finally completed your quest to merge the         data."},{"location":"data-science/data/preparation/#recap","title":"Recap","text":"<p>Using the bank marketing data, we have seen how to find and remove duplicated  data, explored different merge methods and ended up with a single data set.</p> <p>In the next chapter, we will explore this data further, look for missing  values and perform some basic data transformations.</p> <ol> <li> <p>Decision Support Systems, Volume 62, June 2014, Pages 22-31: https://doi.org/10.1016/j.dss.2014.03.001 \u21a9</p> </li> </ol>"},{"location":"data-science/data/preprocessing/","title":"Data preprocessing","text":"Continue your data preprocessing quest! \ud83e\uddd9\u200d\u2642\ufe0f  <code>scikit-learn</code> <p>This chapter introduces the package <code>scikit-learn</code>, the swiss-army knife for data preprocessing, transformation and machine learning.</p> <p>We will continue to work with the Portuguese retail bank data  set<sup>1</sup> and preprocess it further. Alongside we start to explore  <code>scikit-learn</code>'s functionalities.</p> <p>Check-out the excellent scikit-learn documentation.</p>"},{"location":"data-science/data/preprocessing/#prerequisites","title":"Prerequisites","text":"<p>If you have followed the previous chapter closely, your project structure  looks like this:</p> <pre><code>\ud83d\udcc1 bank_marketing/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 data/\n\u251c\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank.tsv\n\u251c\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank-merged.csv\n\u2514\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank-social.csv\n</code></pre> <p>With <code>bank-merged.csv</code> being the <code>\"inner\"</code> join of <code>bank.csv</code> and  <code>social.csv</code>, minus all duplicated customer data. </p> <p>If you are missing the file <code>bank-merged.csv</code>, we strongly recommend you to  go back and complete the previous chapter. For the sake of completeness,  we provide a distilled version of the code from  Data preparation:</p> Merge the data sets <pre><code># Steps from the Data preparation chapter\nimport pandas as pd\n\ndata = pd.read_csv(\"bank.tsv\", sep=\"\\t\")\ndata_social = pd.read_csv(\"bank-social.csv\", sep=\";\")\n\ndata = data.drop_duplicates()\ndata_social = data_social.drop_duplicates()\n\ndata_merged = data.merge(data_social, on=\"id\", how=\"inner\")\ndata_merged.to_csv(\"data/bank-merged.csv\", index=False)\n</code></pre> <p> Merged data  </p> <p>Again, we urge you to use a virtual environment which by now, should be second  nature anyway.</p> Create a new notebook <p>To follow along, create a new Jupyter notebook within your project.</p>"},{"location":"data-science/data/preprocessing/#missing-values","title":"Missing values","text":"<p>After dropping duplicates and merging the data, the next step is to check  for missing values. First, we read the data.</p> <pre><code>import pandas as pd\n\ndata = pd.read_csv(\"data/bank-merged.csv\")\n</code></pre> <p>We chain a couple of methods to count the missing values in each column.</p> <pre><code>print(data.isna().sum())\n</code></pre> <p>The <code>isna()</code> method checks each element and whether it is a missing  value or not. The result is a <code>DataFrame</code> with boolean values of the same  shape as the initial <code>DataFrame</code> (in our case <code>data</code>), with <code>True</code>  being a missing value. With the chaining of <code>sum()</code> we simply sum the  <code>True</code> values (missing values) for each column.</p> <p>A truncated version of the output is shown below:</p> Column Missing Values id 0 age 0 default 0 housing 0 ... ... job 0 marital 0 education 0 ... ... <p>It seems like the columns have no missing values. To sum missing values of  the whole <code>DataFrame</code>, we can chain another <code>sum()</code>.</p> <pre><code>print(data.isna().sum().sum())\n</code></pre> <p>The output once more indicates that the whole data set has <code>0</code>  missing values. So far so good, but this is not the end of the story (who  saw that coming \ud83e\udd2f).</p> <p></p>          Plot twist...      <p>Although, it seems like we don't have to bother with missing values, they  are simply a bit more hidden.</p>"},{"location":"data-science/data/preprocessing/#missing-values-in-disguise","title":"Missing values in disguiseHow are missing values encoded in this specific data set?","text":"<p><code>pandas</code> considers types like <code>None</code> or <code>np.nan</code> as  missing. However in practice, missing values are encoded in various ways. For instance, strings like <code>\"NA\"</code> or integers like <code>-999</code>  are used. Consequently, we can't detect these ways of encoding with  simply calling <code>isna()</code>.</p> <p>Since we have to manually detect these encoded missing values, it is  essential to have a good understanding of the data. Let's get more  familiarized with the data.</p> <p>Visit the UCI Machine Learning Repository  here which also  hosts the data set and some additional information. Interestingly, the section  Dataset Information states:</p> <p>Has Missing Values?</p> <p>No</p> <p>Although that might be technical correct (the data contains no empty values),  we have to dig deeper.</p> Detect the encoding of missing values <p>Open the UCI Machine Learning Repository. Look at the Variables Table. How are the missing values encoded in the  data set?</p> <p>Use the following quiz question to validate your answer.</p> <p>Remember, the bigger picture  by getting more familiar with the data, we can train a better fitting  model to predict the target variable <code>y</code> (subscribed to term deposit or  not).</p> \"unknown\"-1\"NA\"999Submit<p>Correct, the label \"unknown\" is used for missing values. Nominal attributes  like occupation, marital status and ordinal attributes, for  example education, contain \"unknown\" values.</p>"},{"location":"data-science/data/preprocessing/#missing-values-uncovered","title":"Missing values uncovered","text":"<p>Now that we uncovered the encoding of missing values, we replace them with  <code>None</code> to properly detect them and handle them more easily.</p> Replace encoding with <code>None</code> <p>Since, you've detected the particular encoding of missing values, replace  them with <code>None</code> across the whole data frame.</p> <p>Use the <code>DataFrame.replace()</code> method and read the  docs,  especially the Examples section for usage guidance.</p> <p>After solving the question, we (again) sum up the missing values per column.</p> <pre><code>print(data.isna().sum())\n</code></pre> <p>A truncated version of the output:</p> Column Missing Values id 0 age 0 default 760 housing 97 ... ... job 35 marital 11 education 161 ... ... <p>At first glance, a lot of columns contain missing values. Let's calculate  the ratio to get a better feeling.</p> <pre><code>count_missing = data.isna().sum()\nn_rows = len(data)\n\nmissing_ratio = (count_missing / n_rows) * 100\nprint(missing_ratio.round(2))\n</code></pre> Column Missing Values (%) id 0.00 age 0.00 default 19.35 housing 2.47 ... ... job 0.89 marital 0.28 education 4.10 ... ... <p>Compared to the initial observation where we found <code>0</code>  missing values across the whole data set, it's a stark contrast.</p> <p>Looking at the attribute default, nearly a fifth of the observations are  missing (19.35 %). Other attributes contain less missing values, yet we still  need to handle them. Therefore, we explore different strategies to deal  with missing values.</p> Info <p>Though it might not seem much, being able to detect these missing values  will prove invaluable in the future.</p> <p>By identifying and properly handling these gaps, we might be able to  train a better fitting model as unaddressed missing values can lead to  biased predictions. Most importantly, most algorithms can't handle  missing values at all.</p>"},{"location":"data-science/data/preprocessing/#sources","title":"Sources","text":"<p>We have extensively covered how to detect missing values but have not  talked about their possible origins.</p> <p>The reasons for missing values can be manifold:</p> <ul> <li>Data collection issues<ul> <li>Non-responses in a survey</li> <li>Equipment failures</li> <li>Simple human errors when entering data</li> </ul> </li> <li>Technical challenges<ul> <li>Preprocessing errors (i.e., merging data sets from multiple sources)</li> </ul> </li> <li>Intentional omissions<ul> <li>Privacy concerns or legal restrictions</li> </ul> </li> </ul> <p>... or the information is simply not available.</p>"},{"location":"data-science/data/preprocessing/#drop-columnsrows","title":"Drop columns/rows","text":"<p>One simple way to handle missing values is to drop (i.e. remove) the  respective columns which contain any missing values.</p> <pre><code>data_dropped = data.dropna(axis=1)\n</code></pre> <p><code>axis=1</code> specified the columns to be dropped.</p> <p>To comprehend the impact of this operation, we calculate the number of  columns that were removed.</p> <p><pre><code>print(data.shape[1] - data_dropped.shape[1])\n</code></pre> This operation removed <code>6</code> out of <code>21</code> columns/attributes.</p> Remove rows with missing values <p>Contrary, we can leave all columns and instead drop the rows containing  missing values.</p> <ol> <li>Use the <code>DataFrame.dropna()</code> method to remove rows with missing values.</li> <li>Calculate the number of rows that were removed.</li> </ol>"},{"location":"data-science/data/preprocessing/#set-a-threshold","title":"Set a threshold","text":"<p>Instead of dropping all rows/columns with gaps, we can set a threshold to only  drop columns/rows with a certain amount of missing values.</p> <p>To specify a threshold, make use of the <code>thresh</code> parameter, which takes an <code>int</code> value of non-missing values that a column/row must have,  to not be dropped.</p> <p>As an example, we would like to remove all columns holding more than 10 %  missing values.</p> <pre><code>import math\n\nthreshold = 0.1  # 10 % threshold\nthreshold = (1 - threshold) * len(data)\nprint(threshold)\n\nthreshold = math.ceil(threshold)  # (1)!\nprint(threshold)\n\ndata_dropped_threshold = data.dropna(axis=1, thresh=threshold)\ndiff = data.shape[1] - data_dropped_threshold.shape[1]\nprint(f\"Number of columns dropped: {diff}\")\n</code></pre> <ol> <li>The <code>math.ceil()</code> function is used to round up the threshold      value to the next integer.</li> </ol> &gt;&gt;&gt; Output<pre><code>3535.2000000000003\n3536\nNumber of columns dropped: 1\n</code></pre> <p>A single column was dropped and therefore exceeded the 10 % threshold of  missing values.</p> <p>Depending on the data at hand, dropping rows or columns might be a valid  option, if you're dealing with a small number of missing values. However, in  other cases these operations might lead to a significant loss of information. Since, we are dealing with a substantial amount of missing values, we are  looking for more sophisticated ways to handle them.</p>"},{"location":"data-science/data/preprocessing/#imputation-techniques","title":"Imputation techniques","text":"<p>What about filling in the missing values? The process of replacing missing  values is called imputation.</p> <p></p>      Data imputation  <p>There are various imputation techniques available, each with its own advantages and disadvantages.</p>"},{"location":"data-science/data/preprocessing/#fill-manually","title":"Fill manually","text":"<p>Of course, there is always the option to fill the values manually which  could be time-consuming and infeasible for large data sets.</p>"},{"location":"data-science/data/preprocessing/#global-constant","title":"Global constant","text":"<p>The simplest way to impute missing values is to replace them with a global constant, i.e., filling gaps across all columns with the same value.</p> <pre><code>data_filled = data.fillna(\"no\")\n</code></pre> <p>This method is straightforward and easy to implement. However, there are  some drawbacks:</p> <ul> <li>how to choose the global constant?</li> <li>introduces further challenges with mixed attributes (i.e.,    nominal/ordinal and numerical attributes)</li> </ul>"},{"location":"data-science/data/preprocessing/#central-tendency","title":"Central tendency","text":"<p>Another common approach is to replace missing values with the mean, median, or mode of the respective column.</p> <p>Fill a nominal attribute with the mode:</p> <pre><code>job_mode = data[\"job\"].mode()\nprint(job_mode)\n\ndata[\"job_filled\"] = data[\"job\"].fillna(job_mode[0])\n</code></pre> &gt;&gt;&gt; Output<pre><code>0    admin.\n</code></pre> <p>Fill a numerical attribute with the mean:</p> <pre><code>age_mean = data[\"age\"].mean()\nprint(age_mean)\n\ndata[\"age_filled\"] = data[\"age\"].fillna(age_mean)\n</code></pre> &gt;&gt;&gt; Output<pre><code>np.float64(40.1433299389002)\n</code></pre> Info <p>Since the bank data does not contain any numerical attribute with  missing values, the above code snippet assumed gaps in age. As there  are none, the operation did not change the data. </p>"},{"location":"data-science/data/preprocessing/#machine-learning","title":"Machine Learning","text":"<p>Lastly, we can use machine learning algorithms to predict the missing values. The idea is to estimate the missing values based on the other attributes.  Linear regression, k-nearest neighbors, or decision trees are common choices.</p> Info <p>As we have not covered machine learning yet, we won't get into the details. But feel free to return to this section. Especially,  this scikit-learn comparison of imputation techniques (including k-nearest  neighbors) is a good starting point for further exploration.</p>"},{"location":"data-science/data/preprocessing/#transformation","title":"Transformation","text":"<p>Step by step, we are getting closer to actually training a machine learning  model. Beforehand, we introduce data transformations that are commonly applied to improve the fit of the model.</p> <p>For starters, install the <code>scikit-learn</code> package within your activated  environment.</p> <pre><code>pip install scikit-learn\n</code></pre> <p></p> <code>scikit-learn</code> the swiss-army knife for data          preprocessing and machine learning in Python.      <p>From now on, we will heavily use <code>scikit-learn</code>'s functionalities.</p>"},{"location":"data-science/data/preprocessing/#discretize-numerical-attributes","title":"Discretize numerical attributes","text":"<p>When dealing with noisy data, it is often beneficial to discretize  numerical (continuous) attributes.</p> Noise in data <p>Noise is a random error or variance in a measured variable. It is  meaningless information that can distort the data.</p> <p>Noise can be identified using basic statistical methods and  visualization techniques like boxplots or scatter plots.</p> <p>The process of discretizing is called binning. I.e., the continuous data  is separated into intervals (bins). Bins can generally lead to a smoothing effect which in turn reduce the noise.</p> <p>As an example, we pick the attribute age and visualize it with a boxplot.</p> Create a static boxplot <p>To create a static version of the boxplot, perfect for a quick overview:</p> <pre><code>import matplotlib.pyplot as plt\n\ndata[\"age\"].plot(kind=\"box\")  # (1)!\nplt.show()\n</code></pre> <ol> <li>The <code>plot()</code> method uses <code>matplotlib</code> as backend.</li> </ol> <p> </p> <p>Since, age contains outliers, we discretize the attribute age into five  bins with the same width.</p> <pre><code>from sklearn.preprocessing import KBinsDiscretizer\n\nbins = KBinsDiscretizer(n_bins=5, strategy=\"uniform\", encode=\"ordinal\")\nbins.fit(data[[\"age\"]])\nage_binned = bins.transform(data[[\"age\"]])  # (1)!\n</code></pre> <ol> <li>The additional square brackets in <code>data[[\"age\"]]</code> are used to      select the column age as a <code>DataFrame</code> (instead of a <code>Series</code>).      This is necessary for the <code>transform()</code> method as a      two-dimensional input is required.</li> </ol> <p>The above snippet returns 5 bins with a width of 14 years. Inspect the bin  edges with:</p> <pre><code>print(bins.bin_edges_)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[array([18., 32., 46., 60., 74., 88.])]\n</code></pre> <p>Though the actual binning is just two three lines of code, we have a couple of  things to dissect.</p> Working with <code>scikit-learn</code> <p>Although the package is named <code>scikit-learn</code>, it is imported as  <code>import sklearn</code>. Package names on  PyPI (Python Package Index) can be different from the import name.</p> <p><code>scikit-learn</code> frequently uses classes (e.g., <code>KBinsDiscretizer</code>) to represent different models and preprocessing techniques. Two important  methods that many of these classes implement are <code>fit</code> and <code>transform</code>.</p> <ul> <li> <p><code>fit(X)</code>: This method is used to learn the parameters from the  data (referred to as <code>X</code>). </p> </li> <li> <p><code>transform(X)</code>: This method is used to apply the learned  parameters to the data  <code>X</code>.</p> </li> </ul> <p>Put simply, think about the <code>fit(X)</code> method as scikit-learn takes  a look at the data and learns from it. The <code>transform(X)</code>  method then transfers this knowledge and applies it to the data.</p> <p>The <code>fit_transform()</code> method combines both of these steps in one.</p> <p>Alternatively, use <code>strategy=\"quantile\"</code> to bin the data based on quantiles and thus create bins with the same number of observations.</p> <pre><code>bins = KBinsDiscretizer(n_bins=5, strategy=\"quantile\", encode=\"ordinal\")\nage_binned = bins.fit_transform(data[[\"age\"]])\n\nprint(bins.bin_edges_)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[array([18., 31., 36., 41., 50., 88.])]\n</code></pre> <p>No matter the strategy <code>\"uniform\"</code> or <code>\"quantile\"</code>, a  matrix is returned with the</p> <p>bin identifier encoded as an integer value.</p> <p><code>KBinsDiscretizer</code> docs</p>"},{"location":"data-science/data/preprocessing/#normalization","title":"Normalization","text":"<p>Normalization is a common preprocessing step to scale the data to a standard range, which can improve the performance and training stability of machine learning models. Two popular normalization techniques are Min-Max normalization and Z-Score normalization.</p>"},{"location":"data-science/data/preprocessing/#min-max-normalization","title":"Min-Max Normalization","text":"<p>Min-Max normalization scales the data to a fixed range, usually [0, 1].</p> Definition: Min-Max Normalization \\[ X' = \\frac{X - X_{min}}{X_{max} - X_{min}} \\] <p>where \\(X\\) is the original value, \\(X_{min}\\) is the minimum value of the  feature, and \\(X_{max}\\) is the maximum value of the feature.</p> <p>This technique is useful when you want to ensure that all features have the  same scale without distorting differences in the ranges of values.</p> <p>To illustrate the normalization, we use the attribute euribor3m (3 month Euribor rate).</p> <p>Euribor is short for Euro Interbank Offered Rate. The Euribor rates are based on the average interest rates at which a large panel of European banks borrow funds from one another.</p> <p>euribor-rates.eu</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\n\nprint(f\"X_min: {data['euribor3m'].min()}, X_max: {data['euribor3m'].max()}\")\n\nscaler = MinMaxScaler()\nscaler.fit(data[[\"euribor3m\"]])\nscaled = scaler.transform(data[[\"euribor3m\"]])\n\nprint(scaled)\nprint(f\"Min: {scaled.min()}, Max: {scaled.max()}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>X_min: 0.635, X_max: 4.97\n\n[[0.15640138]\n [0.97347174]\n [0.99815456]\n ...\n [0.16585928]\n [0.99907728]\n [0.80392157]]\n\nMin: 0.0, Max: 1.0\n</code></pre> Normalization of new data <p>Assume new data is added:</p> <p><pre><code>new_data = pd.DataFrame({\"euribor3m\": [0.5, 5.0, 2.5]})\n</code></pre> We would like to transform these three new interest rates using the Min  Max normalization. Remember that the <code>MinMaxScaler</code> was already fitted on the original  data with \\(X_{min}=0.635\\) and \\(X_{max}=4.97\\).</p> <p>Answer the following quiz question. Look at the formula again and try  to answer the question without executing code.</p> What happens if you call <code>transform(new_data)</code>?An error is raised, since the new data has not been fitted.The new data is normalized.The normalization works, but the range [0, 1] is not preserved.Submit<p>Correct, since the newly added Euribor rates of 0.5 and 5.0, are lower or  higher than the previous minimum and maximum respectively, the normalization  will not preserve the range [0, 1], i.e. resulting in the normalized values:  <code>[[-0.03114187], [1.00692042], [0.43021915]]</code> <p>"},{"location":"data-science/data/preprocessing/#z-score-normalization","title":"Z-Score Normalization","text":"<p>Z-Score normalization, also known as standardization, scales the data based on the mean and standard deviation of an attribute. </p> Definition: Z-Score Normalization \\[ X' = \\frac{X - \\mu}{\\sigma} \\] <p>where \\(\\mu\\) is the mean of the feature and \\(\\sigma\\) is the standard  deviation.</p> <p>This technique centers the data around zero with a standard deviation of one,  which is useful for algorithms assuming normally distributed data.</p> Apply Z-Score normalization <p>Use the <code>StandardScaler</code> from <code>scikit-learn</code> to apply Z-Score normalization to the attribute  campaign (number of times a customer was contacted).</p> <ol> <li>Fit the <code>StandardScaler</code> on the data.</li> <li>Transform the data.</li> <li>Calculate and print the mean and standard deviation of the transformed  data.</li> </ol>"},{"location":"data-science/data/preprocessing/#one-hot-encoding","title":"One-Hot Encoding","text":"<p>So far we have focused on numerical attributes. But what about  categorical variables? Since, many machine learning algorithms can't handle  categorical attributes directly, they need to be encoded. One common technique is to one-hot encode these attributes.</p> <p>Imagine the toy example below to illustrate the concept of one-hot encoding  on the feature job.</p>        Your browser does not support the video, consider updating your browser.      Definition: One-Hot Encoding <p>One-hot encoding is a technique to convert categorical attributes into  numerical attributes. Each category is represented as a binary vector  where only one bit is set to 1 (hot) and the rest are set to 0 (cold).</p> <p>The class <code>OneHotEncoder</code> from <code>scikit-learn</code> can be used to encode categorical attributes to a one-hot  encoded representation.</p> Apply One-Hot Encoding <p>Use the <code>OneHotEncoder</code> to encode the the attribute job from the  following toy <code>DataFrame</code> (same as in the video).</p> <pre><code>toy_data = pd.DataFrame(\n    {\"id\": [1, 2, 3, 4], \"job\": [\"engineer\", \"student\", \"teacher\", \"student\"]}\n)\n</code></pre> <ol> <li>Apply an instance of <code>OneHotEncoder(sparse_output=False)</code> to  job.</li> <li>Check if the resulting matrix matches with the one in the video.</li> </ol>"},{"location":"data-science/data/preprocessing/#label-encoding","title":"Label Encoding","text":"<p>Lastly, we introduce label encoding. Label encoding is another technique to encode categorical attributes. Instead of creating a binary vector for each category, label encoding assigns a unique integer to each category.</p> <p><code>scikit-learn</code>'s <code>LabelEncoder</code> is specifically designed to encode the target variable (i.e., the attribute we  want to predict). In our case, we apply the <code>LabelEncoder</code> to the column named <code>\"y\"</code>.</p> <p><code>\"y\"</code>  represents if the client  subscribed to a term deposit or not.</p> <pre><code>from sklearn.preprocessing import LabelEncoder\n\nprint(f\"Unique values: {data['y'].unique()}\")\n\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(data[\"y\"])\nprint(y_encoded)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Unique values: ['no' 'yes']\n[0 0 0 ... 0 0 0]\n</code></pre> <p>As <code>\"y\"</code> contains the values <code>\"yes\"</code> and <code>\"no\"</code>,  we retrieve a binary representation with <code>0</code> and <code>1</code>.</p>"},{"location":"data-science/data/preprocessing/#recap","title":"Recap","text":"<p>In this chapter, we have extensively covered missing values. The challenges to  detect them in the first place and how to properly encode them. We explored  different strategies to deal with missing values, from dropping columns/rows  to imputation techniques.</p> <p>Using <code>scikit-learn</code> we were able to easily apply transformation to the  Portuguese retail bank data set. We discretized (<code>KBinsDiscretizer</code>) numerical attributes, normalized them (<code>MinMaxScaler</code>, <code>StandardScaler</code>), and encoded  categorical features with one-hot encoding (<code>OneHotEncoder</code>). Lastly, we  briefly covered the encoding of target variables with the <code>LabelEncoder</code>.</p> <p>With all these preprocessing steps, we are now well-equipped to dive into  the machine learning part and are closer to training our first model.</p> <ol> <li> <p>Decision Support Systems, Volume 62, June 2014, Pages 22-31: https://doi.org/10.1016/j.dss.2014.03.001 \u21a9</p> </li> </ol>"},{"location":"data-science/pipelines/evaluation/","title":"Evaluation","text":""},{"location":"data-science/pipelines/persistence/","title":"Persistence","text":""},{"location":"data-science/pipelines/pipelines/","title":"Pipelines","text":""},{"location":"databasics/Data/","title":"Data vs. Big Data","text":"<p>In its simplest form, data represents information stored digitally , serving as a foundation for analysis and decision-making. Data acts as building blocks for information and knowledge when it is organized and processed. Data is the fundamental element upon which insights, decisions, and actions are based in fields like science, business, and technology.</p>"},{"location":"databasics/Data/#classical-data","title":"Classical Data","text":"<p>Classical data refers to traditional datasets that have been analyzed for decades. These datasets are typically structured, meaning they are organized in a predictable way, such as rows and columns in a relational database. Classical data is generally smaller in volume, ranging from megabytes (MB) to gigabytes (GB), and can be processed using standard tools like SQL databases or spreadsheets . Key characteristics of classical data include:</p> <ul> <li>Structured Format: Data is arranged in a predefined schema, such as tables with rows and columns.</li> <li>Low to Moderate Volume: Datasets are relatively small and manageable.</li> <li>Predictability: The data follows a consistent structure, making it easier to analyze using traditional methods.</li> <li>Centralized Storage: Classical data is typically stored in centralized databases, managed by database management systems (DBMS).</li> </ul> <p>Classical data is ideal for tasks where data volumes are manageable, the structure is well-defined, and computational requirements are not excessive. Examples include customer transaction records, survey data, or small-scale experimental data.</p>"},{"location":"databasics/Data/#big-data","title":"Big Data","text":"<p>Big Data refers to datasets that are large, complex, or fast-changing, often surpassing the capabilities of traditional data tools. Big Data is characterized by the \"three Vs\": Volume, Velocity, and Variety. These datasets are often unstructured or semi-structured and do not fit neatly into traditional database formats. The size of Big Data can range from terabytes (TB) to petabytes (PB) or more. Key characteristics of Big Data include:</p> <ul> <li>Volume: The data size is massive, requiring distributed systems like Hadoop or cloud-based solutions for storage and processing.</li> <li>Velocity: Data is generated and processed at high speeds, often in real-time.</li> <li>Variety: Big Data comes in various formats:<ul> <li>Structured: Data that can be mapped in databases like addresses, product lists, HR management</li> <li>Unstructured: Files like PDF, scanned mails, presentations, images, and videos.</li> <li>Semi-structured: Partly structured, partly unstructured like mails (recipient, sender and subject show a structure, but content is unstructured)</li> </ul> </li> <li>Complexity: The diversity and interconnectedness of data sources add to the complexity of managing and analyzing Big Data.</li> </ul> Visualisation of Big Data according to IBM (Source: BlogDozouza) <p>Big Data requires specialized tools and technologies to handle its characteristics. Distributed computing frameworks like Apache Hadoop and Spark, NoSQL databases, and advanced machine learning algorithms are often employed to extract insights from these vast and complex datasets. Examples of Big Data include social media interactions, sensor data from the Internet of Things (IoT), and genomic sequences.</p> 60 Seconds in the Internet (Source: eDiscovery Today &amp; LTMG)"},{"location":"databasics/Data/#understanding-the-difference","title":"Understanding the Difference","text":"<p>Understanding the differences between classical data and Big Data is essential for professionals in data analysis, data engineering, or data science. Classical data involves structured and manageable datasets processed with conventional tools, while Big Data deals with vast, fast-moving, and complex datasets requiring innovative storage solutions and advanced processing methods.</p> Classical Data Big Data Scale Small Massive Structure Structured Structured, Semi-structured, Unstructured Processing Requirements Traditional tools Advanced algorithms Storage Centralized database Distributed storage Analytical Approaches Statistics Machine learning, Data mining, Real-time processing"},{"location":"databasics/DataBasics/","title":"Data Basics","text":"<p>Before starting with Data Science, it's essential to get to know the data you'll be working with. This means thoroughly examining the attributes and values to understand their characteristics. Real-world data is often messy, large, and diverse, which can make it difficult to handle. For example, data from sensors might include missing or corrupted values, while social media data might be unstructured and include text, images, or even videos. In financial data, outliers or extreme values may skew analysis results.</p> <p>Having a deep understanding of the data is crucial for successful data preprocessing, which is the first major step after acquiring the data. For instance, in sensor data, you may need to filter noise, while in social media data, you might need to convert unstructured text into a structured format for analysis. The main objective is to gather useful insights about the data, which will aid in the preprocessing stage, such as identifying patterns, missing values, or outliers. Grasping these aspects early on provides a solid foundation for the rest of your data analysis process.</p>"},{"location":"databasics/DataBasics/#dataset-objects-attributes","title":"Dataset, Objects, Attributes","text":"<p>In order to do that, we need to distinguish between the terms data set, object and attribute:</p> <ul> <li>Data Set: A data set is a collection of related data organized in a structured format. It consists of multiple objects, each described by a set of attributes. A data set can be represented as a table, where each row corresponds to a data object and each column corresponds to an attribute. Data sets are commonly used in data analysis, machine learning, and other data-driven tasks, serving as the primary source of input for these processes.</li> <li>Object: An object (or sometimes records, instances, or entries) is an individual unit of data within a data set. It represents a single entity or instance, such as a person, a product, or an event, depending on the context of the data. Each data object is characterized by a set of attributes, which define its specific properties or features. In a tabular data set, a data object corresponds to a row, with each attribute value for that object stored in the respective columns.</li> <li>Attributes: Attributes (or sometimes variable, field, dimension, feature or observations) are the characteristics or properties that describe data objects in a data set. Each attribute represents a specific feature of the data object and is associated with a particular value. For example, in a data set of customer information, attributes might include \"Name,\" \"Age,\" \"Gender,\" and \"Purchase History.\" In a tabular representation, attributes are typically the column headers, with each column containing the attribute values for the corresponding data objects (rows). Attributes can be of different types as described later.</li> </ul> Example: Data Set <p>  The displayed data shows a dataset consisting of three objects and seven attributes.</p>"},{"location":"databasics/DataBasics/#qualitative-vs-quantitative","title":"Qualitative vs Quantitative","text":"<p>A variable is called qualitative (categorical) when each observation distinctly falls into a specific category. Qualitative variables express different qualitative properties, but do not convey any magnitude. Conversely, a variable is termed quantitative (numerical) when it measures the magnitude of a property. Quantitative variables can be either Discrete (The variable can only take on a finite number of values) or Continuous (The variable can take on any value within a given interval).</p> Example: Qualitative vs Quantitative Attributes <ul> <li>Qualitative: Race, religious affiliation, gender, children in the household (yes/no)</li> <li>Quantitative: Age, test scores, number of children in a household<ul> <li>Discrete: Number of children in a household  </li> <li>Continuous: Height, weight, length measurements</li> </ul> </li> </ul>"},{"location":"databasics/DataBasics/#attribute-types","title":"Attribute Types","text":"<p>We now know, that attributes define the properties of data objects and are crucial in determining the methods and algorithms that can be applied during analysis. Different types of attributes - such as nominal, ordinal, interval, and ratio - each have unique characteristics that influence how they should be handled and interpreted. Recognizing and appropriately categorizing these attribute types is a key step in ensuring accurate data analysis and meaningful results.</p>"},{"location":"databasics/DataBasics/#nominal","title":"Nominal","text":"<p>Nominal attributes refer to those that are associated with names or labels. </p> <pre><code>cars = ['BMW', 'Audi', 'VW', 'Skoda', 'Tesla', 'Audi']\n</code></pre> <p>The values of nominal attributes are typically symbols or names representing different categories, codes, or states. These values are used to classify data into distinct groups, often referred to as categorical attributes. Importantly, the values of nominal attributes do not have any inherent or meaningful order; they simply indicate membership within a particular category without implying any rank or sequence. </p> Example: Nominal Attributes <ul> <li>Occupation: teacher, dentist, programmer, farmer,...</li> <li>Hair color: black, brown, blond, red, gray, white,...</li> </ul> <p>You can only determine whether two people have the same hair color or not. It is not possible to establish a greater-than or less-than relationship, and the differences between hair colors cannot be meaningfully interpreted.</p> <p>The symbols or names associated with nominal attributes can also be represented by numbers.</p> <pre><code>cars_num = [1, #'BMW'\n            2, #'Audi'\n            3, #'VW'\n            4, #'Skoda'\n            5, #'Tesla'\n            2] #'Audi'  \n</code></pre> <p>However, in such cases, these numbers are not meant to be used quantitatively, meaning that mathematical operations on them are not meaningful. For instance, calculating the mean or median of these numbers would not make sense. </p> <pre><code>import statistics \nstatistics.mean(cars_num)\n</code></pre> &gt;&gt;&gt; Output<pre><code>2.8333333333333335\n</code></pre> <p>But what does that mean? Is it half <code>'Audi'</code>, half <code>'VW'</code>? And we also could have used the numer <code>32</code> for <code>'Audi'</code> and <code>0</code> for <code>'VW'</code>. So its meaningless! </p> <p>Instead, the mode, which identifies the most frequently occurring value, can be used and is particularly useful in this context.</p> <pre><code>statistics.mode(cars)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Audi\n</code></pre> Example: Nominal Attributes Represented by Numbers <ul> <li>Customer ID: 0001, 0002, 0003, 0004, 0005,...</li> <li>Hair color: black=001, brown=010, blond=011,...</li> </ul> <p>Binary attributes are a specific type of nominal attribute that consist of only two categories. These categories are often represented by the numbers 0 and 1, where 0 indicates the absence of the attribute and 1 indicates its presence. This binary classification is commonly used in data analysis to represent simple variables</p> Example: Binary Attributes <ul> <li>Smoker: Yes=1, No=0</li> <li>Medical Test: Positive=1, Negative=0</li> </ul>"},{"location":"databasics/DataBasics/#ordinal","title":"Ordinal","text":"<p>For certain types of attributes, the possible values have a meaningful order or ranking among them, indicating that one value can be considered greater or less than another. However, while this order is significant, the exact magnitude or distance between successive values is not known. This means that while the sequence of values is meaningful, we cannot quantify the difference between them with precise measurements. Therefore it is possible and meaningful to calculate the median and mode. Mean on the other hand is not meaningful.</p> <pre><code>drinks = ['small', 'small', 'small', 'medium', 'medium', 'medium', 'large']\n\nprint('Median: ' + statistics.median(drinks))\nprint('Mode: ' + statistics.mode(drinks))\n</code></pre> &gt;&gt;&gt; Output<pre><code>Median: medium\nMode: small\n</code></pre> Example: Ordinal Attributes <ul> <li>Professional Rank: private, specialist, corporal, sergeant</li> <li>Drink size: small, medium, large</li> </ul> <p>As with the nominal scale, you can determine whether two drinks are the same size or not. Additionally, you can say whether one drink is larger than another. However, it is still not possible to meaningfully interpret the differences between sizes. You cannot specify by how much one drink is larger than another.</p>"},{"location":"databasics/DataBasics/#numerical","title":"Numerical","text":"<p>Numeric attributes are quantitative in nature, meaning they represent measurable quantities. These attributes can be expressed as either integer or real values. One of the key characteristics of numeric attributes is their ability to quantify the difference between values, allowing for meaningful comparisons. Statistical measures such as the mean, median, and mode can be calculated from numeric attributes, and these measures are both possible and useful for analyzing the data.</p> <pre><code>income = [1005, 2500, 2500, 5100, 6011, 10500]\n\nprint('Mean: ' + str(statistics.mean(income)))\nprint('Median: ' + str(statistics.median(income)))\nprint('Mode: ' + str(statistics.mode(income)))\n</code></pre> &gt;&gt;&gt; Output<pre><code>Mean: 4602.67\nMedian: 3800.0\nMode: 2500\n</code></pre>"},{"location":"databasics/DataBasics/#interval-scaled","title":"Interval-scaled","text":"<p>Interval-scaled attributes can be measured on a scale with equal-sized units, allowing for consistent and comparable intervals between values. These attributes have an inherent order and they can take on positive, zero, or negative values. This means that the ranking of values is possible and meaningful, providing a clear sense of progression or regression along the scale.</p> Example: Interval-scaled Attributes <ul> <li>Calendar dates: For instance, the years 2002 and 2010 are eight years apart</li> <li>Celsius temperature: \\(20^\\circ C\\) is five degrees higher than a temperature of \\(15^\\circ C\\)</li> </ul> <p>As with the ordinal scale, you can determine whether two temperatures are the same and whether one temperature is higher than another. Additionally, the difference between temperatures can be meaningfully interpreted. However, because the zero point is arbitrary, ratios cannot be meaningfully interpreted.</p>"},{"location":"databasics/DataBasics/#ratio-scaled","title":"Ratio-scaled","text":"<p>Ratio-scaled attributes possess an inherent zero-point, which indicates the complete absence of the attribute. This characteristic allows us to meaningfully discuss one value as being a multiple of another. Because of this, ratio-scaled data supports a wide range of mathematical operations, including meaningful comparisons of both differences and ratios between values.</p> Example: Ratio-scaled Attributes <ul> <li>Kelvin temperature: has true zero-point</li> <li>Count attributes: years of experience, number of words, Income in Euros</li> </ul> <p>As with the interval scale, you can determine whether two people have the same income and whether one person earns more than another. Additionally, the differences between incomes can be meaningfully interpreted. Furthermore, the ratio between two incomes can now also be interpreted, such as determining how many times one income is compared to another.</p> Task: Attribute Types <p>Name the types of attributes in the following data set and justify  </p>"},{"location":"databasics/Terms/","title":"Definition of Key Terms","text":"<p>Johannes Kepler (1571-1630) is often considered one of the first Data Scientists due to his groundbreaking work in analyzing astronomical data to uncover the laws governing planetary motion. In the early 1600s, Kepler meticulously studied large amounts of observational data gathered by the astronomer Tycho Brahe. By applying mathematical models and repeated analysis, Kepler discovered that planets move in elliptical orbits around the sun, challenging the long-standing belief in circular orbits. His findings were published in the Rudolphine Tables in 1627, which were the most precise planetary position tables of the time. These tables, based on Kepler's laws of planetary motion, were crucial for navigation and scientific research for over a century, illustrating his innovative use of data to revolutionize our understanding of the solar system. This work laid the foundation for modern data analysis practices.</p> <ul> <li> <p> Johannes Keppler (Source: Wikipedia) </p> </li> <li> <p> Rudolphine Tables (Source: Wikipedia) </p> </li> </ul> <p>In the world of data analysis and processing, terms like Data Science, Machine Learning, Artificial Intelligence (AI), Business Intelligence (BI), Predictive Analytics, and Data Engineering are often used. Each of these terms refers to distinct but interconnected fields and methods. To gain a better understanding of these areas, it is important to clearly define these terms and explore how they are related.</p> Definitions of Key Terms <p>To familiarize yourself with fundamental concepts, research the following terms:</p> <ul> <li> <p>Data Science: Investigate what Data Science encompasses and how it differs from other disciplines. What are the central tasks and methods involved in Data Science?</p> </li> <li> <p>Machine Learning: Research how Machine Learning is defined and what role it plays within Data Science. Explore the different types of Machine Learning and their applications.</p> </li> <li> <p>Artificial Intelligence (AI): What is Artificial Intelligence, and how does it differ from Machine Learning? What are the application areas of AI that extend beyond Machine Learning?</p> </li> <li> <p>Business Intelligence (BI): BI is a commonly used term in businesses. What does BI mean, and how does it differ from Data Science? What tools and methods are typical for BI?</p> </li> <li> <p>Predictive Analytics: Investigate what Predictive Analytics is and how it is used in the context of Data Science and BI. How is Predictive Analytics related to Machine Learning?</p> </li> <li> <p>Data Engineering: Find out what tasks and responsibilities are included in Data Engineering and how it differs from Data Science. Why is Data Engineering an essential part of successful data projects?</p> </li> </ul>"},{"location":"python/","title":"Why Python?","text":"<p>This crash course will teach you the basics of the   Python programming language.</p>"},{"location":"python/#motivation","title":"Motivation","text":"Creation of a 3D surface plot of the Lattenspitze. \ud83c\udfd4\ufe0f     That's the power of Python - ease of use paired with a wide range of      functionalities stemming from a large developer community! \ud83e\uddbe  <ul> <li> <p> Ease of use</p> <p><code>Python</code> with its syntax is easy to learn and yet very powerful.</p> </li> <li> <p> Flexible</p> <p><code>Python</code> is a versatile language that can be used for web development,  data analysis, artificial intelligence, and many more.</p> </li> </ul> <p>The below section should give you an impression of what you can do with  <code>Python</code>. This is not an extensive list by all means. It might sound  trashy but if you can imagine something you probably can build it in  <code>Python</code>.</p> <p>Don't worry about the code snippets too much, after finishing the  course you'll have a better understanding of them and will be able to run  and modify code yourself. For now, the snippets illustrate the capabilities  of the language and what complex things you can achieve with little code.</p>"},{"location":"python/#examples","title":"Examples","text":""},{"location":"python/#visualizations","title":"Visualizations","text":"<p>You can create stunning and interactive visualizations<sup>1</sup>.</p> <pre><code># Source: https://plotly.com/python/tile-county-choropleth/\nimport plotly.express as px\n\ndf = px.data.election()\ngeojson = px.data.election_geojson()\n\nfig = px.choropleth_map(\n    df,\n    geojson=geojson,\n    color=\"Bergeron\",\n    locations=\"district\",\n    featureidkey=\"properties.district\",\n    center={\"lat\": 45.5517, \"lon\": -73.7073},\n    map_style=\"carto-positron\",\n    zoom=9,\n)\nfig.show()\n</code></pre>"},{"location":"python/#machine-learningai","title":"Machine Learning/AI","text":"<p>... or you can easily train your own machine learning models. In this  example, with just a few lines of code, a decision tree is fit and  visualized<sup>2</sup>.</p> <pre><code># Source: https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#train-tree-classifier\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\niris = load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nclf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)\n\nplot_tree(clf)\nplt.savefig(\"tree.svg\")\n</code></pre>"},{"location":"python/#automation","title":"Automation","text":"<p>... or automate tasks that you would have otherwise done manually. This code snippet fetches some data (from an online service) and writes it to a file<sup>3</sup>.</p> <pre><code>import json\nfrom pathlib import Path\n\nimport httpx\n\nurl = \"https://pokeapi.co/api/v2/pokemon/charmander\"\n\nresponse = httpx.get(url)\n\nwith Path(\"charmander.json\").open(\"w\") as file:\n    json.dump(response.json(), file, indent=4)\n</code></pre>"},{"location":"python/#web-development","title":"Web Development","text":"<p>You can create websites, just like this one. In fact, all the  heavy lifting of this site is done by <code>Python</code> and tools developed by its  community.</p>"},{"location":"python/#getting-started","title":"Getting Started...","text":"<p>In the next sections, we will install <code>Python</code> including the code editor  <code>Visual Studio Code</code>.</p> Info <p>Both Python and Visual Studio Code are already pre-installed on PCs in the MCI computer rooms. If you are working with your own computer,  please proceed to the installation page.</p> <ol> <li> <p>Plotly is a Python graphing library that  lets you create interactive, publication-quality graphs.\u00a0\u21a9</p> </li> <li> <p>Scikit-learn is a Python library  for machine learning.\u00a0\u21a9</p> </li> <li> <p>HTTPX is a Python library to interact  with APIs.\u00a0\u21a9</p> </li> </ol>"},{"location":"python/comparisons_and_logic/","title":"Comparisons &amp; Logical operators","text":""},{"location":"python/comparisons_and_logic/#comparisons","title":"Comparisons","text":"<p>Now, that we have covered all basic <code>Python</code> types, we can start comparing them. As the name suggests, comparisons are used to compare two values. The result  of a comparison is a boolean value.</p>"},{"location":"python/comparisons_and_logic/#equality","title":"Equality","text":"<pre><code>print(\"Abc\" == \"abc\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>False\n</code></pre> <p>We can compare any type with each other. In the above case, the comparison  checks if both strings are equal, using the <code>==</code> operator. The result is  <code>False</code>, because the case of the strings do not match.</p> <p>Let's check if two integers are equal:</p> <pre><code>print(1 == 1)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre>"},{"location":"python/comparisons_and_logic/#inequality","title":"Inequality","text":"<p>We can also check if two values are not equal with the <code>!=</code> operator:</p> <pre><code>user_name = \"Eric\"\nprint(user_name != \"admin\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre> <pre><code>print(2 != 2.1)\nprint(2 != 2)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\nFalse\n</code></pre>"},{"location":"python/comparisons_and_logic/#numerical-comparisons","title":"Numerical comparisons","text":"<p>... are done with:</p> Operator Description <code>&lt;</code> less than <code>&gt;</code> greater than <code>&lt;=</code> less than or equal <code>&gt;=</code> greater than or equal <pre><code>print(1 &lt; 2)\nprint(1 &gt; 2)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\nFalse\n</code></pre> <pre><code>print(10.2 &lt;= 10.2)\nprint(9.99 &gt;= 10)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\nFalse\n</code></pre>"},{"location":"python/comparisons_and_logic/#logical-operators","title":"Logical Operators","text":"<p>You may want to check multiple conditions at the same time. For example, sometimes you might need two conditions to evaluate to <code>True</code> in  order to perform an action. Hence, logical operators are introduced.  There are three logical operators:</p> Operator Meaning Example Result <code>and</code> Returns <code>True</code> if both statements are <code>True</code> <code>True and True</code> <code>True</code> <code>or</code> Returns <code>True</code> if one of the statements is <code>True</code> <code>True or False</code> <code>True</code> <code>not</code> Reverses a result <code>not True</code> <code>False</code>"},{"location":"python/comparisons_and_logic/#and","title":"<code>and</code>","text":"<pre><code>age = 20\nprint(age &gt;= 18 and age &lt;= 25) # True and True -&gt; True\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre>"},{"location":"python/comparisons_and_logic/#or","title":"<code>or</code>","text":"<pre><code>age = 20\nprint(age &gt;= 50 or age &lt;= 25) # False or True -&gt; True\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre>"},{"location":"python/comparisons_and_logic/#not","title":"<code>not</code>","text":"<pre><code>age = 20\nprint(not(age &gt;= 18)) # not(True) -&gt; False\n</code></pre> &gt;&gt;&gt; Output<pre><code>False\n</code></pre> Evaluate password security requirements: Part 1 <p>You are given two variables that describe properties of a password:</p> <ul> <li><code>password_length</code> - represents how many characters are in the password     (<code>int</code>)</li> <li><code>has_special_characters</code> - represents whether the password contains      special characters (<code>True</code>/<code>False</code>)</li> </ul> <p>Variables to use:</p> <pre><code>password_length = 18\nhas_special_characters = False\n</code></pre> <p>Task:</p> <p>Write code that checks if this password is secure based on these requirements:</p> <ol> <li>The password must be longer than 10 characters</li> <li>The password must contain special characters</li> </ol> <p>Use comparisons and logical operators to create a single expression  that evaluates whether BOTH requirements are met.</p> Evaluate password security requirements: Part 2 <p>To increase security, a third variable is introduced alongside the  previous password properties:</p> <p><code>already_used</code> - represents whether this password has been used before     (<code>True</code>/<code>False</code>)</p> <p>Variables to use:</p> <pre><code>password_length = 18\nhas_special_characters = True\nalready_used = False\n</code></pre> <p>Task:</p> <p>Write code that checks if this password is secure based on these three requirements:</p> <ol> <li>The password must be longer than 10 characters</li> <li>The password must contain special characters</li> <li>The password must not have been used before</li> </ol> <p>Build on your previous solution and evaluate whether all THREE  requirements are met.</p>"},{"location":"python/comparisons_and_logic/#recap","title":"Recap","text":"<p>We have covered the basic comparison and logical operators in <code>Python</code> .</p> <ul> <li> <p>Comparisons</p> <ul> <li><code>==</code> for equality</li> <li><code>!=</code> for inequality</li> <li><code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code> for numerical comparisons</li> </ul> </li> <li> <p>Logical operators</p> <ul> <li><code>and</code></li> <li><code>or</code></li> <li><code>not</code></li> </ul> </li> </ul>"},{"location":"python/functions/","title":"Functions","text":"<p>In this section, you\u2019ll learn to write functions, which are named blocks of code that are designed to do one specific task. If you need to perform that task multiple times throughout your program, you don\u2019t need to type all the code for the same task again and again; you just call the function dedicated to handling that task. By defining functions, your programs will get easier to write, read, test, and fix.</p>"},{"location":"python/functions/#defining-a-function","title":"Defining a function","text":"<p>Here\u2019s a simple function named <code>greet_user()</code> that prints a greeting:</p> <pre><code>def greet_user():\n    print(\"Hello!\")\n</code></pre> <p>This example shows the simplest structure of a function. With the keyword <code>def</code> we define a function, followed by the name of our function.  Within the parentheses we can (optionally) specify any information the function needs to do its job. This information is defined in the form of parameters (more on that later).</p> <p>Any indented lines that follow <code>def greet_user():</code> make up the body of the function. The line <code>print(\"Hello!\")</code> is the only line of actual code in the body of this function, so <code>greet_user()</code> has just one job: <code>print(\"Hello!\")</code>.</p> <p>When you want to use a function, you have to call it. To do so, simply write the function name, followed by any required information in parentheses.</p> <pre><code># call the function\ngreet_user()\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello!\n</code></pre>"},{"location":"python/functions/#detour-docstrings","title":"Detour: docstrings","text":"<p>As previously discussed, it is always good practice to add comments to your code in order to improve readability. That applies to functions as well. In  case of functions, one can add a docstring, which is essentially a longer  comment that describes the function. A docstring is written in triple quotes <code>\"\"\"...\"\"\"</code> and is placed directly after the function definition.</p> <pre><code>def greet_user():\n    \"\"\"Display a simple greeting.\"\"\"\n    print(\"Hello!\")\n</code></pre> <p>Now, you can display the docstring by calling the built-in <code>help()</code>  function with the function name as an argument:</p> <pre><code>help(greet_user)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Help on function greet_user in module __main__:\ngreet_user()\n    Display a simple greeting.\n</code></pre> <p>Docstrings facilitate the proper documentation of your code. Most of all, they will help you in the long run to remember what your code does.</p>"},{"location":"python/functions/#parameters","title":"Parameters","text":"<p>After some modification, the function <code>greet_user()</code> should not only tell the user \"Hello!\" but also greet them by name. Therefore, we have to  define a parameter called <code>user_name</code>. Now, each time you call the function,  you need to pass a <code>user_name</code> such as <code>\"admin\"</code> to the function.</p> <pre><code>def greet_user(user_name):\n    \"\"\"\n    Display a simple greeting.\n    Pass a string with the user's name.\n    \"\"\"\n    print(f\"Hello, {user_name}!\")\n\ngreet_user(\"admin\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello, admin!\n</code></pre> Info <p>As you can see in the example above, a docstring can span multiple lines!</p> <p>Up until now, the functions had no parameters at all or just a single  parameter. However, you can define as many parameters as you like, seperated  by a comma (<code>,</code>):</p> <pre><code>def greet(first_name, last_name):\n    print(f\"Hello, {first_name} {last_name}!\")\n</code></pre> Break-even point <p>Remember the task to calculate the break-even point? Now, you'll wrap  following formula within a function: $$ \\text{BEP (units)} = \\frac{\\text{Fixed Costs}}{\\text{Price per Unit} - \\text{Variable Cost per Unit}} $$</p> <p>Write a function called <code>calculate_bep()</code> that takes the following parameters:</p> <ul> <li><code>fixed_costs</code></li> <li><code>price_per_unit</code></li> <li><code>variable_cost_per_unit</code></li> </ul> <p>Print the calculation result (break-even point) within the function.  Call the function with following arguments:</p> <pre><code>fixed_costs = 30000\nprice_per_unit = 75\nvariable_cost_per_unit = 45\n</code></pre>"},{"location":"python/functions/#terminology","title":"Terminology","text":"<p>parameter vs. argument</p> <p>A parameter is the variable inside the parentheses of the function definition - <code>def greet_user(user_name):</code>. Here <code>user_name</code> is the parameter. When you call the function with, for example <code>greet_user(\"admin\")</code>, the  value <code>\"admin\"</code> is referred to as an argument. You can think of the parameter  as a placeholder and the argument as the actual value.</p>"},{"location":"python/functions/#positional-arguments","title":"Positional arguments","text":"<p>When you call a function, <code>Python</code> must match each argument in the function  call with a parameter in the function definition. The simplest way to do this  is based on the order of the arguments provided which is referred to as positional arguments.</p> <pre><code>def add_numbers(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    print(a + b)\n\nadd_numbers(3, 5)\n</code></pre> &gt;&gt;&gt; Output<pre><code>8\n</code></pre>"},{"location":"python/functions/#order-matters","title":"Order matters!","text":"<p>You can get unexpected results, if you mix up the order of the arguments in a function call when using positional arguments:</p> <pre><code>def perform_calculation(a, b):\n    \"\"\"Calculate something.\"\"\"\n    print(a + b**b)\n\na = 12\nb = 5\n\n# correct order\nperform_calculation(a, b)\n# incorrect order (produces a different result)\nperform_calculation(b, a)\n</code></pre> &gt;&gt;&gt; Output<pre><code>3137\n8916100448261\n</code></pre> <p>Next up, we'll introduce keyword arguments to avoid such mistakes.</p>"},{"location":"python/functions/#keyword-arguments","title":"Keyword arguments","text":"<p>A keyword argument is a name-value pair that you pass to a function. You directly associate the name and the value within the argument, so when you pass the argument to the function, there\u2019s no confusion.</p> <pre><code>perform_calculation(a=12, b=5)\n\n#  you can switch the order of the named keyword arguments\nperform_calculation(b=5, a=12)\n</code></pre> <p>No matter the order, the result is the same!</p> &gt;&gt;&gt; Output<pre><code>3137\n3137\n</code></pre>"},{"location":"python/functions/#default-values","title":"Default values","text":"<p>When writing a function, you can optionally define a default value for each parameter. If an argument for a parameter is provided in the function call, <code>Python</code> uses the argument value. If not, the parameter\u2019s default value is used. Using default values can simplify your function calls and clarify the ways in which your functions are typically used. Let's look at an example.</p> <pre><code>def describe_lab(lab_name, lab_supervisor=\"Miriam\"):\n    print(f\"{lab_name} is supervised by {lab_supervisor}\")\n\n# use the default value for lab_supervisor\ndescribe_lab(lab_name=\"Chemistry\")\n\n# provide a value for lab_supervisor\ndescribe_lab(lab_name=\"IT\", lab_supervisor=\"Alex\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Chemistry is supervised by Miriam\nIT is supervised by Alex\n</code></pre>"},{"location":"python/functions/#return-values","title":"Return values","text":"<p>A function doesn\u2019t have to display its output directly with <code>print()</code>.  Instead, usually a function executes a task and returns the result. The result, the return value can be of any type. To return a value, use the <code>return</code> statement.</p> <pre><code>def square_number(number):\n    square = number ** 2\n    return square\n</code></pre> <p>When you call the function, you can assign the result to a variable.</p> <pre><code>result = square_number(5)\n</code></pre> Password check <p>Write a function to check if a user is able to log-in.</p> <p>You have to determine if the given username and password (in  plain text) are within the database. The database is a <code>dict</code>  with IDs as keys and another <code>dict</code> as value, containing  username and hashed password.</p> <p>Given 'database':</p> <pre><code>very_secure_db = {\n    0: {\n        \"username\": \"SwiftShark22\",\n        \"password\": \"ef92b778bafe771e89245b89ecbc08a44a4e166c06659911881f383d4473e94f\",\n    },\n    1: {\n        \"username\": \"FierceFalcon66\",\n        \"password\": \"07ac6e7d83aa285293fc325fecd04a51e933ab94d43dbc6434ddca652718fb95\",\n    },\n    2: {\n        \"username\": \"admin\",\n        \"password\": \"6feb4c700de1982f91ee7a1b40ca4ded05d155af3987597cb179f430dd60da0b\",\n    },\n    3: {\n        \"username\": \"BraveWolf11\",\n        \"password\": \"c430e4368aff7c1bc75c3865343730500d7c1a5f65758ade56026b08e94686cc\",\n    },\n}\n</code></pre> <p> </p> <p>Use following function to convert a password to its hash:</p> <pre><code>import hashlib\n\ndef hash_password(password):\n    return hashlib.sha256(password.encode()).hexdigest()\n</code></pre> <p>Now, write a function that takes at least two arguments, <code>username</code> and  <code>password</code> (in plain text!). Check if the given username and hashed  password are within the <code>very_secure_db</code>. Return <code>True</code> if the  user is able to log-in, otherwise <code>False</code>.</p> <p>Call your function for following users:</p> <pre><code>user1 = (\"SwiftShark22\", \"password123\")\nuser2 = (\"FierceFalcon\", \"sdkjf34\u00a7\")\nuser3 = (\"admin\", \"1b40ca4ded0\")\n</code></pre> Info <p>As you have seen in the above example, functions help you to structure your code. For instance, the function <code>hash_password()</code> was reused multiple  times (to generate the <code>very_secure_db</code> and within your own function).</p> <p>Functions also help you to break down complex problems. You can write a  function for each subtask and then combine them to solve the problem as a  whole.</p>"},{"location":"python/functions/#recap","title":"Recap","text":"<p>This section introduced the concept of functions to better structure your  code, make it more readable and reusable. We have covered:</p> <ul> <li>How to define a function</li> <li>Docstrings as a tool to document your functions</li> <li>Parameters vs arguments</li> <li>Positional and keyword arguments</li> <li>Defining default values for parameters</li> <li>The <code>return</code> statement</li> <li>How to use functions to solve smaller subtasks and structure your code</li> </ul>"},{"location":"python/ide/","title":"IDE","text":"<p>After the successful installation of Python, we use an IDE (=Integrated Development Environment) which is simply put, a place to write and  execute <code>Python</code> code. There are many IDEs available, but we recommend using  Visual Studio Code (VS Code/VSC).</p>"},{"location":"python/ide/#visual-studio-code","title":"Visual Studio Code","text":""},{"location":"python/ide/#general-information","title":"General Information","text":"<p>VSCode is a free, open-source code editor developed by Microsoft . It has gained immense popularity among developers for its versatility and extensive extension ecosystem, making it a powerful tool for various  programming tasks, including Python and Jupyter Notebook programming.  Some key features of VSCode include:</p> <ul> <li>Cross-Platform: VSCode is available for Windows , macOS , and Linux ,  making it accessible to developers on different operating systems.</li> <li>Lightweight: It\u2019s known for its speed and efficiency. VSCode launches quickly and consumes minimal system resources.</li> <li>Extensible: VSCode supports a wide range of programming languages and  technologies through extensions. You can customize the editor with extensions  to add new features, integrations, and tools. VSCode offers intelligent code  completion and suggestions, which can significantly boost your productivity  while writing code. Additionally, there is an extension for GitHub Copilot which gives you real-time AI-based suggestions (free for students; sign-up here)</li> <li>Version Control: It has built-in <code>Git</code> support, making it easy to manage  version control and collaborate with others using Git repositories.</li> <li>Large Community: VSCode has a large and active community, which means you can find plenty of resources, extensions, and tutorials to enhance your coding experience.</li> </ul>"},{"location":"python/ide/#setup","title":"Setup","text":"<p>Download the installer from the official website. The installation is straightforward, so we won't cover it in detail. </p>"},{"location":"python/ide/#extensions","title":"Extensions","text":"<p>As already mentioned, VSCode can be used for a wide range of programming languages. To do this, we need to install the corresponding extensions.  Therefore, start VSCode and click on the sidebar on <code>Extensions</code>. Then search  and install <code>Jupyter</code> and <code>Python</code> (both from Microsoft). </p> Install the Python and Jupyter extension <p>Now, restart VSCode.</p>"},{"location":"python/ide/#jupyter-notebook","title":"Jupyter notebook","text":"<p>Next, we create a new file to execute our first Python code.  To do so, we use Jupyter notebooks. Jupyter notebooks are basically composed of cells. A cell can either contain  code or text. However, first, we have to create our first notebook.</p> <p>Hence, we first select a folder in which we want to save our work. We go to  <code>File</code> <code>Open Folder</code> and choose a folder. Then click on explorer in the sidebar where your folder should be opened. Right  click somewhere in the explorer and select <code>New File</code>. Type a name for  your file with the extension <code>*.ipynb</code>. If not automatically, open the new file. Click on <code>Select Kernel</code> in the  upper right corner of VSCode and select <code>Python Environment</code>  your <code>Python</code> installation.</p> Select your Python kernel. <p>If your firewall asks, allow access.</p> Allow access. <p>Now, add your first code cell with the <code>+ Code</code> button in the upper left  corner. Add following line.</p> <pre><code>print(\"Hello World!\")\n</code></pre> Run your first code snippet. <p>After clicking on <code>Run All</code>, a popup will appear to install the <code>ipykernel</code>. Click on <code>Install</code>. </p> Last missing piece - the ipykernel. <p>After the installation, you should be greeted with following output</p> &gt;&gt;&gt; Output<pre><code>Hello World!\n</code></pre> <p>Congratulations \ud83c\udf89, you've successfully executed your first <code>Python</code>  code!</p>"},{"location":"python/ide/#more-on-jupyter-notebooks","title":"... more on Jupyter notebooks","text":""},{"location":"python/ide/#why","title":"Why?","text":"<p>One of the key features of Jupyter Notebook is the combination of code cells  with rich text elements, allowing you to create comprehensive documents that  blend code, visualizations, and explanatory text. This makes it a powerful tool for creating data analysis reports, sharing research findings, or documenting  code workflows.</p> <p>In addition to code execution and documentation capabilities, Jupyter Notebook  offers a wide range of extensions and integrations with popular data science  libraries, plotting libraries, and other tools. It provides a flexible and  interactive environment for data manipulation, visualization, and analysis.</p>"},{"location":"python/ide/#cells","title":"Cells","text":"<p>As previously discussed, Jupyter notebooks are composed of cells. A cell can  contain Python code or text. To add a text cell, click on <code>+ Markdown</code>.  Markdown  is a lightweight markup language with  plain text formatting syntax. You can simply write text, add images and links within a markdown cell. This guide offers a nice comprehensive overview of Markdown.</p> Info <p>Don't worry about Markdown too much, it is simple to use and 'supports'  plain text. So just start writing.</p>"},{"location":"python/ide/#execution","title":"Execution","text":"<p>You can execute cells one by one. Either by clicking on the <code>Exceute Cell</code>  button on the left side of your current cell. Or by using the shortcut  Ctrl+Enter.</p> <p>Run all cells with the corresponding <code>Run All</code> button on top.</p>"},{"location":"python/ide/#coming-up","title":"Coming up ...","text":"<p>Next, we will cover some basic Python concepts, and you will get more familiar  with code cells.</p>"},{"location":"python/installation/","title":"Installation","text":"<p>First, we need to install Python. You can download the latest version of Python from the official website python.org.</p> <p>We recommend to install the latest version of  Python (as of September 2024, the latest stable release is <code>3.12.6</code>).</p> <p>Depending on your operating system, the installation process may vary slightly. Below, we will cover the installation process on a Windows machine. If you are using macOS, python.org offers a corresponding installer (64-bit). </p>"},{"location":"python/installation/#windows","title":"Windows","text":"<p>Execute the downloaded file (installer). When installing Python, make sure that you check the box <code>Add python.exe to PATH</code>.</p> <p>After the successful installation, we recommend to open a command prompt (use the Windows search with the keyword <code>cmd</code>) and verify the installation by  typing </p> <pre><code>python --version\n</code></pre> <p>Which should result in:</p> CMD Output<pre><code>Python 3.12.6\n</code></pre>"},{"location":"python/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"python/installation/#path-issues","title":"PATH issues","text":"<p>If you didn't check the box <code>Add python.exe to PATH</code> during  installation, or you encounter an error message along the lines of </p> <pre><code>'python' is not recognized as an internal or external command\n</code></pre> <p>you need to add Python to your PATH (which means the  Python executable is simply not found).</p> <p>We cover two options to fix the PATH issue, either use the command prompt  or the GUI.</p> Option 1: GUIOption 2: Command prompt <p>Step 1:</p> <p>First, we need to find the path to the executable. </p> <p>Open the  Windows search and type <code>python</code>. Select <code>Dateispeicherort \u00f6ffnen</code> (open file location). Open the context menu of <code>Python</code> (that's just a shortcut) and select <code>Eigenschaften</code> (properties)  <code>Dateipfad \u00f6ffnen</code> (open file path).  Lastly, copy the path of the newly opened explorer window.</p> <p>      Determine the path to the Python executable. </p> <p>Step 2:</p> <p>Now, we need to add the path to the environment variables. Again use  the  Windows search and type  <code>Umgebungsvariablen</code> (Environment variables). Select the Path value in the  <code>Benutzervariablen f\u00fcr &lt;user-name&gt;</code> (User variables) section. Click on  <code>Neu</code> (New) and paste the copied path.</p> <p>          Add the path to the user variables.      </p> <p>Step 1:</p> <p>Determine the path to the  Python executable  using the Python launcher <code>py</code> (which is part of the Python installation and  is on PATH by default).</p> <pre><code>py -3.12 -c \"import sys; print(sys.executable)\"\n</code></pre> <p>In my case, the output is:</p> CMD Output<pre><code>C:\\Users\\ztklotz\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n</code></pre> <p>Copy your path without the <code>python.exe</code> part.</p> <p>Step 2:</p> <p>Set the PATH variable using the command prompt.</p> <pre><code>setx PATH \"%PATH%;&lt;copied-path&gt;\"\n</code></pre> <p>For instance (using my path):</p> <pre><code>setx PATH \"%PATH%;C:\\Users\\ztklotz\\AppData\\Local\\Programs\\Python\\Python312\"\n</code></pre> <p>Step 3:</p> <p>Again, verify the installation by typing <code>python --version</code> in your command  prompt.</p> <p>With  Python installed, the next step is to set up a code editor. In the following section, we will install Visual Studio Code (VS Code).</p>"},{"location":"python/packages/","title":"Package Management","text":""},{"location":"python/packages/#introduction","title":"Introduction","text":"<p>One reason, why <code>Python</code> is widespread, is its vibrant community. This community develops code to solve a variety of problems in the widest range of scientific fields. This code is bundled and shared for free (as open-source) in the form of packages. You can download and use these packages. The usage of packages will facilitate your coding process as they offer implementations to solve common problems. Therefore, you won't have to reinvent the wheel.</p> <p>For example the package <code>pandas</code> is the go-to tool for data manipulation and analysis. With <code>pandas</code> you can read text and Excel files   among a lot of other formats and it offers a lot of functionality to manipulate and even plot your data. Hence, you will rarely see <code>Python</code> projects that are not dependent on <code>pandas</code>. Apart from <code>pandas</code> there are a wide variety of popular packages:</p> <ul> <li><code>scipy</code> - statistics (which will be covered in the next   course)</li> <li><code>tqdm</code> - build progress bars</li> <li><code>scikit-learn</code> - for machine learning</li> <li><code>numpy</code> - scientific computing</li> <li>... and many many more</li> </ul> <p>This section serves as a guide on how to install and manage packages. Additionally, the concept of virtual environments is explained.</p>"},{"location":"python/packages/#standard-library","title":"Standard library","text":"<p><code>Python</code> comes with a couple of modules which do not need to be installed and can be used 'out of the box'. For simplicity, we will call these modules packages as well. If you're interested in the difference between packages and modules, Real Python has a nice article on the topic. Here is an extensive list of all the packages that <code>Python</code> ships with.</p> <p>Let's use the <code>random</code> package to generate some random numbers.  First, we have to import the package with the following command:</p> <pre><code>import random\n\n# with the package imported we can use its functions\n# e.g., random integer (between 1 and 100)\nprint(random.randint(1, 100))\n</code></pre> &gt;&gt;&gt; Output<pre><code>42\n</code></pre> <p>Note, the output will be different when you run the code, since it is random.</p> <p>The corresponding documentation is available here. Generally speaking, almost all packages offer an online documentation page. It is good practice, to consult these documentation sites as they offer a lot of information on how to use their package and which methods/functions are available. Usually, functionalities are illustrated with examples that can be a good starting point for your project.</p> Info <p>If you remember, <code>random</code> was used in one of the previous  sections on control structures to generate  passwords of variable length.</p> Calculate the median <p>Use the built-in <code>statistics</code> package to calculate the median of  the below given values. Use Google to search for the <code>statistics</code>  documentation page and try to find the appropriate function.</p> <pre><code>values = [13, 58, 90, 34, 49, 41]\n</code></pre> <p>We will continue with another exercise.</p> Variance of random values <p>Generate a list of random values (can be integers and/or floats) and  calculate the variance. Hint: Use both the <code>random</code> and  <code>statistics</code> package.</p>"},{"location":"python/packages/#installing-packages","title":"Installing packages","text":"<p>To get access to all the packages available online, we need to install them using a package manager. One such manager is <code>pip</code> which is  automatically installed alongside <code>Python</code>. To check if <code>pip</code> is available  on your system open a new terminal within VSC by navigating in the menu bar  <code>Terminal</code> <code>New Terminal</code> </p> VSC Terminal <p>and execute the following command:</p> <pre><code>pip\n</code></pre> <p>... you should see a list of commands and their description:</p> <pre><code>Usage:   \n  pip &lt;command&gt; [options]\n\nCommands:\n  install                     Install packages.\n  download                    Download packages.\n  uninstall                   Uninstall packages.\n  freeze                      Output installed packages in requirements format.\n  inspect                     Inspect the python environment.\n  list                        List installed packages.\n  show                        Show information about installed packages.\n...\n</code></pre> Info <p>You can run shell commands directly from your notebook by using an  exclamation mark (<code>!</code>) as a prefix (e.g., <code>!pip</code>). However, in some cases, such as when uninstalling a package, this approach may cause issues.  Therefore, it's often recommended to use the terminal instead.</p> <p>Now, we'll install our first package, called <code>seaborn</code>. To install a package use pip's <code>install</code> command followed by the package name  (<code>pip install &lt;package-name&gt;</code>). Don't worry, it might take a couple of seconds.</p> <pre><code>pip install seaborn\n</code></pre> <p><code>seaborn</code> is a quite common package to visualize data. Now, run the following code to create your first plot. The code snippet was copied from the <code>seaborn</code> documentation here.</p> <pre><code># taken from https://seaborn.pydata.org/examples/grouped_boxplot.html\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\n\n# Load the example tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Draw a nested boxplot to show bills by day and time\nsns.boxplot(x=\"day\", y=\"total_bill\",\n            hue=\"smoker\", palette=[\"m\", \"g\"],\n            data=tips)\nsns.despine(offset=10, trim=True)\n</code></pre> <p>You don't have to fully understand the code snippet. It's more about the successful usage of a package. You might have noticed, that you didn't solely install <code>seaborn</code>. Among <code>seaborn</code>, <code>pip</code> also installed <code>pandas</code> (for data handling). We can 'verify' that by checking the type of <code>tips</code> (from the code snippet above).</p> <pre><code>print(type(tips))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> <p>Most of the time, a package does not 'stand on its own'. It uses the functionalities of other packages as well. In our case, <code>seaborn</code> also needs <code>pandas</code> to properly function. Hence, a lot of packages are dependent on each other.</p> Remove a package <p>Remove the <code>seaborn</code> package. Like above, use <code>pip</code> within a terminal to  list all commands and find the appropriate one. Execute the command  to remove the package.</p>"},{"location":"python/packages/#pypi","title":"PyPI","text":"<p>You might wonder where <code>pip</code> downloads the packages?! In short, all packages are downloaded from the Python Package Index (PyPI). That's where the open-source community (usually) publishes their packages. Simply put, if you type <code>pip install seaborn</code>, <code>pip</code> looks for a package called <code>seaborn</code> on PyPI and downloads it. <code>PyPI</code> is a valuable resource if you're searching for packages, certain versions, etc.</p>"},{"location":"python/packages/#virtual-environments","title":"Virtual environments","text":"<p>Previously, we have installed the package <code>seaborn</code>. The package itself was available system-wide as we did not create a virtual environment beforehand. That might not sound too bad, but it's actually considered bad practice. But what is good practice and what the heck is a virtual environment?</p> <p>To answer the latter, simply put, a virtual environment is a folder which encapsulates all packages for a specific project. Each project should have its own virtual environment. With a package manager like <code>pip</code>, you install the necessary packages into the project's virtual environment. <code>pip</code> lets you manage these packages/dependencies.</p>"},{"location":"python/packages/#why","title":"Why?","text":"<p>To summarize, the <code>pip</code>/virtual environment combination facilitates:</p> <ul> <li>Dependency management: You can keep track of the packages that your   project needs to function.</li> <li>Version management: You can specify the exact versions of a package that   your project needs. This is important, because different versions of a    package may have different functionalities or bugs.</li> <li>Environment management: It's easier to work on multiple projects on a   single machine as you can install multiple versions of a package on a   per-project basis.</li> <li>Shareable: Your projects will be shareable with other developers as they   can easily install all dependencies with a single command. No more it worked   on my machine excuses!</li> </ul> It works on my machine... byu/Shaheenthebean inProgrammerHumor"},{"location":"python/packages/#how","title":"How?","text":""},{"location":"python/packages/#create-a-virtual-environment","title":"Create a virtual environment","text":"<p>To create a virtual environment, open a new command prompt within VSCode (you can use the shortcut Ctrl + <code>\u00f6</code>).</p> <p>Execute the following command:</p> <pre><code>python -m venv .venv\n</code></pre> <p>This command creates a new folder structure. The folder is called <code>.venv</code>. Instead of <code>.venv</code> you can choose any name you want. However, this section assumes that you named it <code>.venv</code>.</p>"},{"location":"python/packages/#activate-a-environment","title":"Activate a environment","text":"<p>Now, we have to activate the environment in order to use it. Depending on your operating system, the command is slightly different.</p> Windows macOS/Linux / <p>As a Windows user type</p> <pre><code>.venv\\Scripts\\activate\n</code></pre> <p>If an error occurs (\"the execution of scripts is deactivated on this  system\") run</p> <pre><code>Set-ExecutionPolicy Unrestricted -Scope Process\n</code></pre> <p>... and use the previous command again.</p> <p>Type</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>to activate your environment.</p>"},{"location":"python/packages/#deactivate-a-environment","title":"Deactivate a environment","text":"<p>Deactivating the environment is the same on all operating systems. To deactivate it, simply use</p> <pre><code>deactivate\n</code></pre> <p>in your command prompt/terminal.</p> Fit a machine learning model <p>Assuming your virtual environment is activated, try to get the following code cell running. </p> <pre><code># Taken from https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#train-tree-classifier\n\n# pyplot is a submodule of matplotlib and can be directly imported with the `from` statement\nfrom matplotlib import pyplot \n\n# or you can import functions (like load_iris()) directly from its submodule (datasets)\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\niris = load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nclf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)\n\nplot_tree(clf)\npyplot.show()\n</code></pre> <p>Install the packages <code>matplotlib</code> and <code>scikit-learn</code> with <code>pip</code>. Then try to execute the code cell  (the code was taken from here).</p> <p>Congratulations \ud83c\udf89, you've just fitted a machine learning model (simple decision tree) on a data set and visualized the model. That's the power of <code>Python</code> - easily accessible packages with a lot of functionality ready to use. \ud83e\uddbe</p> <p>Don't worry too much about the actual code lines above. Again, the important thing is to get the code running. With the above exercise, you've  reproduced the result from the motivational section  Why Python?.</p>"},{"location":"python/packages/#requirementstxt","title":"<code>requirements.txt</code>","text":"<p>In the following exercise, you will learn how to export all your packages  (your project's dependencies) to a file. We will cover a simple command that  facilitates sharing your project/code with co-developers.</p> Export dependencies <p>Assume you want to share the code snippet from the previous task with  someone. First, your colleague might not know which packages you used to  get the code running. With no more information, one has to read the code  and manually determine which packages are necessary. To circumvent such situations, you export all your packages to a file.  Open a command prompt/terminal and execute</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>A <code>requirements.txt</code> is written which contains all your used  packages.</p> <p>Your colleague can now take the file and install all packages needed, at once.</p> <p><pre><code>pip install -r requirements.txt\n</code></pre> ... is the corresponding command.</p> Info <p>A <code>requirements.txt</code> file is a common way to share project dependencies. However, it will also help you, to restore your environment, in case  something goes wrong. Hence, keep your requirements file up-to-date.</p>"},{"location":"python/packages/#other-choicesoutlook","title":"Other choices?/Outlook","text":"<p>Apart from <code>pip</code> there are a couple of other package managers available. For example, there are</p> <ul> <li><code>uv</code></li> <li><code>pipenv</code></li> <li><code>poetry</code></li> <li><code>miniconda</code></li> </ul> <p>... and this is by no means an extensive list. All of these tools let you install and manage packages. Nevertheless, they have their differences. In the end, it is up to you, the developer which tool fits best. <code>pip</code> is always a solid choice (and the go-to choice to get the hang of package/virtual environment management). However, if you're working on larger scale projects with a couple of other developers, one of these package managers might offer some functionalities which facilitates the development workflow.</p>"},{"location":"python/packages/#recap","title":"Recap","text":"<p>In this section, you have learned how to install packages and manage them  within virtual environments. The topics covered:</p> <ul> <li><code>pip</code></li> <li>How to install/uninstall packages</li> <li>PyPI - the package hub</li> <li>Concept and benefits of virtual environments</li> <li>Creation and basic usage of a virtual environment</li> </ul>"},{"location":"python/pandas/","title":"Pandas","text":""},{"location":"python/pandas/#pandas","title":"<code>pandas</code>","text":""},{"location":"python/pandas/#introduction","title":"Introduction","text":"<p>As the last topic in our <code>Python</code>  Crash Course, we provide a brief introduction to <code>pandas</code> in order to handle data  sets. The package will be heavily used in the upcoming chapters (e.g., Statistics). Therefore, knowledge of the package is needed to properly follow the  chapters from now on.</p> Info <p>At the time of writing, <code>pandas</code> version <code>2.2.3</code> was used. Keep in mind, that <code>pandas</code> is actively developed and some functionalities might  change in the future. However, as always, we try to keep the content  up-to-date.</p> <p>This section is heavily based on the excellent 10 minutes to pandas  guide.</p>"},{"location":"python/pandas/#the-data-set","title":"The data set","text":"<p>We will use a custom Spotify data set, containing the current<sup>1</sup> top 50  songs in Austria. You can find the corresponding playlist here.</p> Info <p>If you're interested in the creation of the data set, you can find the code  below. Note, <code>pandas</code> was the only package needed, and we will cover some of  the used functionalities in this section.</p> Create Spotify data set <pre><code># Create a Spotify data set, containing the top 50 tracks in Austria\n\n# Original data (Top Spotify Songs in 73 Countries (Daily Updated)) from:\n# https://www.kaggle.com/datasets/asaniczka/top-spotify-songs-in-73-countries-daily-updated/data\nfrom pathlib import Path\n\nimport pandas as pd\n\n# read initial data set\ndata = pd.read_csv(Path(\"data/universal_top_spotify_songs.csv\"))\n\n# only Austrian chart topping songs\ndata = data[data[\"country\"] == \"AT\"]\n\n# subset by latest snapshot date\nlatest_snapshot = data[\"snapshot_date\"].max()\ndata = data[data[\"snapshot_date\"] == latest_snapshot]\n# sort by daily_rank\ndata = data.sort_values(by=\"daily_rank\").reset_index(drop=True)\n\n# write data to csv\ndata.to_csv(Path(\"data/spotify-top50.csv\"), index=False)\n\n# excerpt of the data set for inclusion in markdown\nwith Path(\"data/spotify-top50.md\").open(\"w\", encoding=\"UTF-8\") as f:\n    prefix = (\n        f\"Excerpt of the data set (snapshot date: **{latest_snapshot}**):\\n\\n\"\n    )\n    _data = data[\n        [\n            \"daily_rank\",\n            \"name\",\n            \"artists\",\n            \"popularity\",\n            \"is_explicit\",\n            \"energy\",\n        ]\n    ]\n    # only top 5 songs\n    _data = _data.head(5)\n    markdown = _data.to_markdown(index=False)\n    f.write(prefix + markdown)\n</code></pre> <p>Excerpt of the data set (snapshot date: 2024-09-25):</p> daily_rank name artists popularity is_explicit energy 1 The Emptiness Machine Linkin Park 93 True 0.872 2 Rote Flaggen Berq 76 True 0.336 3 Bauch Beine Po Shirin David 80 True 0.746 4 Die With A Smile Lady Gaga, Bruno Mars 100 False 0.592 5 BIRDS OF A FEATHER Billie Eilish 99 False 0.507 <p>Download the whole data set to follow this section:</p> <p>Spotify Top 50 Austria </p>"},{"location":"python/pandas/#prerequisites","title":"Prerequisites","text":"<p>For this section, we recommend, to make a new project folder with a  Jupyter notebook. Additionally, create a new virtual environment and  activate it. Please refer to the previous section on packages and virtual  environments if you're having trouble. Lastly, install <code>pandas</code>.</p> <p>You should end up with a project structure similar to the following:</p> <pre><code>\ud83d\udcc1 pandas-course/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc4 spotify-top50.csv\n\u2514\u2500\u2500 \ud83d\udcc4 pandas-course.ipynb\n</code></pre>"},{"location":"python/pandas/#tabular-data","title":"Tabular data","text":"<p>Before we dive into <code>pandas</code>, let's briefly discuss tabular data. At its simplest, tabular data consists of rows and columns. Looking at the Spotify  table above; each row contains information about a specific track  (e.g., <code>name</code>, <code>artists</code>), while each column represents a specific attribute  (e.g., <code>popularity</code>, <code>energy</code>).</p> <p>Tabular data has a clear structure which makes it easy to work with. On the other hand sources for tabular data can be manifold. However, one of the most  common format is the XLSX ( - Excel) or CSV  ( - Comma Separated Values) format which is the one we are working with in this chapter. Nevertheless, tabular data is  also present in various other text based formats like TXT, TSV or even in  databases (e.g. MySQL, PostgreSQL).</p> <p>No matter the source, <code>pandas</code> is the go-to tool to work with tabular data.</p>"},{"location":"python/pandas/#getting-started","title":"Getting started","text":"<p>Let's explore some of <code>pandas</code> functionalities on the example of the Spotify data set. First, we need to import the package.</p> <pre><code>import pandas as pd\n</code></pre> <p>The <code>as</code> statement is used to create an alias for the package in  order to quickly reference it within our next code snippets. An alias  simply reduces the amount of characters you have to type. Moreover, the  alias <code>pd</code> is commonly used for <code>pandas</code>. Therefore, you can more  easily  employ code snippets you find online.</p>"},{"location":"python/pandas/#reading-files","title":"Reading files","text":"<p>With the package imported, we can already read the data set (given as <code>.csv</code>).</p> <pre><code>data = pd.read_csv(\"spotify-top50.csv\")\n</code></pre> <p>The above code snippet assumes, that both data set and notebook are located at the same directory level. Else, you have to adjust the path  <code>\"spotify-top50.csv\"</code> accordingly.</p> <p>Besides <code>.csv</code> files, <code>pandas</code> supports reading from various other file types like Excel, text files or a SQL database. The <code>pandas</code> documentation  provides a comprehensive overview of different file types and their corresponding function. Have a look, to get an idea which file formats are supported not  only for reading but also for writing. </p>"},{"location":"python/pandas/#displaying-data","title":"Displaying data","text":"<p>With a data set at hand, we will most likely want to view it. To view the  first rows of our data frame use the <code>head()</code> method.</p> <pre><code>print(data.head())\n</code></pre> &gt;&gt;&gt; Output<pre><code>               spotify_id                   name  ...    tempo  time_signature\n0  2PnlsTsOTLE5jnBnNe2K0A  The Emptiness Machine  ...  184.115               4\n1  7bkUa9kDFGxgCC7d36dzFI           Rote Flaggen  ...  109.940               3\n2  64f3yNXsi2Vk76odcHCOnw         Bauch Beine Po  ...  123.969               4\n3  2plbrEY59IikOBgBGLjaoe       Die With A Smile  ...  157.969               3\n4  6dOtVTDdiauQNBQEDOtlAB     BIRDS OF A FEATHER  ...  104.978               4\n</code></pre> <p>To display the last rows of the data frame, use the <code>tail()</code> method.</p> <pre><code>print(data.tail())\n</code></pre> &gt;&gt;&gt; Output<pre><code>                spotify_id  ... time_signature\n45  6leQi7NakJQS1vHRtZsroe  ...              4\n46  5E4jBLx4P0UBji68bBThSw  ...              4\n47  6qzetQfgRVyAGEg8QhqzYD  ...              4\n48  3WOhcATHxK2SLNeP5W3v1v  ...              4\n49  7xLbQTeLpeqlxxTPLSiM20  ...              4\n</code></pre> <p>Columns can be viewed with:</p> <pre><code>print(data.columns)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Index(['spotify_id', 'name', 'artists', 'daily_rank', 'daily_movement',\n       'weekly_movement', 'country', 'snapshot_date', 'popularity',\n       'is_explicit', 'duration_ms', 'album_name', 'album_release_date',\n       'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'],\n      dtype='object')\n</code></pre> <p>Similarly, we can print the row indices.</p> <pre><code>print(data.index)\n</code></pre> <p>&gt;&gt;&gt; Output<pre><code>RangeIndex(start=0, stop=50, step=1)\n</code></pre> This data set is consecutively indexed from <code>0</code> to <code>49</code>. If  you recall, a range does not include its <code>stop</code> value (<code>50</code>).</p> <p>By default, (if not otherwise specified) <code>pandas</code> will assign a range index to a data set in order to label the rows.</p> <p>The data set dimensions are accessed with the <code>shape</code> attribute.</p> <pre><code>print(data.shape)\n</code></pre> &gt;&gt;&gt; Output<pre><code>(50, 25)\n</code></pre> <p>The data set has <code>50</code> rows and <code>25</code> columns.</p>"},{"location":"python/pandas/#data-structures","title":"Data structures","text":"<p><code>pandas</code> has two main data structures: <code>Series</code> and <code>DataFrame</code>. As you would expect, a <code>DataFrame</code> is a two-dimensional data structure such as  our whole Spotify data set assigned to the variable <code>data</code>:</p> <pre><code>print(type(data))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> <p>Whereas, a single column of a <code>DataFrame</code> is referred to as a <code>Series</code>.  Generally, selections of the <code>DataFrame</code> can be accessed with square  brackets (<code>[]</code>). To get a column, you can simply use its name.</p> <pre><code>print(data[\"artists\"])\n\nprint(type(data[\"artists\"]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>0                                       Linkin Park\n1                                              Berq\n2                                      Shirin David\n3                             Lady Gaga, Bruno Mars\n...                                             ...\n\n&lt;class 'pandas.core.series.Series'&gt;\n</code></pre> <p>A <code>DataFrame</code> is composed of at least one <code>Series</code>.</p>"},{"location":"python/pandas/#detour-series-and-dataframe-from-scratch","title":"Detour: <code>Series</code> and <code>DataFrame</code> from scratch","text":"<p>It's not always the case that you have a data set (in form of a file) at hand. Sometimes you have to create a <code>Series</code> or <code>DataFrame</code> yourself.</p> <p>A <code>Series</code> can be easily created from a list.</p> <pre><code>austrian_artists = [\"Bibiza\", \"Wanda\", \"Bilderbuch\"]\naustrian_artists = pd.Series(austrian_artists)\n</code></pre> <p>To initiate a <code>DataFrame</code>, you can use a dictionary (among others).</p> <pre><code>austrian_artists = {\n    \"name\": [\"Bibiza\", \"Wanda\", \"Bilderbuch\"],\n    \"album\": [\"bis einer weint\", \"Amore\", \"mea culpa\"],\n    \"release_year\": [2024, 2014, 2019]\n}\naustrian_artists = pd.DataFrame(austrian_artists)\n</code></pre> <p>Dictionary keys are used as column names and the corresponding values as the column values.</p> Info <p>Apart from a <code>dict</code>, a <code>DataFrame</code> can be created from multiple other data structures like a <code>list</code> or <code>tuple</code>. For an  extensive guide, visit the <code>pandas</code> documentation on Intro to data  structures (specifically the  section DataFrame). </p>"},{"location":"python/pandas/#selecting-data","title":"Selecting data","text":"<p>Let's dive deeper into selecting data. To access specific rows, you can use  a slice (just like with lists).</p> <pre><code># rows 5 and 6\nprint(data[5:7])\n</code></pre> &gt;&gt;&gt; Output<pre><code>               spotify_id        name  ...    tempo  time_signature\n5  0io16MKpbeDIdYzmGpQaES  Embrace It  ...  114.933               4\n6  3aJT51ya8amzpT3TKDVipL         FTW  ...   91.937               4\n</code></pre> <p>Select multiple columns by passing a list of column names.</p> <pre><code>print(data[[\"name\", \"artists\"]])\n</code></pre> &gt;&gt;&gt; Output<pre><code>                                  name    artists\n0                The Emptiness Machine    Linkin Park\n1                         Rote Flaggen    Berq\n2                       Bauch Beine Po    Shirin David\n...\n</code></pre>"},{"location":"python/pandas/#boolean-indexing","title":"Boolean indexing","text":"<p>Most of the time, we want to filter the data based on criterias.  For example, we can select the tracks with a tempo higher than <code>120</code> beats per minute (BPM).</p> <pre><code>high_tempo = data[data[\"tempo\"] &gt; 120]\n</code></pre> <p>Let's break the example down:</p> <ul> <li>First, we select the column <code>tempo</code> from the data set with <code>data[\"tempo\"]</code>.</li> <li>Next, we expand our expression to <code>data[\"tempo\"] &gt; 120</code>.   This will return a <code>Series</code> of boolean values.</li> <li>Lastly, we wrap the expression in another set of square brackets to    filter the whole data set based on our boolean values.</li> </ul> <p>We end up with 27 tracks that meet the criteria. <code>high_tempo</code> is a new  <code>DataFrame</code> containing entries that exceed <code>120</code> BPM.</p> Danceable tracks <p> Saturday Night Fever </p> <p>We assume that tracks with a danceability score higher than <code>0.8</code> are danceable. </p> <p>How many of the tracks are danceable?</p> <p>Danceability describes how suitable a track is for dancing based on a  combination of musical elements including tempo, rhythm stability, beat  strength, and overall regularity. A value of 0.0 is least danceable and  1.0 is most danceable.</p> <p>-- Spotify for Developers</p>"},{"location":"python/pandas/#mathematical-operations","title":"Mathematical operations","text":"<p><code>pandas</code> supports mathematical operations on both <code>Series</code> and <code>DataFrame</code>. For instance, we weigh the popularity of a track by its energy level.</p> <p>Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. </p> <p>-- Spotify for Developers</p> <pre><code>weighted_popularity = data[\"popularity\"].mul(data[\"energy\"])\nprint(type(weighted_popularity))\n\n# assign the Series to the DataFrame\ndata[\"weighted_popularity\"] = weighted_popularity\n</code></pre> <p>The <code>mul()</code> method is used to multiply the <code>popularity</code> and <code>energy</code> columns. The resulting <code>Series</code> is assigned to <code>data</code> as a new column.</p> &gt;&gt;&gt; Output<pre><code>&lt;class 'pandas.core.series.Series'&gt;\n</code></pre> Track length <p> </p> <p>Since songs are getting shorter and shorter, we want to know how long the tracks in our data set are.  To do so, calculate the length in minutes.</p> <ul> <li>Explore the given data set to find the appropriate column (which    holds information on the song length). </li> <li>Calculate the length in minutes (hint: use the <code>pandas</code> documentation    or Google.)</li> <li>Assign the result to the data frame.</li> <li>Use boolean indexing, to check if there are any tracks longer than    <code>4</code> minutes.</li> <li>Lastly, calculate the average track length in minutes.</li> </ul>"},{"location":"python/pandas/#basic-statistics","title":"Basic statistics","text":"<p><code>pandas</code> provides a variety of methods to calculate basic statistics. For  instance, <code>min()</code>, <code>max()</code>, <code>mean()</code> can be easily retrieved for a numeric  <code>Series</code> in the data set.</p> <pre><code>print(data[\"tempo\"].min())\n</code></pre> &gt;&gt;&gt; Output<pre><code>80.969\n</code></pre> <p>Conveniently, statistics can be calculated for each column at once using  the <code>DataFrame</code>. In this example, we calculate the standard deviation.</p> <pre><code>print(data.std())\n</code></pre> <p>If you execute the above snippet, a <code>TypeError</code> is raised.</p> Fix the error <p>Try to determine, why the error was raised in the first place. Now, circumvent/fix the error.</p> <p>Hint: Look at the documentation of the <code>std()</code> method and its  parameters.</p> <p>If you want to calculate multiple statistics, you can call the <code>describe()</code>  method.</p> <pre><code>stats = data.describe()\nprint(stats)\n</code></pre> &gt;&gt;&gt; Output<pre><code>       daily_rank  daily_movement  ...       tempo  time_signature\ncount    50.00000        50.00000  ...   50.000000       50.000000\nmean     25.50000         1.04000  ...  125.087260        3.920000\nstd      14.57738         8.14902  ...   26.751323        0.340468\nmin       1.00000       -22.00000  ...   80.969000        3.000000\n25%      13.25000        -3.00000  ...  104.990750        4.000000\n50%      25.50000         1.00000  ...  123.981500        4.000000\n75%      37.75000         3.00000  ...  137.487250        4.000000\nmax      50.00000        29.00000  ...  184.115000        5.000000\n</code></pre> <p><code>describe()</code> provides descriptive statistics for each column. The result of  <code>describe()</code> is a <code>DataFrame</code> itself.</p>"},{"location":"python/pandas/#other-functionalities","title":"Other functionalities","text":"<p><code>pandas</code> offers a plethora of functionalities. There's simply too much to  cover in a brief introductory section. Still, there are some common  <code>DataFrame</code> methods/properties that are worth mentioning:</p> <ul> <li><code>sort_values()</code>: Sort the data frame by a specific column.</li> <li><code>groupby()</code>: Group the data frame by a column.</li> <li><code>merge()</code>: Merge two data frames.</li> <li><code>T</code>: Transpose of the data frame.</li> <li><code>drop_duplicates()</code>: Remove duplicates.</li> <li><code>dropna()</code>: Remove missing values.</li> </ul> <p>All methods are linked to its corresponding documentation with examples  that help you get started.</p>"},{"location":"python/pandas/#recap","title":"Recap","text":"<p>We covered <code>pandas</code> and some selected functionalities which should provide  you with a solid foundation to work with tabular data sets. Moreover, you  should be able to follow the code portions in the upcoming courses more easily.</p> \ud83c\udf89 <p>Congratulations, you've reached the end of the Python  Crash Course!</p> <p>parserTongue byu/Ange1ofD4rkness inProgrammerHumor</p> <p>With the knowledge gained, you're now well-equipped to write your own  scripts. Additionally, you're able to automate tasks, write functions to  tackle complex problems and work with data sets. You can leverage the functionalities of Python  packages and using virtual environments, you can efficiently manage the packages required for your projects.</p> <p>Lastly, let's have a brief look at the upcoming chapter.</p>"},{"location":"python/pandas/#whats-next","title":"What's next?","text":"<p>The upcoming statistics chapter introduces foundational tools to further  analyse data. Among the topics are descriptive and inferential statistics which are previewed in this section. After completion, you  will be equipped with methods to explore, summarize and effectively visualize your data sets. </p> Maybe a not so effective chart, if the actual sales numbers are missing..."},{"location":"python/pandas/#example-descriptive-statistics","title":"Example: Descriptive statistics","text":"<p>One common tool within the descriptive realm is the box plot. For instance, using our Spotify data set, we can visualize the loudness (in dB) based on  whether a track contains explicit lyrics.</p> <p>The above plot suggests that songs with explicit lyrics  tend to be slightly louder than those without (looking at the median).  Apart from the more in-depth explanation of box plot, concepts are  introduced to further validate such claims.</p>"},{"location":"python/pandas/#example-inferential-statistics","title":"Example: Inferential statistics","text":"<p>To illustrate another example, the inferential statistics part contains a  section on linear regression which equips you with the necessary knowledge to  fit your first model. The following code snippet models the popularity of a track with the given features (such as danceability, loudness, tempo, liveness,  etc.).</p> <pre><code>import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndata = pd.read_csv(\"spotify-top50.csv\")\n\nX = data[\n    [\n        \"danceability\",\n        \"loudness\",\n        \"speechiness\",\n        \"acousticness\",\n        \"instrumentalness\",\n        \"liveness\",\n        \"valence\",\n        \"tempo\",\n    ]\n]\ny = data[\"popularity\"]\n\n# fit the model\nreg = LinearRegression().fit(X, y)\n</code></pre> <p>With the model at hand, we can predict the popularity of a track and compare it to the actual value.</p> <p>You will learn how to interpret these results and measure the goodness of  fit (i.e., if the popularity is accurately modelled) and apply it to your  own data sets.</p> <p>See you in the Statistics chapter! </p> <ol> <li> <p>The full data set is available on Kaggle and contains the most streamed songs for multiple different countries. For our purpose, the data was subset.\u00a0\u21a9</p> </li> </ol>"},{"location":"python/variables/","title":"Variables","text":"Info <p>The structure of following sections is based on the official Python tutorial.</p> <p>Many thanks to @jhumci for providing the initial resource materials!</p>"},{"location":"python/variables/#getting-started","title":"Getting started","text":"<p>    Important    </p> <p>We encourage you to execute all upcoming code snippets on your machine. You can easily copy each code snippet to your clipboard, by clicking the icon in the top right corner. By doing so, you will be prepared for all upcoming tasks within the sections. Tasks are indicated by a -icon.</p> <p>We recommend to create a new notebook for each chapter, e.g. create <code>variables.ipynb</code> for this chapter. Doing so, your notebooks will follow the structure of this crash course.</p> <p>You will encounter multiple info boxes   throughout the course. They provide additional information, tips, and tricks.  Be sure to read them thoroughly.</p> <p>Let's start with the first task.</p> Notebook &amp; first code cell <p>Create a new Jupyter notebook and name it <code>variables.ipynb</code>. Paste the following code snippet into a new cell and execute it.</p> <pre><code>print(\"Hello World!\")\n</code></pre> <p>The output should be:</p> &gt;&gt;&gt; Output<pre><code>Hello World!\n</code></pre> Info <p>The upcoming content contains a lot of code snippets. They are easily  recognizable due to their colourful syntax highlighting, such as:</p> <pre><code>print(1+1)\n</code></pre> <p>Code snippets are an integral part, to illustrate concepts, which are  introduced and explained along the way. Commonly, these code snippets are accompanied by an output block to display the result, for instance:</p> &gt;&gt;&gt; Output<pre><code>2\n</code></pre> <p>Nevertheless note, output blocks can be missing as there is not always  an explicit result.</p> <p>Again, execute and experiment with all code snippets on your machine to  verify the results and get familiar with <code>Python</code> !</p>"},{"location":"python/variables/#variable","title":"Variable","text":"<p>Computers can store lots of information. To do so, in <code>Python</code>  we use variables. A variable is a name that refers to a value. The following code snippet, assigns the value <code>4</code> to the variable <code>number</code>. In  general, you pick the variable name on the left hand side, assign a value  with <code>=</code> and the value itself is on the right hand side.</p> <pre><code>number = 4\n</code></pre> <p>You can change the value of a variable in your program at any time, and Python will always keep track of its current value.</p> <pre><code>number = 4\nnumber = 4000\n</code></pre> <p>You will notice that none of the cells had any output. To display the value of a variable we use the <code>print()</code> function.</p>"},{"location":"python/variables/#print","title":"<code>print()</code>","text":"<pre><code>number = 4\nprint(number)\n</code></pre> &gt;&gt;&gt; Output<pre><code>4\n</code></pre> <p>Now, we can also verify that in the above snippet the value of <code>number</code> was actually changed.</p> <pre><code>number = 4\nprint(number)\nnumber = 4000\nprint(number)\n</code></pre> &gt;&gt;&gt; Output<pre><code>4\n4000\n</code></pre> Info <p>Within a notebook, the variables are stored in the background and can be  overwritten at any time. Therefore, it is good practice to execute all  cells from top to bottom of the notebook in the right order so that  nothing unexpected is stored in a variable.</p>"},{"location":"python/variables/#comments","title":"Comments","text":"<p>Comments exist within your code but are not executed. They are used to describe your code and are ignored by the <code>Python</code> interpreter. Comments are prefaced by a <code>#</code>.</p> <pre><code># this is a comment\nprint(\"Hello World!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello World!\n</code></pre> Info <p>Comments help you and others understand what your code is doing. It is good practice to use comments as a tool for documentation.</p>"},{"location":"python/variables/#variable-naming","title":"Variable naming","text":"<p>When you\u2019re using variables in <code>Python</code>, you need to adhere to a few rules and guidelines. Breaking some of these rules will cause errors; other guidelines just help you write code that\u2019s easier to read and understand. Be sure to keep the following rules in mind:</p> <ul> <li>Variable names are lower case and can contain only letters, numbers, and   underscores.   They can start with a letter or an underscore, but not with a number.   For instance, you can call a variable <code>message_1</code> but not <code>1_message</code>.</li> <li>Whitespace is not allowed in variable names, but an underscores <code>_</code> can be   used to separate words in variable names. For example, <code>greeting_message</code>   works, but <code>greeting message</code> won't.</li> <li>Avoid using <code>Python</code> keywords and function names as variable names;   that is, do not use words that <code>Python</code> has reserved for a particular   programmatic purpose, such as the word <code>print</code>.</li> <li>Variable names should be short but descriptive. For example, <code>name</code> is better   than <code>n</code>, <code>student_name</code> is better than <code>s_n</code>, and <code>name_length</code> is better   than <code>length_of_persons_name</code>.</li> </ul>"},{"location":"python/variables/#errors-nameerror","title":"Errors (<code>NameError</code>)","text":"<p>Every programmer makes mistakes and even after years of experience, mistakes are part of the process. With time, you get more efficient in debugging  (=process of finding and fixing errors).</p> Debugging tactics byu/0ajs0jas inProgrammerHumor <p>Let\u2019s look at an error you\u2019re likely to make early on and learn how to fix it. We\u2019ll write some code that throws an error message on purpose. Copy the code and run your cell.</p> <pre><code>message = \"Hello Python Crash Course reader!\"\nprint(mesage)\n</code></pre> <p>Which should result in:</p> <pre><code>NameError: name 'mesage' is not defined\n</code></pre> <p>When an error occurs in your program, the <code>Python</code> interpreter does its best to help you figure out where the problem is. The interpreter provides a traceback which is a record of where the interpreter ran into trouble when trying to execute your code. Here\u2019s an example of the traceback that <code>Python</code> provides, after you\u2019ve accidentally misspelled a variable\u2019s name:</p> <pre><code>Traceback (most recent call last):\n  File \"C:\\\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-35-c8f2adeaed02&gt;\", line 2, in &lt;module&gt;\n    print(mesage)\n          ^^^^^^\nNameError: name 'mesage' is not defined. Did you mean: 'message'?\n</code></pre> <p>The output reports that an error occurs in line 2. The interpreter shows this line to help us spot the error quickly and tells us what kind of error it found. In this case, it found a <code>NameError</code> and reports that the variable <code>mesage</code> has not been defined. A name error usually means we made a spelling mistake when entering the variable\u2019s name or that the variable simply does not exist.</p> Your first fix <p>Fix the <code>NameError</code> in your code cell.</p>"},{"location":"python/variables/#recap","title":"Recap","text":"<p>In this section, we have covered variables in <code>Python</code> .</p> <p>You have learned (about):</p> <ul> <li>To create and assign a value to a variable</li> <li><code>print()</code> to display the value of a variable</li> <li>Comments</li> <li>Naming conventions for variables</li> <li>How to fix a <code>NameError</code></li> </ul>"},{"location":"python/containers/dict/","title":"Dictionaries","text":"<p>In this section, you\u2019ll learn how to use dictionaries which allow you to connect pieces of related information. Dictionaries let you model a variety of real-world objects more accurately. We will create, modify and  access elements of a dictionary.</p>"},{"location":"python/containers/dict/#creating-a-dictionary","title":"Creating a dictionary","text":"<pre><code>experiment = {\"description\": \"resource optimization\"}\nprint(type(experiment))\n</code></pre> <p>Above code snippet creates a simple dictionary and prints its type:</p> &gt;&gt;&gt; Output<pre><code>&lt;class 'dict'&gt;\n</code></pre> <p>In <code>Python</code>, a dictionary is wrapped in curly braces (<code>{}</code>), with a series  of key-value pairs inside the braces. Each key is connected to a value, and  you can use a key to access the value associated with that key.  A key\u2019s value can be any type, like a string, integer, list, or even  another dictionary. In the above example, the key is <code>\"description\"</code>  and its value <code>\"resource optimization\"</code></p> <p>Every key is connected to its value by a colon. Individual key-value pairs are separated by commas. You can store as many key-value pairs as you want in a dictionary.</p> <pre><code>experiment = {\n    \"description\": \"resource optimization\",\n    \"sample_weight_in_grams\": 5,\n}\n</code></pre>"},{"location":"python/containers/dict/#accessing-values","title":"Accessing values","text":"<p>To get the value associated with a key, give the name of the dictionary and then place the key inside a set of square brackets.</p> <pre><code>experiment = {\"sample_weight_in_grams\": 5}\nprint(experiment[\"sample_weight_in_grams\"])\n</code></pre> &gt;&gt;&gt; Output<pre><code>5\n</code></pre> Create a dictionary <p>Manage the cost of raw materials in a dictionary. The dictionary should  contain the following key-value pairs:</p> <ul> <li><code>\"steel\"</code>: <code>100</code></li> <li><code>\"aluminium\"</code>: <code>150</code></li> <li><code>\"copper\"</code>: <code>200</code></li> <li><code>\"plastic\"</code>: <code>50</code></li> </ul> <p>Create the dictionary and print the price of copper.</p>"},{"location":"python/containers/dict/#adding-key-value-pairs","title":"Adding key-value pairs","text":"<p>You can add new key-value pairs to a dictionary at any time. For example,  to add a new key-value pair, you would give the name of the dictionary followed by the new key in square brackets along with the new value.</p> <pre><code>experiment = {}\nexperiment[\"description\"] = \"resource optimization\"\nprint(experiment)\n</code></pre> <p>In the above example, we start with an empty dictionary and add a key-value pair to it.</p> &gt;&gt;&gt; Output<pre><code>{'description': 'resource optimization'}\n</code></pre> <p>However, we can't add the same key a second time to the dictionary. Every key is unique within the dictionary.</p>"},{"location":"python/containers/dict/#modifying-values","title":"Modifying values","text":"<p>Values can be overwritten:</p> <pre><code>experiment = {\"sample_weight_in_grams\": 10}\nexperiment[\"sample_weight_in_grams\"] = 10.2\n</code></pre>"},{"location":"python/containers/dict/#removing-key-value-pairs","title":"Removing key-value pairs","text":"<p>We can remove key-value-pairs using the key and the <code>del</code> statement:</p> <pre><code>experiment = {\n    \"supervisor\": \"Alex\",\n    \"sample_weight_in_grams\": 10,\n}\n\nprint(experiment)\n\ndel experiment[\"supervisor\"]\n\nprint(experiment)\n</code></pre> &gt;&gt;&gt; Output<pre><code>{'supervisor': 'Alex', 'sample_weight_in_grams': 10}\n{'sample_weight_in_grams': 10}\n</code></pre> Modify a dictionary <p>Remember that a value can hold any data type? You are given a dictionary with production data.</p> <pre><code>production = {\n    \"singapore\": {\"steel\": 100, \"aluminium\": 150},\n    \"taipeh\": {\"steel\": 200, \"aluminium\": 250},\n    \"linz\": {\"steel\": 300, \"aluminium\": 350, \"copper\": 100},\n}\n</code></pre> <p>Each key represents a location and has another dictionary as value. This dictionary contains the production quantity of different materials.</p> <ul> <li>Remove <code>linz</code> from the dictionary.</li> <li>Add a new location <code>vienna</code> with the production of 200 steel  and 250 aluminium.</li> <li>Print the <code>aluminium</code> value of <code>taipeh</code> (try accessing it step by step and use variables for each step).</li> </ul> Info <p>At first, the above example might seem a bit too overcomplicated. However,  nesting (in this example: storing a dictionary within a dictionary) is  common practice and as already discussed, lets you represent more  complex data structures. Even databases like Redis and MongoDB are at it's core key value stores, just like our dictionary above.</p>"},{"location":"python/containers/dict/#recap","title":"Recap","text":"<p>We have covered following topics in this section:</p> <ul> <li>Dictionaries store key-value pairs</li> <li>How to get and modify values</li> <li>Adding key-value pairs</li> <li>Removing key-value pairs with <code>del</code></li> </ul> <p>Lastly, as part of the <code>Containers</code> topic, we will have a look at tuples.</p>"},{"location":"python/containers/list/","title":"Lists","text":""},{"location":"python/containers/list/#introduction","title":"Introduction","text":"<p>In <code>Python</code>, a container is a type of data structure that holds and organizes multiple values or objects. Containers are used to store collections of elements, allowing you to group related data together for easier management and manipulation. <code>Python</code> provides several built-in container types, each with its own characteristics and use cases. In this first section, we cover  <code>list</code> objects, followed by dictionaries and tuples.</p>"},{"location":"python/containers/list/#what-is-a-list","title":"What is a <code>list</code>?","text":"<p>A <code>list</code> is a collection of items. You can make a <code>list</code>  including the letters of the alphabet or the digits from <code>0</code> to  <code>9</code>. You can put anything you want into a <code>list</code> and the items in your <code>list</code> don\u2019t have to be related in any particular way.</p> <p>Because a <code>list</code> usually contains more than one element, it\u2019s a good  idea to use the plural for a variable of type <code>list</code>, such as  <code>letters</code>, <code>digits</code>, or <code>names</code>.</p> <p>A <code>list</code> is created with an opening and closing square bracket  <code>[]</code>. Individual elements in the <code>list</code> are separated by commas. Here\u2019s a  simple example of a <code>list</code>:</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['John', 'Paul', 'George', 'Ringo']\n</code></pre> <pre><code>print(type([]))  # print the type of an empty list\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'list'&gt;\n</code></pre>"},{"location":"python/containers/list/#accessing-elements","title":"Accessing elements","text":"<p>You can access any element in a <code>list</code> by using the <code>index</code> of the desired item. To access an element in a <code>list</code>, write the name of the <code>list</code> followed by the index of the item enclosed in square brackets. For example, let\u2019s pull out  the first Beatle in <code>beatles</code>:</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles[0])\n</code></pre> &gt;&gt;&gt; Output<pre><code>John\n</code></pre>"},{"location":"python/containers/list/#indexerror","title":"<code>IndexError</code>","text":"Info <p>In Python, index positions start at 0, not 1. This is true for most  programming languages. If you\u2019re receiving unexpected results, determine  whether you are making a simple off-by-one error.</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles[4])\n</code></pre> <p>... results in</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;ipython-input-7-68dd8df4c868&gt;\", line 2, in &lt;module&gt;\n    print(beatles[4])\n          ~~~~~~~^^^\nIndexError: list index out of range\n</code></pre> <p>... since there is no official 5<sup>th</sup> Beatle. </p> <p>There is a special syntax for accessing the last element in a <code>list</code>. Use the index <code>-1</code> to access the last element, <code>-2</code> to access the second-to-last element, and so on.</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles[-1])  # Ringo\nprint(beatles[-2])  # George\n</code></pre> Indexing <p>Define a list that stores following programming languages:</p> <ul> <li>R</li> <li>Python</li> <li>Julia</li> <li>Java</li> <li>C++</li> </ul> <p>and use <code>print()</code> to output: <code>\"My favourite language is Python!\"</code></p>"},{"location":"python/containers/list/#list-manipulation","title":"List manipulation","text":"<p><code>Python</code> provides several ways to add or remove data to existing lists.</p>"},{"location":"python/containers/list/#adding-elements","title":"Adding elements","text":"<p>The simplest way to add a new element to a <code>list</code>, is to append it.  When you append an item to a <code>list</code>, the new element is added at  the end.</p> <pre><code>numbers = [1, 2, 3]\nprint(numbers)\nnumbers.append(4)\nprint(numbers)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1, 2, 3]\n[1, 2, 3, 4]\n</code></pre> <p>The <code>append()</code> method makes it easy to build lists dynamically. For example, you can start with an empty <code>list</code> and then add items by  repeatedly calling <code>append()</code>.</p> <pre><code>numbers = [1.0, 2.0, 0.5]\nnumbers.append(4.0)\nnumbers.append(3.0)\nnumbers.append(\"one hundred\")\n\nprint(numbers)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1.0, 2.0, 0.5, 4.0, 3.0, 'one hundred']\n</code></pre> <p>Up until now, our lists contained only one type of elements -  strings or integers. However, as in the example above, you can store  multiple different types of data in a <code>list</code>. Moreover, you can do  nesting (for example, you can store a <code>list</code> within a <code>list</code> - more on that later). Hence, lists can represent complex data structures. Nevertheless, don't mix and match every imaginable data type within a single  <code>list</code> (just because you can) as it makes the handling of your  <code>list</code> quite difficult.</p> Info <p>Later, we will learn how to perform the same task without repeatedly  calling the same <code>append()</code> method over and over.</p>"},{"location":"python/containers/list/#inserting-elements","title":"Inserting elements","text":"<p>You can add a new element at any position in your <code>list</code> by using the <code>insert()</code> method. You do this by specifying the index of the new element and the value of the new item.</p> <pre><code>pokemon = [\"Charmander\", \"Charizard\"]\npokemon.insert(1, \"Charmeleon\")\nprint(pokemon)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['Charmander', 'Charmeleon', 'Charizard']\n</code></pre>"},{"location":"python/containers/list/#removing-elements","title":"Removing elements","text":"<p>To remove an item from a <code>list</code>, you can use the <code>remove()</code>  method. You need to specify the value which you want to remove. However,  this will only remove the first occurrence of the item.</p> <pre><code>pokemon = [\"Charmander\", \"Squirtle\", \"Charmeleon\", \"Charizard\", \"Squirtle\"]\npokemon.remove(\"Squirtle\")\nprint(pokemon)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['Charmander', 'Charmeleon', 'Charizard', 'Squirtle']\n</code></pre>"},{"location":"python/containers/list/#popping-elements","title":"Popping elements","text":"<p>Sometimes you\u2019ll want to use the value of an item after you remove it from a <code>list</code>. The <code>pop()</code> method removes a specified element of a  <code>list</code>. Additionally, the item is returned so you can work with that  item after removing it. </p> <p>The term pop comes from thinking of a <code>list</code> as a stack of items and popping one item off the top of the stack. In this analogy, the top of a stack  corresponds to the end of a <code>list</code>.</p> <pre><code>pokemon = [\"Charmander\", \"Charmeleon\", \"Bulbasaur\", \"Charizard\"]\nbulbasaur = pokemon.pop(2)\nprint(pokemon)\nprint(bulbasaur)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['Charmander', 'Charmeleon', 'Charizard']\nBulbasaur\n</code></pre> List manipulation <p>Define a <code>list</code> with a couple of elements (of your choice). Play around with the methods <code>append()</code>, <code>insert()</code>,  <code>remove()</code> and <code>pop()</code>. Print the <code>list</code> after  each operation to see the changes.</p>"},{"location":"python/containers/list/#organizing-a-list","title":"Organizing a <code>list</code>","text":"<p>For various reasons, often, your lists will be unordered. If you want to  present your <code>list</code> in a particular order, you can use the method  <code>sort()</code>, or the function <code>sorted()</code>.</p>"},{"location":"python/containers/list/#sort","title":"<code>sort()</code>","text":"<p>The <code>sort()</code> method operates on the <code>list</code> itself and  changes its order.</p> <pre><code>numbers = [5, 4, 1, 3, 2]\nnumbers.sort()  # sort in ascending order\nprint(numbers)\n\nnumbers.sort(reverse=True)  # sort in descending order\nprint(numbers)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1, 2, 3, 4, 5]\n[5, 4, 3, 2, 1]\n</code></pre>"},{"location":"python/containers/list/#sorted","title":"<code>sorted()</code>","text":"<p>The <code>sorted()</code> function maintains the original order of a  <code>list</code> and returns a sorted <code>list</code> as well.</p> <pre><code>numbers = [5, 4, 1, 3, 2]\nsorted_numbers = sorted(numbers)\n\nprint(f\"Original list: {numbers}; Sorted list: {sorted_numbers}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Original list: [5, 4, 1, 3, 2]; Sorted list: [1, 2, 3, 4, 5]\n</code></pre>"},{"location":"python/containers/list/#length","title":"Length","text":"<p>You can easily find the length of a <code>list</code> with <code>len()</code>.</p> <pre><code>print(len([3.0, 1.23, 0.5]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>3\n</code></pre>"},{"location":"python/containers/list/#slicing","title":"Slicing","text":"<p>To make a slice (part of a <code>list</code>), you specify the index of the  first and last elements you want to work with. Elements up until the second index are  included. To output the first three elements in a <code>list</code>, you would request indices 0 through 3, which would return elements 0, 1, and 2.</p> <pre><code>players = [\"charles\", \"martina\", \"michael\", \"florence\", \"eli\"]\nprint(players[0:3])\n</code></pre> &gt;&gt;&gt; Output<pre><code>['charles', 'martina', 'michael']\n</code></pre> Slicing <p>Define a <code>list</code> of your choice with at least <code>5</code>  elements. </p> <ul> <li>Now, perform a slice from the second up to and including the fourth  element.</li> <li>Next, omit the first index in the slice (only omit the number!). What  happens?</li> <li>Lastly, re-add the first index and omit the second index of your  slice. Print the result.</li> </ul>"},{"location":"python/containers/list/#copy","title":"Copy","text":"<p>To copy a <code>list</code>, you can use the <code>copy()</code> method.</p> <pre><code>original_list = [1, 2, 3]\ncopied_list = original_list.copy()\n\n# perform some changes to both lists\noriginal_list.append(4)\ncopied_list.insert(0, \"zero\")\n\nprint(f\"Original list: {original_list}, Copied list: {copied_list}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Original list: [1, 2, 3, 4], Copied list: ['zero', 1, 2, 3]\n</code></pre>"},{"location":"python/containers/list/#be-careful","title":"Be careful!","text":"<p>You might wonder why we can't simply do something along the lines of  <code>copied_list = original_list</code>. With lists, we have to be careful,  as this syntax simply creates a reference to the original <code>list</code>. Let's look at an example:</p> <pre><code>original_list = [1, 2, 3]\ncopied_list = original_list\n\n# perform some changes to the original list\noriginal_list.append(4)\n\nprint(f\"Original list: {original_list}, Copied list: {copied_list}\")\n</code></pre> <p>which leaves us with: &gt;&gt;&gt; Output<pre><code>Original list: [1, 2, 3, 4], Copied list: [1, 2, 3, 4]\n</code></pre></p> <p>As you can see, the changes to the original <code>list</code> are reflected in  the copied one. You can read about this in more detail  here.</p> Note <p>We can actually check whether both lists point to the same object in memory by using <code>id()</code> which returns the memory address of an object.  Just remember, to be careful when copying lists and check if your program  behaves as intended!</p> <pre><code>original_list = [1, 2, 3]\ncopied_list = original_list\n\nprint(id(original_list) == id(copied_list))  # True\n</code></pre> Unknown method <p>Use the given <code>list</code> (don't worry about the syntax, it's just a  short expression to create a huge <code>list</code>):</p> <pre><code>long_list = [True] * 1000\n</code></pre> <ul> <li>Check the length of the <code>list</code>.</li> <li>Apply a method that deletes all elements in the <code>list</code> and  returns an empty list <code>[]</code>. You might need to use Google,  since it is a method not previously discussed.</li> <li>Check the length of the <code>list</code> again.</li> </ul>"},{"location":"python/containers/list/#recap","title":"Recap","text":"<p>We extensively covered lists and their manipulation.</p> <ul> <li>Accessing elements with indices (including slicing)</li> <li>The <code>IndexError</code></li> <li>Adding elements with <code>append()</code> and <code>insert()</code></li> <li>Removing elements with <code>remove()</code> and <code>pop()</code></li> <li>Sorting with <code>sort()</code> and <code>sorted()</code></li> <li>Length of a <code>list</code> with <code>len()</code></li> <li>Make a copy with <code>copy()</code></li> </ul>"},{"location":"python/containers/tuple/","title":"Tuples","text":"<p>Lists and dictionaries work well for storing and manipulating data during the execution of a program. Both lists and dictionaries are mutable. However, sometimes you\u2019ll want to create a collection of elements that are immutable (can't change). Tuples allow you to do just that.</p> <pre><code># coordinates of MCI IV\ncoordinates = (47.262996862335854, 11.393082185178823)\nprint(type(coordinates))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'tuple'&gt;\n</code></pre> <p>A <code>tuple</code> is created with round brackets (<code>()</code>). As with lists and dictionaries, the elements are separated by commas. Tuples can hold any type of data.</p>"},{"location":"python/containers/tuple/#accessing-elements","title":"Accessing elements","text":"<p>With indexing, the individual elements of a <code>tuple</code> can be retrieved.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\nprint(coordinates[0])\nprint(coordinates[1])\n</code></pre> &gt;&gt;&gt; Output<pre><code>47.262996862335854\n11.39308218517882\n</code></pre>"},{"location":"python/containers/tuple/#immutability","title":"Immutability","text":"<p>Let's try to change the value of an element in a <code>tuple</code>.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\ncoordinates[0] = 50.102\n</code></pre> <p>we will encounter following error:</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;ipython-input-29-d74dc80ea879&gt;\", line 2, in &lt;module&gt;\n    coordinates[0] = 50.102\n    ~~~~~~~~~~~^^^\nTypeError: 'tuple' object does not support item assignment\n</code></pre> <p>As a <code>tuple</code> is immutable, you can only redefine the entire  <code>tuple</code>.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\n# redefine the entire tuple\ncoordinates = (5.513615392318705, 95.2060492604128)\n</code></pre>"},{"location":"python/containers/tuple/#tuple-unpacking","title":"<code>tuple</code> unpacking","text":"<p>Tuples can be unpacked, to use them separately.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\nlatitude, longitude = coordinates\n</code></pre> Info <p>Tuples are often used for constants. In the above examples, we used  coordinates. As these coordinates are not going to change, a  <code>tuple</code> is a fitting data type.</p> Tuple unpacking <p>Use the following <code>tuple</code> with cities. <pre><code>cities = (\"New York\", \"Los Angeles\", \"Chicago\")\n</code></pre></p> <ul> <li>Print the first city.</li> <li>Use <code>tuple</code> unpacking and print the resulting variables.</li> </ul>"},{"location":"python/containers/tuple/#recap","title":"Recap","text":"<p>In this rather short section, we introduced tuples and covered:</p> <ul> <li>Mutability vs. immutability</li> <li>How to define a <code>tuple</code></li> <li>Access elements with indexing</li> <li>... and <code>tuple</code> unpacking</li> </ul>"},{"location":"python/control-structures/for/","title":"Loops - <code>for</code>","text":""},{"location":"python/control-structures/for/#introduction","title":"Introduction","text":"<p>In this section, you\u2019ll learn how to loop through elements using just a few lines of code. Looping allows you to take the same action, or set of actions with every item in an iterable. Among iterables are for example, lists or dictionaries. As a result, you'll be able to streamline tedious tasks. First, we'll loop over lists.</p>"},{"location":"python/control-structures/for/#looping-over-lists","title":"Looping over lists","text":"<p>You\u2019ll often want to run through all entries in a <code>list</code>, performing the same task with each item. In the below example, we loop through a <code>list</code> of passwords and print the length of each one.</p> <pre><code>passwords = [\"1234\", \"password\", \"admin\", \"123456\"]\nfor password in passwords:\n    print(f\"Password: {password} is {len(password)} characters long.\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Password: 1234 is 4 characters long.\nPassword: password is 8 characters long.\nPassword: admin is 5 characters long.\nPassword: 123456 is 6 characters long.\n</code></pre> <p>A loop is written with the <code>for</code> statement. The <code>password</code> is a temporary variable that holds the current item in the <code>list</code>. You can  choose any name you want for the temporary variable that holds each value.  However, it\u2019s helpful to choose a meaningful name that represents a single item from the <code>list</code>. For example:</p> <pre><code>for experiment in experiments:\n    ...\nfor user in users:\n    ...\n</code></pre> Info <p>When you\u2019re using loops for the first time, keep in mind that the set  of steps is repeated once for each item in the <code>list</code>, no matter  how many items are in the <code>list</code>. If you have a million items  in your <code>list</code>, <code>Python</code> repeats these steps a million times.</p>"},{"location":"python/control-structures/for/#scope","title":"Scope","text":"<p>Python uses indentation (whitespace) to indicate, what is part of the loop.  With an indentation being four characters of whitespace. For a faster way to  intend, use the tab key Tab.</p> <p>Let's extend the example from above:</p> <pre><code>passwords = [\"1234\", \"password\", \"admin\", \"123456\"]\nfor password in passwords:\n    print(f\"Password: {password} is {len(password)} characters long.\")\n\nprint(\"All passwords have been checked.\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Password: 1234 is 4 characters long.\nPassword: password is 8 characters long.\nPassword: admin is 5 characters long.\nPassword: 123456 is 6 characters long.\nAll passwords have been checked.\n</code></pre> <p>You can easily see that only the first <code>print</code> statement is part of  the loop, simply because it is indented. The second <code>print</code> statement is executed after the loop has finished as it is outside the loop.</p>"},{"location":"python/control-structures/for/#indentationerror","title":"<code>IndentationError</code>","text":"<p>In longer programs, you\u2019ll notice blocks of code indented at a few different levels. These indentation levels help you gain a general sense of the overall program\u2019s organization.</p> <p>As you begin to write code that relies on proper indentation, you\u2019ll need to watch for a few common indentation errors.</p>"},{"location":"python/control-structures/for/#expected-indentation","title":"Expected indentation","text":"<pre><code>for number in [1, 2, 3]:\nprint(number)\n</code></pre> <pre><code>  Cell In[4], line 2\n    print(number)\n    ^\nIndentationError: expected an indented block after 'for' statement on line 1\n</code></pre> <p>As the <code>IndentationError</code> states, <code>Python</code> expects an indented  block of code after the <code>for</code> statement.</p>"},{"location":"python/control-structures/for/#unexpected-indentation","title":"Unexpected indentation","text":"<pre><code>message = \"Hello\"\n    print(message)\n</code></pre> <pre><code>  Cell In[9], line 2\n    print(message)\n    ^\nIndentationError: unexpected indent\n</code></pre> <p>In this case, the code snippet contains an unnecessary indentation.</p> Square numbers <p>Square each number in a given list and print the result. First, initialize a list of numbers from 1 to 10. Square each number and <code>print</code> it. Use a <code>for</code> loop.</p>"},{"location":"python/control-structures/for/#range","title":"<code>range()</code>","text":"<p>The <code>range()</code> function makes it easy to generate a series of numbers.  For example, you can use <code>range()</code> to print a series of numbers like this:</p> <pre><code>for value in range(3):\n  print(value)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0\n1\n2\n</code></pre> <p>Remember, that <code>Python</code> 'starts counting at <code>0</code>'. <code>3</code> is not  included in the output, as <code>range()</code> generates a sequence up to, but not including, the number you provide. You can also pass two arguments to <code>range()</code>, the first and the last number of the sequence. In this case, the sequence will start at the first number and end at the last number minus one.</p> <pre><code>for value in range(3, 6):\n    print(value)\n</code></pre> &gt;&gt;&gt; Output<pre><code>3\n4\n5\n</code></pre> <code>range()</code> <p>Use <code>range()</code> to build a <code>list</code> which holds the numbers from 15 to 20 - including 20.</p> Savings account growth <p>Write a <code>for</code> loop to calculate the growth of savings over a  period of time. Use following formula to calculate the future value of  savings in year \\(t\\):</p> \\[ \\text{A} = \\text{P} \\times \\left(1 + \\frac{\\text{r}}{100} \\right)^{\\text{t}} \\] <p>where:</p> <ul> <li>\\(\\text{A}\\) is the future value of the savings account or investment.</li> <li>\\(\\text{P}\\) is the present value of the savings account or investment.</li> <li>\\(\\text{r}\\) is the annual interest rate.</li> <li>\\(\\text{t}\\) is the number of years the money is invested for.</li> </ul> <p>Given values:</p> <ul> <li>\\(\\text{P} = 1000\\)</li> <li>\\(\\text{r} = 5\\)</li> </ul> <p>Print the future value of the savings account over a period of 10 years.  Skip each second year. Use  Python's documentation on range() as a starting point.</p>"},{"location":"python/control-structures/for/#detour-simple-statistics-on-lists-with-numbers","title":"Detour: Simple statistics on lists with numbers","text":"<p>A few functions are specific to lists of numbers. For example, you can easily find the minimum, maximum, and sum of a list of numbers:</p> <pre><code>numbers = [1.0, 8.38, 3.14, 7.0, 2.71]\nprint(\n    f\"Minimum: {min(numbers)}\",\n    f\"Maximum: {max(numbers)}\",\n    f\"Sum: {sum(numbers)}\", sep=\"\\n\"\n)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Minimum: 1.0\nMaximum: 8.38\nSum: 22.23\n</code></pre> Calculate the average <p>Calculate the average of the following list: <pre><code>numbers = [4.52, 3.14, 2.71, 1.0, 8.38]\n</code></pre></p>"},{"location":"python/control-structures/for/#list-comprehensions","title":"List comprehensions","text":"<p>... are a concise way to create lists.</p> <p>A list comprehension combines a <code>for</code> loop to create a new list in a single line.</p> Rewrite a list comprehension <p>Rewrite the following list comprehension in a regular for-loop to  achieve the same result:</p> <pre><code>squares = [value**2 for value in range(1,11)]\nprint(squares)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n</code></pre>"},{"location":"python/control-structures/for/#looping-over-dictionaries","title":"Looping over dictionaries","text":"<p>As previously discussed, you can not only loop over a <code>list</code>, but  also iterate over a variety of different data types, such as dictionaries. You can loop over a dictionary\u2019s key-value pairs, solely over the keys  or just the values.</p>"},{"location":"python/control-structures/for/#items","title":"<code>items()</code>","text":"<p>Using the <code>.items()</code> method, we can loop over the key-value pairs. Take note, that the method returns two values, which we store in two separate variables (<code>key</code> and <code>value</code>).</p> <p>We can freely choose the variable names in the <code>for</code>-loop. It does  not have to be <code>key</code> and <code>value</code> respectively.</p> <pre><code>parts = {\n    \"P100\": \"Bolt\",\n    \"P200\": \"Screw\",\n    \"P300\": \"Hinge\",\n}\n\nfor key, value in parts.items():\n    print(key, value)\n</code></pre> &gt;&gt;&gt; Output<pre><code>P100 Bolt\nP200 Screw\nP300 Hinge\n</code></pre>"},{"location":"python/control-structures/for/#values-keys","title":"<code>values()</code>, <code>keys()</code>","text":"Dictionary methods <p>Define a (non-empty) dictionary of your choice and use both methods <code>.values()</code> and <code>.keys()</code> to access solely values and keys respectively.</p>"},{"location":"python/control-structures/for/#recap","title":"Recap","text":"<p>With the introduction of the <code>for</code> loop, you can now start to  automate re-occurring tasks. We have covered:</p> <ul> <li>Looping over lists</li> <li>Indentation and possible resulting <code>IndentationError</code></li> <li><code>range()</code> to generate a series of numbers</li> <li>Simple statistics on lists of numbers</li> <li>List comprehensions</li> <li>Specific methods to loop over dictionaries</li> </ul>"},{"location":"python/control-structures/if/","title":"More control structures","text":""},{"location":"python/control-structures/if/#introduction","title":"Introduction","text":"<p>In this section, we will cover additional control structures. First, we  discuss the <code>if</code> statement, which allows us to execute code based on a  condition. Followed by the <code>elif</code>, <code>else</code> and  <code>while</code> statements.</p>"},{"location":"python/control-structures/if/#if","title":"<code>if</code>","text":"<p>The <code>if</code> statement lets you evaluate conditions. The simplest kind of  <code>if</code> statement has one condition and one action. Here is some  pseudocode:</p> <pre><code>if condition is True:\n    do something\n</code></pre> <p>You can put any condition in the first line and just about any action in the  indented block following the test. If the condition evaluates to <code>True</code>, <code>Python</code> executes the indented code following the <code>if</code> statement.  If the test evaluates to <code>False</code>, the indented code block (following the  <code>if</code>) is ignored.</p> <pre><code>user = \"admin\"\n\nif user == \"admin\":\n  print(f\"Welcome {user}!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Welcome admin!\n</code></pre> <p>First, the condition <code>user == \"admin\"</code> is evaluated. If it  evaluates to <code>True</code>, the indented print is executed. If the condition evaluates to <code>False</code>, the indented code block is ignored.</p> <p>Indentation plays the same role in <code>if</code> statements as it did in  <code>for</code> loops (see the previous section).</p> Password strength: Part 1 <p> </p> <p>In the section on comparisons and logical operators, you had to check whether a password meets certain criterias. The following example expands on this task as you are  given a list of passwords. You have to check if each password exceeds a length of 12 characters.</p> <p>Execute the first code cell to generate some random passwords  (note every time you rerun the code snippet, different passwords will be  generated).</p> <pre><code># generate passwords - simply execute the code to generate some random\n# passwords\nimport random\nimport string\n\npasswords = []\nfor i in range(10):\n    length = random.randint(3, 25)\n    password = \"\".join(random.choices(string.ascii_letters, k=length))\n    passwords.append(password)\n</code></pre> <p>The <code>list</code> <code>passwords</code> should look something like this:</p> <pre><code>['PWgOYxQgnxgXm',\n 'gpOMVTmCSjAcndowkUd',\n 'ADKIEthzsGBr',\n 'VRLzOIZtEz',\n 'uOckmTJjeonUyMlnG',\n 'gjOpWuHrIbG',\n 'doxIylbRkNLdvdLNgVgYsDGzd',\n 'KvUdsgZhPIrS',\n 'LrdpffEqlBVQYr',\n 'ncyqXNLnVstVxlx']\n</code></pre> <p>Now, loop over the passwords and check if each password exceeds the  character limit of 12. If so, print the password.</p>"},{"location":"python/control-structures/if/#else","title":"<code>else</code>","text":"<p>Previously, every time the condition in the <code>if</code> statement  evaluated to <code>False</code>,  no action was taken. Hence, the <code>else</code> clause is introduced which  allows you to define a set of actions that are executed when the conditional test fails.</p> <pre><code>user = \"random_user\"\n\nif user == \"admin\":\n  print(f\"Welcome {user}!\")\nelse:\n  print(\"Only admins can enter this area!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Only admins can enter this area!\n</code></pre> Password strength: Part 2 <p>Let's expand on our previous example. Re-use your code to check the length of the generated passwords. Now, we would like to store all passwords that did not meet our criteria in the empty list <code>invalid_passwords</code>.</p> <p>Hint: Introduce an <code>else</code> statement to save the invalid passwords.</p>"},{"location":"python/control-structures/if/#elif","title":"<code>elif</code>","text":"<p>Often, you\u2019ll need to test more than two possible situations, and to evaluate these, you can use an <code>if-elif-else</code> syntax. <code>Python</code> executes only one block in an <code>if-elif-else</code> chain. It runs each conditional test in order until one passes. When a test passes, the code following that test is executed and the rest is skipped.</p> <pre><code>user = \"xX_user_Xx\"\nregistered_users = [\n    \"admin\",\n    \"guest\",\n    \"SwiftShark22\",\n    \"FierceFalcon66\",\n    \"BraveWolf11\"\n]\n\nif user == \"admin\":\n  print(f\"Welcome {user}!\")\nelif user not in registered_users:\n  print(\"Please create an account first!\")\nelse:\n  print(\"Only admins can enter this area!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Please create an account first!\n</code></pre> Info <p>As you might have noticed, you can use a single <code>if</code> statement or <code>if</code> in combination with <code>else</code>. For multiple conditions  you can add as many <code>elif</code> parts as you wish.</p>"},{"location":"python/control-structures/if/#while","title":"<code>while</code>","text":"<p>The <code>for</code> loop takes an iterable and executes a block of  code once for each element. In contrast, the <code>while</code> loop runs as  long as a certain condition is <code>True</code>.</p> <p>For instance, you can use a <code>while</code> loop to count up through a  series of numbers. Here is an example:</p> <pre><code># set a counter\ncurrent_number = 1\n\nwhile current_number &lt;= 5:\n  print(current_number)\n  # increment the counter value by one\n  current_number += 1\n</code></pre> &gt;&gt;&gt; Output<pre><code>1\n2\n3\n4\n5\n</code></pre> <p>Note, that the variable, that is checked in the <code>while</code>-condition  must be defined prior to the loop, otherwise we will encounter a  <code>NameError</code>.</p> Infinite loops <p>Moreover, the variable must be updated within the loop to avoid an infinite loop. For example, if <code>current_number</code> is not incremented by one, the condition <code>current_number &lt;= 5</code> will always evaluate to <code>True</code>, leaving us stuck in an infinite loop. In such cases, simply click the <code>Stop</code> button (on the left-hand side of the  respective code cell) to interrupt the execution.</p> Addition assignment <p>In the above example, we used the <code>+=</code> operator, referred to as  addition assignment. It is a shorthand for incrementing a variable by a  certain value.</p> <pre><code>a = 10\na += 5\nprint(a)\n</code></pre> &gt;&gt;&gt; Output<pre><code>15\n</code></pre> <p>The above code is equivalent to <code>a = a + 5</code>. This shorthand assignment can be used with all arithmetic operators, such as subtraction <code>-=</code> or division <code>/=</code>.</p> While loop <p>Write some code, to print all even numbers up to 42 using a while  loop.</p>"},{"location":"python/control-structures/if/#detour-user-input","title":"Detour: User input","text":"<p>Most programs are written to solve an end user\u2019s problem. To do so, usually  we need to get some information from the user. For a simple example, let\u2019s  say someone wants to enter a username.</p> <p>You can store the user input in the variable <code>user_name</code> like in the example  below.</p> <pre><code>user_name = input(\"Please enter your username:\")\n</code></pre> <p>However, the <code>input()</code> function always returns a string.</p> <pre><code>age = input(\"Please enter your age:\")\nprint(type(age))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'str'&gt;\n</code></pre> <p>... use casting to  convert the input to the desired type.</p>"},{"location":"python/control-structures/if/#break","title":"<code>break</code>","text":"<p>To exit any loop immediately without running any remaining 'loop code', use  the <code>break</code> statement. The <code>break</code> statement directs the flow of your program; you can use it to control which lines of code are executed and  which aren\u2019t, so the program only executes code that you want it to, when you want it to.</p> <pre><code>for i in range(5):\n    if i == 3:\n        break\n    print(i)\n\nprint(\"Continue running the program...\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>0\n1\n2\nContinue running the program...\n</code></pre>"},{"location":"python/control-structures/if/#continue","title":"<code>continue</code>","text":"<p>Rather than breaking out of a loop entirely, you can use the <code>continue</code>  statement to return to the beginning of the loop based on the result of a  condition.</p> <pre><code>for i in range(5):\n    if i == 3:\n        continue\n    print(i)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0\n1\n2\n4\n</code></pre>"},{"location":"python/control-structures/if/#recap","title":"Recap","text":"<p>In this section we have expanded on control structures. We discussed:</p> <ul> <li><code>if</code> statements and how to use them</li> <li><code>else</code> clauses</li> <li><code>elif</code> statements for multiple conditions</li> <li><code>while</code> loops</li> <li><code>break</code> and <code>continue</code> statements for more 'fine-grained'   control</li> </ul>"},{"location":"python/types/bool_and_none/","title":"Boolean &amp; None","text":"<p>In this section, we introduce two more data types, namely boolean (<code>bool</code>) and None (<code>NoneType</code>). Let's start with the latter one,  <code>NoneType</code>.</p>"},{"location":"python/types/bool_and_none/#none-nonetype","title":"None (<code>NoneType</code>)","text":"<p><code>NoneType</code> is a special data type in Python that represents the absence of a value.</p> <pre><code>nothing = None\nprint(type(nothing))\n</code></pre> <p>... which outputs:</p> &gt;&gt;&gt; Output<pre><code>&lt;class 'NoneType'&gt;\n</code></pre> <p><code>None</code> can be used as a placeholder for a variable which will be assigned a value later on. Furthermore, if a program was not able to return  a value, <code>None</code> can be used as a return value. </p> <p>Later, <code>None</code> will play a bigger role. For now, we simply keep in mind that <code>None</code> is a thing.</p>"},{"location":"python/types/bool_and_none/#detour-typeerror","title":"Detour: <code>TypeError</code>","text":"... yet another error. <p>Often, you\u2019ll want to use a variable\u2019s value within a message. For example, say you want to wish someone a happy birthday. You might write code like this:</p> <pre><code>age = 23\nmessage = \"Happy \" + age + \"rd Birthday!\"\nprint(message)\n</code></pre> <p>... results in.</p> <pre><code>Traceback (most recent call last):\n  File \"C:\\\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-34-80a141e301d6&gt;\", line 2, in &lt;module&gt;\n    message = \"Happy \" + age + \"rd Birthday!\"\n              ~~~~~~~~~^~~~~\nTypeError: can only concatenate str (not \"int\") to str\n</code></pre> <p>You might expect this code to print the simple birthday greeting, <code>Happy 23rd birthday!</code>. But if you run this code, you\u2019ll see that it generates an  error.</p> <p>This is a <code>TypeError</code>. It means Python encounters an unexpected  type in <code>age</code>, as strings were mixed with integers in the expression. We will  easily fix the <code>TypeError</code> in the next section.</p>"},{"location":"python/types/bool_and_none/#casting","title":"Casting","text":"<p>When you use integers within strings like this, you need to specify explicitly  that you want Python to use the integer as a string of characters.  You can do this by wrapping the variable in the <code>str()</code> function, which tells <code>Python</code> to represent non-string values as strings:</p> <pre><code>age = 23\nmessage = \"Happy \" + str(age) + \"rd Birthday!\"\nprint(message)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Happy 23rd Birthday!\n</code></pre> <p>Changing the type of <code>age</code> to string is called casting.</p> Info <p>A <code>TypeError</code> can stem from various 'sources'. This is just one  common example.</p> <p>In the following example the integers <code>3</code> and <code>2</code> were  implicitly cast to floating point numbers, to calculate the result,  which is a floating point number.</p> <pre><code>print(type(3 / 2))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'float'&gt;\n</code></pre> <p>With the function <code>int()</code> any value can be explicitly cast into an  integer, if possible. The value to be cast is passed as the input parameter.</p> <pre><code>number = 3.0\nprint(type(number))\n\n# casting\nnumber = int(number)\nprint(type(number))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'float'&gt;\n&lt;class 'int'&gt;\n</code></pre> <p>To explicitly cast a value into a float, use the function <code>float()</code>.</p> Casting <p>For each of the given variables, check whether you can cast them to another type. First, print the type of each variable. Then, use <code>int()</code>, and <code>float()</code> to cast the variables.</p> <pre><code># variables\nfirst = \"12\"\nsecond = \"1.2\"\nthird = 12\n</code></pre> Note <p>Remember the f-string (<code>f\"...\"</code>) from the previous section? Try a slightly modified example from above.</p> <pre><code>age = 23\nmessage = f\"Happy {age}rd Birthday!\"\nprint(message)\n</code></pre> <p>You'll notice, that there's no need for any explicit casting of <code>age</code>.</p> <p>Whenver, you want to include a variable in a string, remember that  f-strings might be more convenient. \ud83d\ude09</p>"},{"location":"python/types/bool_and_none/#booleans","title":"Booleans","text":"<p>Computers work with binary (e.g. <code>True</code> or <code>False</code>). Such information can be stored in a single bit. A boolean is either <code>True</code> or <code>False</code>. Booleans are often used to keep  track of certain conditions, such as whether a game is running or whether a  user can edit certain content on a website:</p> <pre><code>game_active = True\ncan_edit = False\n\nprint(type(True))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'bool'&gt;\n</code></pre>"},{"location":"python/types/bool_and_none/#recap","title":"Recap","text":"<p>In this section, we have introduced two more data types in Python:</p> <ul> <li>None (<code>NoneType</code>)</li> <li>and Booleans (<code>bool</code>)</li> </ul> <p>Now, we have covered all basic types! \ud83c\udf89 With that knowledge, we can already  start to do comparisons and logical operations.</p>"},{"location":"python/types/numbers/","title":"Integers &amp; Floats","text":""},{"location":"python/types/numbers/#integers","title":"Integers","text":"<p><code>Python</code>  treats numbers in several different ways, depending on how they are used. Let\u2019s first look at how <code>Python</code> manages whole  numbers called integers (<code>int</code>).</p> <p>Any number without decimal places is automatically interpreted as an integer.</p> <pre><code>number = 10176\n</code></pre> <p>We can verify the type of the variable <code>number</code> with <code>type()</code>.</p> <pre><code>number = 10176\nprint(type(number))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'int'&gt;\n</code></pre>"},{"location":"python/types/numbers/#arithmetic-operators","title":"Arithmetic operators","text":"<p>Of course, with integers, you can perform basic arithmetic operations. These are:</p> Operator Description <code>+</code> Addition <code>-</code> Subtraction <code>*</code> Multiplication <code>/</code> Division <code>**</code> Exponentiation <code>%</code> Modulo (Used to calculate the remainder of a division) <code>//</code> Floor division <pre><code># Modulo\n20 % 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>2\n</code></pre> <pre><code># Floor division\n20 // 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>6\n</code></pre> Info <p>The floor division <code>//</code> is often referred to as integer division. It rounds down to the nearest whole number.</p> Integer division<pre><code>20 // 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>6\n</code></pre> <p>Contrary, the divison operator <code>/</code> does not round the result.</p> Float division<pre><code>20 / 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>6.666666666666667\n</code></pre> <p>Hence, <code>/</code> is referred to as float division as it always returns  a  <code>float</code> (a number with decimal places). More on floats in a  second.</p> <p>Moreover, you can use multiple operations in one expression. You can also use parentheses to modify the order of operations so Python can evaluate your  expression in the order you specify. For example:</p> <pre><code>2 + 3 * 4\n</code></pre> &gt;&gt;&gt; Output<pre><code>14\n</code></pre> <pre><code>(2 + 3) * 4\n</code></pre> &gt;&gt;&gt; Output<pre><code>20\n</code></pre>"},{"location":"python/types/numbers/#floats","title":"Floats","text":"<p>Any number with decimal places is automatically interpreted as a <code>float</code>.</p> <pre><code>number = 10176.0\nprint(type(number))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'float'&gt;\n</code></pre> <p>All previously introduced arithmetic operations can be used for floats as well.</p> <pre><code>3.0 + 4.5\n# operations with floats and integers\n3.0 * (4 / 2)\n</code></pre>"},{"location":"python/types/numbers/#limitations","title":"Limitations","text":"Info <p>Floats are not 100% precise. \"[...] In general, the decimal floating-point  numbers you enter are only approximated by the binary floating-point  numbers actually stored in the machine.\" (The Python Tutorial, 2024)<sup>1</sup></p> <p>So be aware that these small numerical errors could add up in complex  calculations.</p> <pre><code>34.3 + 56.4\n</code></pre> <p>... results in</p> &gt;&gt;&gt; Output<pre><code>90.69999999999999\n</code></pre> Calculate the BEP <p>Use variables and arithmetic operations to calculate the break-even point (the number of units that need to be sold to cover the costs) for a product. The break-even point is given as:</p> \\[ \\text{BEP (units)} = \\frac{\\text{Fixed Costs}}{\\text{Price per Unit} - \\text{Variable Cost per Unit}} \\] <p>Calculate the \\(\\text{BEP}\\) for the following values:</p> <ul> <li>Fixed Costs: 30000</li> <li>Variable Cost per Unit: 45</li> <li>Price per Unit: 75</li> </ul> <p>Assign each given value to a variable. Print the result in a sentence, e.g.  <code>\"The break-even point is X units.\"</code></p>"},{"location":"python/types/numbers/#recap","title":"Recap","text":"<p>This section was all about numbers in Python. We have covered:</p> <ul> <li>Whole numbers  <code>int</code></li> <li>Decimal numbers  <code>float</code></li> <li>Floating-point arithmetic issues and limitations</li> </ul> <p>Next up, we will introduce the <code>bool</code> and <code>NoneType</code> type in Python.</p> <ol> <li> <p>The Python Tutorial \u21a9</p> </li> </ol>"},{"location":"python/types/strings/","title":"Strings","text":"<p>So far, we have already stored some text in a variable. For example  <code>\"Hello World!\"</code> which is called a string. A string is a primitive data type. Integer, float, boolean and None are also primitive data types which we  will cover later.</p> <p>A string is simply a series of characters. Anything inside quotes is considered a string in <code>Python</code>, and you can use single (<code>'</code>) or double  quotes (<code>\"</code>) around your strings like this:</p> <pre><code>text = \"This is a string.\"\nanother_text = 'This is also a string.'\n</code></pre> <p>This flexibility allows you to use quotes and apostrophes within your strings:</p> <pre><code>text = \"One of Python's strengths is its diverse and supportive community.\"\nprint(text)\n</code></pre> &gt;&gt;&gt; Output<pre><code>One of Python's strengths is its diverse and supportive community.\n</code></pre> <pre><code>text = 'I told my friend, \"Python is my favorite language!\"'\nprint(text)\n</code></pre> &gt;&gt;&gt; Output<pre><code>I told my friend, \"Python is my favorite language!\"\n</code></pre>"},{"location":"python/types/strings/#type","title":"<code>type()</code>","text":"<p>Let's check the type of the variable <code>text</code>.</p> <pre><code>text = \"The language 'Python' is named after Monty Python, not the snake.\"\nprint(type(text))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'str'&gt;\n</code></pre> <p><code>type()</code> comes in handy to check the type of variables. In this  case, we can verify that <code>text</code> is indeed a string. Just like  <code>print()</code>, <code>type()</code>  is an important tool in your programming arsenal.</p> Info <p>It is advisable to consistently enclose your strings with either single  <code>'...'</code> or double quotes <code>\"...\"</code>. This will make your  code easier to read and maintain.</p>"},{"location":"python/types/strings/#string-methods","title":"String methods","text":"<p>One of the simplest string manipulation, is to change the case of  the words in a string.</p> <pre><code>name = \"paul atreides\"\nprint(name.title())\n</code></pre> &gt;&gt;&gt; Output<pre><code>Paul Atreides\n</code></pre> <p>A method is an action performed on an object (in our case the  variable). The dot (.) in <code>name.title()</code> tells <code>Python</code> to  make the <code>title()</code> method act on the variable <code>name</code> which holds  the value <code>\"paul atreides\"</code>.</p> <p>Every method is followed by a set of parentheses, because methods often need additional information to do their work. That information is provided inside the parentheses. The <code>title()</code> method doesn\u2019t need any additional information, so its parentheses are empty.</p>"},{"location":"python/types/strings/#methods-vs-functions","title":"Methods vs. functions","text":"<p>We have already encountered functions like <code>print()</code> and <code>type()</code>. Functions are standalone entities that perform a specific task.</p> <p>On the other hand, methods are associated with objects. In this case, the  <code>title()</code> method is associated with the string object <code>name</code>.</p> String methods <p>You start with the variable <code>input_string</code> that holds the value  <code>\"fEyD rAuThA\"</code>. </p> <pre><code>input_string = \"fEyD rAuThA\"\n</code></pre> <p>Experiment and apply a combination of the following methods:</p> <ul> <li><code>capitalize()</code></li> <li><code>title()</code></li> <li><code>istitle()</code></li> <li><code>isupper()</code></li> <li><code>upper()</code></li> <li><code>lower()</code></li> </ul> <p>Eventually you should end up with the string <code>\"Feyd Rautha\"</code>,  print it.</p>"},{"location":"python/types/strings/#concatenation","title":"Concatenation","text":"<p>It\u2019s often useful to combine strings. For example, you might want to store first and last name in separate variables, and then combine them when you want to display someone\u2019s full name.</p> <p>Python uses the plus symbol (<code>+</code>) to combine strings. In this  example, we use <code>+</code> to create a full name by combining a  <code>first_name</code>, a space, and a <code>last_name</code>:</p> <pre><code>first_name = \"paul\"\nlast_name = \"atreides\"\nfull_name = first_name + \" \" + last_name\nprint(full_name)\n</code></pre> &gt;&gt;&gt; Output<pre><code>paul atreides\n</code></pre> <p>Here, the full name is used in a sentence that greets the user, and the <code>title()</code> method is used to format the name appropriately. This code returns a simple but nicely formatted greeting:</p> <pre><code>full_name = \"paul atreides\"\nprint(\"Hello, \" + full_name.title() + \"!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello, Paul Atreides!\n</code></pre> <p>Another way to nicely format strings is by using f-strings. To achieve the same result as above, simply put an <code>f\"...\"</code> in front of the string and use  curly braces <code>{}</code> to insert the variables. </p> <pre><code>full_name = \"Alia Atreides\"\n\nprint(f\"Hello, {full_name}!\")\n\n# you can even apply methods directly to the variables \n# (within the curly braces)\nprint(f\"Hello, {full_name.lower()}!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello, Alia Atreides!\nHello, alia atreides!\n</code></pre> A quote <p>Find a quote from a famous person you admire. Store the  quote and name in variables and print both with an f-string.</p> <p>Your output should look something like the following,  including the quotation marks: </p> <p><code>Frank Herbert (Dune opening quote): \"I must not fear. Fear is the  mind-killer.\"</code></p>"},{"location":"python/types/strings/#recap","title":"Recap","text":"<p>This section was all about strings, we have covered:</p> <ul> <li>How to create strings</li> <li>Use <code>type()</code> to check a variable's type</li> <li>String methods, such as <code>title()</code> and <code>lower()</code></li> <li>Distinction between methods and functions</li> <li>String concatenation with <code>+</code> and f-strings (<code>f\"...\"</code>)</li> </ul> <p>Next, up we will introduce numbers in Python, namely integers and floats. </p>"},{"location":"python-extensive/","title":"Home","text":"Under Construction <p>The <code>Python Extensive Course</code> is currently under construction.  </p> <p>Please check back later for updates.</p> <p></p>"},{"location":"python-extensive/DataBasics/","title":"Data Basics","text":""},{"location":"python-extensive/DataBasics/#data-basics","title":"Data Basics","text":"<p>Before starting with Data Science, it's essential to get to know the data you'll be working with. This means thoroughly examining the attributes and values to understand their characteristics. Real-world data is often messy, large, and diverse, which can make it difficult to handle. For example, data from sensors might include missing or corrupted values, while social media data might be unstructured and include text, images, or even videos. In financial data, outliers or extreme values may skew analysis results.</p> <p>Having a deep understanding of the data is crucial for successful data preprocessing, which is the first major step after acquiring the data. For instance, in sensor data, you may need to filter noise, while in social media data, you might need to convert unstructured text into a structured format for analysis. The main objective is to gather useful insights about the data, which will aid in the preprocessing stage, such as identifying patterns, missing values, or outliers. Grasping these aspects early on provides a solid foundation for the rest of your data analysis process.</p>"},{"location":"python-extensive/DataBasics/#dataset-objects-attributes","title":"Dataset, Objects, Attributes","text":"<p>In order to do that, we need to distinguish between the terms data set, object and attribute:</p> <ul> <li>Data Set: A data set is a collection of related data organized in a structured format. It consists of multiple objects, each described by a set of attributes. A data set can be represented as a table, where each row corresponds to a data object and each column corresponds to an attribute. Data sets are commonly used in data analysis, machine learning, and other data-driven tasks, serving as the primary source of input for these processes.</li> <li>Object: An object (or sometimes records, instances, or entries) is an individual unit of data within a data set. It represents a single entity or instance, such as a person, a product, or an event, depending on the context of the data. Each data object is characterized by a set of attributes, which define its specific properties or features. In a tabular data set, a data object corresponds to a row, with each attribute value for that object stored in the respective columns.</li> <li>Attributes: Attributes (or sometimes variable, field, dimension, feature or observations) are the characteristics or properties that describe data objects in a data set. Each attribute represents a specific feature of the data object and is associated with a particular value. For example, in a data set of customer information, attributes might include \"Name,\" \"Age,\" \"Gender,\" and \"Purchase History.\" In a tabular representation, attributes are typically the column headers, with each column containing the attribute values for the corresponding data objects (rows). Attributes can be of different types as described later.</li> </ul> Example: Data Set <p>  The displayed data shows a dataset consisting of three objects and seven attributes.</p>"},{"location":"python-extensive/DataBasics/#qualitative-vs-quantitative","title":"Qualitative vs Quantitative","text":"<p>A variable is called qualitative (categorical) when each observation distinctly falls into a specific category. Qualitative variables express different qualitative properties, but do not convey any magnitude. Conversely, a variable is termed quantitative (numerical) when it measures the magnitude of a property. Quantitative variables can be either Discrete (The variable can only take on a finite number of values) or Continuous (The variable can take on any value within a given interval).</p> Example: Qualitative vs Quantitative Attributes <ul> <li>Qualitative: Race, religious affiliation, gender, children in the household (yes/no)</li> <li>Quantitative: Age, test scores, number of children in a household<ul> <li>Discrete: Number of children in a household  </li> <li>Continuous: Height, weight, length measurements</li> </ul> </li> </ul>"},{"location":"python-extensive/DataBasics/#attribute-types","title":"Attribute Types","text":"<p>We now know, that attributes define the properties of data objects and are crucial in determining the methods and algorithms that can be applied during analysis. Different types of attributes - such as nominal, ordinal, interval, and ratio - each have unique characteristics that influence how they should be handled and interpreted. Recognizing and appropriately categorizing these attribute types is a key step in ensuring accurate data analysis and meaningful results.</p>"},{"location":"python-extensive/DataBasics/#nominal","title":"Nominal","text":"<p>Nominal attributes refer to those that are associated with names or labels. </p> <pre><code>cars = ['BMW', 'Audi', 'VW', 'Skoda', 'Tesla', 'Audi']\n</code></pre> <p>The values of nominal attributes are typically symbols or names representing different categories, codes, or states. These values are used to classify data into distinct groups, often referred to as categorical attributes. Importantly, the values of nominal attributes do not have any inherent or meaningful order; they simply indicate membership within a particular category without implying any rank or sequence. </p> Example: Nominal Attributes <ul> <li>Occupation: teacher, dentist, programmer, farmer,...</li> <li>Hair color: black, brown, blond, red, gray, white,...</li> </ul> <p>You can only determine whether two people have the same hair color or not. It is not possible to establish a greater-than or less-than relationship, and the differences between hair colors cannot be meaningfully interpreted.</p> <p>The symbols or names associated with nominal attributes can also be represented by numbers.</p> <pre><code>cars_num = [1, #'BMW'\n            2, #'Audi'\n            3, #'VW'\n            4, #'Skoda'\n            5, #'Tesla'\n            2] #'Audi'  \n</code></pre> <p>However, in such cases, these numbers are not meant to be used quantitatively, meaning that mathematical operations on them are not meaningful. For instance, calculating the mean or median of these numbers would not make sense. </p> <pre><code>import statistics \nstatistics.mean(cars_num)\n</code></pre> &gt;&gt;&gt; Output<pre><code>2.8333333333333335\n</code></pre> <p>But what does that mean? Is it half <code>'Audi'</code>, half <code>'VW'</code>? And we also could have used the numer <code>32</code> for <code>'Audi'</code> and <code>0</code> for <code>'VW'</code>. So its meaningless! </p> <p>Instead, the mode, which identifies the most frequently occurring value, can be used and is particularly useful in this context.</p> <pre><code>statistics.mode(cars)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Audi\n</code></pre> Example: Nominal Attributes Represented by Numbers <ul> <li>Customer ID: 0001, 0002, 0003, 0004, 0005,...</li> <li>Hair color: black=001, brown=010, blond=011,...</li> </ul> <p>Binary attributes are a specific type of nominal attribute that consist of only two categories. These categories are often represented by the numbers 0 and 1, where 0 indicates the absence of the attribute and 1 indicates its presence. This binary classification is commonly used in data analysis to represent simple variables</p> Example: Binary Attributes <ul> <li>Smoker: Yes=1, No=0</li> <li>Medical Test: Positive=1, Negative=0</li> </ul>"},{"location":"python-extensive/DataBasics/#ordinal","title":"Ordinal","text":"<p>For certain types of attributes, the possible values have a meaningful order or ranking among them, indicating that one value can be considered greater or less than another. However, while this order is significant, the exact magnitude or distance between successive values is not known. This means that while the sequence of values is meaningful, we cannot quantify the difference between them with precise measurements. Therefore it is possible and meaningful to calculate the median and mode. Mean on the other hand is not meaningful.</p> <pre><code>drinks = ['small', 'small', 'small', 'medium', 'medium', 'medium', 'large']\n\nprint('Median: ' + statistics.median(drinks))\nprint('Mode: ' + statistics.mode(drinks))\n</code></pre> &gt;&gt;&gt; Output<pre><code>Median: medium\nMode: small\n</code></pre> Example: Ordinal Attributes <ul> <li>Professional Rank: private, specialist, corporal, sergeant</li> <li>Drink size: small, medium, large</li> </ul> <p>As with the nominal scale, you can determine whether two drinks are the same size or not. Additionally, you can say whether one drink is larger than another. However, it is still not possible to meaningfully interpret the differences between sizes. You cannot specify by how much one drink is larger than another.</p>"},{"location":"python-extensive/DataBasics/#numerical","title":"Numerical","text":"<p>Numeric attributes are quantitative in nature, meaning they represent measurable quantities. These attributes can be expressed as either integer or real values. One of the key characteristics of numeric attributes is their ability to quantify the difference between values, allowing for meaningful comparisons. Statistical measures such as the mean, median, and mode can be calculated from numeric attributes, and these measures are both possible and useful for analyzing the data.</p> <pre><code>income = [1005, 2500, 2500, 5100, 6011, 10500]\n\nprint('Mean: ' + str(statistics.mean(income)))\nprint('Median: ' + str(statistics.median(income)))\nprint('Mode: ' + str(statistics.mode(income)))\n</code></pre> &gt;&gt;&gt; Output<pre><code>Mean: 4602.67\nMedian: 3800.0\nMode: 2500\n</code></pre>"},{"location":"python-extensive/DataBasics/#interval-scaled","title":"Interval-scaled","text":"<p>Interval-scaled attributes can be measured on a scale with equal-sized units, allowing for consistent and comparable intervals between values. These attributes have an inherent order and they can take on positive, zero, or negative values. This means that the ranking of values is possible and meaningful, providing a clear sense of progression or regression along the scale.</p> Example: Interval-scaled Attributes <ul> <li>Calendar dates: For instance, the years 2002 and 2010 are eight years apart</li> <li>Celsius temperature: \\(20^\\circ C\\) is five degrees higher than a temperature of \\(15^\\circ C\\)</li> </ul> <p>As with the ordinal scale, you can determine whether two temperatures are the same and whether one temperature is higher than another. Additionally, the difference between temperatures can be meaningfully interpreted. However, because the zero point is arbitrary, ratios cannot be meaningfully interpreted.</p>"},{"location":"python-extensive/DataBasics/#ratio-scaled","title":"Ratio-scaled","text":"<p>Ratio-scaled attributes possess an inherent zero-point, which indicates the complete absence of the attribute. This characteristic allows us to meaningfully discuss one value as being a multiple of another. Because of this, ratio-scaled data supports a wide range of mathematical operations, including meaningful comparisons of both differences and ratios between values.</p> Example: Ratio-scaled Attributes <ul> <li>Kelvin temperature: has true zero-point</li> <li>Count attributes: years of experience, number of words, Income in Euros</li> </ul> <p>As with the interval scale, you can determine whether two people have the same income and whether one person earns more than another. Additionally, the differences between incomes can be meaningfully interpreted. Furthermore, the ratio between two incomes can now also be interpreted, such as determining how many times one income is compared to another.</p> Task: Attribute Types <p>Name the types of attributes in the following data set and justify  </p>"},{"location":"python-extensive/comparisons_and_logic/","title":"Comparisons and logic","text":""},{"location":"python-extensive/comparisons_and_logic/#comparisons-logical-operators","title":"Comparisons &amp; Logical operators","text":""},{"location":"python-extensive/comparisons_and_logic/#comparisons","title":"Comparisons","text":"<p>Now, that we have covered all basic <code>Python</code> types, we can start comparing them. As the name suggests, comparisons are used to compare two values. The result  of a comparison is a boolean value.</p>"},{"location":"python-extensive/comparisons_and_logic/#equality","title":"Equality","text":"<pre><code>print(\"Abc\" == \"abc\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>False\n</code></pre> <p>We can compare any type with each other. In the above case, the comparison  checks if both strings are equal, using the <code>==</code> operator. The result is  <code>False</code>, because the case of the strings do not match.</p> <p>Let's check if two integers are equal:</p> <pre><code>print(1 == 1)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre>"},{"location":"python-extensive/comparisons_and_logic/#inequality","title":"Inequality","text":"<p>We can also check if two values are not equal with the <code>!=</code> operator:</p> <pre><code>user_name = \"Eric\"\nprint(user_name != \"admin\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre> <pre><code>print(2 != 2.1)\nprint(2 != 2)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\nFalse\n</code></pre>"},{"location":"python-extensive/comparisons_and_logic/#numerical-comparisons","title":"Numerical comparisons","text":"<p>... are done with:</p> Operator Description <code>&lt;</code> less than <code>&gt;</code> greater than <code>&lt;=</code> less than or equal <code>&gt;=</code> greater than or equal <pre><code>print(1 &lt; 2)\nprint(1 &gt; 2)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\nFalse\n</code></pre> <pre><code>print(10.2 &lt;= 10.2)\nprint(9.99 &gt;= 10)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\nFalse\n</code></pre>"},{"location":"python-extensive/comparisons_and_logic/#logical-operators","title":"Logical Operators","text":"<p>You may want to check multiple conditions at the same time. For example, sometimes you might need two conditions to evaluate to <code>True</code> in  order to perform an action. Hence, logical operators are introduced.  There are three logical operators:</p> Operator Meaning Example Result <code>and</code> Returns <code>True</code> if both statements are <code>True</code> <code>True and True</code> <code>True</code> <code>or</code> Returns <code>True</code> if one of the statements is <code>True</code> <code>True or False</code> <code>True</code> <code>not</code> Reverses a result <code>not True</code> <code>False</code>"},{"location":"python-extensive/comparisons_and_logic/#and","title":"<code>and</code>","text":"<pre><code>age = 20\nprint(age &gt;= 18 and age &lt;= 25) # True and True -&gt; True\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre>"},{"location":"python-extensive/comparisons_and_logic/#or","title":"<code>or</code>","text":"<pre><code>age = 20\nprint(age &gt;= 50 or age &lt;= 25) # False or True -&gt; True\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre>"},{"location":"python-extensive/comparisons_and_logic/#not","title":"<code>not</code>","text":"<pre><code>age = 20\nprint(not(age &gt;= 18)) # not(True) -&gt; False\n</code></pre> &gt;&gt;&gt; Output<pre><code>False\n</code></pre> Evaluate password security requirements: Part 1 <p>You are given two variables that describe properties of a password:</p> <ul> <li><code>password_length</code> - represents how many characters are in the password     (<code>int</code>)</li> <li><code>has_special_characters</code> - represents whether the password contains      special characters (<code>True</code>/<code>False</code>)</li> </ul> <p>Variables to use:</p> <pre><code>password_length = 18\nhas_special_characters = False\n</code></pre> <p>Task:</p> <p>Write code that checks if this password is secure based on these requirements:</p> <ol> <li>The password must be longer than 10 characters</li> <li>The password must contain special characters</li> </ol> <p>Use comparisons and logical operators to create a single expression  that evaluates whether BOTH requirements are met.</p> Evaluate password security requirements: Part 2 <p>To increase security, a third variable is introduced alongside the  previous password properties:</p> <p><code>already_used</code> - represents whether this password has been used before     (<code>True</code>/<code>False</code>)</p> <p>Variables to use:</p> <pre><code>password_length = 18\nhas_special_characters = True\nalready_used = False\n</code></pre> <p>Task:</p> <p>Write code that checks if this password is secure based on these three requirements:</p> <ol> <li>The password must be longer than 10 characters</li> <li>The password must contain special characters</li> <li>The password must not have been used before</li> </ol> <p>Build on your previous solution and evaluate whether all THREE  requirements are met.</p>"},{"location":"python-extensive/comparisons_and_logic/#recap","title":"Recap","text":"<p>We have covered the basic comparison and logical operators in <code>Python</code> .</p> <ul> <li> <p>Comparisons</p> <ul> <li><code>==</code> for equality</li> <li><code>!=</code> for inequality</li> <li><code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code> for numerical comparisons</li> </ul> </li> <li> <p>Logical operators</p> <ul> <li><code>and</code></li> <li><code>or</code></li> <li><code>not</code></li> </ul> </li> </ul>"},{"location":"python-extensive/functions/","title":"Functions","text":""},{"location":"python-extensive/functions/#functions","title":"Functions","text":"<p>In this section, you\u2019ll learn to write functions, which are named blocks of code that are designed to do one specific task. If you need to perform that task multiple times throughout your program, you don\u2019t need to type all the code for the same task again and again; you just call the function dedicated to handling that task. By defining functions, your programs will get easier to write, read, test, and fix.</p>"},{"location":"python-extensive/functions/#defining-a-function","title":"Defining a function","text":"<p>Here\u2019s a simple function named <code>greet_user()</code> that prints a greeting:</p> <pre><code>def greet_user():\n    print(\"Hello!\")\n</code></pre> <p>This example shows the simplest structure of a function. With the keyword <code>def</code> we define a function, followed by the name of our function.  Within the parentheses we can (optionally) specify any information the function needs to do its job. This information is defined in the form of parameters (more on that later).</p> <p>Any indented lines that follow <code>def greet_user():</code> make up the body of the function. The line <code>print(\"Hello!\")</code> is the only line of actual code in the body of this function, so <code>greet_user()</code> has just one job: <code>print(\"Hello!\")</code>.</p> <p>When you want to use a function, you have to call it. To do so, simply write the function name, followed by any required information in parentheses.</p> <pre><code># call the function\ngreet_user()\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello!\n</code></pre>"},{"location":"python-extensive/functions/#detour-docstrings","title":"Detour: docstrings","text":"<p>As previously discussed, it is always good practice to add comments to your code in order to improve readability. That applies to functions as well. In  case of functions, one can add a docstring, which is essentially a longer  comment that describes the function. A docstring is written in triple quotes <code>\"\"\"...\"\"\"</code> and is placed directly after the function definition.</p> <pre><code>def greet_user():\n    \"\"\"Display a simple greeting.\"\"\"\n    print(\"Hello!\")\n</code></pre> <p>Now, you can display the docstring by calling the built-in <code>help()</code>  function with the function name as an argument:</p> <pre><code>help(greet_user)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Help on function greet_user in module __main__:\ngreet_user()\n    Display a simple greeting.\n</code></pre> <p>Docstrings facilitate the proper documentation of your code. Most of all, they will help you in the long run to remember what your code does.</p>"},{"location":"python-extensive/functions/#parameters","title":"Parameters","text":"<p>After some modification, the function <code>greet_user()</code> should not only tell the user \"Hello!\" but also greet them by name. Therefore, we have to  define a parameter called <code>user_name</code>. Now, each time you call the function,  you need to pass a <code>user_name</code> such as <code>\"admin\"</code> to the function.</p> <pre><code>def greet_user(user_name):\n    \"\"\"\n    Display a simple greeting.\n    Pass a string with the user's name.\n    \"\"\"\n    print(f\"Hello, {user_name}!\")\n\ngreet_user(\"admin\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello, admin!\n</code></pre> Info <p>As you can see in the example above, a docstring can span multiple lines!</p> <p>Up until now, the functions had no parameters at all or just a single  parameter. However, you can define as many parameters as you like, seperated  by a comma (<code>,</code>):</p> <pre><code>def greet(first_name, last_name):\n    print(f\"Hello, {first_name} {last_name}!\")\n</code></pre> Break-even point <p>Remember the task to calculate the break-even point? Now, you'll wrap  following formula within a function: $$ \\text{BEP (units)} = \\frac{\\text{Fixed Costs}}{\\text{Price per Unit} - \\text{Variable Cost per Unit}} $$</p> <p>Write a function called <code>calculate_bep()</code> that takes the following parameters:</p> <ul> <li><code>fixed_costs</code></li> <li><code>price_per_unit</code></li> <li><code>variable_cost_per_unit</code></li> </ul> <p>Print the calculation result (break-even point) within the function.  Call the function with following arguments:</p> <pre><code>fixed_costs = 30000\nprice_per_unit = 75\nvariable_cost_per_unit = 45\n</code></pre>"},{"location":"python-extensive/functions/#terminology","title":"Terminology","text":"<p>parameter vs. argument</p> <p>A parameter is the variable inside the parentheses of the function definition - <code>def greet_user(user_name):</code>. Here <code>user_name</code> is the parameter. When you call the function with, for example <code>greet_user(\"admin\")</code>, the  value <code>\"admin\"</code> is referred to as an argument. You can think of the parameter  as a placeholder and the argument as the actual value.</p>"},{"location":"python-extensive/functions/#positional-arguments","title":"Positional arguments","text":"<p>When you call a function, <code>Python</code> must match each argument in the function  call with a parameter in the function definition. The simplest way to do this  is based on the order of the arguments provided which is referred to as positional arguments.</p> <pre><code>def add_numbers(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    print(a + b)\n\nadd_numbers(3, 5)\n</code></pre> &gt;&gt;&gt; Output<pre><code>8\n</code></pre>"},{"location":"python-extensive/functions/#order-matters","title":"Order matters!","text":"<p>You can get unexpected results, if you mix up the order of the arguments in a function call when using positional arguments:</p> <pre><code>def perform_calculation(a, b):\n    \"\"\"Calculate something.\"\"\"\n    print(a + b**b)\n\na = 12\nb = 5\n\n# correct order\nperform_calculation(a, b)\n# incorrect order (produces a different result)\nperform_calculation(b, a)\n</code></pre> &gt;&gt;&gt; Output<pre><code>3137\n8916100448261\n</code></pre> <p>Next up, we'll introduce keyword arguments to avoid such mistakes.</p>"},{"location":"python-extensive/functions/#keyword-arguments","title":"Keyword arguments","text":"<p>A keyword argument is a name-value pair that you pass to a function. You directly associate the name and the value within the argument, so when you pass the argument to the function, there\u2019s no confusion.</p> <pre><code>perform_calculation(a=12, b=5)\n\n#  you can switch the order of the named keyword arguments\nperform_calculation(b=5, a=12)\n</code></pre> <p>No matter the order, the result is the same!</p> &gt;&gt;&gt; Output<pre><code>3137\n3137\n</code></pre>"},{"location":"python-extensive/functions/#default-values","title":"Default values","text":"<p>When writing a function, you can optionally define a default value for each parameter. If an argument for a parameter is provided in the function call, <code>Python</code> uses the argument value. If not, the parameter\u2019s default value is used. Using default values can simplify your function calls and clarify the ways in which your functions are typically used. Let's look at an example.</p> <pre><code>def describe_lab(lab_name, lab_supervisor=\"Miriam\"):\n    print(f\"{lab_name} is supervised by {lab_supervisor}\")\n\n# use the default value for lab_supervisor\ndescribe_lab(lab_name=\"Chemistry\")\n\n# provide a value for lab_supervisor\ndescribe_lab(lab_name=\"IT\", lab_supervisor=\"Alex\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Chemistry is supervised by Miriam\nIT is supervised by Alex\n</code></pre>"},{"location":"python-extensive/functions/#return-values","title":"Return values","text":"<p>A function doesn\u2019t have to display its output directly with <code>print()</code>.  Instead, usually a function executes a task and returns the result. The result, the return value can be of any type. To return a value, use the <code>return</code> statement.</p> <pre><code>def square_number(number):\n    square = number ** 2\n    return square\n</code></pre> <p>When you call the function, you can assign the result to a variable.</p> <pre><code>result = square_number(5)\n</code></pre> Password check <p>Write a function to check if a user is able to log-in.</p> <p>You have to determine if the given username and password (in  plain text) are within the database. The database is a <code>dict</code>  with IDs as keys and another <code>dict</code> as value, containing  username and hashed password.</p> <p>Given 'database': <pre><code>very_secure_db = {\n    0: {\n        \"username\": \"SwiftShark22\",\n        \"password\": \"ef92b778bafe771e89245b89ecbc08a44a4e166c06659911881f383d4473e94f\",\n    },\n    1: {\n        \"username\": \"FierceFalcon66\",\n        \"password\": \"07ac6e7d83aa285293fc325fecd04a51e933ab94d43dbc6434ddca652718fb95\",\n    },\n    2: {\n        \"username\": \"admin\",\n        \"password\": \"6feb4c700de1982f91ee7a1b40ca4ded05d155af3987597cb179f430dd60da0b\",\n    },\n    3: {\n        \"username\": \"BraveWolf11\",\n        \"password\": \"c430e4368aff7c1bc75c3865343730500d7c1a5f65758ade56026b08e94686cc\",\n    },\n}\n</code></pre></p> <p> </p> <p>Use following function to convert a password to its hash: <pre><code>import hashlib\n\ndef hash_password(password):\n    return hashlib.sha256(password.encode()).hexdigest()\n</code></pre></p> <p>Now, write a function that takes at least two arguments, <code>username</code> and  <code>password</code> (in plain text!). Check if the given username and hashed  password are within the <code>very_secure_db</code>. Return <code>True</code> if the  user is able to log-in, otherwise <code>False</code>.</p> <p>Call your function for following users:</p> <pre><code>user1 = (\"SwiftShark22\", \"password123\")\nuser2 = (\"FierceFalcon\", \"sdkjf34\u00a7\")\nuser3 = (\"admin\", \"1b40ca4ded0\")\n</code></pre> Info <p>As you have seen in the above example, functions help you to structure your code. For instance, the function <code>hash_password()</code> was reused multiple  times (to generate the <code>very_secure_db</code> and within your own function).</p> <p>Functions also help you to break down complex problems. You can write a  function for each subtask and then combine them to solve the problem as a  whole.</p>"},{"location":"python-extensive/functions/#recap","title":"Recap","text":"<p>This section introduced the concept of functions to better structure your  code, make it more readable and reusable. We have covered:</p> <ul> <li>How to define a function</li> <li>Docstrings as a tool to document your functions</li> <li>Parameters vs arguments</li> <li>Positional and keyword arguments</li> <li>Defining default values for parameters</li> <li>The <code>return</code> statement</li> <li>How to use functions to solve smaller subtasks and structure your code</li> </ul>"},{"location":"python-extensive/git/","title":"Bonus: GIT","text":""},{"location":"python-extensive/ide/","title":"IDE","text":""},{"location":"python-extensive/ide/#ide","title":"IDE","text":"<p>After the successful installation of Python, we use an IDE (=Integrated Development Environment) which is simply put, a place to write and  execute <code>Python</code> code. There are many IDEs available, but we recommend using  Visual Studio Code (VS Code/VSC).</p>"},{"location":"python-extensive/ide/#visual-studio-code","title":"Visual Studio Code","text":""},{"location":"python-extensive/ide/#general-information","title":"General Information","text":"<p>VSCode is a free, open-source code editor developed by Microsoft . It has gained immense popularity among developers for its versatility and extensive extension ecosystem, making it a powerful tool for various  programming tasks, including Python and Jupyter Notebook programming.  Some key features of VSCode include:</p> <ul> <li>Cross-Platform: VSCode is available for Windows , macOS , and Linux ,  making it accessible to developers on different operating systems.</li> <li>Lightweight: It\u2019s known for its speed and efficiency. VSCode launches quickly and consumes minimal system resources.</li> <li>Extensible: VSCode supports a wide range of programming languages and  technologies through extensions. You can customize the editor with extensions  to add new features, integrations, and tools. VSCode offers intelligent code  completion and suggestions, which can significantly boost your productivity  while writing code. Additionally, there is an extension for GitHub Copilot which gives you real-time AI-based suggestions (free for students; sign-up here)</li> <li>Version Control: It has built-in <code>Git</code> support, making it easy to manage  version control and collaborate with others using Git repositories.</li> <li>Large Community: VSCode has a large and active community, which means you can find plenty of resources, extensions, and tutorials to enhance your coding experience.</li> </ul>"},{"location":"python-extensive/ide/#setup","title":"Setup","text":"<p>Download the installer from the official website. The installation is straightforward, so we won't cover it in detail. </p>"},{"location":"python-extensive/ide/#extensions","title":"Extensions","text":"<p>As already mentioned, VSCode can be used for a wide range of programming languages. To do this, we need to install the corresponding extensions.  Therefore, start VSCode and click on the sidebar on <code>Extensions</code>. Then search  and install <code>Jupyter</code> and <code>Python</code> (both from Microsoft). </p> Install the Python and Jupyter extension <p>Now, restart VSCode.</p>"},{"location":"python-extensive/ide/#jupyter-notebook","title":"Jupyter notebook","text":"<p>Next, we create a new file to execute our first Python code.  To do so, we use Jupyter notebooks. Jupyter notebooks are basically composed of cells. A cell can either contain  code or text. However, first, we have to create our first notebook.</p> <p>Hence, we first select a folder in which we want to save our work. We go to  <code>File</code> <code>Open Folder</code> and choose a folder. Then click on explorer in the sidebar where your folder should be opened. Right  click somewhere in the explorer and select <code>New File</code>. Type a name for  your file with the extension <code>*.ipynb</code>. If not automatically, open the new file. Click on <code>Select Kernel</code> in the  upper right corner of VSCode and select <code>Python Environment</code>  your <code>Python</code> installation.</p> Select your Python kernel. <p>If your firewall asks, allow access.</p> Allow access. <p>Now, add your first code cell with the <code>+ Code</code> button in the upper left  corner. Add following line.</p> <pre><code>print(\"Hello World!\")\n</code></pre> Run your first code snippet. <p>After clicking on <code>Run All</code>, a popup will appear to install the <code>ipykernel</code>. Click on <code>Install</code>. </p> Last missing piece - the ipykernel. <p>After the installation, you should be greeted with following output</p> &gt;&gt;&gt; Output<pre><code>Hello World!\n</code></pre> <p>Congratulations \ud83c\udf89, you've successfully executed your first <code>Python</code>  code!</p>"},{"location":"python-extensive/ide/#more-on-jupyter-notebooks","title":"... more on Jupyter notebooks","text":""},{"location":"python-extensive/ide/#why","title":"Why?","text":"<p>One of the key features of Jupyter Notebook is the combination of code cells  with rich text elements, allowing you to create comprehensive documents that  blend code, visualizations, and explanatory text. This makes it a powerful tool for creating data analysis reports, sharing research findings, or documenting  code workflows.</p> <p>In addition to code execution and documentation capabilities, Jupyter Notebook  offers a wide range of extensions and integrations with popular data science  libraries, plotting libraries, and other tools. It provides a flexible and  interactive environment for data manipulation, visualization, and analysis.</p>"},{"location":"python-extensive/ide/#cells","title":"Cells","text":"<p>As previously discussed, Jupyter notebooks are composed of cells. A cell can  contain Python code or text. To add a text cell, click on <code>+ Markdown</code>.  Markdown  is a lightweight markup language with  plain text formatting syntax. You can simply write text, add images and links within a markdown cell. This guide offers a nice comprehensive overview of Markdown.</p> Info <p>Don't worry about Markdown too much, it is simple to use and 'supports'  plain text. So just start writing.</p>"},{"location":"python-extensive/ide/#execution","title":"Execution","text":"<p>You can execute cells one by one. Either by clicking on the <code>Exceute Cell</code>  button on the left side of your current cell. Or by using the shortcut  Ctrl+Enter.</p> <p>Run all cells with the corresponding <code>Run All</code> button on top.</p>"},{"location":"python-extensive/ide/#coming-up","title":"Coming up ...","text":"<p>Next, we will cover some basic Python concepts, and you will get more familiar  with code cells.</p>"},{"location":"python-extensive/installation/","title":"Installation","text":""},{"location":"python-extensive/installation/#installation","title":"Installation","text":"<p>First, we need to install Python. You can download the latest version of Python from the official website python.org.</p> <p>We recommend to install the latest version of  Python (as of September 2024, the latest stable release is <code>3.12.6</code>).</p> <p>Depending on your operating system, the installation process may vary slightly. Below, we will cover the installation process on a Windows machine. If you are using macOS, python.org offers a corresponding installer (64-bit). </p>"},{"location":"python-extensive/installation/#windows","title":"Windows","text":"<p>Execute the downloaded file (installer). When installing Python, make sure that you check the box <code>Add python.exe to PATH</code>.</p> <p>After the successful installation, we recommend to open a command prompt (use the Windows search with the keyword <code>cmd</code>) and verify the installation by  typing </p> <pre><code>python --version\n</code></pre> <p>Which should result in:</p> CMD Output<pre><code>Python 3.12.6\n</code></pre>"},{"location":"python-extensive/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"python-extensive/installation/#path-issues","title":"PATH issues","text":"<p>If you didn't check the box <code>Add python.exe to PATH</code> during  installation, or you encounter an error message along the lines of </p> <pre><code>'python' is not recognized as an internal or external command\n</code></pre> <p>you need to add Python to your PATH (which means the  Python executable is simply not found).</p> <p>We cover two options to fix the PATH issue, either use the command prompt  or the GUI.</p> Option 1: GUIOption 2: Command prompt <p>Step 1:</p> <p>First, we need to find the path to the executable. </p> <p>Open the  Windows search and type <code>python</code>. Select <code>Dateispeicherort \u00f6ffnen</code> (open file location). Open the context menu of <code>Python</code> (that's just a shortcut) and select <code>Eigenschaften</code> (properties)  <code>Dateipfad \u00f6ffnen</code> (open file path).  Lastly, copy the path of the newly opened explorer window.</p> <p>      Determine the path to the Python executable. </p> <p>Step 2:</p> <p>Now, we need to add the path to the environment variables. Again use  the  Windows search and type  <code>Umgebungsvariablen</code> (Environment variables). Select the Path value in the  <code>Benutzervariablen f\u00fcr &lt;user-name&gt;</code> (User variables) section. Click on  <code>Neu</code> (New) and paste the copied path.</p> <p>          Add the path to the user variables.      </p> <p>Step 1:</p> <p>Determine the path to the  Python executable  using the Python launcher <code>py</code> (which is part of the Python installation and  is on PATH by default). <pre><code>py -3.12 -c \"import sys; print(sys.executable)\"\n</code></pre> CMD Output<pre><code>C:\\Users\\ztklotz\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n</code></pre> <pre><code>setx PATH \"%PATH%;&lt;copied-path&gt;\"\n</code></pre> <pre><code>setx PATH \"%PATH%;C:\\Users\\ztklotz\\AppData\\Local\\Programs\\Python\\Python312\"\n</code></pre></p> <p>In my case, the output is:</p> <p>Copy your path without the <code>python.exe</code> part.</p> <p>Step 2:</p> <p>Set the PATH variable using the command prompt.</p> <p>For instance (using my path):</p> <p>Step 3:</p> <p>Again, verify the installation by typing <code>python --version</code> in your command  prompt.</p> <p>With  Python installed, the next step is to set up a code editor. In the following section, we will install Visual Studio Code (VS Code).</p>"},{"location":"python-extensive/oop/","title":"Object-Oriented Programming","text":""},{"location":"python-extensive/oop/#introduction","title":"Introduction","text":"<p>As code complexity grows, managing and understanding functionality becomes increasingly challenging. Object-Oriented Programming (OOP) addresses this by breaking down large tasks into smaller, modular components.</p>"},{"location":"python-extensive/oop/#example-self-driving-car","title":"Example: Self-Driving Car","text":"<p>Imagine developing software for a self-driving car. Instead of coding everything in a single block, you can organize the project into modules like camera systems, lane detection, navigation, and battery management. Each module is easier to develop, test, and maintain. Moreover, these modules can be reused in other applications, such as drone control, if similar functionalities are required.</p>"},{"location":"python-extensive/oop/#how-oop-helps","title":"How OOP Helps","text":"<p>OOP enhances modularity by allowing each module (or class) to operate independently. Different team members can work on separate classes, improving scalability and collaboration. Think of it like running a restaurant: instead of handling every task yourself, roles like chef, waiter, and cleaner are delegated to specialized staff, making operations efficient and scalable.</p> <p></p>"},{"location":"python-extensive/oop/#core-concepts-of-oop","title":"Core Concepts of OOP","text":"<p>Object-Oriented Programming (OOP) is based on the concept of objects, which are instances of classes (modules).</p> <p>Classes and Objects:  </p> <ul> <li>Class: A blueprint defining attributes (properties) and methods (actions) for objects.</li> <li>Object: An instance of a class, created using the class blueprint.</li> </ul> <p>Objects combine attributes (what they \"have\") and methods (what they \"do\"). </p> <p>Attributes and Methods:  </p> <ul> <li>Attributes: Properties specific to the object (e.g., resolution, lens type).</li> <li>Methods: Functions defining object behavior (e.g., capturing images, detecting objects).</li> </ul> <p>For instance, a <code>Camera</code> class in a self-driving car may define attributes like resolution and lens type, along with methods for capturing images or detecting objects. Multiple objects (e.g., front and rear cameras) can be created from this class, each with unique attribute values.</p> <p>OOP makes it easy to reuse and extend code, simplifying the development of complex systems like self-driving cars.</p> <p></p>"},{"location":"python-extensive/oop/#class-definition","title":"Class Definition","text":"<p>Classes define the structure of objects, specifying their attributes and methods. Use the syntax: <code>class ClassName:</code>  Just like with functions, the code inside the class is indented. Classes can be defined in the same script or in a separate script file, which can then be included using <code>import</code>.  Changes to the class definition only apply to new objects, meaning that all objects from the old definition must be removed from the workspace.     </p> <p>A class for a <code>Camera</code> module in a self-driving car could be defined as follows:</p> <pre><code>class Camera:\n    pass\n</code></pre> Info <p><code>pass</code> is a placeholder that indicates that no action is executed. It is used to define an empty code block.</p> <p>You can create Camera objects \u2014 instances of the class \u2014 by assigning them using  <code>Camera()</code>.</p> <p><pre><code>class Camera:\n    pass\n\n# Creating instances of the Camera class\nfront_camera = Camera()\nrear_camera = Camera()\n\nprint(type(front_camera)) \nprint(type(rear_camera))  \n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class '__main__.Camera'&gt;\n&lt;class '__main__.Camera'&gt;\n</code></pre> This demonstrates that <code>front_camera</code> and <code>rear_camera</code> are instances of the <code>Camera</code> class, even though the class currently has no defined attributes or methods.    </p> <p>Variables or attributes can be defined within a class. As a result, all objects of the class will have this variable with the specified value. The attribute can be accessed using a dot operator.</p> <p><pre><code>class Camera:\n    # Defining a class attribute\n    lens_type = \"wide-angle\" \n\n# Creating an instance of the Camera class\nfront_camera = Camera()\n\n# Accessing the attribute using the dot operator\nprint(front_camera.lens_type)\n</code></pre> The <code>lens_type</code> attribute is a class attribute, meaning it is shared by all instances of the class.   Changing the value of the class attribute (<code>Camera.lens_type</code>) affects all objects created from the class, as they share the same attribute. </p> <pre><code>class Camera:\n    # Defining a class attribute\n    lens_type = \"wide-angle\"  \n\n# Creating two instances of the Camera class\nfront_camera = Camera()\nrear_camera = Camera()\n\n# Accessing the shared attribute\nprint(f\"Front camera lens type: {front_camera.lens_type}\")  \nprint(f\"Rear camera lens type: {rear_camera.lens_type}\")    \n\n# Changing the class attribute\nCamera.lens_type = \"telephoto\"\n\n# Both instances reflect the updated value\nprint(f\"Front camera lens type: {front_camera.lens_type}\")\nprint(f\"Rear camera lens type: {rear_camera.lens_type}\") \n</code></pre>"},{"location":"python-extensive/oop/#initialization-method","title":"Initialization Method","text":"<p>The <code>__init__(self, property)</code> method is called each time a new object is instantiated.   </p> <p>Attributes are characteristics that describe an object (e.g., camera_type, lens_type). Within the <code>__init__</code> method, the term <code>self</code> refers to the object being created, and additional attributes can be added to it. This initialization method ensures that the Camera object is set up with specific values (e.g., camera_type and lens_type) right when it is created. An error message occurs if these specific values are missing.</p> <pre><code>class Camera:\n    # Setting the attributes camera_type and lens_tpye\n    def __init__(self, camera_type, lens_type):\n        self.camera_type = camera_type\n        self.lens_type = lens_type\n\n# Creating an instance of the Camera class       \nfront_camera = Camera(\"front\",\"wide-angle\")\nprint(f\"{front_camera.camera_type} and {front_camera.lens_type}\")\n\n# Creating another instance of the Camera class\nrear_camera = Camera()\n</code></pre> Initialization Method <p>Why does this code generate an error message? Identify the cause and modify the code to ensure it runs without errors.</p> <p>Initialization parameters allow optional customization when creating an object. If no values are provided, default values will be used.</p> <p><pre><code>class Camera:\n    # Setting the attributes camera_type, lens_tpye and resolution\n    def __init__(self, camera_type, lens_type, resolution=1080):\n        self.camera_type = camera_type\n        self.lens_type = lens_type\n        self.resolution = resolution\n\n# Creating an instance of the Camera class w/o defining the resolution      \nfront_camera = Camera(\"front\",\"wide-angle\")\nprint(f\"{front_camera.camera_type}, {front_camera.lens_type} and {front_camera.resolution}\")\n</code></pre> <pre><code># Creating an instance of the Camera class with defining the resolution       \nfront_camera = Camera(\"front\",\"wide-angle\",720)\nprint(f\"{front_camera.camera_type}, {front_camera.lens_type} and {front_camera.resolution}\")\n</code></pre></p>"},{"location":"python-extensive/oop/#encapsulation","title":"Encapsulation","text":"<p>Encapsulation separates what a class shows (public properties and methods) from its hidden internal details (private implementation). If data is public, it can be directly accessed and changed using the dot operator.</p> <p><pre><code>class Camera:\n    # Setting the attributes camera_type and lens_tpye\n    def __init__(self, camera_type, lens_type):\n        self.camera_type = camera_type\n        self.lens_type = lens_type\n\n# Creating an instance of the Camera class          \nfront_camera = Camera(\"front\",\"wide-angle\")\nprint(front_camera.camera_type)\n</code></pre> <pre><code># Changing the attribute camera_type\nfront_camera.camera_type = \"rear\"\nprint(front_camera.camera_type)\n</code></pre> If data is private, it allows access only through specific methods, protecting the object's internal workings. Using double underscores before the attribute name (<code>__name</code>), restricts access to the private attribute. </p> <p><pre><code>class Camera:\n    # Setting the private attributes camera_type and lens_tpye\n    def __init__(self, camera_type, lens_type):\n        self.__camera_type = camera_type\n        self.__lens_type = lens_type\n\n# Creating an instance of the Camera class \nfront_camera = Camera(\"front\",\"wide-angle\")\n</code></pre> <pre><code># Incorrect usage: Accessing the attribute camera_type\nprint(front_camera.camera_type)\n</code></pre> &gt;&gt;&gt; Output<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[8], line 2\n      1 # Incorrect usage: Accessing the attribute camera_type\n----&gt; 2 print(front_camera.camera_type)\n\nAttributeError: 'Camera' object has no attribute 'camera_type'\n</code></pre></p> <p>By defining appropriate methods, interface functions can be provided to allow the user to modify and read private attributes (e.g., change_camera_type, display_data). The dot operator is used when calling the function.</p> <pre><code>class Camera:\n    # Setting the private attributes camera_type and lens_tpye\n    def __init__(self, camera_type, lens_type):\n        self.__camera_type = camera_type\n        self.__lens_type = lens_type\n\n    # Creating the method change_camera_type\n    def change_camera_type(self, camera_type_new):\n        self.__camera_type = camera_type_new\n\n    # Creating the method display\n    def disp(self):\n        print(f\"Object of Camera Class:\\n\")\n        print(f\"Camera Type: {self.__camera_type}\")\n        print(f\"Lens Type: {self.__lens_type}\")\n</code></pre> Encapsulation <p>Create a new instance of the <code>Camera</code> class (e.g., <code>front_camera</code>), then update its <code>camera_type</code> attribute.</p>"},{"location":"python-extensive/oop/#definition-of-the-data-structure","title":"Definition of the Data Structure","text":"<p>The structure of data \u2014 such as data types and dimensions \u2014 can still be freely chosen by the user, which may lead to undesired behavior. For example, the attribute camera_type might be assigned a list (list) instead of a string (str), or resolution might be given a string (str) instead of an integer (int) without any warning about the incorrect input. Also a method could receive an attribute with the correct type but an invalid value (e.g., a negative number where only positives make sense).  </p> Causing errors for others <p>Up until now, you have encountered various different errors. For example, we encountered a <code>NameError</code> when misspelling a  variable name, a <code>TypeError</code> when using an incorrect data type,  or a <code>IndentationError</code> when the code was not properly indented.</p> <p>Now it's your time to raise an error (or often called exception) yourself,  which can be a helpful and informative way to guide the user in case of  incorrect use.  Here is a comprehensive list of all built-in exceptions in Python. </p> <p>To prevent this, data structures can be validated within the class definition. If the input is incorrect, a general error message can be raised using: <code>raise ValueError(\"Error message\")</code>  To specifically check if an attribute is of an incorrect type (e.g., passing a string when a number is expected), you can raise a <code>TypeError</code> with a descriptive message:   <code>raise TypeError(\"Error message\")</code> </p> <p></p> <p>You can check the data type with the command: <code>isinstance(variable, data_type)</code> </p> <p>Possible attributes of a category can also be defined in a list, for example: <code>__camera_types = [\"front\", \"rear\", \"left\", \"right\", \"top\", \"not stated\"]</code> <code>orientation = [\"horizontal\", \"vertical\", \"not stated\"]</code> </p> <p>By validating the data types during object creation, you can ensure the object behaves as expected and avoid unexpected errors later in the program.   </p> <pre><code>class Camera:\n    __camera_types = [\"front\",\"rear\",\"left\",\"right\",\"top\",\"not stated\"]\n\n    # Setting the attributes camera_type, lens_type, resolution and orientation with restrictions\n    def __init__(self, camera_type, lens_type, resolution, orientation=\"not stated\"):\n        if camera_type not in self.__camera_types:\n            raise ValueError(f\"Camera type must be one of {self.__camera_types}.\")  # Check if camera_type is one attribute from list.\n        if not isinstance(lens_type, str):                      \n            raise TypeError(\"Lens type must be a string.\")  # Check if lens_type is a string.\n        if not isinstance(resolution, int):                                                         \n            raise TypeError(\"Resolution must be an integer.\")  # Check if resolution is an integer.\n        if orientation not in [\"horizontal\",\"vertical\",\"not stated\"]:\n            raise ValueError(\"Orientation must be either 'horizontal', 'vertical' or 'not stated'.\")  # Check if orientation is one attribute from list.\n        self.__camera_type = camera_type\n        self.__lens_type = lens_type\n        self.resolution = resolution\n        self.orientation = orientation\n\n    # Creating the method change_camera_type with data type restrictions\n    def change_camera_type(self, camera_type_new):\n        if camera_type_new not in self.__camera_types:\n            raise ValueError(f\"Camera type must be one of {self.__camera_types}.\")\n        self.__camera_type = camera_type_new\n\n    # Creating the method set_orientation with data type restrictions\n    def set_orientation(self, orientation):\n        if orientation not in [\"horizontal\",\"vertical\",\"not stated\"]:\n            raise ValueError(\"Orientation must be either 'horizontal', 'vertical' or 'not stated'.\")\n        self.orientation = orientation\n\n    # Creating the method display\n    def disp(self):\n        print(f\"Object of Camera Class\\n:\")\n        print(f\"Camera Type: {self.__camera_type}\")\n        print(f\"Lens Type: {self.__lens_type}\")\n        print(f\"Resolution: {self.resolution}p\")\n        print(f\"Orientation: {self.orientation}\")\n</code></pre> Data Structure <p>Create two instances of the <code>Camera</code> class. Ensure one is created correctly, and intentionally cause an error with the other.</p>"},{"location":"python-extensive/packages/","title":"Packages","text":""},{"location":"python-extensive/packages/#package-management","title":"Package Management","text":""},{"location":"python-extensive/packages/#introduction","title":"Introduction","text":"<p>One reason, why <code>Python</code> is widespread, is its vibrant community. This community develops code to solve a variety of problems in the widest range of scientific fields. This code is bundled and shared for free (as open-source) in the form of packages. You can download and use these packages. The usage of packages will facilitate your coding process as they offer implementations to solve common problems. Therefore, you won't have to reinvent the wheel.</p> <p>For example the package <code>pandas</code> is the go-to tool for data manipulation and analysis. With <code>pandas</code> you can read text and Excel files   among a lot of other formats and it offers a lot of functionality to manipulate and even plot your data. Hence, you will rarely see <code>Python</code> projects that are not dependent on <code>pandas</code>. Apart from <code>pandas</code> there are a wide variety of popular packages:</p> <ul> <li><code>scipy</code> - statistics (which will be covered in the next   course)</li> <li><code>tqdm</code> - build progress bars</li> <li><code>scikit-learn</code> - for machine learning</li> <li><code>numpy</code> - scientific computing</li> <li>... and many many more</li> </ul> <p>This section serves as a guide on how to install and manage packages. Additionally, the concept of virtual environments is explained.</p>"},{"location":"python-extensive/packages/#standard-library","title":"Standard library","text":"<p><code>Python</code> comes with a couple of modules which do not need to be installed and can be used 'out of the box'. For simplicity, we will call these modules packages as well. If you're interested in the difference between packages and modules, Real Python has a nice article on the topic. Here is an extensive list of all the packages that <code>Python</code> ships with.</p> <p>Let's use the <code>random</code> package to generate some random numbers.  First, we have to import the package with the following command:</p> <pre><code>import random\n\n# with the package imported we can use its functions\n# e.g., random integer (between 1 and 100)\nprint(random.randint(1, 100))\n</code></pre> &gt;&gt;&gt; Output<pre><code>42\n</code></pre> <p>Note, the output will be different when you run the code, since it is random.</p> <p>The corresponding documentation is available here. Generally speaking, almost all packages offer an online documentation page. It is good practice, to consult these documentation sites as they offer a lot of information on how to use their package and which methods/functions are available. Usually, functionalities are illustrated with examples that can be a good starting point for your project.</p> Info <p>If you remember, <code>random</code> was used in one of the previous  sections on control structures to generate  passwords of variable length.</p> Calculate the median <p>Use the built-in <code>statistics</code> package to calculate the median of  the below given values. Use Google to search for the <code>statistics</code>  documentation page and try to find the appropriate function.</p> <pre><code>values = [13, 58, 90, 34, 49, 41]\n</code></pre> <p>We will continue with another exercise.</p> Variance of random values <p>Generate a list of random values (can be integers and/or floats) and  calculate the variance. Hint: Use both the <code>random</code> and  <code>statistics</code> package.</p>"},{"location":"python-extensive/packages/#installing-packages","title":"Installing packages","text":"<p>To get access to all the packages available online, we need to install them using a package manager. One such manager is <code>pip</code> which is  automatically installed alongside <code>Python</code>. To check if <code>pip</code> is available  on your system open a new terminal within VSC by navigating in the menu bar  <code>Terminal</code> <code>New Terminal</code> </p> VSC Terminal <p>and execute the following command:</p> <pre><code>pip\n</code></pre> <p>... you should see a list of commands and their description:</p> <pre><code>Usage:   \n  pip &lt;command&gt; [options]\n\nCommands:\n  install                     Install packages.\n  download                    Download packages.\n  uninstall                   Uninstall packages.\n  freeze                      Output installed packages in requirements format.\n  inspect                     Inspect the python environment.\n  list                        List installed packages.\n  show                        Show information about installed packages.\n...\n</code></pre> Info <p>You can run shell commands directly from your notebook by using an  exclamation mark (<code>!</code>) as a prefix (e.g., <code>!pip</code>). However, in some cases, such as when uninstalling a package, this approach may cause issues.  Therefore, it's often recommended to use the terminal instead.</p> <p>Now, we'll install our first package, called <code>seaborn</code>. To install a package use pip's <code>install</code> command followed by the package name  (<code>pip install &lt;package-name&gt;</code>). Don't worry, it might take a couple of seconds.</p> <pre><code>pip install seaborn\n</code></pre> <p><code>seaborn</code> is a quite common package to visualize data. Now, run the following code to create your first plot. The code snippet was copied from the <code>seaborn</code> documentation here.</p> <pre><code># taken from https://seaborn.pydata.org/examples/grouped_boxplot.html\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\n\n# Load the example tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Draw a nested boxplot to show bills by day and time\nsns.boxplot(x=\"day\", y=\"total_bill\",\n            hue=\"smoker\", palette=[\"m\", \"g\"],\n            data=tips)\nsns.despine(offset=10, trim=True)\n</code></pre> <p>You don't have to fully understand the code snippet. It's more about the successful usage of a package. You might have noticed, that you didn't solely install <code>seaborn</code>. Among <code>seaborn</code>, <code>pip</code> also installed <code>pandas</code> (for data handling). We can 'verify' that by checking the type of <code>tips</code> (from the code snippet above).</p> <pre><code>print(type(tips))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> <p>Most of the time, a package does not 'stand on its own'. It uses the functionalities of other packages as well. In our case, <code>seaborn</code> also needs <code>pandas</code> to properly function. Hence, a lot of packages are dependent on each other.</p> Remove a package <p>Remove the <code>seaborn</code> package. Like above, use <code>pip</code> within a terminal to  list all commands and find the appropriate one. Execute the command  to remove the package.</p>"},{"location":"python-extensive/packages/#pypi","title":"PyPI","text":"<p>You might wonder where <code>pip</code> downloads the packages?! In short, all packages are downloaded from the Python Package Index (PyPI). That's where the open-source community (usually) publishes their packages. Simply put, if you type <code>pip install seaborn</code>, <code>pip</code> looks for a package called <code>seaborn</code> on PyPI and downloads it. <code>PyPI</code> is a valuable resource if you're searching for packages, certain versions, etc.</p>"},{"location":"python-extensive/packages/#virtual-environments","title":"Virtual environments","text":"<p>Previously, we have installed the package <code>seaborn</code>. The package itself was available system-wide as we did not create a virtual environment beforehand. That might not sound too bad, but it's actually considered bad practice. But what is good practice and what the heck is a virtual environment?</p> <p>To answer the latter, simply put, a virtual environment is a folder which encapsulates all packages for a specific project. Each project should have its own virtual environment. With a package manager like <code>pip</code>, you install the necessary packages into the project's virtual environment. <code>pip</code> lets you manage these packages/dependencies.</p>"},{"location":"python-extensive/packages/#why","title":"Why?","text":"<p>To summarize, the <code>pip</code>/virtual environment combination facilitates:</p> <ul> <li>Dependency management: You can keep track of the packages that your   project needs to function.</li> <li>Version management: You can specify the exact versions of a package that   your project needs. This is important, because different versions of a    package may have different functionalities or bugs.</li> <li>Environment management: It's easier to work on multiple projects on a   single machine as you can install multiple versions of a package on a   per-project basis.</li> <li>Shareable: Your projects will be shareable with other developers as they   can easily install all dependencies with a single command. No more it worked   on my machine excuses!</li> </ul> It works on my machine... byu/Shaheenthebean inProgrammerHumor"},{"location":"python-extensive/packages/#how","title":"How?","text":""},{"location":"python-extensive/packages/#create-a-virtual-environment","title":"Create a virtual environment","text":"<p>To create a virtual environment, open a new command prompt within VSCode (you can use the shortcut Ctrl + <code>\u00f6</code>).</p> <p>Execute the following command:</p> <pre><code>python -m venv .venv\n</code></pre> <p>This command creates a new folder structure. The folder is called <code>.venv</code>. Instead of <code>.venv</code> you can choose any name you want. However, this section assumes that you named it <code>.venv</code>.</p>"},{"location":"python-extensive/packages/#activate-a-environment","title":"Activate a environment","text":"<p>Now, we have to activate the environment in order to use it. Depending on your operating system, the command is slightly different.</p> Windows macOS/Linux / <p>As a Windows user type</p> <p><pre><code>.venv\\Scripts\\activate\n</code></pre> <pre><code>Set-ExecutionPolicy Unrestricted -Scope Process\n</code></pre></p> <p>If an error occurs (\"the execution of scripts is deactivated on this  system\") run</p> <p>... and use the previous command again.</p> <p>Type</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>to activate your environment.</p>"},{"location":"python-extensive/packages/#deactivate-a-environment","title":"Deactivate a environment","text":"<p>Deactivating the environment is the same on all operating systems. To deactivate it, simply use</p> <pre><code>deactivate\n</code></pre> <p>in your command prompt/terminal.</p> Fit a machine learning model <p>Assuming your virtual environment is activated, try to get the following code cell running. </p> <pre><code># Taken from https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#train-tree-classifier\n\n# pyplot is a submodule of matplotlib and can be directly imported with the `from` statement\nfrom matplotlib import pyplot \n\n# or you can import functions (like load_iris()) directly from its submodule (datasets)\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\niris = load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nclf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)\n\nplot_tree(clf)\npyplot.show()\n</code></pre> <p>Install the packages <code>matplotlib</code> and <code>scikit-learn</code> with <code>pip</code>. Then try to execute the code cell  (the code was taken from here).</p> <p>Congratulations \ud83c\udf89, you've just fitted a machine learning model (simple decision tree) on a data set and visualized the model. That's the power of <code>Python</code> - easily accessible packages with a lot of functionality ready to use. \ud83e\uddbe</p> <p>Don't worry too much about the actual code lines above. Again, the important thing is to get the code running. With the above exercise, you've  reproduced the result from the motivational section  Why Python?.</p>"},{"location":"python-extensive/packages/#requirementstxt","title":"<code>requirements.txt</code>","text":"<p>In the following exercise, you will learn how to export all your packages  (your project's dependencies) to a file. We will cover a simple command that  facilitates sharing your project/code with co-developers.</p> Export dependencies <p>Assume you want to share the code snippet from the previous task with  someone. First, your colleague might not know which packages you used to  get the code running. With no more information, one has to read the code  and manually determine which packages are necessary. To circumvent such situations, you export all your packages to a file.  Open a command prompt/terminal and execute</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>A <code>requirements.txt</code> is written which contains all your used  packages.</p> <p>Your colleague can now take the file and install all packages needed, at once.</p> <p><pre><code>pip install -r requirements.txt\n</code></pre> ... is the corresponding command.</p> Info <p>A <code>requirements.txt</code> file is a common way to share project dependencies. However, it will also help you, to restore your environment, in case  something goes wrong. Hence, keep your requirements file up-to-date.</p>"},{"location":"python-extensive/packages/#other-choicesoutlook","title":"Other choices?/Outlook","text":"<p>Apart from <code>pip</code> there are a couple of other package managers available. For example, there are</p> <ul> <li><code>uv</code></li> <li><code>pipenv</code></li> <li><code>poetry</code></li> <li><code>miniconda</code></li> </ul> <p>... and this is by no means an extensive list. All of these tools let you install and manage packages. Nevertheless, they have their differences. In the end, it is up to you, the developer which tool fits best. <code>pip</code> is always a solid choice (and the go-to choice to get the hang of package/virtual environment management). However, if you're working on larger scale projects with a couple of other developers, one of these package managers might offer some functionalities which facilitates the development workflow.</p>"},{"location":"python-extensive/packages/#recap","title":"Recap","text":"<p>In this section, you have learned how to install packages and manage them  within virtual environments. The topics covered:</p> <ul> <li><code>pip</code></li> <li>How to install/uninstall packages</li> <li>PyPI - the package hub</li> <li>Concept and benefits of virtual environments</li> <li>Creation and basic usage of a virtual environment</li> </ul>"},{"location":"python-extensive/pandas/","title":"Pandas","text":""},{"location":"python-extensive/pandas/#pandas","title":"<code>pandas</code>","text":""},{"location":"python-extensive/pandas/#introduction","title":"Introduction","text":"Info <p>At the time of writing, <code>pandas</code> version <code>2.2.3</code> was used. Keep in mind, that <code>pandas</code> is actively developed and some functionalities might  change in the future. However, as always, we try to keep the content  up-to-date.</p> <p>This section is heavily based on the excellent 10 minutes to pandas  guide.</p>"},{"location":"python-extensive/pandas/#the-data-set","title":"The data set","text":"<p>We will use a custom Spotify data set, containing the current<sup>1</sup> top 50  songs in Austria. You can find the corresponding playlist here.</p> Info <p>If you're interested in the creation of the data set, you can find the code  below. Note, <code>pandas</code> was the only package needed, and we will cover some of  the used functionalities in this section.</p> Create Spotify data set <pre><code># Create a Spotify data set, containing the top 50 tracks in Austria\n\n# Original data (Top Spotify Songs in 73 Countries (Daily Updated)) from:\n# https://www.kaggle.com/datasets/asaniczka/top-spotify-songs-in-73-countries-daily-updated/data\nfrom pathlib import Path\n\nimport pandas as pd\n\n# read initial data set\ndata = pd.read_csv(Path(\"data/universal_top_spotify_songs.csv\"))\n\n# only Austrian chart topping songs\ndata = data[data[\"country\"] == \"AT\"]\n\n# subset by latest snapshot date\nlatest_snapshot = data[\"snapshot_date\"].max()\ndata = data[data[\"snapshot_date\"] == latest_snapshot]\n# sort by daily_rank\ndata = data.sort_values(by=\"daily_rank\").reset_index(drop=True)\n\n# write data to csv\ndata.to_csv(Path(\"data/spotify-top50.csv\"), index=False)\n\n# excerpt of the data set for inclusion in markdown\nwith Path(\"data/spotify-top50.md\").open(\"w\", encoding=\"UTF-8\") as f:\n    prefix = (\n        f\"Excerpt of the data set (snapshot date: **{latest_snapshot}**):\\n\\n\"\n    )\n    _data = data[\n        [\n            \"daily_rank\",\n            \"name\",\n            \"artists\",\n            \"popularity\",\n            \"is_explicit\",\n            \"energy\",\n        ]\n    ]\n    # only top 5 songs\n    _data = _data.head(5)\n    markdown = _data.to_markdown(index=False)\n    f.write(prefix + markdown)\n</code></pre> <p>Excerpt of the data set (snapshot date: 2024-09-25):</p> daily_rank name artists popularity is_explicit energy 1 The Emptiness Machine Linkin Park 93 True 0.872 2 Rote Flaggen Berq 76 True 0.336 3 Bauch Beine Po Shirin David 80 True 0.746 4 Die With A Smile Lady Gaga, Bruno Mars 100 False 0.592 5 BIRDS OF A FEATHER Billie Eilish 99 False 0.507 <p>Download the whole data set to follow this section:</p> <p>Spotify Top 50 Austria </p>"},{"location":"python-extensive/pandas/#prerequisites","title":"Prerequisites","text":"<p>For this section, we recommend, to make a new project folder with a  Jupyter notebook. Additionally, create a new virtual environment and  activate it. Please refer to the previous section on packages and virtual  environments if you're having trouble. Lastly, install <code>pandas</code>.</p> <p>You should end up with a project structure similar to the following:</p> <pre><code>\ud83d\udcc1 pandas-course/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc4 spotify-top50.csv\n\u2514\u2500\u2500 \ud83d\udcc4 pandas-course.ipynb\n</code></pre>"},{"location":"python-extensive/pandas/#tabular-data","title":"Tabular data","text":"<p>Before we dive into <code>pandas</code>, let's briefly discuss tabular data. At its simplest, tabular data consists of rows and columns. Looking at the Spotify  table above; each row contains information about a specific track  (e.g., <code>name</code>, <code>artists</code>), while each column represents a specific attribute  (e.g., <code>popularity</code>, <code>energy</code>).</p> <p>Tabular data has a clear structure which makes it easy to work with. On the other hand sources for tabular data can be manifold. However, one of the most  common format is the XLSX ( - Excel) or CSV  ( - Comma Separated Values) format which is the one we are working with in this chapter. Nevertheless, tabular data is  also present in various other text based formats like TXT, TSV or even in  databases (e.g. MySQL, PostgreSQL).</p> <p>No matter the source, <code>pandas</code> is the go-to tool to work with tabular data.</p>"},{"location":"python-extensive/pandas/#getting-started","title":"Getting started","text":"<p>Let's explore some of <code>pandas</code> functionalities on the example of the Spotify data set. First, we need to import the package.</p> <pre><code>import pandas as pd\n</code></pre> <p>The <code>as</code> statement is used to create an alias for the package in  order to quickly reference it within our next code snippets. An alias  simply reduces the amount of characters you have to type. Moreover, the  alias <code>pd</code> is commonly used for <code>pandas</code>. Therefore, you can more  easily  employ code snippets you find online.</p>"},{"location":"python-extensive/pandas/#reading-files","title":"Reading files","text":"<p>With the package imported, we can already read the data set (given as <code>.csv</code>).</p> <pre><code>data = pd.read_csv(\"spotify-top50.csv\")\n</code></pre> <p>The above code snippet assumes, that both data set and notebook are located at the same directory level. Else, you have to adjust the path  <code>\"spotify-top50.csv\"</code> accordingly.</p> <p>Besides <code>.csv</code> files, <code>pandas</code> supports reading from various other file types like Excel, text files or a SQL database. The <code>pandas</code> documentation  provides a comprehensive overview of different file types and their corresponding function. Have a look, to get an idea which file formats are supported not  only for reading but also for writing. </p>"},{"location":"python-extensive/pandas/#displaying-data","title":"Displaying data","text":"<p>With a data set at hand, we will most likely want to view it. To view the  first rows of our data frame use the <code>head()</code> method.</p> <pre><code>print(data.head())\n</code></pre> &gt;&gt;&gt; Output<pre><code>               spotify_id                   name  ...    tempo  time_signature\n0  2PnlsTsOTLE5jnBnNe2K0A  The Emptiness Machine  ...  184.115               4\n1  7bkUa9kDFGxgCC7d36dzFI           Rote Flaggen  ...  109.940               3\n2  64f3yNXsi2Vk76odcHCOnw         Bauch Beine Po  ...  123.969               4\n3  2plbrEY59IikOBgBGLjaoe       Die With A Smile  ...  157.969               3\n4  6dOtVTDdiauQNBQEDOtlAB     BIRDS OF A FEATHER  ...  104.978               4\n</code></pre> <p>To display the last rows of the data frame, use the <code>tail()</code> method.</p> <pre><code>print(data.tail())\n</code></pre> &gt;&gt;&gt; Output<pre><code>                spotify_id  ... time_signature\n45  6leQi7NakJQS1vHRtZsroe  ...              4\n46  5E4jBLx4P0UBji68bBThSw  ...              4\n47  6qzetQfgRVyAGEg8QhqzYD  ...              4\n48  3WOhcATHxK2SLNeP5W3v1v  ...              4\n49  7xLbQTeLpeqlxxTPLSiM20  ...              4\n</code></pre> <p>Columns can be viewed with:</p> <pre><code>print(data.columns)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Index(['spotify_id', 'name', 'artists', 'daily_rank', 'daily_movement',\n       'weekly_movement', 'country', 'snapshot_date', 'popularity',\n       'is_explicit', 'duration_ms', 'album_name', 'album_release_date',\n       'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'],\n      dtype='object')\n</code></pre> <p>Similarly, we can print the row indices.</p> <pre><code>print(data.index)\n</code></pre> <p>&gt;&gt;&gt; Output<pre><code>RangeIndex(start=0, stop=50, step=1)\n</code></pre> This data set is consecutively indexed from <code>0</code> to <code>49</code>. If  you recall, a range does not include its <code>stop</code> value (<code>50</code>).</p> <p>By default, (if not otherwise specified) <code>pandas</code> will assign a range index to a data set in order to label the rows.</p> <p>The data set dimensions are accessed with the <code>shape</code> attribute.</p> <pre><code>print(data.shape)\n</code></pre> &gt;&gt;&gt; Output<pre><code>(50, 25)\n</code></pre> <p>The data set has <code>50</code> rows and <code>25</code> columns.</p>"},{"location":"python-extensive/pandas/#data-structures","title":"Data structures","text":"<p><code>pandas</code> has two main data structures: <code>Series</code> and <code>DataFrame</code>. As you would expect, a <code>DataFrame</code> is a two-dimensional data structure such as  our whole Spotify data set assigned to the variable <code>data</code>:</p> <pre><code>print(type(data))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> <p>Whereas, a single column of a <code>DataFrame</code> is referred to as a <code>Series</code>.  Generally, selections of the <code>DataFrame</code> can be accessed with square  brackets (<code>[]</code>). To get a column, you can simply use its name.</p> <pre><code>print(data[\"artists\"])\n\nprint(type(data[\"artists\"]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>0                                       Linkin Park\n1                                              Berq\n2                                      Shirin David\n3                             Lady Gaga, Bruno Mars\n...                                             ...\n\n&lt;class 'pandas.core.series.Series'&gt;\n</code></pre> <p>A <code>DataFrame</code> is composed of at least one <code>Series</code>.</p>"},{"location":"python-extensive/pandas/#detour-series-and-dataframe-from-scratch","title":"Detour: <code>Series</code> and <code>DataFrame</code> from scratch","text":"<p>It's not always the case that you have a data set (in form of a file) at hand. Sometimes you have to create a <code>Series</code> or <code>DataFrame</code> yourself.</p> <p>A <code>Series</code> can be easily created from a list.</p> <pre><code>austrian_artists = [\"Bibiza\", \"Wanda\", \"Bilderbuch\"]\naustrian_artists = pd.Series(austrian_artists)\n</code></pre> <p>To initiate a <code>DataFrame</code>, you can use a dictionary (among others).</p> <pre><code>austrian_artists = {\n    \"name\": [\"Bibiza\", \"Wanda\", \"Bilderbuch\"],\n    \"album\": [\"bis einer weint\", \"Amore\", \"mea culpa\"],\n    \"release_year\": [2024, 2014, 2019]\n}\naustrian_artists = pd.DataFrame(austrian_artists)\n</code></pre> <p>Dictionary keys are used as column names and the corresponding values as the column values.</p> Info <p>Apart from a <code>dict</code>, a <code>DataFrame</code> can be created from multiple other data structures like a <code>list</code> or <code>tuple</code>. For an  extensive guide, visit the <code>pandas</code> documentation on Intro to data  structures (specifically the  section DataFrame). </p>"},{"location":"python-extensive/pandas/#selecting-data","title":"Selecting data","text":"<p>Let's dive deeper into selecting data. To access specific rows, you can use  a slice (just like with lists).</p> <pre><code># rows 5 and 6\nprint(data[5:7])\n</code></pre> &gt;&gt;&gt; Output<pre><code>               spotify_id        name  ...    tempo  time_signature\n5  0io16MKpbeDIdYzmGpQaES  Embrace It  ...  114.933               4\n6  3aJT51ya8amzpT3TKDVipL         FTW  ...   91.937               4\n</code></pre> <p>Select multiple columns by passing a list of column names.</p> <pre><code>print(data[[\"name\", \"artists\"]])\n</code></pre> &gt;&gt;&gt; Output<pre><code>                                  name    artists\n0                The Emptiness Machine    Linkin Park\n1                         Rote Flaggen    Berq\n2                       Bauch Beine Po    Shirin David\n...\n</code></pre>"},{"location":"python-extensive/pandas/#boolean-indexing","title":"Boolean indexing","text":"<p>Most of the time, we want to filter the data based on criterias.  For example, we can select the tracks with a tempo higher than <code>120</code> beats per minute (BPM).</p> <pre><code>high_tempo = data[data[\"tempo\"] &gt; 120]\n</code></pre> <p>Let's break the example down:</p> <ul> <li>First, we select the column <code>tempo</code> from the data set with <code>data[\"tempo\"]</code>.</li> <li>Next, we expand our expression to <code>data[\"tempo\"] &gt; 120</code>.   This will return a <code>Series</code> of boolean values.</li> <li>Lastly, we wrap the expression in another set of square brackets to    filter the whole data set based on our boolean values.</li> </ul> <p>We end up with 27 tracks that meet the criteria. <code>high_tempo</code> is a new  <code>DataFrame</code> containing entries that exceed <code>120</code> BPM.</p> Danceable tracks <p> Saturday Night Fever </p> <p>We assume that tracks with a danceability score higher than <code>0.8</code> are danceable. </p> <p>How many of the tracks are danceable?</p> <p>Danceability describes how suitable a track is for dancing based on a  combination of musical elements including tempo, rhythm stability, beat  strength, and overall regularity. A value of 0.0 is least danceable and  1.0 is most danceable.</p> <p>-- Spotify for Developers</p>"},{"location":"python-extensive/pandas/#mathematical-operations","title":"Mathematical operations","text":"<p><code>pandas</code> supports mathematical operations on both <code>Series</code> and <code>DataFrame</code>. For instance, we weigh the popularity of a track by its energy level.</p> <p>Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. </p> <p>-- Spotify for Developers</p> <pre><code>weighted_popularity = data[\"popularity\"].mul(data[\"energy\"])\nprint(type(weighted_popularity))\n\n# assign the Series to the DataFrame\ndata[\"weighted_popularity\"] = weighted_popularity\n</code></pre> <p>The <code>mul()</code> method is used to multiply the <code>popularity</code> and <code>energy</code> columns. The resulting <code>Series</code> is assigned to <code>data</code> as a new column.</p> &gt;&gt;&gt; Output<pre><code>&lt;class 'pandas.core.series.Series'&gt;\n</code></pre> Track length <p> </p> <p>Since songs are getting shorter and shorter, we want to know how long the tracks in our data set are.  To do so, calculate the length in minutes.</p> <ul> <li>Explore the given data set to find the appropriate column (which    holds information on the song length). </li> <li>Calculate the length in minutes (hint: use the <code>pandas</code> documentation    or Google.)</li> <li>Assign the result to the data frame.</li> <li>Use boolean indexing, to check if there are any tracks longer than    <code>4</code> minutes.</li> <li>Lastly, calculate the average track length in minutes.</li> </ul>"},{"location":"python-extensive/pandas/#basic-statistics","title":"Basic statistics","text":"<p><code>pandas</code> provides a variety of methods to calculate basic statistics. For  instance, <code>min()</code>, <code>max()</code>, <code>mean()</code> can be easily retrieved for a numeric  <code>Series</code> in the data set.</p> <pre><code>print(data[\"tempo\"].min())\n</code></pre> &gt;&gt;&gt; Output<pre><code>80.969\n</code></pre> <p>Conveniently, statistics can be calculated for each column at once using  the <code>DataFrame</code>. In this example, we calculate the standard deviation.</p> <pre><code>print(data.std())\n</code></pre> <p>If you execute the above snippet, a <code>TypeError</code> is raised.</p> Fix the error <p>Try to determine, why the error was raised in the first place. Now, circumvent/fix the error.</p> <p>Hint: Look at the documentation of the <code>std()</code> method and its  parameters.</p> <p>If you want to calculate multiple statistics, you can call the <code>describe()</code>  method.</p> <pre><code>stats = data.describe()\nprint(stats)\n</code></pre> &gt;&gt;&gt; Output<pre><code>       daily_rank  daily_movement  ...       tempo  time_signature\ncount    50.00000        50.00000  ...   50.000000       50.000000\nmean     25.50000         1.04000  ...  125.087260        3.920000\nstd      14.57738         8.14902  ...   26.751323        0.340468\nmin       1.00000       -22.00000  ...   80.969000        3.000000\n25%      13.25000        -3.00000  ...  104.990750        4.000000\n50%      25.50000         1.00000  ...  123.981500        4.000000\n75%      37.75000         3.00000  ...  137.487250        4.000000\nmax      50.00000        29.00000  ...  184.115000        5.000000\n</code></pre> <p><code>describe()</code> provides descriptive statistics for each column. The result of  <code>describe()</code> is a <code>DataFrame</code> itself.</p>"},{"location":"python-extensive/pandas/#other-functionalities","title":"Other functionalities","text":"<p><code>pandas</code> offers a plethora of functionalities. There's simply too much to  cover in a brief introductory section. Still, there are some common  <code>DataFrame</code> methods/properties that are worth mentioning:</p> <ul> <li><code>sort_values()</code>: Sort the data frame by a specific column.</li> <li><code>groupby()</code>: Group the data frame by a column.</li> <li><code>merge()</code>: Merge two data frames.</li> <li><code>T</code>: Transpose of the data frame.</li> <li><code>drop_duplicates()</code>: Remove duplicates.</li> <li><code>dropna()</code>: Remove missing values.</li> </ul> <p>All methods are linked to its corresponding documentation with examples  that help you get started.</p>"},{"location":"python-extensive/pandas/#recap","title":"Recap","text":"<p>We covered <code>pandas</code> and some selected functionalities which should provide  you with a solid foundation to work with tabular data sets. Moreover, you  should be able to follow the code portions in the upcoming courses more easily.</p> <ol> <li> <p>The full data set is available on Kaggle and contains the most streamed songs for multiple different countries. For our purpose, the data was subset.\u00a0\u21a9</p> </li> </ol>"},{"location":"python-extensive/plotting/","title":"Plotting","text":""},{"location":"python-extensive/plotting/#plotting","title":"Plotting","text":"<p>With a couple of practical examples, we will discover tips on how to generate a plot, and outline the various plotting methods and format styles you can explore.</p> Info <p>If you're interested in the creation of the sine-graph above, you can find the code  below. Note, for the data generation the <code>numpy</code> package is used as well as the  <code>matplotlib</code> package for data visualization. </p> <p>We will cover some of the used functionalities and formatting styles in this section.</p> Create Visualization <pre><code># import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definition of variables\nphi_min = 0                # definition of starting angle in degrees\nphi_max = 360              # definition of final angle in degrees\nn = 100                    # number of points\n\n# Calculations and data generation with numpy\n# Gererate time-vector\nt = np.linspace(np.radians(phi_min), np.radians(phi_max), n, endpoint=True)  \ny = np.sin(t)  \n\n# Visualization with matplotlib\nplt.rcParams['text.usetex'] = False    # if True use LATEX font type\n\nplt.figure()\nplt.plot(t, y, 'k')\n\n# Labels for the x- and y-axis\nplt.xlabel(r'Angle $\\theta$ in degrees', fontsize=12) \nplt.ylabel(r'Sine($\\theta$)', fontsize=12)\n\n# Change axes\nstartx, endx = np.radians(phi_min), np.radians(phi_max)\nstarty, endy = -1.1, 1.1\nplt.axis([startx, endx, starty, endy])\n\n# Add grid\nplt.grid()\n\n# Change scale of axes\nax = plt.gca()\naxis_x = np.array([0, 90, 180, 270, 360])\naxis_x = np.radians(axis_x)\nplt.xticks(axis_x, [360, 450, 540, 630, 720])\n\n# Add legend\nax.legend([r\"Sine($\\theta$)\"], loc=\"lower left\", fontsize=13) \n\n# Add title\nplt.title(r\"$sin(\\theta) = cos(\\theta - 90^\\circ)$\", fontsize=24)\n\n# Add text using x- and y-coordinates\nplt.text(3.5, 0.35, r'$1^\\circ=\\frac{2\\pi}{360}~rad$', fontsize=13)\n\n# Show graph\nplt.show()\n</code></pre>"},{"location":"python-extensive/plotting/#introduction","title":"Introduction","text":"Info <p>We will give a brief introduction on plotting data with <code>pandas</code>, which is built on the package <code>matplotlib</code>(the corresponding documentation is available here).</p> <p>This chapter is an extension to the previous <code>pandas</code>  chapter. Therefore, we will use the Spotify data set and we assume that you have imported the data already (to download the data set and reading the file see <code>pandas</code>). </p> <p>This chapter should equip you with the necessary skills to generate various visualizations for your data analysis.  </p>"},{"location":"python-extensive/plotting/#getting-started","title":"Getting started","text":"<p>First, install <code>matplotlib</code> as it is required for <code>pandas</code>' plotting  functionalities. Additionally, you can use <code>matplotlib</code> to customize your  figures, but more on that later. Import both packages to start plotting.</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"python-extensive/plotting/#2-d-plots","title":"2-D plots","text":"<p>Figures can be generated directly using a <code>DataFrame</code>. Simply call its <code>plot()</code> method. The <code>x</code> and <code>y</code> attributes refer to the values of the x and  y-axis. </p> <pre><code>data.plot(x='daily_rank', y='popularity')\n</code></pre> <p>If you visualize a DataFrame, you plot all columns as multiple lines. If the  <code>x</code> values are not explicitly stated, the index of the <code>DataFrame</code> is  utilized. In our case, the index and hence the x-values start with zero and  end with the number of rows minus one (<code>range(0, number of rows)</code>). </p> <pre><code># Mathematical operations (see pandas)\ndata[\"weighted_popularity\"] = data[\"popularity\"].mul(data[\"energy\"]) \ndata_plot = data[['popularity', 'weighted_popularity']]\ndata_plot.plot()\n</code></pre> <p>Note: The <code>plt.show()</code> function in a <code>py</code> script opens one or more interactive windows to show the graphs. For jupyter notebooks the command is not necessary because the graph is enbedded in the document.</p>"},{"location":"python-extensive/plotting/#formatting","title":"Formatting","text":"<p><code>pandas</code> offers a a range of pre-configured plotting styles. You can use plot style arguments to perform format changes by simply adding it to the <code>plot</code> function. There are some common <code>plot</code> arguments that are worth mentioning (for more detail see also the <code>pandas</code> documentation or Google.):</p> <ul> <li><code>style</code>: Color and style of lines or markers (see table below).</li> <li><code>linewidth</code>: Changing thickness of the line. </li> <li><code>legend</code>: Description of the elements in a plot (<code>loc</code> for the location using <code>plt.legend()</code>).</li> <li><code>grid</code>: Adding a grid.</li> <li><code>xlabel</code>, <code>ylabel</code>: Labeling the x- and y-axis.</li> <li><code>xticks</code>, <code>yticks</code>: Changig the annotation of the axis.</li> <li><code>axis</code>: Changing the range of the axis (<code>xlim</code>, <code>ylim</code> individually). </li> <li><code>secondary_y</code>: Adding additional plot.</li> <li><code>subplots</code>: Generating individual plots for each column (<code>layout</code> for the <code>subplots</code>).</li> <li><code>title</code>: Adding a title (set <code>fontsize</code>).</li> <li><code>figsize</code>: Changing the size of the plot.</li> </ul> <p>The following table shows additional arguments for different colors, line styles and marker types.</p> Initials Description Initials Description Initials Description <code>y</code> yellow <code>-</code> solid line <code>+</code> plus-marker <code>m</code> magenta <code>--</code> dashed <code>o</code> circle <code>c</code> cyan <code>:</code> dotted <code>*</code> asterisk <code>r</code> red <code>-.</code> dotdashed <code>.</code> point <code>g</code> green <code>x</code> cross <code>b</code> blue <code>s</code> square <code>w</code> white <code>d</code> diamant <code>k</code> black <code>&gt;</code>, <code>&lt;</code>, <code>^</code>, <code>v</code> triangle"},{"location":"python-extensive/plotting/#mci-wing-formatting-standards","title":"MCI | WING: Formatting standards","text":"Info <p>For laboratory reports and final papers formatting standards exist (see <code>Academic Walkthrough - Formakriterien f\u00fcr schrifliche Abgaben - Abbildungen und Diagramme</code>).  For example:</p> <ul> <li>The line colors are usually set to black or gray with different line styles for black/white printing. </li> <li>The legend is necessary to identify different data series in one graph.</li> <li>For axis labelling the following information is mandatory: the variable name and unit.</li> </ul> <pre><code>data_plot.plot(style=['k-','k--'], xlim = (0,49), ylim=(0, 110), linewidth=0.8, \n               grid=True, xlabel='daily rank', ylabel='popularity points')\nplt.legend(loc='lower right')\n</code></pre> Formatting line plots <p>We want to analyse the tempo of our tracks.</p> <p>Generate a line plot of the tempo (<code>y</code> argument) and the <code>daily_rank</code> as the horizontal axis (<code>x</code> argument). Change the format to the following:</p> <ul> <li>Set the line color to black</li> <li>Delete the legend (hint: set legend to <code>False</code>)</li> <li>Set the labels of the x- and y-axis</li> <li>Set the range of the x-axis from 1 to 50</li> </ul>"},{"location":"python-extensive/plotting/#statistical-plots","title":"Statistical plots","text":"<p><code>pandas</code> supports statistical plots, which present results of the statistical data analysis.  The following table shows some of these plotting methods, which are provided with the <code>kind</code> argument in the <code>plot()</code> function or using the mehod <code>DataFrame.plot.&lt;kind&gt;</code> instead. </p> Initials Description <code>hist</code> histogram (<code>bins</code> change number, <code>density</code> for probability?) <code>scatter</code> scatter plot (two variables representing the x- and y-values) <code>bar</code> bar plot for labeled, not time-series data (<code>stacked</code> for multiple bars/columns) <code>barh</code> horizontal bar plots (also used for gantt charts) <code>box</code> boxplot shows distribution of values within each column (<code>by</code> for grouping) <code>kde</code>, <code>density</code> density plot <code>area</code> area plot <code>pie</code> pie chart (percent of categorical data) <p>Some of the formatting arguments can be used for statistical plots. For a detailed description see the <code>pandas</code>documentation.</p>"},{"location":"python-extensive/plotting/#histogram","title":"Histogram","text":"<p>The histogram counts the number of values in each bin. The range of the bin and the number of bins can be changed using the <code>range</code> and <code>bins</code> argument. Histograms of multiple columns can be drawn at once using <code>subplots</code>. </p> <pre><code>data_plot = data[['liveness', 'acousticness']]\ndata[['liveness', 'acousticness']].plot(kind='hist', layout=(1,2), figsize=(10, 4), subplots=True, \n                                        color='k', alpha=0.5)\n</code></pre> Change arguments <p>Make the following changes to the histogram above and see what happens.</p> <ul> <li>Generate a probability density function (hint: use the argument <code>density</code>).</li> <li>Add the argument <code>bins</code> and change the number of bins to 20.</li> </ul>"},{"location":"python-extensive/plotting/#scatter-plot","title":"Scatter plot","text":"<p>The scatter plot can be used to show correlations between two variables. Therefore, the horizontal (<code>x</code> argument) and vertical (<code>y</code> argument) coordinates are defined by two columns of the <code>DataFrame</code>.</p> <pre><code>data.plot.scatter(x='loudness', y='energy', color='k')\n</code></pre> Scatter plot  <p>Generate a scatter plot to show, if there is a relationship between the variabel <code>speechiness</code>and the variable <code>tempo</code>.</p>"},{"location":"python-extensive/plotting/#detour-data-categorisation-for-statistical-analysis","title":"Detour: Data categorisation for statistical analysis","text":"<p><code>Categorical</code> data in <code>pandas</code> correspond to categorical variables in statistics. This data type has a limited number of possible values, which are called <code>categories</code>. </p> <p>For example, some artists have more than one song in this list. The calculation of the maximum number of tracks one artist has, can be generated as follows.</p> <pre><code>number_artists = pd.Categorical(data['artists']).value_counts()\nprint(number_artists.max())\nprint(number_artists[number_artists == 3])\n</code></pre> &gt;&gt;&gt; Output<pre><code>3\nLACAZETTE    3\nName: count, dtype: int64\n</code></pre> <p>Let's break the example down:</p> <ul> <li>The datatype <code>Categorical</code> shows a list of unique artists (output: 46 different artists).</li> <li>The method <code>value_counts()</code> creates a <code>Series</code> with the number of counts for each artist.</li> <li>The method <code>max()</code> calculates the maximum number of tracks for one artist in the data set.</li> <li>Lastly, we use boolean indexing to show the name of the artist.</li> </ul> <p>The <code>cut()</code> method discretizes data according to intervals (<code>bins</code>) and chosen names (<code>labels</code>).  The <code>DataFrame</code> can easily be extended by the <code>Series</code> with <code>categorical</code> data.</p> <pre><code>data['tempo_cat'] = pd.cut(x=data['tempo'], bins=[0, 110, 140, 200], \n                           labels=['slow', 'medium', 'fast'])\n</code></pre> <p><code>Categorical</code> data can be used for grouping in box plots, as we will see below.</p> <p>Further we can generate a <code>DataFrame</code> which contains the different categories in the first column and the number of counts in the second column using the <code>value_counts()</code> method. </p> <pre><code>data_count = pd.DataFrame(data['tempo_cat'].value_counts())\ndata_count\n</code></pre> &gt;&gt;&gt; Output<pre><code>tempo_cat   count\nmedium      24\nslow        15\nfast        11\n</code></pre> <p>We will see, that this <code>DataFrame</code> can be used for statistical graphs like <code>bar</code> plots or <code>pie</code> charts.</p>"},{"location":"python-extensive/plotting/#box-plot","title":"Box plot","text":"<p>The boxplot is generated to visualize the statistical values for each column of a <code>DataFrame</code>.  <code>pandas</code> also supports many arguments of the <code>matplotlib</code> package for boxplots (look at the <code>matplotlib</code> documentation). </p> <pre><code>data.plot.box(column=['popularity','weighted_popularity'], by='tempo_cat', \n              color='k', ylabel='points', figsize=(10,4))\n</code></pre> Box plot  <p>Generate a box plot to show the difference between the variables <code>acousticness</code>, <code>speechiness</code> and <code>liveness</code>.</p>"},{"location":"python-extensive/plotting/#bar-plot","title":"Bar plot","text":"<p>The bar plot presents rectangular bars, which represent the values of the <code>DataFrame</code> for different categories (<code>x</code> axis).</p> <pre><code>data_count.plot.bar(color='k', alpha=0.5, legend=False, xlabel='tempo', \n                    ylabel='count')\n</code></pre>"},{"location":"python-extensive/plotting/#pie-chart","title":"Pie chart","text":"<p>The pie chart shows the percentage of each category from the absolute values of the count table.  The different formatting styles for the pie chart can be done using the <code>autopct</code> argument (for more information see the <code>matplotlib</code> documentation).  </p> <pre><code>data_count.plot.pie(y='count', autopct=\"%1.0f%%\")\n</code></pre> Danceable tracks <p> Saturday Night Fever </p> <p>We assume that tracks with a danceability score higher than <code>0.8</code> are most danceable and the tracks less than <code>0.7</code> are less danceable. </p> <ul> <li>Categorize the data using the categories <code>less_danceable</code>, <code>danceable</code> and <code>most_danceable</code>.</li> <li>Generate a boxplot to explore the relationship between the <code>tempo</code> grouped by the different categories for <code>danceability</code>.</li> <li>Display the number of tracks for each category in a <code>DataFrame</code> (hint: use the <code>value_counts()</code> method).</li> <li>Visualize the number of tracks for each category with a <code>bar</code> chart.</li> <li>Visualize the number of tracks for each category in a <code>pie</code> chart.</li> </ul>"},{"location":"python-extensive/plotting/#recap","title":"Recap","text":"<p>We provided the basis to generate good looking visualization of your data analysis using <code>pandas</code>.  The introduced functionalities and arguments can be used to change the format of your graphs to your liking. </p>"},{"location":"python-extensive/python-index/","title":"Motivation","text":""},{"location":"python-extensive/python-index/#why-python","title":"Why Python?","text":"<p>This crash course will teach you the basics of the   Python programming language.</p>"},{"location":"python-extensive/python-index/#motivation","title":"Motivation","text":"Creation of a 3D surface plot of the Lattenspitze. \ud83c\udfd4\ufe0f     That's the power of Python - ease of use paired with a wide range of      functionalities stemming from a large developer community! \ud83e\uddbe  <ul> <li> <p> Ease of use</p> <p><code>Python</code> with its syntax is easy to learn and yet very powerful.</p> </li> <li> <p> Flexible</p> <p><code>Python</code> is a versatile language that can be used for web development,  data analysis, artificial intelligence, and many more.</p> </li> </ul> <p>The below section should give you an impression of what you can do with  <code>Python</code>. This is not an extensive list by all means. It might sound  trashy but if you can imagine something you probably can build it in  <code>Python</code>.</p> <p>Don't worry about the code snippets too much, after finishing the  course you'll have a better understanding of them and will be able to run  and modify code yourself. For now, the snippets illustrate the capabilities  of the language and what complex things you can achieve with little code.</p>"},{"location":"python-extensive/python-index/#examples","title":"Examples","text":""},{"location":"python-extensive/python-index/#visualizations","title":"Visualizations","text":"<p>You can create stunning and interactive visualizations<sup>1</sup>.</p> <pre><code># Source: https://plotly.com/python/tile-county-choropleth/\nimport plotly.express as px\n\ndf = px.data.election()\ngeojson = px.data.election_geojson()\n\nfig = px.choropleth_map(\n    df,\n    geojson=geojson,\n    color=\"Bergeron\",\n    locations=\"district\",\n    featureidkey=\"properties.district\",\n    center={\"lat\": 45.5517, \"lon\": -73.7073},\n    map_style=\"carto-positron\",\n    zoom=9,\n)\nfig.show()\n</code></pre>"},{"location":"python-extensive/python-index/#machine-learningai","title":"Machine Learning/AI","text":"<p>... or you can easily train your own machine learning models. In this  example, with just a few lines of code, a decision tree is fit and  visualized<sup>2</sup>.</p> <pre><code># Source: https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#train-tree-classifier\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\niris = load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nclf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)\n\nplot_tree(clf)\nplt.savefig(\"tree.svg\")\n</code></pre>"},{"location":"python-extensive/python-index/#automation","title":"Automation","text":"<p>... or automate tasks that you would have otherwise done manually. This code snippet fetches some data (from an online service) and writes it to a file<sup>3</sup>.</p> <pre><code>import json\nfrom pathlib import Path\n\nimport httpx\n\nurl = \"https://pokeapi.co/api/v2/pokemon/charmander\"\n\nresponse = httpx.get(url)\n\nwith Path(\"charmander.json\").open(\"w\") as file:\n    json.dump(response.json(), file, indent=4)\n</code></pre>"},{"location":"python-extensive/python-index/#web-development","title":"Web Development","text":"<p>You can create websites, just like this one. In fact, all the  heavy lifting of this site is done by <code>Python</code> and tools developed by its  community.</p>"},{"location":"python-extensive/python-index/#getting-started","title":"Getting Started...","text":"<p>In the next sections, we will install <code>Python</code> including the code editor  <code>Visual Studio Code</code>.</p> Info <p>Both Python and Visual Studio Code are already pre-installed on PCs in the MCI computer rooms. If you are working with your own computer,  please proceed to the installation page.</p> <ol> <li> <p>Plotly is a Python graphing library that  lets you create interactive, publication-quality graphs.\u00a0\u21a9</p> </li> <li> <p>Scikit-learn is a Python library  for machine learning.\u00a0\u21a9</p> </li> <li> <p>HTTPX is a Python library to interact  with APIs.\u00a0\u21a9</p> </li> </ol>"},{"location":"python-extensive/streamlit/","title":"Building Web Applications with Streamlit","text":""},{"location":"python-extensive/streamlit/#introduction","title":"Introduction","text":"<p>As the final topic in our <code>Python</code>  course, we introduce <code>streamlit</code>,  a powerful framework for building web applications. It allows you to turn your Python scripts into  interactive web apps with minimal effort, handling most of the complexities of web development in the background. This knowledge will prove invaluable when showcasing your data analysis or machine learning projects.</p> Info <p>At the time of writing, <code>streamlit</code> version <code>1.41.0</code> was used. Keep in mind that <code>streamlit</code> is actively developed and some functionalities might change in the future. However, as always, we try to keep the content up-to-date.</p> <p>This section is based on the excellent Streamlit documentation.</p>"},{"location":"python-extensive/streamlit/#prerequisites","title":"Prerequisites","text":"<p>For this section, it's best to create a new project folder and a fresh virtual environment, then activate it.  Refer back to the packages and virtual environments section if you need a refresher. Next, create a file named <code>app.py</code>.</p> <p>You should end up with a project structure similar to the following:</p> <pre><code>\ud83d\udcc1 streamlit/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u2514\u2500\u2500 \ud83d\udcc4 app.py\n</code></pre> <p>Finaly, install <code>streamlit</code>:</p> <pre><code>pip install streamlit\n</code></pre> <p>To verify the installation, run:</p> <pre><code>streamlit hello\n</code></pre> <p>This command launches Streamlit's built-in demo app in your default web browser.</p> Switch from Jupyter to standalone Python? <p>Up until now, we\u2019ve primarily used Jupyter Notebooks (<code>.ipynb</code>) for experimentation and data analysis. However, to build shareable, user-friendly applications, standalone Python scripts (<code>.py</code>) are typically more suitable. Streamlit enables you to create interactive web apps directly from Python files, adding a professional edge and new level of interactivity to your projects.</p>"},{"location":"python-extensive/streamlit/#your-first-streamlit-app","title":"Your First Streamlit App","text":"<p>Let\u2019s create a basic Streamlit application. In app.py, add the following code:</p> <pre><code>import streamlit as st\n\nst.title(\"My First Streamlit App\")\nst.write(\"Welcome to Streamlit!\")\n</code></pre> <p>To run the app, execute the following command in your terminal:</p> <pre><code>streamlit run app.py\n</code></pre> <p>Streamlit will open a new tab in your web browser and display your app.</p> Hot Reloading <p>Streamlit automatically detects changes in your source code and updates the web  application in real time. If you notice the hot reloading feature isn\u2019t working as expected, use the \"Rerun\" button in the upper-right corner of the Streamlit app or enable the \"Always rerun\" option.</p>"},{"location":"python-extensive/streamlit/#basic-elements","title":"Basic Elements","text":"<p>Streamlit provides various elements to build your web interface. Let's explore some of the most  commonly used ones.</p>"},{"location":"python-extensive/streamlit/#text-elements","title":"Text Elements","text":"<pre><code>import streamlit as st\n\n# Display text\nst.title(\"Main Title\")\nst.header(\"Header\")\nst.subheader(\"Subheader\")\nst.text(\"Simple text\")\nst.markdown(\"**Bold** and *italic* text\")\n\n# Information boxes\nst.info(\"Info message\")\nst.warning(\"Warning message\")\nst.error(\"Error message\")\nst.success(\"Success message\")\n</code></pre>"},{"location":"python-extensive/streamlit/#input-widgets","title":"Input Widgets","text":"<p>Streamlit offers multiple widgets for capturing user input:</p> <pre><code># Text input\nname = st.text_input(\"Enter your name\")\nif name:\n    st.write(f\"Hello, {name}!\")\n\n# Numeric input\nage = st.number_input(\"Enter your age\", min_value=0, max_value=120, value=25)\n\n# Slider\ntemperature = st.slider(\"Select temperature\", min_value=-10.0, max_value=40.0, value=20.0)\n\n# Checkbox\nagree = st.checkbox(\"I agree to the terms\")\nif agree:\n    st.write(\"Thank you for agreeing!\")\n\n# Select box\noption = st.selectbox(\n    \"What's your favorite color?\",\n    [\"Red\", \"Green\", \"Blue\"]\n)\n</code></pre> Temperature Converter <p>Create a simple temperature converter application that:</p> <ol> <li>Accepts temperature input in Celsius via <code>st.number_input</code></li> <li>Converts this temperature to Fahrenheit using \u00b0F = (\u00b0C \u00d7 9/5) + 32</li> <li>Displays the result with <code>st.write</code></li> </ol> <p>The Ultimate temperature conversion guide.... byu/noobmaster69_is_hela inmemes</p>"},{"location":"python-extensive/streamlit/#data-display","title":"Data Display","text":"<p>Streamlit makes it easy to display <code>pandas</code> dataframes and other data structures:</p> <pre><code>import streamlit as st\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\ndata = pd.DataFrame({\n    'Name': ['John', 'Anna', 'Peter'],\n    'Age': [28, 22, 35],\n    'City': ['New York', 'Paris', 'London']\n})\n\n# Display data\nst.dataframe(data)  # Interactive dataframe\nst.table(data)      # Static table\n</code></pre> <p>Furthermore, Streamlit allows us to edit dataframe right in the application: </p> <pre><code># Display data and allow user to edit it\nedited_table = st.data_editor(data)\n\noldest_name = edited_table.loc[edited_table[\"Age\"].idxmax()][\"Name\"]\nst.markdown(f\"The oldest Person is **{oldest_name}** \ud83c\udf88\")\n</code></pre>"},{"location":"python-extensive/streamlit/#charts-and-plots","title":"Charts and Plots","text":"<p>Streamlit seamlessly integrates with plotting libraries such as Matplotlib, Seaborn, or Plotly (make sure to install these libraries first).</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Generate sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Matplotlib\nfig, ax = plt.subplots()\nax.plot(x, y)\nst.pyplot(fig)\n\n# Seaborn\nsns.set_theme()\ntips = sns.load_dataset(\"tips\")\nfig = plt.figure(figsize=(10, 6))\nsns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\")\nst.pyplot(fig)\n</code></pre> Interactive Plots <p>Streamlit also supports interactive plotting libraries like Plotly:</p> <pre><code>import plotly.express as px\n\nfig = px.scatter(tips, x=\"total_bill\", y=\"tip\", color=\"size\")\nst.plotly_chart(fig)\n</code></pre>"},{"location":"python-extensive/streamlit/#layout-and-containers","title":"Layout and Containers","text":"<p>Streamlit provides several options to manage your layout and organize the user interface.</p>"},{"location":"python-extensive/streamlit/#columns","title":"Columns","text":"<pre><code>col1, col2 = st.columns(2)\n\nwith col1:\n    st.header(\"Column 1\")\n    st.write(\"This is the first column\")\n\nwith col2:\n    st.header(\"Column 2\")\n    st.write(\"This is the second column\")\n</code></pre>"},{"location":"python-extensive/streamlit/#tabs","title":"Tabs","text":"<pre><code>tab1, tab2 = st.tabs([\"Tab 1\", \"Tab 2\"])\n\nwith tab1:\n    st.header(\"Tab 1 Content\")\n    st.write(\"This is the first tab\")\n\nwith tab2:\n    st.header(\"Tab 2 Content\")\n    st.write(\"This is the second tab\")\n</code></pre>"},{"location":"python-extensive/streamlit/#expanders","title":"Expanders","text":"<pre><code>with st.expander(\"Click to expand\"):\n    st.write(\"This content is hidden by default\")\n    st.image(\"https://upload.wikimedia.org/wikipedia/commons/7/7d/Seal_point_Sphynx_Kitten.jpg\")\n</code></pre> Did You Know? <p>Streamlit automatically adjusts its layout to fit different screen sizes, ensuring your app looks great on both desktop and mobile devices.</p>"},{"location":"python-extensive/streamlit/#file-upload-and-download","title":"File Upload and Download","text":"<p>Streamlit makes it easy to handle file uploads and downloads:</p> <pre><code># File upload\nuploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\nif uploaded_file is not None:\n    data = pd.read_csv(uploaded_file)\n    st.write(data)\n\n# File download\nst.download_button(\n    label=\"Download data as CSV\",\n    data=data.to_csv(index=False),\n    file_name='sample_data.csv',\n    mime='text/csv',\n)\n</code></pre> CSV Data Analyzer <p>Create a Streamlit application that:</p> <ol> <li>Lets users upload the 'Student Data' CSV file from before</li> <li>Displays basic statistics using <code>df.describe()</code></li> <li>Creates a line chart based on the 'Student Data' dataset using ploty and visualize it in the streamlit application</li> </ol> <p>Bonus: Add error handling for invalid file formats and empty data frames.</p>"},{"location":"python-extensive/streamlit/#optional-session-state","title":"Optional: Session State","text":"<p>Streamlit is stateless by default, meaning it reruns your entire script on any user interaction.  Session state allows you to persist data between reruns:</p> <pre><code>if 'counter' not in st.session_state:\n    st.session_state.counter = 0\n\nif st.button('Increment'):\n    st.session_state.counter += 1\n\nst.write('Counter:', st.session_state.counter)\n</code></pre> When to Use Session State <p>Use session state when you need to:</p> <ul> <li>Persist data between reruns</li> <li>Share data between different parts of your app</li> <li>Implement counters or progress tracking</li> <li>Store user preferences</li> </ul>"},{"location":"python-extensive/streamlit/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Performance</p> <ul> <li>Cache expensive computations using <code>@st.cache_data</code></li> <li>Use appropriate container widgets to organize your layout</li> <li>Minimize the use of heavy computations in the main thread</li> </ul> </li> <li> <p>User Experience</p> <ul> <li>Add proper error handling</li> <li>Include loading indicators for long operations</li> <li>Provide clear instructions and feedback</li> <li>Use appropriate input validation</li> </ul> </li> <li> <p>Code Organization</p> <ul> <li>Split your code into logical functions</li> <li>Use config files for constants</li> <li>Follow Python naming conventions</li> </ul> </li> </ol>"},{"location":"python-extensive/streamlit/#example-data-dashboard","title":"Example: Data Dashboard","text":"<p>Let's create a simple dashboard that combines various Streamlit features:</p> <pre><code>import streamlit as st\nimport pandas as pd\nimport plotly.express as px\nimport seaborn as sns\n\n# Page config\nst.set_page_config(page_title=\"Data Dashboard\", layout=\"wide\")\n\n# Title\nst.title(\"\ud83d\udcca Data Dashboard\")\n\n# Sidebar\nst.sidebar.header(\"Settings\")\ndataset = st.sidebar.selectbox(\n    \"Select Dataset\",\n    [\"Iris\", \"Diamonds\", \"Tips\"]\n)\n\n# Load data\n@st.cache_data\ndef load_data(dataset_name):\n    if dataset_name == \"Iris\":\n        return sns.load_dataset(\"iris\")\n    elif dataset_name == \"Diamonds\":\n        return sns.load_dataset(\"diamonds\")\n    else:\n        return sns.load_dataset(\"tips\")\n\ndata = load_data(dataset)\n\n# Display data overview\ncol1, col2 = st.columns(2)\n\nwith col1:\n    st.subheader(\"Data Preview\")\n    st.dataframe(data.head())\n\nwith col2:\n    st.subheader(\"Basic Statistics\")\n    st.dataframe(data.describe())\n\n# Visualizations\nst.subheader(\"Data Visualization\")\ntab1, tab2 = st.tabs([\"Distribution\", \"Relationships\"])\n\nwith tab1:\n    numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n    col = st.selectbox(\"Select Column\", numeric_cols)\n    fig = px.histogram(data, x=col)\n    st.plotly_chart(fig, use_container_width=True)\n\nwith tab2:\n    x_col = st.selectbox(\"Select X axis\", numeric_cols, key=\"x\")\n    y_col = st.selectbox(\"Select Y axis\", numeric_cols, key=\"y\")\n    fig = px.scatter(data, x=x_col, y=y_col)\n    st.plotly_chart(fig, use_container_width=True)\n</code></pre>"},{"location":"python-extensive/streamlit/#deployment","title":"Deployment","text":"<p>Streamlit applications can be deployed in various ways:</p> <ol> <li> <p>Streamlit Cloud (Recommended for small projects) </p> <ul> <li>Push your code to a GitHub repository.</li> <li>Log in to Streamlit Cloud.</li> <li>Connect your repository and deploy the app.</li> </ul> </li> <li> <p>Docker </p> </li> <li> <p>Kubernetes </p> </li> </ol> Production Deployment <p>When deploying to production:</p> <ul> <li>Use requirements.txt or Poetry for dependency management</li> <li>Set up proper environment variables</li> <li>Configure authentication if needed</li> <li>Monitor application performance</li> <li>Set up error logging</li> </ul>"},{"location":"python-extensive/streamlit/#recap","title":"Recap","text":"<p>In this chapter, we covered the fundamentals of building web applications with Streamlit. We  explored:</p> <ul> <li>Basic Streamlit elements and widgets</li> <li>Data visualization and interaction</li> <li>Layout management</li> <li>File handling</li> <li>Session state</li> <li>Best practices</li> <li>Deployment options</li> </ul> <p>With these skills, you can now transform your Python scripts into interactive web applications  that can be shared with others.</p> \ud83c\udf89 <p>Congratulations! You've completed the Streamlit chapter. You're now equipped to create interactive web applications using Python.</p> <p> </p>"},{"location":"python-extensive/tkinter/","title":"Graphical User Interfaces with Tkinter","text":""},{"location":"python-extensive/tkinter/#introduction","title":"Introduction","text":"<p>A Graphical User Interface (GUI) allows users to interact with a program through visual elements like buttons, text fields, and menus, rather than typing commands in a terminal. GUIs thus make applications more accessible to non-technical users.</p> <p></p>"},{"location":"python-extensive/tkinter/#how-guis-relate-to-oop","title":"How GUIs relate to OOP?","text":"<p>Each GUI element (widget), like a button or text field, can be thought of as an object of the corresponding class. These objects:    </p> <ul> <li>have attributes (e.g., size, color, labels),</li> <li>have methods (e.g., click, enable/disable), and</li> <li>interact with each other through events,</li> </ul> <p>following the principles of object-oriented programming (OOP).</p> <p>Tkinter is the standard <code>Python</code>  library for creating graphical user interfaces. It provides a practical introduction to GUI development and event-driven programming. By using Tkinter, interactive applications such as calculators, form-based tools, or simple games, can be build. The Tkinter documentation can be found here:   Nevertheless, there are many other GUI libraries that you could be interested in. The <code>Python</code>  wiki lists several alternative GUI frameworks and tools.</p>"},{"location":"python-extensive/tkinter/#creating-a-basic-tkinter-application","title":"Creating a Basic Tkinter Application","text":""},{"location":"python-extensive/tkinter/#the-tkinter-main-loop","title":"The Tkinter Main Loop","text":"<p>The root window is responsible for running the main event loop (<code>mainloop()</code>), which keeps the GUI application running, listens for events (e.g., clicks, keystrokes), and updates the interface. The main event loop ends when the window is closed. Without the main loop, the GUI application would close immediately after being displayed. <pre><code>import tkinter as tk\n\n# Create root window\nroot = tk.Tk()\nroot.title(\"My First GUI Program\")\nroot.minsize(width=300, height=400)\n\n# Running the main loop event\nroot.mainloop()\n</code></pre></p>"},{"location":"python-extensive/tkinter/#widgets-and-controls","title":"Widgets and Controls","text":"<p>Widgets and controls are the building blocks of a Tkinter GUI. Widgets are structured in a hierarchy, where some widgets serve as containers (parent widgets - e.g., windows or frames) that hold other widgets (child widgets). This hierarchical arrangement defines the relationships and organization of widgets, influencing their layout, behavior, and interaction.   They are placed using a layout manager (place, pack, or grid).   Widgets feature attributes like <code>text</code>, <code>font</code>, <code>bg</code> (background color), <code>fg</code> (foreground color), and <code>width</code>, which can be set during creation or updated later. </p> <p>Collection of Different Widgets:  The following code provides a collection of different widgets: <code>Label</code>, <code>Input</code>, <code>Button</code>, <code>Text</code>, <code>Spinbox</code>, <code>Scale</code>, <code>Checkbutton</code>, <code>Radiobutton</code>, and <code>Listbox</code>.  Each widget is an object of its class (e.g., button, label) and is part of the same parent widget (<code>root</code> window).</p> <p>Widget Callbacks:  Some widgets support event handling. Each widget's behavior is managed by a specific method:    </p> <ul> <li>Button: <code>button_clicked()</code></li> <li>Spinbox: <code>spinbox_used()</code></li> <li>Scale: <code>scale_used(value)</code></li> <li>Checkbutton: <code>checkbutton_used()</code></li> <li>Radiobutton: <code>radio_used()</code></li> <li>Listbox: <code>listbox_used(event)</code></li> </ul> <p>Dynamic Behavior:   Widgets like the spinbox, scale, and listbox dynamically print or process user interactions.</p> <p>Event Binding:   The <code>listbox</code> widget uses the <code>bind</code> method to associate the selection event with the <code>listbox_used</code> method.</p> Run the Code <p>Interact with the widgets to see their behavior in action (check the console for printed outputs).</p> <pre><code>import tkinter as tk\n\n# Set window attributes\nroot = tk.Tk()\nroot.title(\"My First GUI Program and Widgets\")\nroot.geometry(\"1024x768\")\n\n# Label\nmy_label = tk.Label(text=\"My First Label\", font=(\"Arial\", 24, \"bold\"))\nmy_label.pack()\n\n# Input\nmy_input = tk.Entry(width=10)\nmy_input.pack()\n\n# Button callback\ndef button_clicked():\n    print(\"Do Something Cool\")\n# Button\nmy_button = tk.Button(text=\"Click Me\", command=button_clicked)\nmy_button.pack()\n\n# Text\ntext = tk.Text(height=5, width=30)\ntext.focus()\ntext.insert(tk.END, \"My first try on a multi-line text entry.\")\nprint(text.get(\"1.0\", tk.END))  # Prints the current text starting from line 1, character 0 (\"1.0\" == the very beginning of the text widget)\ntext.pack()\n\n# Spinbox callback\ndef spinbox_used():\n    print(spinbox.get())\n# Spinbox\nspinbox = tk.Spinbox(from_=0, to=3, width=5, command=spinbox_used)\nspinbox.pack()\n\n# Scale callback\ndef scale_used(value):\n    print(value)\n# Scale\nscale = tk.Scale(from_=0, to=1000, command=scale_used)\nscale.pack()\n\n# Checkbutton callback\ndef checkbutton_used():\n    print(checked_state.get())\n# Checkbutton\nchecked_state = tk.IntVar()  # Variable to hold the checked state\ncheckbutton = tk.Checkbutton(text=\"Is This On?\", variable=checked_state, command=checkbutton_used)\ncheckbutton.pack()\n\n# Radiobutton callback\ndef radio_used():\n    print(radio_state.get())\n# Radiobutton\nradio_state = tk.IntVar()  # Variable to hold the radio button selection\nradiobutton1 = tk.Radiobutton(text=\"Answer1\", value=1, variable=radio_state, command=radio_used)\nradiobutton2 = tk.Radiobutton(text=\"Answer2\", value=2, variable=radio_state, command=radio_used)\nradiobutton1.pack()\nradiobutton2.pack()\n\n# Listbox callback\ndef listbox_used(event):\n    print(listbox.get(listbox.curselection()))\n# Listbox\nlistbox = tk.Listbox(height=4)\nfor item in [\"Red\", \"Green\", \"Blue\", \"Yellow\"]:\n    listbox.insert(tk.END, item)\nlistbox.bind(\"&lt;&lt;ListboxSelect&gt;&gt;\", listbox_used)\nlistbox.pack()\n\nroot.mainloop()\n</code></pre>"},{"location":"python-extensive/tkinter/#tkinter-layout-manager","title":"Tkinter Layout Manager","text":"<p>The layout of a graphical user interface is largely determined by the arrangement and design of the GUI components. Tkinter provides three different layout managers that automatically generate the layout based on the settings defined in the program. Without using a layout manager, the GUI element will not appear on the screen.</p> <ol> <li>Place Manager: Positions components based on specific coordinates (x, y), offering precise control over placement.</li> <li>Pack Manager: Automatically arranges components in a container, either vertically or horizontally, based on the order they are added.</li> <li>Grid Manager: Organizes components in a grid with rows and columns, allowing more complex layouts with alignment options.</li> </ol> <p>Key Concepts of the Grid Layout Manager:   </p> <ol> <li> <p>Rows and Columns:</p> <ul> <li>The grid is organized into rows and columns. You can place widgets in a specific row and column using the <code>grid(row, column)</code> method.</li> <li>You can also span multiple rows or columns using <code>rowspan</code> and <code>columnspan</code> attributes.</li> </ul> </li> <li> <p>Control Placement:</p> <ul> <li>You can specify the row, column, and sticky options to control widget alignment (e.g., top, bottom, left, right).</li> </ul> </li> <li> <p>Column and Row Configuration:</p> <ul> <li>Tkinter allows you to configure the weight of rows and columns to define how they should expand when the window is resized. You can do this using <code>grid_columnconfigure()</code> and <code>grid_rowconfigure()</code>.</li> </ul> </li> </ol> <pre><code>import tkinter as tk\n\nroot = tk.Tk()\nroot.title(\"Grid Layout Example\")\n\n# Label in the first row and first column\nname_label = tk.Label(text=\"Name:\")\nname_label.grid(row=0, column=0)\n\n# Entry in the first row and second column\nname_entry = tk.Entry()\nname_entry.grid(row=0, column=1)\n\n# Label in the second row and first column\nage_label = tk.Label(text=\"Age:\")\nage_label.grid(row=1, column=0)\n\n# Entry in the second row and second column\nage_entry = tk.Entry()\nage_entry.grid(row=1, column=1)\n\n# Button in the third row, spanning both columns\nsubmit_button = tk.Button(text=\"Submit\")\nsubmit_button.grid(row=2, columnspan=2)  # Spanning both columns\n\nroot.mainloop()\n</code></pre> Tkinter Hints and Suggestions <ol> <li> <p>Modularize Code</p> <ul> <li>Break the GUI into smaller classes or methods, particularly for large application.</li> <li>Use reusable components: e.g., set an <code>InputForm</code> class for repeated input forms.</li> </ul> </li> <li> <p>Use Meaningful Variable and Method Names</p> <ul> <li>Name widgets and methods clearly to reflect their purpose. Avoid generic names like <code>button1</code> or <code>label1</code>, use for example <code>submit_button</code> instead.</li> </ul> </li> <li> <p>Avoid Hardcoding Layouts</p> <ul> <li>Use layout managers (<code>pack</code> or <code>grid</code>) instead of coding absolute positions (<code>place(x, y)</code>).</li> <li>Avoid mixing layout manager in the same container.</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Add errror handling for user input or unexpected behavior.</li> <li>Provide clear instructions and feedback.</li> </ul> </li> </ol>"},{"location":"python-extensive/tkinter/#example-create-your-own-to-do-list-application","title":"Example: Create Your Own To-Do List Application","text":"<p>The following code provides the basis for a simple To-Do List application with the following key features:</p> <ul> <li>Add Task: Enter tasks using the <code>Entry</code> widget and add them to the <code>Listbox</code>.</li> <li>Cross Out Task: Mark the selected task with a \"\u2714\" symbol to indicate completion.</li> </ul> To-Do List Enhancements <p>Customize and enhance the To-Do List app with the following features:       1. Change the color scheme of the app.        2. Add a <code>Button</code> to entirely delete tasks from the <code>Listbox</code>.        3. Position the <code>delete_button</code> next to the <code>cross_out_button</code> and use a red color.      </p> <p>Bonus: Highlight important tasks by adding features like colored text or checkboxes to mark tasks as important.</p> <pre><code>import tkinter as tk\n\n# Create the main window\nroot = tk.Tk()\nroot.title(\"To-Do List\")\nroot.minsize(height=400, width=400)\nroot.configure(bg=\"#d0ebff\")  # Light blue background\n\n# Function to add a new task\ndef add_task():\n    task = task_entry.get()\n    if task.strip():\n        task_listbox.insert(tk.END, task)\n        task_entry.delete(0, tk.END)\n\n# Function to cross out a selected task\ndef cross_task():\n    selected = task_listbox.curselection()\n    if selected:\n        current_task = task_listbox.get(selected)\n        task_listbox.delete(selected)\n        task_listbox.insert(selected, f\"\u2714 {current_task}\")\n\n# Entry box for adding tasks\ntask_entry = tk.Entry(width=30, font=(\"Helvetica\", 14))\ntask_entry.grid(row=0, column=1, padx=10, pady=10)\n\n# Add Task button\nadd_button = tk.Button(text=\"Add Task\", font=(\"Helvetica\", 12), bg=\"#b0e0e6\", fg=\"black\", command=add_task)\nadd_button.grid(row=0, column=0, padx=10, pady=10)\n\n# Listbox to display tasks\ntask_listbox = tk.Listbox(width=40, height=15, font=(\"Helvetica\", 12), selectbackground=\"#b3e5fc\", selectforeground=\"black\")\ntask_listbox.grid(row=1, column=0, columnspan=2, padx=10, pady=10)\n\n# Cross Out Task button (centered)\ncross_out_button = tk.Button(text=\"Cross Out Task\", font=(\"Helvetica\", 12), bg=\"#00264d\", fg=\"white\", command=cross_task)\ncross_out_button.grid(row=2, column=0, columnspan=2, padx=10, pady=10)\n\n# Start the main loop\nroot.mainloop()\n</code></pre> <p></p>"},{"location":"python-extensive/variables/","title":"Variables","text":""},{"location":"python-extensive/variables/#variables","title":"Variables","text":"Info <p>The structure of following sections is based on the official Python tutorial.</p> <p>Many thanks to @jhumci for providing the initial resource materials!</p>"},{"location":"python-extensive/variables/#getting-started","title":"Getting started","text":"<p>    Important    </p> <p>We encourage you to execute all upcoming code snippets on your machine. You can easily copy each code snippet to your clipboard, by clicking the icon in the top right corner. By doing so, you will be prepared for all upcoming tasks within the sections. Tasks are indicated by a -icon.</p> <p>We recommend to create a new notebook for each chapter, e.g. create <code>variables.ipynb</code> for this chapter. Doing so, your notebooks will follow the structure of this crash course.</p> <p>You will encounter multiple info boxes   throughout the course. They provide additional information, tips, and tricks.  Be sure to read them thoroughly.</p> <p>Let's start with the first task.</p> Notebook &amp; first code cell <p>Create a new Jupyter notebook and name it <code>variables.ipynb</code>. Paste the following code snippet into a new cell and execute it.</p> <pre><code>print(\"Hello World!\")\n</code></pre> <p>The output should be:</p> &gt;&gt;&gt; Output<pre><code>Hello World!\n</code></pre> Info <pre><code>print(1+1)\n</code></pre> <p>The upcoming content contains a lot of code snippets. They are easily  recognizable due to their colourful syntax highlighting, such as:</p> <p>Code snippets are an integral part, to illustrate concepts, which are  introduced and explained along the way. Commonly, these code snippets are accompanied by an output block to display the result, for instance:</p> &gt;&gt;&gt; Output<pre><code>2\n</code></pre> <p>Nevertheless note, output blocks can be missing as there is not always  an explicit result.</p> <p>Again, execute and experiment with all code snippets on your machine to  verify the results and get familiar with <code>Python</code> !</p>"},{"location":"python-extensive/variables/#variable","title":"Variable","text":"<p>Computers can store lots of information. To do so, in <code>Python</code>  we use variables. A variable is a name that refers to a value. The following code snippet, assigns the value <code>4</code> to the variable <code>number</code>. In  general, you pick the variable name on the left hand side, assign a value  with <code>=</code> and the value itself is on the right hand side.</p> <pre><code>number = 4\n</code></pre> <p>You can change the value of a variable in your program at any time, and Python will always keep track of its current value.</p> <pre><code>number = 4\nnumber = 4000\n</code></pre> <p>You will notice that none of the cells had any output. To display the value of a variable we use the <code>print()</code> function.</p>"},{"location":"python-extensive/variables/#print","title":"<code>print()</code>","text":"<pre><code>number = 4\nprint(number)\n</code></pre> &gt;&gt;&gt; Output<pre><code>4\n</code></pre> <p>Now, we can also verify that in the above snippet the value of <code>number</code> was actually changed.</p> <pre><code>number = 4\nprint(number)\nnumber = 4000\nprint(number)\n</code></pre> &gt;&gt;&gt; Output<pre><code>4\n4000\n</code></pre> Info <p>Within a notebook, the variables are stored in the background and can be  overwritten at any time. Therefore, it is good practice to execute all  cells from top to bottom of the notebook in the right order so that  nothing unexpected is stored in a variable.</p>"},{"location":"python-extensive/variables/#comments","title":"Comments","text":"<p>Comments exist within your code but are not executed. They are used to describe your code and are ignored by the <code>Python</code> interpreter. Comments are prefaced by a <code>#</code>.</p> <pre><code># this is a comment\nprint(\"Hello World!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello World!\n</code></pre> Info <p>Comments help you and others understand what your code is doing. It is good practice to use comments as a tool for documentation.</p>"},{"location":"python-extensive/variables/#variable-naming","title":"Variable naming","text":"<p>When you\u2019re using variables in <code>Python</code>, you need to adhere to a few rules and guidelines. Breaking some of these rules will cause errors; other guidelines just help you write code that\u2019s easier to read and understand. Be sure to keep the following rules in mind:</p> <ul> <li>Variable names are lower case and can contain only letters, numbers, and   underscores.   They can start with a letter or an underscore, but not with a number.   For instance, you can call a variable <code>message_1</code> but not <code>1_message</code>.</li> <li>Whitespace is not allowed in variable names, but an underscores <code>_</code> can be   used to separate words in variable names. For example, <code>greeting_message</code>   works, but <code>greeting message</code> won't.</li> <li>Avoid using <code>Python</code> keywords and function names as variable names;   that is, do not use words that <code>Python</code> has reserved for a particular   programmatic purpose, such as the word <code>print</code>.</li> <li>Variable names should be short but descriptive. For example, <code>name</code> is better   than <code>n</code>, <code>student_name</code> is better than <code>s_n</code>, and <code>name_length</code> is better   than <code>length_of_persons_name</code>.</li> </ul>"},{"location":"python-extensive/variables/#errors-nameerror","title":"Errors (<code>NameError</code>)","text":"<p>Every programmer makes mistakes and even after years of experience, mistakes are part of the process. With time, you get more efficient in debugging  (=process of finding and fixing errors).</p> Debugging tactics byu/0ajs0jas inProgrammerHumor <p>Let\u2019s look at an error you\u2019re likely to make early on and learn how to fix it. We\u2019ll write some code that throws an error message on purpose. Copy the code and run your cell.</p> <pre><code>message = \"Hello Python Crash Course reader!\"\nprint(mesage)\n</code></pre> <p>Which should result in:</p> <pre><code>NameError: name 'mesage' is not defined\n</code></pre> <p>When an error occurs in your program, the <code>Python</code> interpreter does its best to help you figure out where the problem is. The interpreter provides a traceback which is a record of where the interpreter ran into trouble when trying to execute your code. Here\u2019s an example of the traceback that <code>Python</code> provides, after you\u2019ve accidentally misspelled a variable\u2019s name:</p> <pre><code>Traceback (most recent call last):\n  File \"C:\\\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-35-c8f2adeaed02&gt;\", line 2, in &lt;module&gt;\n    print(mesage)\n          ^^^^^^\nNameError: name 'mesage' is not defined. Did you mean: 'message'?\n</code></pre> <p>The output reports that an error occurs in line 2. The interpreter shows this line to help us spot the error quickly and tells us what kind of error it found. In this case, it found a <code>NameError</code> and reports that the variable <code>mesage</code> has not been defined. A name error usually means we made a spelling mistake when entering the variable\u2019s name or that the variable simply does not exist.</p> Your first fix <p>Fix the <code>NameError</code> in your code cell.</p>"},{"location":"python-extensive/variables/#recap","title":"Recap","text":"<p>In this section, we have covered variables in <code>Python</code> .</p> <p>You have learned (about):</p> <ul> <li>To create and assign a value to a variable</li> <li><code>print()</code> to display the value of a variable</li> <li>Comments</li> <li>Naming conventions for variables</li> <li>How to fix a <code>NameError</code></li> </ul>"},{"location":"python-extensive/containers/dict/","title":"Dict","text":""},{"location":"python-extensive/containers/dict/#dictionaries","title":"Dictionaries","text":"<p>In this section, you\u2019ll learn how to use dictionaries which allow you to connect pieces of related information. Dictionaries let you model a variety of real-world objects more accurately. We will create, modify and  access elements of a dictionary.</p>"},{"location":"python-extensive/containers/dict/#creating-a-dictionary","title":"Creating a dictionary","text":"<pre><code>experiment = {\"description\": \"resource optimization\"}\nprint(type(experiment))\n</code></pre> <p>Above code snippet creates a simple dictionary and prints its type:</p> &gt;&gt;&gt; Output<pre><code>&lt;class 'dict'&gt;\n</code></pre> <p>In <code>Python</code>, a dictionary is wrapped in curly braces (<code>{}</code>), with a series  of key-value pairs inside the braces. Each key is connected to a value, and  you can use a key to access the value associated with that key.  A key\u2019s value can be any type, like a string, integer, list, or even  another dictionary. In the above example, the key is <code>\"description\"</code>  and its value <code>\"resource optimization\"</code></p> <p>Every key is connected to its value by a colon. Individual key-value pairs are separated by commas. You can store as many key-value pairs as you want in a dictionary.</p> <pre><code>experiment = {\n    \"description\": \"resource optimization\",\n    \"sample_weight_in_grams\": 5,\n}\n</code></pre>"},{"location":"python-extensive/containers/dict/#accessing-values","title":"Accessing values","text":"<p>To get the value associated with a key, give the name of the dictionary and then place the key inside a set of square brackets.</p> <pre><code>experiment = {\"sample_weight_in_grams\": 5}\nprint(experiment[\"sample_weight_in_grams\"])\n</code></pre> &gt;&gt;&gt; Output<pre><code>5\n</code></pre> Create a dictionary <p>Manage the cost of raw materials in a dictionary. The dictionary should  contain the following key-value pairs:</p> <ul> <li><code>\"steel\"</code>: <code>100</code></li> <li><code>\"aluminium\"</code>: <code>150</code></li> <li><code>\"copper\"</code>: <code>200</code></li> <li><code>\"plastic\"</code>: <code>50</code></li> </ul> <p>Create the dictionary and print the price of copper.</p>"},{"location":"python-extensive/containers/dict/#adding-key-value-pairs","title":"Adding key-value pairs","text":"<p>You can add new key-value pairs to a dictionary at any time. For example,  to add a new key-value pair, you would give the name of the dictionary followed by the new key in square brackets along with the new value.</p> <pre><code>experiment = {}\nexperiment[\"description\"] = \"resource optimization\"\nprint(experiment)\n</code></pre> <p>In the above example, we start with an empty dictionary and add a key-value pair to it.</p> &gt;&gt;&gt; Output<pre><code>{'description': 'resource optimization'}\n</code></pre> <p>However, we can't add the same key a second time to the dictionary. Every key is unique within the dictionary.</p>"},{"location":"python-extensive/containers/dict/#modifying-values","title":"Modifying values","text":"<p>Values can be overwritten:</p> <pre><code>experiment = {\"sample_weight_in_grams\": 10}\nexperiment[\"sample_weight_in_grams\"] = 10.2\n</code></pre>"},{"location":"python-extensive/containers/dict/#removing-key-value-pairs","title":"Removing key-value pairs","text":"<p>We can remove key-value-pairs using the key and the <code>del</code> statement:</p> <pre><code>experiment = {\n    \"supervisor\": \"Alex\",\n    \"sample_weight_in_grams\": 10,\n}\n\nprint(experiment)\n\ndel experiment[\"supervisor\"]\n\nprint(experiment)\n</code></pre> &gt;&gt;&gt; Output<pre><code>{'supervisor': 'Alex', 'sample_weight_in_grams': 10}\n{'sample_weight_in_grams': 10}\n</code></pre> Modify a dictionary <p>Remember that a value can hold any data type? You are given a dictionary with production data. <pre><code>production = {\n    \"singapore\": {\"steel\": 100, \"aluminium\": 150},\n    \"taipeh\": {\"steel\": 200, \"aluminium\": 250},\n    \"linz\": {\"steel\": 300, \"aluminium\": 350, \"copper\": 100},\n}\n</code></pre></p> <p>Each key represents a location and has another dictionary as value. This dictionary contains the production quantity of different materials.</p> <ul> <li>Remove <code>linz</code> from the dictionary.</li> <li>Add a new location <code>vienna</code> with the production of 200 steel  and 250 aluminium.</li> <li>Print the <code>aluminium</code> value of <code>taipeh</code> (try accessing it step by step and use variables for each step).</li> </ul> Info <p>At first, the above example might seem a bit too overcomplicated. However,  nesting (in this example: storing a dictionary within a dictionary) is  common practice and as already discussed, lets you represent more  complex data structures. Even databases like Redis and MongoDB are at it's core key value stores, just like our dictionary above.</p>"},{"location":"python-extensive/containers/dict/#recap","title":"Recap","text":"<p>We have covered following topics in this section:</p> <ul> <li>Dictionaries store key-value pairs</li> <li>How to get and modify values</li> <li>Adding key-value pairs</li> <li>Removing key-value pairs with <code>del</code></li> </ul> <p>Lastly, as part of the <code>Containers</code> topic, we will have a look at tuples.</p>"},{"location":"python-extensive/containers/list/","title":"List","text":""},{"location":"python-extensive/containers/list/#lists","title":"Lists","text":""},{"location":"python-extensive/containers/list/#introduction","title":"Introduction","text":"<p>In <code>Python</code>, a container is a type of data structure that holds and organizes multiple values or objects. Containers are used to store collections of elements, allowing you to group related data together for easier management and manipulation. <code>Python</code> provides several built-in container types, each with its own characteristics and use cases. In this first section, we cover  <code>list</code> objects, followed by dictionaries and tuples.</p>"},{"location":"python-extensive/containers/list/#what-is-a-list","title":"What is a <code>list</code>?","text":"<p>A <code>list</code> is a collection of items. You can make a <code>list</code>  including the letters of the alphabet or the digits from <code>0</code> to  <code>9</code>. You can put anything you want into a <code>list</code> and the items in your <code>list</code> don\u2019t have to be related in any particular way.</p> <p>Because a <code>list</code> usually contains more than one element, it\u2019s a good  idea to use the plural for a variable of type <code>list</code>, such as  <code>letters</code>, <code>digits</code>, or <code>names</code>.</p> <p>A <code>list</code> is created with an opening and closing square bracket  <code>[]</code>. Individual elements in the <code>list</code> are separated by commas. Here\u2019s a  simple example of a <code>list</code>:</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['John', 'Paul', 'George', 'Ringo']\n</code></pre> <pre><code>print(type([]))  # print the type of an empty list\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'list'&gt;\n</code></pre>"},{"location":"python-extensive/containers/list/#accessing-elements","title":"Accessing elements","text":"<p>You can access any element in a <code>list</code> by using the <code>index</code> of the desired item. To access an element in a <code>list</code>, write the name of the <code>list</code> followed by the index of the item enclosed in square brackets. For example, let\u2019s pull out  the first Beatle in <code>beatles</code>:</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles[0])\n</code></pre> &gt;&gt;&gt; Output<pre><code>John\n</code></pre>"},{"location":"python-extensive/containers/list/#indexerror","title":"<code>IndexError</code>","text":"Info <p>In Python, index positions start at 0, not 1. This is true for most  programming languages. If you\u2019re receiving unexpected results, determine  whether you are making a simple off-by-one error.</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles[4])\n</code></pre> <p>... results in</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;ipython-input-7-68dd8df4c868&gt;\", line 2, in &lt;module&gt;\n    print(beatles[4])\n          ~~~~~~~^^^\nIndexError: list index out of range\n</code></pre> <p>... since there is no official 5<sup>th</sup> Beatle. </p> <p>There is a special syntax for accessing the last element in a <code>list</code>. Use the index <code>-1</code> to access the last element, <code>-2</code> to access the second-to-last element, and so on.</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles[-1])  # Ringo\nprint(beatles[-2])  # George\n</code></pre> Indexing <p>Define a list that stores following programming languages:</p> <ul> <li>R</li> <li>Python</li> <li>Julia</li> <li>Java</li> <li>C++</li> </ul> <p>and use <code>print()</code> to output: <code>\"My favourite language is Python!\"</code></p>"},{"location":"python-extensive/containers/list/#list-manipulation","title":"List manipulation","text":"<p><code>Python</code> provides several ways to add or remove data to existing lists.</p>"},{"location":"python-extensive/containers/list/#adding-elements","title":"Adding elements","text":"<p>The simplest way to add a new element to a <code>list</code>, is to append it.  When you append an item to a <code>list</code>, the new element is added at  the end.</p> <pre><code>numbers = [1, 2, 3]\nprint(numbers)\nnumbers.append(4)\nprint(numbers)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1, 2, 3]\n[1, 2, 3, 4]\n</code></pre> <p>The <code>append()</code> method makes it easy to build lists dynamically. For example, you can start with an empty <code>list</code> and then add items by  repeatedly calling <code>append()</code>.</p> <pre><code>numbers = [1.0, 2.0, 0.5]\nnumbers.append(4.0)\nnumbers.append(3.0)\nnumbers.append(\"one hundred\")\n\nprint(numbers)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1.0, 2.0, 0.5, 4.0, 3.0, 'one hundred']\n</code></pre> <p>Up until now, our lists contained only one type of elements -  strings or integers. However, as in the example above, you can store  multiple different types of data in a <code>list</code>. Moreover, you can do  nesting (for example, you can store a <code>list</code> within a <code>list</code> - more on that later). Hence, lists can represent complex data structures. Nevertheless, don't mix and match every imaginable data type within a single  <code>list</code> (just because you can) as it makes the handling of your  <code>list</code> quite difficult.</p> Info <p>Later, we will learn how to perform the same task without repeatedly  calling the same <code>append()</code> method over and over.</p>"},{"location":"python-extensive/containers/list/#inserting-elements","title":"Inserting elements","text":"<p>You can add a new element at any position in your <code>list</code> by using the <code>insert()</code> method. You do this by specifying the index of the new element and the value of the new item.</p> <pre><code>pokemon = [\"Charmander\", \"Charizard\"]\npokemon.insert(1, \"Charmeleon\")\nprint(pokemon)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['Charmander', 'Charmeleon', 'Charizard']\n</code></pre>"},{"location":"python-extensive/containers/list/#removing-elements","title":"Removing elements","text":"<p>To remove an item from a <code>list</code>, you can use the <code>remove()</code>  method. You need to specify the value which you want to remove. However,  this will only remove the first occurrence of the item.</p> <pre><code>pokemon = [\"Charmander\", \"Squirtle\", \"Charmeleon\", \"Charizard\", \"Squirtle\"]\npokemon.remove(\"Squirtle\")\nprint(pokemon)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['Charmander', 'Charmeleon', 'Charizard', 'Squirtle']\n</code></pre>"},{"location":"python-extensive/containers/list/#popping-elements","title":"Popping elements","text":"<p>Sometimes you\u2019ll want to use the value of an item after you remove it from a <code>list</code>. The <code>pop()</code> method removes a specified element of a  <code>list</code>. Additionally, the item is returned so you can work with that  item after removing it. </p> <p>The term pop comes from thinking of a <code>list</code> as a stack of items and popping one item off the top of the stack. In this analogy, the top of a stack  corresponds to the end of a <code>list</code>.</p> <pre><code>pokemon = [\"Charmander\", \"Charmeleon\", \"Bulbasaur\", \"Charizard\"]\nbulbasaur = pokemon.pop(2)\nprint(pokemon)\nprint(bulbasaur)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['Charmander', 'Charmeleon', 'Charizard']\nBulbasaur\n</code></pre> List manipulation <p>Define a <code>list</code> with a couple of elements (of your choice). Play around with the methods <code>append()</code>, <code>insert()</code>,  <code>remove()</code> and <code>pop()</code>. Print the <code>list</code> after  each operation to see the changes.</p>"},{"location":"python-extensive/containers/list/#organizing-a-list","title":"Organizing a <code>list</code>","text":"<p>For various reasons, often, your lists will be unordered. If you want to  present your <code>list</code> in a particular order, you can use the method  <code>sort()</code>, or the function <code>sorted()</code>.</p>"},{"location":"python-extensive/containers/list/#sort","title":"<code>sort()</code>","text":"<p>The <code>sort()</code> method operates on the <code>list</code> itself and  changes its order.</p> <pre><code>numbers = [5, 4, 1, 3, 2]\nnumbers.sort()  # sort in ascending order\nprint(numbers)\n\nnumbers.sort(reverse=True)  # sort in descending order\nprint(numbers)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1, 2, 3, 4, 5]\n[5, 4, 3, 2, 1]\n</code></pre>"},{"location":"python-extensive/containers/list/#sorted","title":"<code>sorted()</code>","text":"<p>The <code>sorted()</code> function maintains the original order of a  <code>list</code> and returns a sorted <code>list</code> as well.</p> <pre><code>numbers = [5, 4, 1, 3, 2]\nsorted_numbers = sorted(numbers)\n\nprint(f\"Original list: {numbers}; Sorted list: {sorted_numbers}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Original list: [5, 4, 1, 3, 2]; Sorted list: [1, 2, 3, 4, 5]\n</code></pre>"},{"location":"python-extensive/containers/list/#length","title":"Length","text":"<p>You can easily find the length of a <code>list</code> with <code>len()</code>.</p> <pre><code>print(len([3.0, 1.23, 0.5]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>3\n</code></pre>"},{"location":"python-extensive/containers/list/#slicing","title":"Slicing","text":"<p>To make a slice (part of a <code>list</code>), you specify the index of the  first and last elements you want to work with. Elements up until the second index are  included. To output the first three elements in a <code>list</code>, you would request indices 0 through 3, which would return elements 0, 1, and 2.</p> <pre><code>players = [\"charles\", \"martina\", \"michael\", \"florence\", \"eli\"]\nprint(players[0:3])\n</code></pre> &gt;&gt;&gt; Output<pre><code>['charles', 'martina', 'michael']\n</code></pre> Slicing <p>Define a <code>list</code> of your choice with at least <code>5</code>  elements. </p> <ul> <li>Now, perform a slice from the second up to and including the fourth  element.</li> <li>Next, omit the first index in the slice (only omit the number!). What  happens?</li> <li>Lastly, re-add the first index and omit the second index of your  slice. Print the result.</li> </ul>"},{"location":"python-extensive/containers/list/#copy","title":"Copy","text":"<p>To copy a <code>list</code>, you can use the <code>copy()</code> method.</p> <pre><code>original_list = [1, 2, 3]\ncopied_list = original_list.copy()\n\n# perform some changes to both lists\noriginal_list.append(4)\ncopied_list.insert(0, \"zero\")\n\nprint(f\"Original list: {original_list}, Copied list: {copied_list}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Original list: [1, 2, 3, 4], Copied list: ['zero', 1, 2, 3]\n</code></pre>"},{"location":"python-extensive/containers/list/#be-careful","title":"Be careful!","text":"<p>You might wonder why we can't simply do something along the lines of  <code>copied_list = original_list</code>. With lists, we have to be careful,  as this syntax simply creates a reference to the original <code>list</code>. Let's look at an example:</p> <pre><code>original_list = [1, 2, 3]\ncopied_list = original_list\n\n# perform some changes to the original list\noriginal_list.append(4)\n\nprint(f\"Original list: {original_list}, Copied list: {copied_list}\")\n</code></pre> <p>which leaves us with: &gt;&gt;&gt; Output<pre><code>Original list: [1, 2, 3, 4], Copied list: [1, 2, 3, 4]\n</code></pre></p> <p>As you can see, the changes to the original <code>list</code> are reflected in  the copied one. You can read about this in more detail  here.</p> Note <p>We can actually check whether both lists point to the same object in memory by using <code>id()</code> which returns the memory address of an object.  Just remember, to be careful when copying lists and check if your program  behaves as intended!</p> <pre><code>original_list = [1, 2, 3]\ncopied_list = original_list\n\nprint(id(original_list) == id(copied_list))  # True\n</code></pre> Unknown method <p>Use the given <code>list</code> (don't worry about the syntax, it's just a  short expression to create a huge <code>list</code>):</p> <pre><code>long_list = [True] * 1000\n</code></pre> <ul> <li>Check the length of the <code>list</code>.</li> <li>Apply a method that deletes all elements in the <code>list</code> and  returns an empty list <code>[]</code>. You might need to use Google,  since it is a method not previously discussed.</li> <li>Check the length of the <code>list</code> again.</li> </ul>"},{"location":"python-extensive/containers/list/#recap","title":"Recap","text":"<p>We extensively covered lists and their manipulation.</p> <ul> <li>Accessing elements with indices (including slicing)</li> <li>The <code>IndexError</code></li> <li>Adding elements with <code>append()</code> and <code>insert()</code></li> <li>Removing elements with <code>remove()</code> and <code>pop()</code></li> <li>Sorting with <code>sort()</code> and <code>sorted()</code></li> <li>Length of a <code>list</code> with <code>len()</code></li> <li>Make a copy with <code>copy()</code></li> </ul>"},{"location":"python-extensive/containers/tuple/","title":"Tuple","text":""},{"location":"python-extensive/containers/tuple/#tuples","title":"Tuples","text":"<p>Lists and dictionaries work well for storing and manipulating data during the execution of a program. Both lists and dictionaries are mutable. However, sometimes you\u2019ll want to create a collection of elements that are immutable (can't change). Tuples allow you to do just that.</p> <pre><code># coordinates of MCI IV\ncoordinates = (47.262996862335854, 11.393082185178823)\nprint(type(coordinates))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'tuple'&gt;\n</code></pre> <p>A <code>tuple</code> is created with round brackets (<code>()</code>). As with lists and dictionaries, the elements are separated by commas. Tuples can hold any type of data.</p>"},{"location":"python-extensive/containers/tuple/#accessing-elements","title":"Accessing elements","text":"<p>With indexing, the individual elements of a <code>tuple</code> can be retrieved.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\nprint(coordinates[0])\nprint(coordinates[1])\n</code></pre> &gt;&gt;&gt; Output<pre><code>47.262996862335854\n11.39308218517882\n</code></pre>"},{"location":"python-extensive/containers/tuple/#immutability","title":"Immutability","text":"<p>Let's try to change the value of an element in a <code>tuple</code>.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\ncoordinates[0] = 50.102\n</code></pre> <p>we will encounter following error:</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;ipython-input-29-d74dc80ea879&gt;\", line 2, in &lt;module&gt;\n    coordinates[0] = 50.102\n    ~~~~~~~~~~~^^^\nTypeError: 'tuple' object does not support item assignment\n</code></pre> <p>As a <code>tuple</code> is immutable, you can only redefine the entire  <code>tuple</code>.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\n# redefine the entire tuple\ncoordinates = (5.513615392318705, 95.2060492604128)\n</code></pre>"},{"location":"python-extensive/containers/tuple/#tuple-unpacking","title":"<code>tuple</code> unpacking","text":"<p>Tuples can be unpacked, to use them separately.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\nlatitude, longitude = coordinates\n</code></pre> Info <p>Tuples are often used for constants. In the above examples, we used  coordinates. As these coordinates are not going to change, a  <code>tuple</code> is a fitting data type.</p> Tuple unpacking <p>Use the following <code>tuple</code> with cities. <pre><code>cities = (\"New York\", \"Los Angeles\", \"Chicago\")\n</code></pre></p> <ul> <li>Print the first city.</li> <li>Use <code>tuple</code> unpacking and print the resulting variables.</li> </ul>"},{"location":"python-extensive/containers/tuple/#recap","title":"Recap","text":"<p>In this rather short section, we introduced tuples and covered:</p> <ul> <li>Mutability vs. immutability</li> <li>How to define a <code>tuple</code></li> <li>Access elements with indexing</li> <li>... and <code>tuple</code> unpacking</li> </ul>"},{"location":"python-extensive/control-structures/for/","title":"For","text":""},{"location":"python-extensive/control-structures/for/#loops-for","title":"Loops - <code>for</code>","text":""},{"location":"python-extensive/control-structures/for/#introduction","title":"Introduction","text":"<p>In this section, you\u2019ll learn how to loop through elements using just a few lines of code. Looping allows you to take the same action, or set of actions with every item in an iterable. Among iterables are for example, lists or dictionaries. As a result, you'll be able to streamline tedious tasks. First, we'll loop over lists.</p>"},{"location":"python-extensive/control-structures/for/#looping-over-lists","title":"Looping over lists","text":"<p>You\u2019ll often want to run through all entries in a <code>list</code>, performing the same task with each item. In the below example, we loop through a <code>list</code> of passwords and print the length of each one.</p> <pre><code>passwords = [\"1234\", \"password\", \"admin\", \"123456\"]\nfor password in passwords:\n    print(f\"Password: {password} is {len(password)} characters long.\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Password: 1234 is 4 characters long.\nPassword: password is 8 characters long.\nPassword: admin is 5 characters long.\nPassword: 123456 is 6 characters long.\n</code></pre> <p>A loop is written with the <code>for</code> statement. The <code>password</code> is a temporary variable that holds the current item in the <code>list</code>. You can  choose any name you want for the temporary variable that holds each value.  However, it\u2019s helpful to choose a meaningful name that represents a single item from the <code>list</code>. For example:</p> <pre><code>for experiment in experiments:\n    ...\nfor user in users:\n    ...\n</code></pre> Info <p>When you\u2019re using loops for the first time, keep in mind that the set  of steps is repeated once for each item in the <code>list</code>, no matter  how many items are in the <code>list</code>. If you have a million items  in your <code>list</code>, <code>Python</code> repeats these steps a million times.</p>"},{"location":"python-extensive/control-structures/for/#scope","title":"Scope","text":"<p>Python uses indentation (whitespace) to indicate, what is part of the loop.  With an indentation being four characters of whitespace. For a faster way to  intend, use the tab key Tab.</p> <p>Let's extend the example from above:</p> <pre><code>passwords = [\"1234\", \"password\", \"admin\", \"123456\"]\nfor password in passwords:\n    print(f\"Password: {password} is {len(password)} characters long.\")\n\nprint(\"All passwords have been checked.\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Password: 1234 is 4 characters long.\nPassword: password is 8 characters long.\nPassword: admin is 5 characters long.\nPassword: 123456 is 6 characters long.\nAll passwords have been checked.\n</code></pre> <p>You can easily see that only the first <code>print</code> statement is part of  the loop, simply because it is indented. The second <code>print</code> statement is executed after the loop has finished as it is outside the loop.</p>"},{"location":"python-extensive/control-structures/for/#indentationerror","title":"<code>IndentationError</code>","text":"<p>In longer programs, you\u2019ll notice blocks of code indented at a few different levels. These indentation levels help you gain a general sense of the overall program\u2019s organization.</p> <p>As you begin to write code that relies on proper indentation, you\u2019ll need to watch for a few common indentation errors.</p>"},{"location":"python-extensive/control-structures/for/#expected-indentation","title":"Expected indentation","text":"<pre><code>for number in [1, 2, 3]:\nprint(number)\n</code></pre> <pre><code>  Cell In[4], line 2\n    print(number)\n    ^\nIndentationError: expected an indented block after 'for' statement on line 1\n</code></pre> <p>As the <code>IndentationError</code> states, <code>Python</code> expects an indented  block of code after the <code>for</code> statement.</p>"},{"location":"python-extensive/control-structures/for/#unexpected-indentation","title":"Unexpected indentation","text":"<pre><code>message = \"Hello\"\n    print(message)\n</code></pre> <pre><code>  Cell In[9], line 2\n    print(message)\n    ^\nIndentationError: unexpected indent\n</code></pre> <p>In this case, the code snippet contains an unnecessary indentation.</p> Square numbers <p>Square each number in a given list and print the result. First, initialize a list of numbers from 1 to 10. Square each number and <code>print</code> it. Use a <code>for</code> loop.</p>"},{"location":"python-extensive/control-structures/for/#range","title":"<code>range()</code>","text":"<p>The <code>range()</code> function makes it easy to generate a series of numbers.  For example, you can use <code>range()</code> to print a series of numbers like this:</p> <pre><code>for value in range(3):\n  print(value)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0\n1\n2\n</code></pre> <p>Remember, that <code>Python</code> 'starts counting at <code>0</code>'. <code>3</code> is not  included in the output, as <code>range()</code> generates a sequence up to, but not including, the number you provide. You can also pass two arguments to <code>range()</code>, the first and the last number of the sequence. In this case, the sequence will start at the first number and end at the last number minus one.</p> <pre><code>for value in range(3, 6):\n    print(value)\n</code></pre> &gt;&gt;&gt; Output<pre><code>3\n4\n5\n</code></pre> <code>range()</code> <p>Use <code>range()</code> to build a <code>list</code> which holds the numbers from 15 to 20 - including 20.</p> Savings account growth <p>Write a <code>for</code> loop to calculate the growth of savings over a  period of time. Use following formula to calculate the future value of  savings in year \\(t\\):</p> \\[ \\text{A} = \\text{P} \\times \\left(1 + \\frac{\\text{r}}{100} \\right)^{\\text{t}} \\] <p>where:</p> <ul> <li>\\(\\text{A}\\) is the future value of the savings account or investment.</li> <li>\\(\\text{P}\\) is the present value of the savings account or investment.</li> <li>\\(\\text{r}\\) is the annual interest rate.</li> <li>\\(\\text{t}\\) is the number of years the money is invested for.</li> </ul> <p>Given values:</p> <ul> <li>\\(\\text{P} = 1000\\)</li> <li>\\(\\text{r} = 5\\)</li> </ul> <p>Print the future value of the savings account over a period of 10 years.  Skip each second year. Use  Python's documentation on range() as a starting point.</p>"},{"location":"python-extensive/control-structures/for/#detour-simple-statistics-on-lists-with-numbers","title":"Detour: Simple statistics on lists with numbers","text":"<p>A few functions are specific to lists of numbers. For example, you can easily find the minimum, maximum, and sum of a list of numbers:</p> <pre><code>numbers = [1.0, 8.38, 3.14, 7.0, 2.71]\nprint(\n    f\"Minimum: {min(numbers)}\",\n    f\"Maximum: {max(numbers)}\",\n    f\"Sum: {sum(numbers)}\", sep=\"\\n\"\n)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Minimum: 1.0\nMaximum: 8.38\nSum: 22.23\n</code></pre> Calculate the average <p>Calculate the average of the following list: <pre><code>numbers = [4.52, 3.14, 2.71, 1.0, 8.38]\n</code></pre></p>"},{"location":"python-extensive/control-structures/for/#list-comprehensions","title":"List comprehensions","text":"<p>... are a concise way to create lists.</p> <p>A list comprehension combines a <code>for</code> loop to create a new list in a single line.</p> Rewrite a list comprehension <p>Rewrite the following list comprehension in a regular for-loop to  achieve the same result: <pre><code>squares = [value**2 for value in range(1,11)]\nprint(squares)\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n</code></pre>"},{"location":"python-extensive/control-structures/for/#looping-over-dictionaries","title":"Looping over dictionaries","text":"<p>As previously discussed, you can not only loop over a <code>list</code>, but  also iterate over a variety of different data types, such as dictionaries. You can loop over a dictionary\u2019s key-value pairs, solely over the keys  or just the values.</p>"},{"location":"python-extensive/control-structures/for/#items","title":"<code>items()</code>","text":"<p>Using the <code>.items()</code> method, we can loop over the key-value pairs. Take note, that the method returns two values, which we store in two separate variables (<code>key</code> and <code>value</code>).</p> <p>We can freely choose the variable names in the <code>for</code>-loop. It does  not have to be <code>key</code> and <code>value</code> respectively.</p> <pre><code>parts = {\n    \"P100\": \"Bolt\",\n    \"P200\": \"Screw\",\n    \"P300\": \"Hinge\",\n}\n\nfor key, value in parts.items():\n    print(key, value)\n</code></pre> &gt;&gt;&gt; Output<pre><code>P100 Bolt\nP200 Screw\nP300 Hinge\n</code></pre>"},{"location":"python-extensive/control-structures/for/#values-keys","title":"<code>values()</code>, <code>keys()</code>","text":"Dictionary methods <p>Define a (non-empty) dictionary of your choice and use both methods <code>.values()</code> and <code>.keys()</code> to access solely values and keys respectively.</p>"},{"location":"python-extensive/control-structures/for/#recap","title":"Recap","text":"<p>With the introduction of the <code>for</code> loop, you can now start to  automate re-occurring tasks. We have covered:</p> <ul> <li>Looping over lists</li> <li>Indentation and possible resulting <code>IndentationError</code></li> <li><code>range()</code> to generate a series of numbers</li> <li>Simple statistics on lists of numbers</li> <li>List comprehensions</li> <li>Specific methods to loop over dictionaries</li> </ul>"},{"location":"python-extensive/control-structures/if/","title":"If","text":""},{"location":"python-extensive/control-structures/if/#more-control-structures","title":"More control structures","text":""},{"location":"python-extensive/control-structures/if/#introduction","title":"Introduction","text":"<p>In this section, we will cover additional control structures. First, we  discuss the <code>if</code> statement, which allows us to execute code based on a  condition. Followed by the <code>elif</code>, <code>else</code> and  <code>while</code> statements.</p>"},{"location":"python-extensive/control-structures/if/#if","title":"<code>if</code>","text":"<p>The <code>if</code> statement lets you evaluate conditions. The simplest kind of  <code>if</code> statement has one condition and one action. Here is some  pseudocode:</p> <pre><code>if condition is True:\n    do something\n</code></pre> <p>You can put any condition in the first line and just about any action in the  indented block following the test. If the condition evaluates to <code>True</code>, <code>Python</code> executes the indented code following the <code>if</code> statement.  If the test evaluates to <code>False</code>, the indented code block (following the  <code>if</code>) is ignored.</p> <pre><code>user = \"admin\"\n\nif user == \"admin\":\n  print(f\"Welcome {user}!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Welcome admin!\n</code></pre> <p>First, the condition <code>user == \"admin\"</code> is evaluated. If it  evaluates to <code>True</code>, the indented print is executed. If the condition evaluates to <code>False</code>, the indented code block is ignored.</p> <p>Indentation plays the same role in <code>if</code> statements as it did in  <code>for</code> loops (see the previous section).</p> Password strength: Part 1 <p> </p> <p>In the section on comparisons and logical operators, you had to check whether a password meets certain criterias. The following example expands on this task as you are  given a list of passwords. You have to check if each password exceeds a length of 12 characters.</p> <p>Execute the first code cell to generate some random passwords  (note every time you rerun the code snippet, different passwords will be  generated).</p> <pre><code># generate passwords - simply execute the code to generate some random\n# passwords\nimport random\nimport string\n\npasswords = []\nfor i in range(10):\n    length = random.randint(3, 25)\n    password = \"\".join(random.choices(string.ascii_letters, k=length))\n    passwords.append(password)\n</code></pre> <p>The <code>list</code> <code>passwords</code> should look something like this: <pre><code>['PWgOYxQgnxgXm',\n 'gpOMVTmCSjAcndowkUd',\n 'ADKIEthzsGBr',\n 'VRLzOIZtEz',\n 'uOckmTJjeonUyMlnG',\n 'gjOpWuHrIbG',\n 'doxIylbRkNLdvdLNgVgYsDGzd',\n 'KvUdsgZhPIrS',\n 'LrdpffEqlBVQYr',\n 'ncyqXNLnVstVxlx']\n</code></pre></p> <p>Now, loop over the passwords and check if each password exceeds the  character limit of 12. If so, print the password.</p>"},{"location":"python-extensive/control-structures/if/#else","title":"<code>else</code>","text":"<p>Previously, every time the condition in the <code>if</code> statement  evaluated to <code>False</code>,  no action was taken. Hence, the <code>else</code> clause is introduced which  allows you to define a set of actions that are executed when the conditional test fails.</p> <pre><code>user = \"random_user\"\n\nif user == \"admin\":\n  print(f\"Welcome {user}!\")\nelse:\n  print(\"Only admins can enter this area!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Only admins can enter this area!\n</code></pre> Password strength: Part 2 <p>Let's expand on our previous example. Re-use your code to check the length of the generated passwords. Now, we would like to store all passwords that did not meet our criteria in the empty list <code>invalid_passwords</code>.</p> <p>Hint: Introduce an <code>else</code> statement to save the invalid passwords.</p>"},{"location":"python-extensive/control-structures/if/#elif","title":"<code>elif</code>","text":"<p>Often, you\u2019ll need to test more than two possible situations, and to evaluate these, you can use an <code>if-elif-else</code> syntax. <code>Python</code> executes only one block in an <code>if-elif-else</code> chain. It runs each conditional test in order until one passes. When a test passes, the code following that test is executed and the rest is skipped.</p> <pre><code>user = \"xX_user_Xx\"\nregistered_users = [\n    \"admin\",\n    \"guest\",\n    \"SwiftShark22\",\n    \"FierceFalcon66\",\n    \"BraveWolf11\"\n]\n\nif user == \"admin\":\n  print(f\"Welcome {user}!\")\nelif user not in registered_users:\n  print(\"Please create an account first!\")\nelse:\n  print(\"Only admins can enter this area!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Please create an account first!\n</code></pre> Info <p>As you might have noticed, you can use a single <code>if</code> statement or <code>if</code> in combination with <code>else</code>. For multiple conditions  you can add as many <code>elif</code> parts as you wish.</p>"},{"location":"python-extensive/control-structures/if/#while","title":"<code>while</code>","text":"<p>The <code>for</code> loop takes an iterable and executes a block of  code once for each element. In contrast, the <code>while</code> loop runs as  long as a certain condition is <code>True</code>.</p> <p>For instance, you can use a <code>while</code> loop to count up through a  series of numbers. Here is an example:</p> <pre><code># set a counter\ncurrent_number = 1\n\nwhile current_number &lt;= 5:\n  print(current_number)\n  # increment the counter value by one\n  current_number += 1\n</code></pre> &gt;&gt;&gt; Output<pre><code>1\n2\n3\n4\n5\n</code></pre> <p>Note, that the variable, that is checked in the <code>while</code>-condition  must be defined prior to the loop, otherwise we will encounter a  <code>NameError</code>.</p> Infinite loops <p>Moreover, the variable must be updated within the loop to avoid an infinite loop. For example, if <code>current_number</code> is not incremented by one, the condition <code>current_number &lt;= 5</code> will always evaluate to <code>True</code>, leaving us stuck in an infinite loop. In such cases, simply click the <code>Stop</code> button (on the left-hand side of the  respective code cell) to interrupt the execution.</p> Addition assignment <p>In the above example, we used the <code>+=</code> operator, referred to as  addition assignment. It is a shorthand for incrementing a variable by a  certain value.</p> <pre><code>a = 10\na += 5\nprint(a)\n</code></pre> &gt;&gt;&gt; Output<pre><code>15\n</code></pre> <p>The above code is equivalent to <code>a = a + 5</code>. This shorthand assignment can be used with all arithmetic operators, such as subtraction <code>-=</code> or division <code>/=</code>.</p> While loop <p>Write some code, to print all even numbers up to 42 using a while  loop.</p>"},{"location":"python-extensive/control-structures/if/#detour-user-input","title":"Detour: User input","text":"<p>Most programs are written to solve an end user\u2019s problem. To do so, usually  we need to get some information from the user. For a simple example, let\u2019s  say someone wants to enter a username.</p> <p>You can store the user input in the variable <code>user_name</code> like in the example  below.</p> <pre><code>user_name = input(\"Please enter your username:\")\n</code></pre> <p>However, the <code>input()</code> function always returns a string.</p> <pre><code>age = input(\"Please enter your age:\")\nprint(type(age))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'str'&gt;\n</code></pre> <p>... use casting to  convert the input to the desired type.</p>"},{"location":"python-extensive/control-structures/if/#break","title":"<code>break</code>","text":"<p>To exit any loop immediately without running any remaining 'loop code', use  the <code>break</code> statement. The <code>break</code> statement directs the flow of your program; you can use it to control which lines of code are executed and  which aren\u2019t, so the program only executes code that you want it to, when you want it to.</p> <pre><code>for i in range(5):\n    if i == 3:\n        break\n    print(i)\n\nprint(\"Continue running the program...\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>0\n1\n2\nContinue running the program...\n</code></pre>"},{"location":"python-extensive/control-structures/if/#continue","title":"<code>continue</code>","text":"<p>Rather than breaking out of a loop entirely, you can use the <code>continue</code>  statement to return to the beginning of the loop based on the result of a  condition.</p> <pre><code>for i in range(5):\n    if i == 3:\n        continue\n    print(i)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0\n1\n2\n4\n</code></pre>"},{"location":"python-extensive/control-structures/if/#recap","title":"Recap","text":"<p>In this section we have expanded on control structures. We discussed:</p> <ul> <li><code>if</code> statements and how to use them</li> <li><code>else</code> clauses</li> <li><code>elif</code> statements for multiple conditions</li> <li><code>while</code> loops</li> <li><code>break</code> and <code>continue</code> statements for more 'fine-grained'   control</li> </ul>"},{"location":"python-extensive/data/api/","title":"API","text":"<p>An application programming interface (API) is a set of rules and protocols that allows one software application to interact with another. In other words, it is a way to communicate with a server. Some of these servers are openly available and host data that can be accessed by anyone. Others require authentication and are therefore paid services.</p> <p>To illustrate the practical interaction with APIs, we will retrieve cryptocurrency data from the CoinCap API.</p> <p>We will pull a list of available cryptocurrencies, the latest price history  of a specific coin, plot a line chart to visualize the data and  lastly perform a conversion from USD to EUR.</p> Disclaimer <p>This section merely demonstrates APIs on the example of cryptocurrency  market data.</p> <p>Cryptocurrencies involve significant financial risk.  Investors should conduct thorough research and consult financial professionals  before making investment decisions. The code examples presented herein are  for illustrative purposes only and do not constitute financial advice  nor do we endorse any companies mentioned.</p> Reading the documentation <p>Open the CoinCap API documentation (here) and  browse through the site for a minute or two.</p>"},{"location":"python-extensive/data/api/#rate-limits","title":"Rate limits","text":"<p>During the task, you should have noticed that the API provides information  on rate limits. Rate limits are the number of requests that can be made to the server in a given time frame. If you as the user exceed the rate limit,  the server will respond with an error message. In this specific case, we can make up to 200 requests per minute which is more than enough for our use case. These rate limits are set by the provider and  can vary from one API to another. Some APIs may not have any rate limits at  all.</p> <p>But how can we even request data from the server to retrieve a list of  cryptocurrencies? To answer this question, the concept of endpoints is  introduced.</p>"},{"location":"python-extensive/data/api/#endpoints","title":"Endpoints","text":"<p>An endpoint is a specific URL that the API uses to perform a specific action. For example, the endpoint <code>/assets</code> returns a list of all cryptocurrencies. To send a request to the server, we need to specify the endpoint in the URL. The server will then respond with the requested data (if everything went  smoothly).</p> <p>To request all cryptocurrencies, we can use the following URL: <pre><code>https://api.coincap.io/v2/assets\n</code></pre></p> <p><code>https://api.coincap.io/v2</code> is simply the URL of the API and <code>/assets</code> is  the endpoint of our interest.</p> Send your first request <p>Open the URL <code>https://api.coincap.io/v2/assets</code> in your browser and  observe the response.</p> Which Python type does the output of your request most closely resemble?A pandas DataFrameA simple listA simple dictionarySubmit<p>Correct! The server response you got was actually in the form of a  <code>JSON</code> file.  This is a common format for APIs to return data. We can easily read the  <code>JSON</code> with <code>Python</code> and convert it to a dictionary. </p> <p>Since we don't want to manually use the browser anytime we want to retrieve  data, we now replicate the request in <code>Python</code> .  To send requests we can make use of the appropriately named <code>requests</code> package.</p> Setup <p>Create a new virtual environment and install the <code>requests</code> package.</p> <pre><code>pip install requests\n</code></pre> <p>The below snippet sends a request to the <code>/assets</code> endpoint and stores the response in a variable.</p> <pre><code>import requests\n\nresponse = requests.get(url=\"https://api.coincap.io/v2/assets\")\ndata = response.json()  # assign the response to a variable\n</code></pre> Validate the above quiz question <p>What type is returned by the <code>response.json()</code> method?  Check the <code>type()</code> of the <code>data</code> variable.</p>"},{"location":"python-extensive/data/api/#methods","title":"Methods","text":"<p>In the above code snippet, we applied <code>requests</code> <code>get()</code> method. The <code>get</code> method solely retrieves data from the server, that is no data is  changed on the server-side. If you have another look at the CoinCap API docs you will discover that all endpoints like <code>/assets</code>, <code>/rates</code>, or <code>/markets</code>  are prefaced by the <code>GET</code> method.</p> <p>Nevertheless, <code>GET</code> is not the only method, there are also <code>POST</code>, <code>PUT</code>, <code>DELETE</code>, and <code>PATCH</code>. Following table provides a brief overview:</p> Method Description <code>requests</code> method GET Retrieve data from the server <code>requests.get()</code> POST Create data on the server <code>requests.post()</code> PUT Update data on the server <code>requests.put()</code> DELETE Delete data on the server <code>requests.delete()</code> PATCH Partially update data on the server <code>requests.patch()</code> <p>Don't worry about these methods too much for now as we will continue solely with <code>GET</code> methods.</p> <p></p> Info <p>If you need to revisit the topic of HTTP methods or simply want to dive  deeper, here's a great article.</p>"},{"location":"python-extensive/data/api/#endpoints-continued","title":"Endpoints continued...","text":"<p>Let's revisit the code snippet from above and extend it. After requesting the  <code>/assets</code> endpoint we convert the response (the <code>dict</code>) into a tabular  format in order to process the data more easily.</p> <pre><code>import requests\n\nresponse = requests.get(url=\"https://api.coincap.io/v2/assets\")\ndata = response.json()\n\nprint(data.keys())  # print all dictionary keys\nprint(data[\"data\"]) # closer look at the value of the \"data\" key\n</code></pre> &gt;&gt;&gt; Output<pre><code>dict_keys(['data', 'timestamp'])\n[{'id': 'bitcoin', 'rank': '1', 'symbol': 'BTC', 'name': 'Bitcoin', ....] \n</code></pre> <p>A closer look at the response reveals that the <code>dict</code> is nested.  The <code>data</code> key is of particular interest, since it contains a list of  dictionaries containing information on cryptocurrencies.  We can convert this list to a <code>pandas</code> <code>DataFrame</code>.</p> <pre><code>import pandas as pd\n\ndata = pd.DataFrame(data[\"data\"])\nprint(data.head())\n</code></pre> id rank symbol name ... bitcoin 1 BTC Bitcoin ... ethereum 2 ETH Ethereum ... tether 3 USDT Tether ... solana 4 SOL Solana ... xrp 5 XRP XRP ... Info <p>The content of your <code>DataFrame</code> can differ slightly as responses  contain the latest data from the server. Since we are dealing with  cryptocurrency market data, changes occur rapidly.</p> <p>Nevertheless, that's the power of APIs as they allow you to  programmatically access up to date information. \ud83e\uddbe</p>"},{"location":"python-extensive/data/api/#query-parameters","title":"Query parameters","text":"<p>To continue on our quest to visualize the latest price history of a cryptocurrency, we need to settle on a single cryptocurrency. The concept  of query parameters is introduced with another practical example.</p> <p>For the following examples, we will use an emerging (at the time of writing)  cryptocurrency called <code>Pepe-Cash</code> (more of a meme-coin).</p> <p> </p> <p>To get access to the price history of <code>Pepe-Cash</code>, we need to consult the API  documentation and find the appropriate endpoint.</p> Which endpoint provides the historic market data of a specific cryptocurrency?/markets/assets/{{id}}/history/rates/assets - The endpoint we used before already contains the information we need.Submit<p>Exactly, by providing a asset <code>id</code>, we can retrieve  the price history from the <code>/assets/{{id}}/history</code> endpoint. </p> <p>With the endpoint name at hand, we can send another request to the server.  But first, we need to construct the URL. Expand the code snippet below, if you  solved the quiz question.</p> URL construction <pre><code>api_url = \"https://api.coincap.io/v2\"\ncoin_id = \"pepe-cash\"\nendpoint = f\"/assets/{coin_id}/history\"\nquery_params = \"?interval=d1\"  # daily interval (if available)\n\nurl = f\"{api_url}{endpoint}{query_params}\"\n</code></pre> <p>Let's walk through the URL construction step by step:</p> <ol> <li><code>api_url</code> is the base URL of the API (nothing new here).</li> <li><code>coin_id</code> <code>pepe-cash</code> is the      identifier of the cryptocurrency we want to retrieve data      for. We have already requested all cryptocurrency identifiers like      <code>bitcoin</code> or <code>ethereum</code> with the <code>/assets</code> endpoint. Have another look      at the table here.</li> <li><code>endpoint</code> contains the endpoint name we want to access, in this      particular case  <code>/assets/pepe-cash/history</code>.</li> <li> <p><code>query_params</code> stands for query parameters which are additional      parameters that are passed to the server. Think of a <code>Python</code>      function with the endpoint being the function name and the query      parameters being the function parameters used for fine-grained control.</p> <p>Query parameters are separated from the URL by a <code>?</code>. In this case, we specified <code>?interval=d1</code>. <code>interval</code> is the  parameter name followed by the value <code>d1</code> which stands for daily  price history intervals. Again, with a <code>Python</code> function you can think  of <code>interval=d1</code> as a named argument.</p> <p>More detailed explanations on both parameters and values are  specified in the API documentation.</p> </li> </ol> <p>Finally, we end up with the URL  <code>\"https://api.coincap.io/v2/assets/pepe-cash/history?interval=d1\"</code></p> <p> <p></p> Quite a complicated URL. </p>"},{"location":"python-extensive/data/api/#request","title":"Request","text":"<p>If you've followed the construction of the URL closely, we can easily send  another request to retrieve market data. This time around it is another <code>GET</code> request, however with a query parameter.</p> <pre><code># construct the URL (same as above)\napi_url = \"https://api.coincap.io/v2\"\ncoin_id = \"pepe-cash\"\nendpoint = f\"/assets/{coin_id}/history\"\nquery_params = \"?interval=d1\"  # daily interval (if available)\n\nurl = f\"{api_url}{endpoint}{query_params}\"\n\n# send the request\nresponse = requests.get(url=url)\n</code></pre> <p>Again, convert the response to a <code>DataFrame</code> and print the first few rows.</p> <pre><code>pepe_history = response.json()\npepe_history = pd.DataFrame(pepe_history[\"data\"])\n\nprint(pepe_history.tail())\n</code></pre> priceUsd time date 0.0176438251661799 1726963200000 2024-09-22T00:00:00.000Z 0.0127411915131318 1727827200000 2024-10-02T00:00:00.000Z 0.0127704751708670 1727913600000 2024-10-03T00:00:00.000Z 0.0131082066240718 1728345600000 2024-10-08T00:00:00.000Z 0.0130405021808657 1728432000000 2024-10-09T00:00:00.000Z <p>We are now looking at the daily (if available) price history of <code>Pepe-Cash</code> in  USD.</p>"},{"location":"python-extensive/data/api/#detour-visualizations","title":"Detour: Visualizations","text":"<p>As a bonus we can plot the price history and try to recreate the price  charts seen on various market platforms. This Visualizations section is  optional and should provide a glimpse into the possibilities of working with APIs.</p> <p></p> <p>Regardless of whether you plot the price chart  dynamically or statically, two preprocessing steps are necessary.</p> <pre><code># convert date and price to their appropriate types\npepe_history[\"date\"] = pd.to_datetime(pepe_history[\"date\"])\npepe_history[\"priceUsd\"] = pepe_history[\"priceUsd\"].astype(float)\n</code></pre> Option 1: Dynamic plot  <code>plotly</code>Option 2: Static plot  <code>matplotlib</code> <pre><code>import plotly.express as px\n\nfig = px.area(\n    data_frame=pepe_history,\n    x=\"date\",\n    y=\"priceUsd\",\n    title=\"Pepe Cash - Price History in USD\",\n    color_discrete_sequence=[\"#009485\"],\n)\nfig.show()\n</code></pre> <p> </p> <pre><code>import matplotlib.pyplot as plt\n\n# pandas plot method:\n# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html\npepe_history.plot(\n    x=\"date\",\n    y=\"priceUsd\",\n    kind=\"area\",\n    title=\"Pepe Cash - Price History in USD\",\n    color=\"#009485\",\n)\nplt.show()\n</code></pre> <p> </p> Bonus: Styling the plot <p>If you want to style the dynamic plot further (to more closely resemble the price charts seen on market platforms) adjust colors, labels and add a  logo.</p> <pre><code>fig = px.area(\n    data_frame=pepe_history,\n    x=\"date\",\n    y=\"priceUsd\",\n    title=\"Pepe Cash - Price History in USD\",\n    color_discrete_sequence=[\"#009485\"],\n    template=\"plotly_dark\"  # dark theme\n)\n# add the logo\nfig.add_layout_image(\n    dict(\n        source=\"https://cryptologos.cc/logos/pepe-pepe-logo.png?v=035\",\n        xref=\"paper\",\n        yref=\"paper\",\n        x=1,\n        y=1.15,\n        sizex=0.2,\n        sizey=0.2,\n        xanchor=\"right\",\n        yanchor=\"top\",\n    )\n)\nfig.show()\n</code></pre> <p> </p> Rate conversion to  <p>Since the price history is in USD, convert the prices to EUR. Conveniently, the API provides an endpoint for current exchange rates. </p> <ol> <li>Use the appropriate <code>/rates/{{id}}</code> endpoint.</li> <li>Use the identifier (<code>id</code>)  <code>euro</code> for the     endpoint.</li> <li>Construct the URL and send a <code>GET</code> request.</li> <li>Extract the exchange rate from the response. Hint:      This time it is easier to deal with the <code>dict</code> and not      perform a conversion to a <code>DataFrame</code>.</li> <li>Convert <code>pepe_history[\"priceUsd\"]</code> to EUR.</li> </ol> <p>Start with the given code snippet below:</p> <pre><code>import requests\n\n# get current Pepe price history in USD\nresponse = requests.get(url=\"https://api.coincap.io/v2/assets/pepe-cash/history?interval=d1\")\npepe_history = pd.DataFrame(response.json()[\"data\"])\npepe_history[\"priceUsd\"] = pepe_history[\"priceUsd\"].astype(float)\n\n# get exchange rate; your solution ...\n</code></pre>"},{"location":"python-extensive/data/api/#conclusion","title":"Conclusion","text":"<p>In this end-to-end example, we have seen how to retrieve data from an API,  store it in a <code>DataFrame</code> and visualize it. With consecutive requests, we  have pulled a list of cryptocurrencies, the price history of a specific  coin and even converted the prices to EUR. </p> <p>Despite this specific use case, concepts like rate limits, endpoints, request methods and query parameters were introduced along the way which are  universal to APIs.</p> Apply your knowledge <p>You should be able to apply your knowledge to other APIs as well. Here  are just a couple of other APIs<sup>1</sup>:</p> <ul> <li>OpenWeatherMap for weather data</li> <li>NASA from astronomy pictures to earth      observation data</li> <li>Google Search      access search results programmatically</li> <li>Spotify access      Spotify's music data</li> <li>Instagram      access Instagram's data</li> </ul> <p>The possibilities are endless. \ud83d\ude80</p> <p>If you want to dive deeper into APIs, we recommend the following resources:</p> <ul> <li>HTTP Status Codes:      Dive deeper into the server responses and their various status codes.</li> <li>FastAPI: Build an API yourself with Python</li> </ul> <ol> <li> <p>Some of these APIs require authentication or are paid services.\u00a0\u21a9</p> </li> </ol>"},{"location":"python-extensive/data/tabular/","title":"Tabular data","text":"<p>With a couple of practical examples, we will discover tips on how to work with tabular data, where to find it, and the various sources and formats you can explore. This chapter will guide you through working with different types of structured data, enabling you to extract, transform, and analyze information from multiple sources.</p> Info <p>This chapter is an extension to the previous <code>pandas</code>  chapter. It should equip you with the necessary skills to acquire data  from various different sources.</p> <p>Our journey will cover a selection of following topics:</p> <ol> <li>Excel: Learn how to read spreadsheets</li> <li>Web Scraping: Extract tables directly from online sources</li> <li>File Writing: Save your data to disk</li> <li>Various Sources: An incomplete list of further data sources</li> </ol>"},{"location":"python-extensive/data/tabular/#prerequisites","title":"Prerequisites","text":"<p>For this chapter we recommend to initialize a new project. Additionally,  create a new virtual environment and activate it.</p> <p>You should end up with a project structure similar to:</p> <pre><code>\ud83d\udcc1 tabular/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u2514\u2500\u2500 \ud83d\udcc4 tabular_data.ipynb\n</code></pre>"},{"location":"python-extensive/data/tabular/#excel","title":"Excel","text":"<p>Let's start off with arguably the most common data source: Excel spreadsheets.</p> <p>Reading Excel files can be straightforward, if they are properly  structured! However, if you see files like these...</p> <p></p> <p>... run, or it will take you several days to parse the file. </p> <p>Although, the example might exaggerate, it is not uncommon to encounter  spreadsheets that are easily readable by humans but hard to parse by machines. Like in the example, the title, empty rows and columns, merged cells,  column names spanning multiple lines, pictures and other formatting can  make it difficult to extract the data in a structured manner.</p>"},{"location":"python-extensive/data/tabular/#reading-excel-files","title":"Reading Excel files","text":"<p>Download the following (structured) file to get started:</p> <p>Student data </p> <p>Data source: Statistik Austria - data.statistik.gv.at<sup>1</sup></p> <p>Place the file within your project directory. The data set contains the  number of students enrolled at universities of applied sciences in Austria  per semester. </p> <p>If you are an MCI student, you are part of this data set!</p> Interested in the creation of the Excel? <p>Since, Statistik Austria provides the data across three files, a <code>Python</code>  script was used to merge everything  into a single Excel. Below you can find the code snippet:</p> <pre><code># Data from:\n# https://data.statistik.gv.at/web/meta.jsp?dataset=OGD_fhsstud_ext_FHS_S_1\nimport pandas as pd\n\nstudents = pd.read_csv(\"OGD_fhsstud_ext_FHS_S_1.csv\", sep=\";\")\nsemester = pd.read_csv(\n    \"OGD_fhsstud_ext_FHS_S_1_C-SEMESTER-0.csv\",\n    sep=\";\",\n    usecols=[\"code\", \"name\"],\n)\nheader = pd.read_csv(\n    \"OGD_fhsstud_ext_FHS_S_1_HEADER.csv\", sep=\";\", usecols=[\"code\", \"name\"]\n)\n\n# replace column codes with their corresponding names\nstudents = students.rename(\n    columns={row[\"code\"]: row[\"name\"] for _, row in header.iterrows()}\n)\n# replace semester codes with their descriptions\nstudents.Berichtssemester = students.Berichtssemester.replace(\n    {row[\"code\"]: row[\"name\"] for _, row in semester.iterrows()}\n)\n\n# get term\nstudents[\"Semester\"] = students.Berichtssemester.str.split(\" \").str[0]\n\n# write Excel\nstudents.to_excel(\"fhsstud.xlsx\", index=False)\n</code></pre> <p>To read the Excel file, we will use <code>pandas</code> in conjunction with <code>openpyxl</code>  (to read and write Excel files):</p> <pre><code>pip install pandas openpyxl\n</code></pre> Tip <p>You can install multiple packages with a single command. Simply  separate the package name with a space.</p> <p>To read the file, it's as simple as:</p> <pre><code>import pandas as pd\n\ndata = pd.read_excel(\"fhsstud.xlsx\")\n</code></pre> <p>As the file is structured, the data loads without any issues. </p>"},{"location":"python-extensive/data/tabular/#reading-specific-sheets","title":"Reading specific sheets","text":"<p>By default, <code>pd.read_excel()</code> loads the first sheet. If you want to read another sheet, you can specify it  with the <code>sheet_name</code> parameter:</p> Create and read a new sheet <ol> <li>Open <code>fhsstud.xlsx</code> within Excel. </li> <li>Manually create a new sheet and fill it with some data of your choice.</li> <li>Save the file.</li> <li>Read the new sheet with <code>pd.read_excel()</code>.</li> </ol>"},{"location":"python-extensive/data/tabular/#detour-visualize-enrolled-students","title":"Detour: Visualize enrolled students","text":"<p>To further consolidate your visualization skills, obtained in the  Plotting chapter, we create a simple plot to visualize the  total of newly enrolled students per winter term in Austria.</p> <p>On a side note, it's quite interesting that the numbers are steadily rising,  with a dip in the winter term 2022/23.</p> Create a static version of the plot <p>Recreate the above plot with <code>pandas</code> and <code>matplotlib</code> as backend.  It does not have to be the same colors, background, title etc.</p> <ol> <li>Subset the data by winter term.</li> <li>Create a suitable plot (e.g., line plot, area plot).</li> </ol>"},{"location":"python-extensive/data/tabular/#writing-excel-files","title":"Writing Excel files","text":"<p>You can't just easily read Excel files, but also write them.</p> <pre><code>data.to_excel(\"fhsstud_copy.xlsx\", index=False)  # (1)!\n</code></pre> <ol> <li>The <code>index=False</code> parameter omits the index to be written to the     file. Have a look at your <code>DataFrame</code>'s index with <code>data.index</code>.</li> </ol> <p>Or you can write multiple sheets:</p> <pre><code>with pd.ExcelWriter(\"fhsstud_multiple.xlsx\") as writer:\n    data.to_excel(writer, sheet_name=\"Students\", index=False)\n    data.to_excel(writer, sheet_name=\"Students-Copy\", index=False)\n</code></pre> <p>Although the same data is written to two different named sheets, you should  get the idea.</p>"},{"location":"python-extensive/data/tabular/#with-statement","title":"<code>with</code> statement","text":"<p>The <code>with</code> statement is used to wrap the execution of a block of code. It is commonly used for resource management, such as opening files or managing database connections, ensuring that resources are properly cleaned up after use</p> <p>In the above example, the <code>with</code> statement is used to open an Excel file for writing and ensures that the Excel writer is properly closed after writing the data.</p>"},{"location":"python-extensive/data/tabular/#web-scraping","title":"Web Scraping","text":"<p>Web scraping is a technique to extract data from websites. It can be used to extract structured data from HTML pages, such as tables.</p> <p>To illustrate web scraping, we pick an example from Wikipedia as our  HTML. We use the english article of the  ATX (Austrian Traded Index) to retrieve a data set with all companies listed in the ATX.</p> Visit the article <ol> <li>Open a new browser tab and visit the  ATX Wikipedia article.</li> <li>Open the source code of the page, the HTML code. To do so, right-click on the page and select <code>View page source</code>.  Alternatively, use the shortcut Ctrl+U. Simply scroll through the HTML code a bit.</li> </ol> <p>You might have noticed that the HTML code is quite complex. Nevertheless,  we can easily extract all the tables on the page with <code>pandas</code>:</p> <pre><code>tables = pd.read_html(\"https://en.wikipedia.org/wiki/Austrian_Traded_Index\")\nprint(type(tables))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'list'&gt;\n</code></pre> <p>Simply by passing the URL to <code>pd.read_html()</code>, we get a list of <code>DataFrame</code> objects. Each <code>DataFrame</code> corresponds to a table found on the page.</p> <p>The second table on the page contains the companies listed in the ATX.  Let's have a look:</p> <pre><code>atx_companies = tables[1]\nprint(atx_companies.head())\n</code></pre> Company Industry Sector Erste Bank Financials Banking Verbund Utilities Electric Utilities OMV Basic Industries Oil &amp; Gas BAWAG Financials Banking Andritz Industrial Goods &amp; Services Industrial Engineering &amp; Machinery <p>The resulting <code>DataFrame</code> <code>atx_companies</code> can be perfectly used as is,  without any further data cleaning.</p> Tip <p>The <code>pd.read_html()</code> function is a powerful tool to extract  tables from HTML pages. However, it might not always work out of the  box by simply passing a URL. Most websites have a complex structure, and the tables might not be directly accessible. In such cases, you might need to use a more sophisticated web scraping packages like</p> <ul> <li><code>BeautifulSoup</code></li> <li><code>Scrapy</code></li> <li><code>Selenium</code></li> </ul> Info <p>Be nice to the websites you scrape! Always check the website's <code>robots.txt</code> file to see if web scraping is allowed. For example,  Wikipedia's <code>robots.txt</code> file can be found at  https://en.wikipedia.org/robots.txt.</p> <p>Respect the website's terms of service and don't overload the  server with requests.</p>"},{"location":"python-extensive/data/tabular/#file-writing","title":"File Writing","text":"<p>So far we have written data solely to Excel files. However, <code>pandas</code> supports a variety of file formats for writing data. Using the <code>atx_companies</code> data  set, we explore two more useful file formats.</p>"},{"location":"python-extensive/data/tabular/#csv","title":"CSV","text":"<p>Writing to a CSV (Comma Separated Values) file is as simple as:</p> <pre><code>atx_companies.to_csv(\"atx_companies.csv\", index=False)\n</code></pre> Tip <p>If you are dealing with large data sets, you might want to consider  compressing the output file and thus reducing the file size.</p> <p>Simply pass a compression algorithm as <code>str</code> (e.g.,<code>\"gzip\"</code>), to the <code>compression</code> parameter:</p> <pre><code>atx_companies.to_csv(\"atx_companies.csv.gz\", index=False, compression=\"gzip\")\n</code></pre> <p>To read a compressed file:</p> <pre><code>_atx_companies = pd.read_csv(\"atx_companies.csv.gz\", compression=\"gzip\")\n</code></pre> <p>Have a look at the documentation of  <code>DataFrame.to_csv()</code> for further options.</p>"},{"location":"python-extensive/data/tabular/#latex","title":"LaTeX","text":"<p>Exporting your data to a LaTeX table can be easily achieved with:</p> <pre><code>atx_companies.to_latex(\"atx_companies.tex\", index=False)\n</code></pre> Additional dependency <p>To use the LaTeX export functionality, <code>Jinja2</code> is required.</p> <pre><code>pip install Jinja2\n</code></pre> <p>Since CSV and LaTeX formats are just a fraction of the supported file formats, navigate to panda's Input/Output section for further reference. </p>"},{"location":"python-extensive/data/tabular/#data-sources","title":"Data Sources","text":"<p>Apart from Excel files and web scraping, there are numerous other  online sources where you can find structured data. Here are a  couple of further links to explore:</p> <ul> <li>data.gv.at - Open government data from Austria    covering various topics like economy, environment, and society.</li> <li>data.statistik.gv.at -    Statistics Austria portal for open data ranging from population,    environment to economy and tourism.</li> <li>UCI Machine Learning Repository -   Popular data repository for machine learning, hosting classic data sets    from various domains.</li> <li>Kaggle - A platform owned by Google to    share data sets, models and code. Although kaggle is free to access, you need   to create an account to download data sets.</li> <li>Google Dataset Search -    Google's search engine for data sets. It helps you find data sets stored    across the web.</li> </ul>"},{"location":"python-extensive/data/tabular/#recap","title":"Recap","text":"<p>This chapter equipped you with the necessary skills to work with Excel  files in <code>Python</code> . Furthermore, we have dipped  our toes into web scraping and learned how to extract structured data from a simple HTML page.</p> <p>Two brief sections on writing data to different file formats and a list of data sources rounded off the chapter.</p> <p>Next up, we will introduce APIs (Application Programming Interfaces) as another possible source of data.</p> <ol> <li> <p>Studien an Fachhochschulen At the time of writing (December 2024), the data was last updated on  2024-07-25.\u00a0\u21a9</p> </li> </ol>"},{"location":"python-extensive/types/bool_and_none/","title":"Bool and none","text":""},{"location":"python-extensive/types/bool_and_none/#boolean-none","title":"Boolean &amp; None","text":"<p>In this section, we introduce two more data types, namely boolean (<code>bool</code>) and None (<code>NoneType</code>). Let's start with the latter one,  <code>NoneType</code>.</p>"},{"location":"python-extensive/types/bool_and_none/#none-nonetype","title":"None (<code>NoneType</code>)","text":"<p><code>NoneType</code> is a special data type in Python that represents the absence of a value.</p> <pre><code>nothing = None\nprint(type(nothing))\n</code></pre> <p>... which outputs:</p> &gt;&gt;&gt; Output<pre><code>&lt;class 'NoneType'&gt;\n</code></pre> <p><code>None</code> can be used as a placeholder for a variable which will be assigned a value later on. Furthermore, if a program was not able to return  a value, <code>None</code> can be used as a return value. </p> <p>Later, <code>None</code> will play a bigger role. For now, we simply keep in mind that <code>None</code> is a thing.</p>"},{"location":"python-extensive/types/bool_and_none/#detour-typeerror","title":"Detour: <code>TypeError</code>","text":"... yet another error. <p>Often, you\u2019ll want to use a variable\u2019s value within a message. For example, say you want to wish someone a happy birthday. You might write code like this:</p> <pre><code>age = 23\nmessage = \"Happy \" + age + \"rd Birthday!\"\nprint(message)\n</code></pre> <p>... results in.</p> <pre><code>Traceback (most recent call last):\n  File \"C:\\\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-34-80a141e301d6&gt;\", line 2, in &lt;module&gt;\n    message = \"Happy \" + age + \"rd Birthday!\"\n              ~~~~~~~~~^~~~~\nTypeError: can only concatenate str (not \"int\") to str\n</code></pre> <p>You might expect this code to print the simple birthday greeting, <code>Happy 23rd birthday!</code>. But if you run this code, you\u2019ll see that it generates an  error.</p> <p>This is a <code>TypeError</code>. It means Python encounters an unexpected  type in <code>age</code>, as strings were mixed with integers in the expression. We will  easily fix the <code>TypeError</code> in the next section.</p>"},{"location":"python-extensive/types/bool_and_none/#casting","title":"Casting","text":"<p>When you use integers within strings like this, you need to specify explicitly  that you want Python to use the integer as a string of characters.  You can do this by wrapping the variable in the <code>str()</code> function, which tells <code>Python</code> to represent non-string values as strings:</p> <pre><code>age = 23\nmessage = \"Happy \" + str(age) + \"rd Birthday!\"\nprint(message)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Happy 23rd Birthday!\n</code></pre> <p>Changing the type of <code>age</code> to string is called casting.</p> Info <p>A <code>TypeError</code> can stem from various 'sources'. This is just one  common example.</p> <p>In the following example the integers <code>3</code> and <code>2</code> were  implicitly cast to floating point numbers, to calculate the result,  which is a floating point number.</p> <pre><code>print(type(3 / 2))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'float'&gt;\n</code></pre> <p>With the function <code>int()</code> any value can be explicitly cast into an  integer, if possible. The value to be cast is passed as the input parameter.</p> <pre><code>number = 3.0\nprint(type(number))\n\n# casting\nnumber = int(number)\nprint(type(number))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'float'&gt;\n&lt;class 'int'&gt;\n</code></pre> <p>To explicitly cast a value into a float, use the function <code>float()</code>.</p> Casting <pre><code># variables\nfirst = \"12\"\nsecond = \"1.2\"\nthird = 12\n</code></pre> <p>For each of the given variables, check whether you can cast them to another type. First, print the type of each variable. Then, use <code>int()</code>, and <code>float()</code> to cast the variables.</p> Note <p>Remember the f-string (<code>f\"...\"</code>) from the previous section? Try a slightly modified example from above.</p> <pre><code>age = 23\nmessage = f\"Happy {age}rd Birthday!\"\nprint(message)\n</code></pre> <p>You'll notice, that there's no need for any explicit casting of <code>age</code>.</p> <p>Whenver, you want to include a variable in a string, remember that  f-strings might be more convenient. \ud83d\ude09</p>"},{"location":"python-extensive/types/bool_and_none/#booleans","title":"Booleans","text":"<p>Computers work with binary (e.g. <code>True</code> or <code>False</code>). Such information can be stored in a single bit. A boolean is either <code>True</code> or <code>False</code>. Booleans are often used to keep  track of certain conditions, such as whether a game is running or whether a  user can edit certain content on a website:</p> <pre><code>game_active = True\ncan_edit = False\n\nprint(type(True))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'bool'&gt;\n</code></pre>"},{"location":"python-extensive/types/bool_and_none/#recap","title":"Recap","text":"<p>In this section, we have introduced two more data types in Python:</p> <ul> <li>None (<code>NoneType</code>)</li> <li>and Booleans (<code>bool</code>)</li> </ul> <p>Now, we have covered all basic types! \ud83c\udf89 With that knowledge, we can already  start to do comparisons and logical operations.</p>"},{"location":"python-extensive/types/numbers/","title":"Numbers","text":""},{"location":"python-extensive/types/numbers/#integers-floats","title":"Integers &amp; Floats","text":""},{"location":"python-extensive/types/numbers/#integers","title":"Integers","text":"<p><code>Python</code>  treats numbers in several different ways, depending on how they are used. Let\u2019s first look at how <code>Python</code> manages whole  numbers called integers (<code>int</code>).</p> <p>Any number without decimal places is automatically interpreted as an integer.</p> <pre><code>number = 10176\n</code></pre> <p>We can verify the type of the variable <code>number</code> with <code>type()</code>.</p> <pre><code>number = 10176\nprint(type(number))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'int'&gt;\n</code></pre>"},{"location":"python-extensive/types/numbers/#arithmetic-operators","title":"Arithmetic operators","text":"<p>Of course, with integers, you can perform basic arithmetic operations. These are:</p> Operator Description <code>+</code> Addition <code>-</code> Subtraction <code>*</code> Multiplication <code>/</code> Division <code>**</code> Exponentiation <code>%</code> Modulo (Used to calculate the remainder of a division) <code>//</code> Floor division <pre><code># Modulo\n20 % 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>2\n</code></pre> <pre><code># Floor division\n20 // 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>6\n</code></pre> Info <p>The floor division <code>//</code> is often referred to as integer division. It rounds down to the nearest whole number.</p> Integer division<pre><code>20 // 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>6\n</code></pre> <p>Contrary, the divison operator <code>/</code> does not round the result.</p> Float division<pre><code>20 / 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>6.666666666666667\n</code></pre> <p>Hence, <code>/</code> is referred to as float division as it always returns  a  <code>float</code> (a number with decimal places). More on floats in a  second.</p> <p>Moreover, you can use multiple operations in one expression. You can also use parentheses to modify the order of operations so Python can evaluate your  expression in the order you specify. For example:</p> <pre><code>2 + 3 * 4\n</code></pre> &gt;&gt;&gt; Output<pre><code>14\n</code></pre> <pre><code>(2 + 3) * 4\n</code></pre> &gt;&gt;&gt; Output<pre><code>20\n</code></pre>"},{"location":"python-extensive/types/numbers/#floats","title":"Floats","text":"<p>Any number with decimal places is automatically interpreted as a <code>float</code>.</p> <pre><code>number = 10176.0\nprint(type(number))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'float'&gt;\n</code></pre> <p>All previously introduced arithmetic operations can be used for floats as well.</p> <pre><code>3.0 + 4.5\n# operations with floats and integers\n3.0 * (4 / 2)\n</code></pre>"},{"location":"python-extensive/types/numbers/#limitations","title":"Limitations","text":"Info <p>Floats are not 100% precise. \"[...] In general, the decimal floating-point  numbers you enter are only approximated by the binary floating-point  numbers actually stored in the machine.\" (The Python Tutorial, 2024)<sup>1</sup></p> <p>So be aware that these small numerical errors could add up in complex  calculations.</p> <pre><code>34.3 + 56.4\n</code></pre> <p>... results in</p> &gt;&gt;&gt; Output<pre><code>90.69999999999999\n</code></pre> Calculate the BEP <p>Use variables and arithmetic operations to calculate the break-even point (the number of units that need to be sold to cover the costs) for a product. The break-even point is given as:</p> \\[ \\text{BEP (units)} = \\frac{\\text{Fixed Costs}}{\\text{Price per Unit} - \\text{Variable Cost per Unit}} \\] <p>Calculate the \\(\\text{BEP}\\) for the following values:</p> <ul> <li>Fixed Costs: 30000</li> <li>Variable Cost per Unit: 45</li> <li>Price per Unit: 75</li> </ul> <p>Assign each given value to a variable. Print the result in a sentence, e.g.  <code>\"The break-even point is X units.\"</code></p>"},{"location":"python-extensive/types/numbers/#recap","title":"Recap","text":"<p>This section was all about numbers in Python. We have covered:</p> <ul> <li>Whole numbers  <code>int</code></li> <li>Decimal numbers  <code>float</code></li> <li>Floating-point arithmetic issues and limitations</li> </ul> <p>Next up, we will introduce the <code>bool</code> and <code>NoneType</code> type in Python.</p> <ol> <li> <p>The Python Tutorial \u21a9</p> </li> </ol>"},{"location":"python-extensive/types/strings/","title":"Strings","text":""},{"location":"python-extensive/types/strings/#strings","title":"Strings","text":"<p>So far, we have already stored some text in a variable. For example  <code>\"Hello World!\"</code> which is called a string. A string is a primitive data type. Integer, float, boolean and None are also primitive data types which we  will cover later.</p> <p>A string is simply a series of characters. Anything inside quotes is considered a string in <code>Python</code>, and you can use single (<code>'</code>) or double  quotes (<code>\"</code>) around your strings like this:</p> <pre><code>text = \"This is a string.\"\nanother_text = 'This is also a string.'\n</code></pre> <p>This flexibility allows you to use quotes and apostrophes within your strings:</p> <pre><code>text = \"One of Python's strengths is its diverse and supportive community.\"\nprint(text)\n</code></pre> &gt;&gt;&gt; Output<pre><code>One of Python's strengths is its diverse and supportive community.\n</code></pre> <pre><code>text = 'I told my friend, \"Python is my favorite language!\"'\nprint(text)\n</code></pre> &gt;&gt;&gt; Output<pre><code>I told my friend, \"Python is my favorite language!\"\n</code></pre>"},{"location":"python-extensive/types/strings/#type","title":"<code>type()</code>","text":"<p>Let's check the type of the variable <code>text</code>.</p> <pre><code>text = \"The language 'Python' is named after Monty Python, not the snake.\"\nprint(type(text))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'str'&gt;\n</code></pre> <p><code>type()</code> comes in handy to check the type of variables. In this  case, we can verify that <code>text</code> is indeed a string. Just like  <code>print()</code>, <code>type()</code>  is an important tool in your programming arsenal.</p> Info <p>It is advisable to consistently enclose your strings with either single  <code>'...'</code> or double quotes <code>\"...\"</code>. This will make your  code easier to read and maintain.</p>"},{"location":"python-extensive/types/strings/#string-methods","title":"String methods","text":"<p>One of the simplest string manipulation, is to change the case of  the words in a string.</p> <pre><code>name = \"paul atreides\"\nprint(name.title())\n</code></pre> &gt;&gt;&gt; Output<pre><code>Paul Atreides\n</code></pre> <p>A method is an action performed on an object (in our case the  variable). The dot (.) in <code>name.title()</code> tells <code>Python</code> to  make the <code>title()</code> method act on the variable <code>name</code> which holds  the value <code>\"paul atreides\"</code>.</p> <p>Every method is followed by a set of parentheses, because methods often need additional information to do their work. That information is provided inside the parentheses. The <code>title()</code> method doesn\u2019t need any additional information, so its parentheses are empty.</p>"},{"location":"python-extensive/types/strings/#methods-vs-functions","title":"Methods vs. functions","text":"<p>We have already encountered functions like <code>print()</code> and <code>type()</code>. Functions are standalone entities that perform a specific task.</p> <p>On the other hand, methods are associated with objects. In this case, the  <code>title()</code> method is associated with the string object <code>name</code>.</p> String methods <p>You start with the variable <code>input_string</code> that holds the value  <code>\"fEyD rAuThA\"</code>. </p> <pre><code>input_string = \"fEyD rAuThA\"\n</code></pre> <p>Experiment and apply a combination of the following methods:</p> <ul> <li><code>capitalize()</code></li> <li><code>title()</code></li> <li><code>istitle()</code></li> <li><code>isupper()</code></li> <li><code>upper()</code></li> <li><code>lower()</code></li> </ul> <p>Eventually you should end up with the string <code>\"Feyd Rautha\"</code>,  print it.</p>"},{"location":"python-extensive/types/strings/#concatenation","title":"Concatenation","text":"<p>It\u2019s often useful to combine strings. For example, you might want to store first and last name in separate variables, and then combine them when you want to display someone\u2019s full name.</p> <p>Python uses the plus symbol (<code>+</code>) to combine strings. In this  example, we use <code>+</code> to create a full name by combining a  <code>first_name</code>, a space, and a <code>last_name</code>:</p> <pre><code>first_name = \"paul\"\nlast_name = \"atreides\"\nfull_name = first_name + \" \" + last_name\nprint(full_name)\n</code></pre> &gt;&gt;&gt; Output<pre><code>paul atreides\n</code></pre> <p>Here, the full name is used in a sentence that greets the user, and the <code>title()</code> method is used to format the name appropriately. This code returns a simple but nicely formatted greeting:</p> <pre><code>full_name = \"paul atreides\"\nprint(\"Hello, \" + full_name.title() + \"!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello, Paul Atreides!\n</code></pre> <p>Another way to nicely format strings is by using f-strings. To achieve the same result as above, simply put an <code>f\"...\"</code> in front of the string and use  curly braces <code>{}</code> to insert the variables. </p> <pre><code>full_name = \"Alia Atreides\"\n\nprint(f\"Hello, {full_name}!\")\n\n# you can even apply methods directly to the variables \n# (within the curly braces)\nprint(f\"Hello, {full_name.lower()}!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello, Alia Atreides!\nHello, alia atreides!\n</code></pre> A quote <p>Find a quote from a famous person you admire. Store the  quote and name in variables and print both with an f-string.</p> <p>Your output should look something like the following,  including the quotation marks: </p> <p><code>Frank Herbert (Dune opening quote): \"I must not fear. Fear is the  mind-killer.\"</code></p>"},{"location":"python-extensive/types/strings/#recap","title":"Recap","text":"<p>This section was all about strings, we have covered:</p> <ul> <li>How to create strings</li> <li>Use <code>type()</code> to check a variable's type</li> <li>String methods, such as <code>title()</code> and <code>lower()</code></li> <li>Distinction between methods and functions</li> <li>String concatenation with <code>+</code> and f-strings (<code>f\"...\"</code>)</li> </ul> <p>Next, up we will introduce numbers in Python, namely integers and floats. </p>"},{"location":"statistics/","title":"Home","text":"<p>In today's data-driven world, the importance of statistics cannot be overstated. Statistics offers the foundational tools for making sense of the vast amount of information surrounding us, helping us draw meaningful conclusions from data and guiding decision-making across a wide array of fields. Whether it's in business, healthcare, engineering, or social sciences, the ability to analyze data effectively has become a crucial skill in the modern age. This chapter aims to introduce the fundamental concepts of statistics, from basic descriptive techniques to more advanced inferential methods.</p> <p>The motivation behind studying statistics lies in its dual role: providing tools for summarizing and understanding data (descriptive statistics), and equipping us with methodologies to infer patterns and make predictions about broader populations from samples (inferential statistics). By mastering these concepts, we enhance our ability to interpret data accurately and make informed decisions, thus fostering a more analytical and evidence-based approach to problem-solving.</p> DID YOU KNOW? byu/hardik_borana_Pro instatisticsmemes <p>The content of this chapter is structured to provide a comprehensive overview of both descriptive and inferential statistics. We begin with univariate and bivariate methods in descriptive statistics, which cover frequency distribution, measures of central tendency, and measures of dispersion. These provide the basic tools to summarize data. We then explore bivariate methods to examine relationships between two variables.</p> <p>Moving into inferential statistics, we delve into probability theory, discussing the essential concepts of probability, random variables, the Law of Large Numbers, and the Central Limit Theorem. These foundational theories pave the way for more complex topics such as hypothesis testing, where we cover hypothesis metrics, T-tests, confidence intervals, and ANOVA. </p> <p>Finally, the chapter concludes with regression analysis, exploring various regression techniques such as linear, multiple, polynomial, and logistic regression. These methods enable us to model relationships between variables, offering deeper insights into data trends and predictions for future outcomes.</p> <p>With this chapter, the goal is to equip you with a solid understanding of statistical principles, empowering you to apply these concepts in practical scenarios and appreciate the critical role statistics plays in our increasingly data-centric world.</p>"},{"location":"statistics/bivariate/Correlation/","title":"Measures of Correlation","text":"<p>This chapter covers the measures of linear relationships, specifically covariance and correlation. These measures help in characterizing the linear relationship between two variables, if such a relationship exists.</p>"},{"location":"statistics/bivariate/Correlation/#covariance","title":"Covariance","text":"<p>In the previous univariate parts, the metrics focused only on a single variable. Covariance allows us to determine the relationship between two variables. </p> Definition: Covariance \\[ \\text{cov}(X, Y) = \\frac{\\sum_{i=1}^{N}(x_i - \\bar{x}) \\cdot (y_i - \\bar{y})}{N} \\] <p>where \\(X\\) and \\(Y\\) are variables consisting of \\(N\\) values, and \\( \\bar{x} \\) and \\( \\bar{y} \\) are their respective means.  </p> <p>The name suggests that it is a type of 'variance,' as \\( \\text{cov}(X, X) = \\sigma^2(X) \\).</p> <pre><code>import plotly.express as px\ndf = px.data.tips()\n\nCovariance = df['total_bill'].cov(df['tip'], ddof=0)\nprint(f\"Covariance: {Covariance}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>8.29\n</code></pre> <p>By default, the degree of freedom <code>ddof=1</code>. In this case the population formular will be used. For the sample formular, the <code>ddof=0</code> needs to be set.</p> Example: Covariance of House Price <p>Given is a table with the size and price of houses. Determine the covariance.</p> <pre><code>size = [60, 72, 111, 67, 90]\nprice = [330000, 490000, 600000, 400000, 455000]\n</code></pre> <p>Solution </p> \\[ \\bar{x} = 80 \\, \\text{m}^2 \\quad \\bar{y} = 455,000 \\, \\text{\u20ac} \\] \\[ \\begin{eqnarray*}                     cov(X,Y)&amp;=&amp;\\frac{[(60-80)\\cdot(330k-455k)]+[(72-80)\\cdot(490k-455k)]+[(111-80)\\cdot(600k-455k)]}{5}\\\\                     &amp;&amp;+\\frac{[(67-80)\\cdot(400k-455k)]+[(90-80)\\cdot(455k-455k)]}{5}\\\\                     &amp;=&amp;1,486,000 \\, \\text{m}^2\\text{\u20ac}             \\end{eqnarray*} \\] <p>The covariance between \\(X\\) and \\(Y\\) is \\(1,486,000 \\, \\text{m}^2\\text{\u20ac}\\).</p> Code <pre><code>import pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame([(60, 330000), (72, 490000), (111, 600000), (67, 400000) , (90, 455000)], columns=['size', 'price'])\n\n# Calculate covariance\nhouse_cov = df['size'].cov(df['price'], ddof=0) # ddof=0 --&gt; Formlar for sample | ddof=1 --&gt; Formlar for population (default)\nprint(f\"Covariance: {house_cov}\")\n</code></pre> <p>Interpretation of Covariance: </p> <ul> <li>Covariance indicates the direction of the relationship:<ul> <li>\\(&gt;0\\): Positive relationship (if \\(X\\) increases, \\(Y\\) increases)</li> <li>\\(=0\\): No relationship</li> <li>\\(&lt;0\\): Negative relationship (if \\(X\\) increases, \\(Y\\) decreases)</li> </ul> </li> <li>Covariance does not provide information about the strength of the relationship since it is not dimensionless.</li> <li>To quantify the strength, we use a normalized measure called the correlation coefficient.  </li> </ul>"},{"location":"statistics/bivariate/Correlation/#correlation-coefficient","title":"Correlation Coefficient","text":""},{"location":"statistics/bivariate/Correlation/#pearson","title":"Pearson","text":"<p>The Pearson correlation coefficient expresses both the direction and strength of the linear relationship between two variables. It is a normalized form of covariance and is symmetric: \\( \\rho_{X,Y} = \\rho_{Y,X} \\).</p> Definition: Pearson  Correlation Coefficient \\[ \\rho = \\frac{\\text{cov}(X, Y)}{\\sigma_x \\cdot \\sigma_y} \\] <p>where \\(X\\) and \\(Y\\) are metric variables, \\( \\sigma_x \\) and \\( \\sigma_y \\) are their standard deviations, and \\( \\text{cov}(X, Y) \\) is the covariance. </p> <pre><code>Pearson = df['total_bill'].corr(df['tip'],method='pearson')\nprint(f\"Pearson Correlation Coefficient: {Pearson}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.68\n</code></pre> Example: Pearson Correlation Cofficient of House Price <p>Given is a table with the size and price of houses.  <pre><code>size = [60, 72, 111, 67, 90]\nprice = [330000, 490000, 600000, 400000, 455000]\n</code></pre> Additionally, we already know:</p> \\[ \\text{cov}(X, Y) = 1,486,000 \\quad \\sigma_x = 18.4 \\quad \\sigma_y = 90,443 \\] <p>Determine the Pearson correlation coefficient.</p> <p>Solution </p> \\[ \\rho = \\frac{1,486,000}{18.4 \\cdot 90,443} = 0.89 \\] <p>The correlation coefficient between \\(X\\) and \\(Y\\) is \\(0.89\\).</p> Code <pre><code>import pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame([(60, 330000), (72, 490000), (111, 600000), (67, 400000) , (90, 455000)], columns=['size', 'price'])\n\n# Calculate pearson correlation coefficient\nhouse_pearson = df['size'].corr(df['price'],method='pearson')\nprint(f\"Pearson Correlation Coefficient: {house_pearson}\")\n</code></pre> <p>Interpretation of the Pearson Correlation Coefficient : </p> <ul> <li>The Pearson correlation coefficient has no dimesion.</li> <li>Its value ranges between \\([-1 \\dots 1]\\) (according to the standard terminology by Jacob Cohen, 1988):<ul> <li>\\(0\\): No correlation</li> <li>\\(&gt; \\pm 0.1\\): Weak correlation</li> <li>\\(&gt; \\pm 0.3\\): Moderate correlation</li> <li>\\(&gt; \\pm 0.5\\): Strong correlation</li> <li>\\(\\pm 1\\): Perfect positive/negative correlation (one variable can be derived from the other)</li> </ul> </li> </ul>"},{"location":"statistics/bivariate/Correlation/#spearman","title":"Spearman","text":"<p>Covariance and the Pearson correlation coefficient require variables to be at least metric. When one of the variables is ordinal, the rank correlation should be calculated, with Spearman's method being a common choice. </p> <pre><code>Spearman = df['total_bill'].corr(df['tip'],method='spearman')\nprint(f\"Spearman Correlation Coefficient: {Spearman}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.68\n</code></pre> <p>In order for <code>df.corr()</code> to deal with ordinal data, the input needs to be numeric. This means that for ordinal data consisting of letters, the data needs to be mapped: </p> <pre><code>day_order = {\n    'Thur' : 4, \n    'Fri'  : 5, \n    'Sat'  : 6,\n    'Sun'  : 7\n    }\ndf['day_ord'] = df['day'].map(day_order)\n\nSpearman = df['day_ord'].corr(df['size'],method='spearman')\nprint(f\"Spearman Correlation Coefficient: {Spearman}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.24\n</code></pre> <p>The interpretation of Spearman's rank correlation is similar to Pearson's.</p> Definition: Spearman Rank Correlation Coefficient \\[ \\rho_s = 1 - \\frac{6 \\cdot \\sum_{i=1}^{N} d_i^2}{N^3 - N} \\quad \\text{where } d_i = R(x_i) - R(y_i) \\] <p>with \\(X\\) and \\(Y\\) as variables consisting of \\(N\\) values and ranks \\(R(x_i)\\) and \\(R(y_i)\\).  </p> <p>Rank Formation</p> <ul> <li> <p>The rank corresponds to the position a value holds when all values are arranged in order.</p> Example: Ranking of Values \\(x_i\\) 2.17 8.00 1.09 2.01 \\(R(x_i)\\) 3 4 1 2 </li> <li> <p>For identical values, the average rank (mean of the relevant ranks) is used.</p> Example: Ranking of Equal Values \\(x_i\\) 1.09 2.17 2.17 2.17 3.02 4.50 \\(R(x_i)\\) 1 3 3 3 5 6 <p>with \\( (2+3+4)/3 = 3 \\)</p> </li> </ul> Example: Spearman Correlation Coefficient of House Price <p>Given is a table with the size and price of houses.  <pre><code>size = [60, 72, 111, 67, 90]\nprice = [330000, 490000, 600000, 400000, 455000]\n</code></pre> Determine the Spearman correlation coefficient.</p> <p>Solution </p> Size R(Size) Price R(Price) \\( d_i \\) 60 1 330k 1 0 72 3 490k 4 -1 111 5 600k 5 0 67 2 400k 2 0 90 4 455k 3 1 \\[ \\rho_s = 1 - \\frac{6 \\cdot [0^2 + 0^2 + (-1)^2 + 1^2 + 0^2]}{5^3 - 5} = 0.9 \\] <p>The Spearman correlation coefficient between \\(X\\) and \\(Y\\) is \\(0.9\\).</p> Code <pre><code>import pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame([(60, 330000), (72, 490000), (111, 600000), (67, 400000) , (90, 455000)], columns=['size', 'price'])\n\n# Calculate spearman correlation coefficient\nhouse_spearman = df['size'].corr(df['price'],method='spearman')\nprint(f\"Spearman Correlation Coefficient: {house_spearman}\")\n</code></pre>"},{"location":"statistics/bivariate/Correlation/#scatter-plot","title":"Scatter Plot","text":"<p>A scatter plot provides a graphical representation of the relationship between two metric variables on a Cartesian coordinate system. </p> <pre><code>import plotly.express as px\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\")\nfig.show()\n</code></pre> <p>Each data pair is treated as a coordinate and is represented by a point on the plot. This allows for identifying relationships, patterns, or trends between the variables.</p> Different Types of Correlation (Source: https://www.geeksforgeeks.org/what-is-correlation-analysis/)  Example: Scatter Plot of House Prices <p></p> Code <pre><code>import pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame([(60, 330000), (72, 490000), (111, 600000), (67, 400000) , (90, 455000)], columns=['size', 'price'])\n\nimport plotly.express as px\n\n# Create a scatter plot\nfig = px.scatter(df, x=\"size\", y=\"price\")\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Size',\n    yaxis_title_text='Price',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;House Prices: Scatter Plot&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Variables: size, price&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre>"},{"location":"statistics/bivariate/Correlation/#recap","title":"Recap","text":"<ul> <li>Measures of linear relationships describe the relationship between two variables.</li> <li>Covariance identifies the direction of the relationship between two metrically scaled variables but not the strength.</li> <li>The Pearson correlation coefficient measures both the direction and strength of the relationship between two metrically scaled variables.</li> <li>If at least one variable is ordinal, the Spearman correlation coefficient is used.</li> <li>A scatter plot can visually represent the relationship between two metric variables.  </li> </ul>"},{"location":"statistics/bivariate/Correlation/#tasks","title":"Tasks","text":"Task: Attribute Correlation <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ncars = fetch_ucirepo(id=9) \n# https://archive.ics.uci.edu/dataset/9/auto+mpg\n\n# data (as pandas dataframes) \ndata = cars.data.features\n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following task: </p> <ol> <li>Analyze the correlation between the variables <code>horsepower</code> and <code>cylinders</code>. Therefore calculate the covariance, pearson correlation coefficient and spearman correlation coefficient. Interpret the results.</li> <li>Generate a scatter plot for the variabels <code>horsepower</code> and <code>cylinders</code>. Compare the before result with the calculated measures. </li> <li>Take a closer look on the different variables and the corresponding attribute type. Is there a variable, where the calculation of the correlation makes no sense? </li> </ol> Task: Income vs. Expenditures <p>Given below are the incomes and weekly consumption expenditures of four households (sample), each measured in euros: <pre><code># \nimport pandas as pd\n\nincome = [150, 250, 175, 165]\nexpenditure = [135, 150, 140, 150]\n\n# convert the lists to pandaframe\ndf = pd.DataFrame({'income': income, 'expenditure': expenditure})\ndf.head()\n</code></pre> Work on the following task: </p> <ol> <li>Calculate the covariance between <code>income</code> and <code>expenditure</code>. Interpret the result.</li> <li>Calculate the covariance when <code>income</code> is measured in Euro cents. How does this affect the interpretation?</li> <li>Switch back to <code>income</code> in Euro. Calculate the Pearson correlation coefficient. Interpret the results.</li> <li>Calculate the correlation coefficient when income is measured in Euro Cents. How does this affect the interpretation?</li> </ol>"},{"location":"statistics/bivariate/Frequency/","title":"Frequency Distribution","text":"<p>Two variables, \\(X\\) and \\(Y\\), consist of \\(n\\) elements. The raw data list consists of the tuples \\((x_1, y_1), \\dots, (x_n, y_n)\\). Possible values are \\(a_1, \\dots, a_k\\) for \\(X\\) and \\(b_1, \\dots, b_m\\) for \\(Y\\). The absolute frequency refers to how often a combination \\((a_i, b_j)\\) occurs.</p> <pre><code>drinks = ['small', 'small', 'small', 'medium', 'medium', 'medium', 'large']\nkcal =   [    123,     154,     123,      201,      201,      201,     434]\n</code></pre> <p>In this example, </p> <ul> <li> <ul> <li>\\( X \\): <code>drinks</code></li> <li>\\( k \\): 3</li> <li>\\( a_1, \\dots, a_k \\): <code>['small', 'medium', 'large']</code></li> </ul> </li> <li> <ul> <li>\\( Y \\): <code>kcal</code></li> <li>\\( m \\): 4</li> <li>\\( b_1, \\dots, b_m \\): <code>[123, 154, 201, 434]</code></li> </ul> </li> </ul> <ul> <li>\\( n \\): <code>7</code></li> <li>\\((x_1, y_1), \\dots, (x_n, y_n)\\): <code>('small', 123), ('small', 154) ... ('large', 434)</code></li> </ul> <p>Representation of frequencies can be done in the form of a table or a graphic. In tabular form, the so-called crosstab (or contingency table) is commonly used. For graphical representation, a histogram (or 2D bar chart) is suitable.  It is important that the data remain the focal point and are presented as accurately and objectively as possible, avoiding distortions like 3D effects or shadows. Titles, axis labels, legends, the data source, and the time of data collection should always be clearly indicated.</p> Definition: Bivariate Frequency <p>Absolute frequency of the combination \\( (a_i, b_j) \\):</p> \\[ h_{ij} = h(a_i, b_j) \\] <p>Relative frequency of the combination \\( (a_i, b_j) \\):</p> \\[ f_{ij} = f(a_i, b_j) = \\frac{h_{ij}}{n} \\]"},{"location":"statistics/bivariate/Frequency/#histogram","title":"Histogram","text":"<p>Histograms are also suitable for bivariate data to represent frequency. A specific type of representation is the density heatmap, which can also be generated using plotly.</p> <pre><code>import plotly.express as px\ndf = px.data.tips()\n\nfig = px.density_heatmap(df, x=\"total_bill\", y=\"tip\")\nfig.show()\n</code></pre> <p>Both absolute and relative frequencies can be visualized using this method (by adding the parameter <code>histnorm='percent'</code>). Multidimensional histograms can be created using the Python package matplotlib.</p> Example: Plotly Heatmap <p></p> Code <pre><code>from ucimlrepo import fetch_ucirepo \nimport plotly.express as px\n\n# fetch dataset \ndrugs = fetch_ucirepo(id=468) \n# https://archive.ics.uci.edu/dataset/462\n\n# data (as pandas dataframes) \ndata = drugs.data.features\n\n# Create a density heatmap\nfig = px.density_heatmap(data, x=\"Region\", y=\"VisitorType\")\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Region',\n    yaxis_title_text='Visitor Type',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Densitiy Heatmap&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: drug_reviews_drugs_com; variable: Region, VisitorType&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre>"},{"location":"statistics/bivariate/Frequency/#crosstab-contingency-table","title":"Crosstab (Contingency Table)","text":"<p>The representation of the joint distribution of discrete features with few categories (if there are many categories, they need to be grouped into categories) can be done using contingency tables. </p> <pre><code>import pandas as pd\nimport plotly.express as px\n\ndf = px.data.tips()\npd.crosstab( df['sex'], df['day'])\n</code></pre> <p>These tables can display both absolute and relative frequencies (by adding the parameter <code>normalize=True</code>). It is important to note that contingency tables use only the nominal scale level, even if a variable could be measured at a higher level (ordinal or numerical).</p> <p>Marginal Frequencies refer to the row and column totals added to a table. The row totals are the marginal frequencies of the variable \\(X\\), calculated as \\( h_{i.} = h_{i1} + \\dots + h_{im} \\) for \\( i = 1, \\dots, k \\). The column totals are the marginal frequencies of the variable \\(Y\\), given by \\( h_{.j} = h_{1j} + \\dots + h_{kj} \\) for \\( j = 1, \\dots, m \\). In <code>Python</code> you just need to add the parameter <code>margins=True</code>.</p> <p>Marginal Distribution refers to the marginal frequencies of a variable, which are the simple frequencies without considering the second variable. The collection of all marginal frequencies for a variable gives the marginal distribution of \\(X\\) (\\(h_{1.}, h_{2.}, \\dots, h_{k.}\\)) or \\(Y\\)(\\(h_{.1}, h_{.2}, \\dots, h_{.m}\\)) in absolute frequencies:</p>"},{"location":"statistics/bivariate/Frequency/#absolute-frequency","title":"Absolute Frequency","text":"Definition: Absolute Crosstab <p>Crosstab of the absolute Frequencies</p> <ul> <li> \\[ \\begin{array}{c|ccc|c}     &amp; b_1 &amp; \\dots &amp; b_m &amp; \\sum \\\\ \\hline     a_1 &amp; h_{11} &amp; \\dots &amp; h_{1m} &amp; h_{1.} \\\\     a_2 &amp; h_{21} &amp; \\dots &amp; h_{2m} &amp; h_{2.} \\\\     \\vdots &amp; \\vdots &amp;  &amp; \\vdots &amp; \\vdots \\\\     a_k &amp; h_{k1} &amp; \\dots &amp; h_{km} &amp; h_{k.} \\\\ \\hline     \\sum &amp; h_{.1} &amp; \\dots &amp; h_{.m} &amp; n \\end{array} \\] </li> <li> <ul> <li>\\( a_i \\): Values of \\( X \\) with \\( i = 1, \\dots, k \\)</li> <li>\\( b_j \\): Values of \\( Y \\) with \\( j = 1, \\dots, m \\)</li> <li>\\( h_{ij} \\): The absolute frequency of the combination \\( (a_i, b_j) \\)</li> <li>\\( h_{1.}, \\dots, h_{k.} \\): The marginal frequencies of \\( X \\)</li> <li>\\( h_{.1}, \\dots, h_{.m} \\): The marginal frequencies of \\( Y \\)</li> <li>\\( n \\): Total number of elements </li> </ul> </li> </ul> Example: Absolute Crosstab Website Visitors <p>Crosstab of the absolute Frequencies </p> \\[ \\begin{array}{r|ccccccccc|c}  &amp; &amp; &amp; &amp; &amp; Region &amp; &amp; &amp; &amp; &amp; \\\\ Visitor Types   &amp; \\textbf{1}  &amp; \\textbf{2}  &amp; \\textbf{3}  &amp; \\textbf{4}  &amp; \\textbf{5}  &amp;\\textbf{ 6}  &amp; \\textbf{7}  &amp; \\textbf{8}  &amp;\\textbf{9}  &amp;  \\sum\\\\\\hline \\textbf{New_Visitor} &amp; 657 &amp;    149&amp;    312&amp;    139&amp;    50&amp; 121&amp;    100&amp;    74&amp; 92&amp; 1694 \\\\ \\textbf{Returning_Visitor} &amp; 4115   &amp;982    &amp;2083   &amp;1038   &amp;268    &amp;683&amp;   659&amp;    359&amp;    364&amp;    10551 \\\\ \\textbf{Other} &amp; 8  &amp;5  &amp;8  &amp;5  &amp;0  &amp;1  &amp;2  &amp;1  &amp;55 &amp;85 \\\\ \\hline \\sum &amp; 4780 &amp;1136   &amp;2403   &amp;1182   &amp;318    &amp;805    &amp;761    &amp;434    &amp;511    &amp;12330\\\\ \\end{array} \\] Code <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ndrugs = fetch_ucirepo(id=468) \n# https://archive.ics.uci.edu/dataset/462\n\n# data (as pandas dataframes) \ndata = drugs.data.features\n\nimport pandas as pd\n\n# Create a crosstab\npd.crosstab( data['VisitorType'],data['Region'], margins=True)\n</code></pre>"},{"location":"statistics/bivariate/Frequency/#relative-frequency","title":"Relative Frequency","text":"Definition: Relative Crosstab <p>Crosstab of the relative Frequencies</p> <ul> <li> \\[ \\begin{array}{c|ccc|c}     &amp; b_1 &amp; \\dots &amp; b_m &amp; \\sum \\\\ \\hline     a_1 &amp; f_{11} &amp; \\dots &amp; f_{1m} &amp; f_{1.} \\\\     a_2 &amp; f_{21} &amp; \\dots &amp; f_{2m} &amp; f_{2.} \\\\     \\vdots &amp; \\vdots &amp;  &amp; \\vdots &amp; \\vdots \\\\     a_k &amp; f_{k1} &amp; \\dots &amp; f_{km} &amp; f_{k.} \\\\ \\hline     \\sum &amp; f_{.1} &amp; \\dots &amp; f_{.m} &amp; 1 \\end{array} \\] </li> <li> <ul> <li>\\( a_i \\): Values of \\( X \\) with \\( i = 1, \\dots, k \\)</li> <li>\\( b_j \\): Values of \\( Y \\) with \\( j = 1, \\dots, m \\)</li> <li>\\( f_{ij} = \\frac{h_{ij}}{n} \\): The relative frequency of the combination \\( (a_i, b_j) \\)</li> <li>\\( f_{i.} = \\frac{h_{i.}}{n} \\): The relative marginal frequencies of \\( X \\)</li> <li>\\( f_{.j} = \\frac{h_{.j}}{n} \\): The relative marginal frequencies of \\( Y \\) </li> </ul> </li> </ul> Example: Relative Crosstab Website Visitors <p>Crosstab of the relative Frequencies in [%]</p> \\[ \\begin{array}{r|ccccccccc|c}  &amp; &amp; &amp; &amp; &amp; Region &amp; &amp; &amp; &amp; &amp; \\\\ Visitor Types   &amp; \\textbf{1}  &amp; \\textbf{2}  &amp; \\textbf{3}  &amp; \\textbf{4}  &amp; \\textbf{5}  &amp;\\textbf{ 6}  &amp; \\textbf{7}  &amp; \\textbf{8}  &amp;\\textbf{9}  &amp;  \\sum\\\\\\hline \\textbf{New_Visitor}        &amp;   5.3 &amp;     1.2 &amp;   2.5 &amp;   1.1 &amp;   0.4 &amp;   1.0 &amp;   0.8 &amp;   0.6 &amp;   0.7 &amp;  13.7 \\\\ \\textbf{Returning_Visitor}  &amp;  33.4 &amp;     8.0 &amp;  16.9 &amp;   8.4 &amp;   2.2 &amp;   5.5 &amp;   5.3 &amp;   2.9 &amp;   3.0 &amp;  85.6 \\\\ \\textbf{Other}              &amp;   0.1 &amp;     0.0 &amp;   0.1 &amp;   0.0 &amp;   0.0 &amp;   0.0 &amp;   0.0 &amp;   0.0 &amp;   0.4 &amp;   0.7 \\\\ \\hline \\sum                        &amp;  38.8 &amp;     9.2 &amp;  19.5 &amp;   9.6 &amp;   2.6 &amp;   6.5 &amp;   6.2 &amp;   3.5 &amp;   4.1 &amp; 100.0\\\\ \\end{array} \\] Code <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ndrugs = fetch_ucirepo(id=468) \n# https://archive.ics.uci.edu/dataset/462\n\n# data (as pandas dataframes) \ndata = drugs.data.features\n\nimport pandas as pd\n\n# Create a crosstab\npd.crosstab( data['VisitorType'],data['Region'], margins=True, normalize='all')\n</code></pre>"},{"location":"statistics/bivariate/Frequency/#conditional-frequency","title":"Conditional frequency","text":"<p>Absolute and relative frequencies are not suitable for determining the relationship between variables. For example, the frequency of regions for <code>New_Visitors</code> and <code>Returning_Visitors</code> cannot be directly compared because the sizes of both groups are different. The conditional relative frequency allows for this comparison by accounting for the differences in group sizes. Therefore, in <code>pd.crosstab()</code> you need to add <code>normalize='index'</code> or <code>normalize='columns'</code></p> Definition: Conditional Frequency <p>Conditional Frequency Distribution of \\( Y \\) given \\( X = a_i \\):</p> \\[ f_{Y,ij} = \\frac{f_{ij}}{f_{i.}} = \\frac{h_{ij}}{h_{i.}} \\] <p>Conditional Frequency Distribution of \\( X \\) given \\( Y = b_j \\):</p> \\[ f_{X,ij} = \\frac{f_{ij}}{f_{.j}} = \\frac{h_{ij}}{h_{.j}} \\] Example: Condtional Frequency of Website Visitors <p>Crosstab of the Conditional Frequencies for given Visitor Types in [%]</p> \\[ \\begin{array}{r|ccccccccc|c}  &amp; &amp; &amp; &amp; &amp; Region &amp; &amp; &amp; &amp; &amp; \\\\ Visitor Types   &amp; \\textbf{1}  &amp; \\textbf{2}  &amp; \\textbf{3}  &amp; \\textbf{4}  &amp; \\textbf{5}  &amp;\\textbf{ 6}  &amp; \\textbf{7}  &amp; \\textbf{8}  &amp;\\textbf{9}  &amp;  \\sum\\\\\\hline \\textbf{New_Visitor}        &amp; 38.8 &amp; 8.8 &amp; 18.4 &amp; 8.2 &amp; 3.0 &amp; 7.1 &amp; 5.9 &amp; 4.4 &amp;  5.4 &amp; 100 \\\\ \\textbf{Returning_Visitor}  &amp; 39.0 &amp; 9.3 &amp; 19.7 &amp; 9.8 &amp; 2.5 &amp; 6.5 &amp; 6.2 &amp; 3.4 &amp;  3.4 &amp; 100 \\\\ \\textbf{Other}              &amp;  9.4 &amp; 5.9 &amp;  9.4 &amp; 5.9 &amp; 0.0 &amp; 1.2 &amp; 2.4 &amp; 1.2 &amp; 64.7 &amp; 100 \\\\ \\hline \\sum                        &amp; 38.8 &amp; 9.2 &amp; 19.5 &amp; 9.6 &amp; 2.6 &amp; 6.5 &amp; 6.2 &amp; 3.5 &amp;  4.1 &amp; 100\\\\ \\end{array} \\] Code <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ndrugs = fetch_ucirepo(id=468) \n# https://archive.ics.uci.edu/dataset/462\n\n# data (as pandas dataframes) \ndata = drugs.data.features\n\nimport pandas as pd\n\n# Create a crosstab\nprint(pd.crosstab( data['VisitorType'],data['Region'], margins=True, normalize='index'))\n</code></pre> Example: Condtional Frequency of Website Visitors <p>Crosstab of the Conditional Frequencies for given Region in [%]</p> \\[ \\begin{array}{r|ccccccccc|c}  &amp; &amp; &amp; &amp; &amp; Region &amp; &amp; &amp; &amp; &amp; \\\\ Visitor Types   &amp; \\textbf{1}  &amp; \\textbf{2}  &amp; \\textbf{3}  &amp; \\textbf{4}  &amp; \\textbf{5}  &amp;\\textbf{ 6}  &amp; \\textbf{7}  &amp; \\textbf{8}  &amp;\\textbf{9}  &amp;  \\sum\\\\\\hline \\textbf{New_Visitor}        &amp; 13.7 &amp; 13.1 &amp; 13.0 &amp; 11.8 &amp; 15.7 &amp; 15.0 &amp; 13.1 &amp; 17.1 &amp; 18.0 &amp; 13.7  \\\\ \\textbf{Returning_Visitor}  &amp; 86.1 &amp; 86.4 &amp; 86.7 &amp; 87.8 &amp; 84.3 &amp; 84.8 &amp; 86.6 &amp; 82.7 &amp; 71.2 &amp; 85.6  \\\\ \\textbf{Other}              &amp;  0.2 &amp;  0.4 &amp;  0.3 &amp;  0.4 &amp;  0.0 &amp;  0.1 &amp;  0.3 &amp;  0.2 &amp; 10.8 &amp;  0.7  \\\\ \\hline \\sum                        &amp; 100&amp; 100&amp; 100&amp; 100&amp; 100&amp; 100&amp; 100&amp; 100&amp; 100&amp; 100\\\\ \\end{array} \\] Code <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ndrugs = fetch_ucirepo(id=468) \n# https://archive.ics.uci.edu/dataset/462\n\n# data (as pandas dataframes) \ndata = drugs.data.features\n\nimport pandas as pd\n\n# Create a crosstab\nprint(pd.crosstab( data['VisitorType'],data['Region'], margins=True, normalize='columns'))\n</code></pre>"},{"location":"statistics/bivariate/Frequency/#recap","title":"Recap","text":"<ul> <li>Frequencies in the bivariate case describe how often a combination of two values occurs.</li> <li>As in the univariate case, a distinction between absolute and relative frequency is made.</li> <li>2D histograms or contingency tables can be used for representation.</li> <li>Relationships between variables are not easily identified in either absolute or relative contingency tables.</li> <li>The conditional frequency examines the frequency distribution of one variable while fixing the second variable.</li> </ul>"},{"location":"statistics/bivariate/Frequency/#tasks","title":"Tasks","text":"Task: Bivariate Frequency <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ncars = fetch_ucirepo(id=9) \n# https://archive.ics.uci.edu/dataset/9/auto+mpg\n\n# data (as pandas dataframes) \ndata = cars.data.features\n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following task: </p> <ol> <li>Generate a 2D Histogram for the variables <code>origin</code> and <code>horsepower</code> (think about attribute types, title, labeling of the axes). Interpret the results. </li> <li>Calculate the crosstab for the absolute frequencies of the variables <code>origin</code> and <code>cylinders</code></li> <li>Calculate the conditional crosstab for the relative frequencies to answer the following question: Whats the cylinder distributed within each origin? </li> </ol>"},{"location":"statistics/hypothesis/ANOVA/","title":"ANOVA","text":"<p>ANOVA, which stands for Analysis of Variance, is a statistical method used to determine whether there are statistically significant differences between the means of three or more groups. Unlike the t-test, which examines whether there is a difference between two groups, ANOVA is used to assess differences among multiple group means simultaneously by comparing the variances within each group to the variances between the groups. The primary goal of ANOVA is to determine whether any of those differences are statistically significant.</p> (Source: Imgflip Meme Generator)"},{"location":"statistics/hypothesis/ANOVA/#types-of-anova","title":"Types of ANOVA","text":"<ul> <li> <p>One-Way ANOVA</p> <p>analyzes the impact of a single independent variable (factor) with three or more levels on a continuous dependent variable.</p> Example One-Way ANOVA <p>A factory manager uses One-Way ANOVA to compare the average production times of widgets produced by Machine A, Machine B, and Machine C.</p> </li> <li> <p>Two-Way ANOVA</p> <p>evaluates the effects of two independent variables (factors) simultaneously and examines the interaction between them on a continuous dependent variable.</p> Example Two-Way ANOVA <p>A researcher employs Two-Way ANOVA to study the effects of different fertilizers and watering schedules on plant growth.</p> </li> <li> <p>Repeated Measures ANOVA</p> <p>assesses the effects of one or more factors when the same subjects are measured multiple times under different conditions.</p> Example Repeated Measures ANOVA <p>A psychologist uses Repeated Measures ANOVA to evaluate participants' stress levels before, during, and after a meditation intervention.</p> </li> <li> <p>MANOVA (Multivariate ANOVA)</p> <p>extends ANOVA by analyzing the effects of one or more independent variables on two or more dependent variables simultaneously.</p> Example MANOVA <p>An educator applies MANOVA to investigate how teaching methods and class sizes influence both student performance and engagement levels.</p> </li> </ul> (Source: Imgflip Meme Generator)"},{"location":"statistics/hypothesis/ANOVA/#one-way-anova","title":"One-Way ANOVA","text":"<p>One-Way ANOVA is a statistical method used to determine whether there are any statistically significant differences between the means of three or more independent (unrelated) groups. Unlike the t-test, which compares the means of two groups, One-Way ANOVA can handle multiple groups simultaneously, making it a powerful tool for analyzing variations within and between groups.</p> Example One-Way ANOVA in Production Scenario <p>In a factory, there are three different machines used to assemble electronic components: Machine A, Machine B, and Machine C. The production manager wants to know if the type of machine impacts the production time of the components.</p> <p>The key concepts of the One-Way ANOVA includes: </p> <ul> <li>Factor: The independent variable that categorizes the data. In One-Way ANOVA, there is only one factor.</li> <li>Levels: The different categories or groups within the factor.</li> <li>Dependent Variable: The outcome or response variable that is measured.</li> <li>Null Hypothesis (H<sub>0</sub>): Assumes that all group means are equal.</li> <li>Alternative Hypothesis (H<sub>1</sub>): Assumes that at least one group mean is different.</li> </ul> Example One-Way ANOVA in Production Scenario <ul> <li>Factor: Machine Types</li> <li>Levels: Machine A, Machine B, Machine C</li> <li>Dependent Variable: Production Time</li> <li> <p>Null Hypothesis (H<sub>0</sub>): there is no difference in the average assembly time among the three machines.</p> \\[     H_0 : \\mu_A = \\mu_B = \\mu_C \\] </li> <li> <p>Alternative Hypothesis (H<sub>1</sub>): at least one group mean is different.</p> </li> </ul>"},{"location":"statistics/hypothesis/ANOVA/#when-to-use-one-way-anova","title":"When to Use One-Way ANOVA","text":"<p>One-Way ANOVA is appropriate when you want to:</p> <ul> <li>Compare the means of three or more independent groups.</li> <li>Assess the impact of a single categorical factor on a continuous dependent variable.</li> <li>Determine if at least one group mean significantly differs from the others.</li> </ul>"},{"location":"statistics/hypothesis/ANOVA/#approach","title":"Approach","text":"<p>At its core, ANOVA partitions the total variance in the data into components attributable to different sources. Here's a simplified breakdown:</p> <ol> <li> <p>Hypothesis Definition</p> <ul> <li>Null Hypothesis (\\( H_0 \\)): All group means are equal.</li> <li>Alternative Hypothesis (\\( H_a \\)): At least one group mean is different.</li> </ul> </li> <li> <p>Sum of Squares (SS)</p> <ol> <li> <p>Total Sum of Squares (SS Total)</p> Definition: SS Total <p>Measure of the total variability in the data</p> \\[ SS_{Total} = \\sum_{i=1}^{N} (Y_i - \\overline{Y})^2 \\] <p>with: </p> <ul> <li>\\( Y_i \\) = individual observations  </li> <li>\\( \\overline{Y} \\) = grand mean</li> <li>\\( N \\) = total number of observations  </li> </ul> </li> <li> <p>Between-Group Sum of Squares (SS Between)</p> Definition: SS Between <p>Measure of the variability due to the factor (e.g., different treatments).</p> \\[ SS_{Between} = \\sum_{j=1}^{k} n_j (\\overline{Y}_j - \\overline{Y})^2 \\] <p>with:</p> <ul> <li>\\( \\overline{Y}_j \\) = mean of group \\( j \\) </li> <li>\\( n_j \\) = number of observations in group \\( j \\)</li> <li>\\( k \\) = number of groups  </li> </ul> </li> <li> <p>Within-Group Sum of Squares (SS Within)</p> Definition: SS Between <p>Measure of the variability within each group.</p> \\[ SS_{Within} = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (Y_{ij} - \\overline{Y}_j)^2 \\] </li> </ol> </li> <li> <p>Degrees of Freedom (df)</p> Definition: Degree of Freedom \\[ df_{Total} = N-1 \\quad | \\quad df_{Between} = k-1 \\quad | \\quad df_{Within} = N-k \\] <p>with:</p> <ul> <li>\\( N \\) = total number of observations  </li> <li>\\( k \\) = number of groups </li> </ul> </li> <li> <p>Mean Squares (MS)</p> Definition: Mean Squares \\[ MS_{Between} = \\frac{SS_{Between}}{df_{Between}} \\quad | \\quad MS_{Within} =  \\frac{SS_{Within}}{df_{Within}} \\] </li> <li> <p>F-Statistic</p> Definition: ANOVA F-Statistic \\[ F = \\frac{MS_{Between}}{MS_{Within}} \\] <p>Interpretation: A higher F-value indicates greater evidence against the null hypothesis. The p-value is derived from the F-distribution and determines the statistical significance of the results.</p> <p>Constructing the ANOVA Table</p> Source of Variation Sum of Squares (SS) Degrees of Freedom (df) Mean Square (MS) F-Statistic p-Value Between Groups \\( SS_{Between} \\) \\( k - 1 \\) \\( MS_{Between} \\) \\( F \\) Within Groups \\( SS_{Within} \\) \\( N - k \\) \\( MS_{Within} \\) Total \\( SS_{Total} \\) \\( N - 1 \\) </li> <li> <p>Interpretation of Results     As we have have seen before in the T-Test and F-Test, if the p-value is smaller than \\( \\alpha\\) (commonly 0.05) we reject H<sub>0</sub>. If \\(p&gt; \\alpha\\) we fail to reject \\( H_0 \\).</p> <p>If the ANOVA is significant, determine which specific groups differ using post-hoc tests like the Tukey HSD test to control for multiple comparisons.</p> </li> </ol> Example One-Way ANOVA in Production Scenario <p>Let\u2019s consider a realistic production scenario. A factory uses three different machines (Machine A, Machine B, and Machine C) to assemble electronic components. The production manager wants to know if the machine type affects the assembly time.</p> <p>The Production times (in minutes) for 5 widgets from each machine:</p> <pre><code># Production Time in Minutes\nA = [12, 14, 13, 15, 14] # Machine A\nB = [16, 18, 17, 19, 18] # Machine B\nC = [11, 10, 12, 11, 10] # Machine C\n</code></pre> Manual Calculation <ul> <li> <p>Calculate Group Means:     <pre><code>k = 3\n\nY_mean_A = np.mean(A)\nY_mean_B = np.mean(B)\nY_mean_C = np.mean(C)\n\nY_mean = (Y_mean_A + Y_mean_B + Y_mean_C) / k\n\nprint(f\"Mean of Machine A: {Y_mean_A} minutes\")\nprint(f\"Mean of Machine B: {Y_mean_B} minutes\")\nprint(f\"Mean of Machine C: {Y_mean_C} minutes\")\n\nprint(f\"Mean of Production Time: {Y_mean} minutes\")\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>Mean of Machine A: 13.6 minutes\nMean of Machine B: 17.6 minutes\nMean of Machine C: 10.8 minutes\nMean of Production Time: 14.0 minutes\n</code></pre> </li> </ul> <ul> <li> <p>Sum of Squares (SS) <pre><code># Calculate Sum of Squares Betwen Groups\n\nSSB = len(A)*(Y_mean_A - Y_mean)**2 + len(B)*(Y_mean_B - Y_mean)**2 + len(C)*(Y_mean_C - Y_mean)**2\nprint(f\"Sum of Squares Between Groups: {SSB}\")\n\n# Calculate Sum of Squares Within Groups\n\nSSW = sum((np.array(A) - Y_mean_A)**2) + sum((np.array(B) - Y_mean_B)**2) + sum((np.array(C) - Y_mean_C)**2)\nprint(f\"Sum of Squares Within Groups: {SSW}\")\n\n# Calculate Sum of Squares Total\n\nSST = SSB + SSW\nprint(f\"Sum of Squares Total: {SST}\")\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>Sum of Squares Between Groups: 116.8\nSum of Squares Within Groups: 13.2\nSum of Squares Total: 130\n</code></pre> </li> </ul> <ul> <li> <p>Degrees of Freedom <pre><code># Degrees of Freedom\nN = len(A) + len(B) + len(C)\n\ndf_B = k - 1\ndf_W = N - k\ndf_T = N - 1\n\nprint(f\"df Between Groups: {df_B}\")\nprint(f\"df Within Groups: {df_W}\")\nprint(f\"df Total: {df_T}\")\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>df Between Groups: 2\ndf Within Groups: 12\ndf Total: 14\n</code></pre> </li> </ul> <ul> <li> <p>Mean Squares <pre><code># Mean Squares\n\nMSB = SSB / df_B\nMSW = SSW / df_W\n\nprint(f\"Mean Squares Between Groups: {MSB}\")\nprint(f\"Mean Squares Within Groups: {MSW}\")\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>Mean Squares Between Groups: 58.4\nMean Squares Within Groups: 1.1\n</code></pre> </li> </ul> <ul> <li> <p>F-Statistic <pre><code># F-Statistic\n\nF = MSB / MSW\nprint(f\"F-Statistic: {F}\")\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>F-Statistic: 53.1\n</code></pre> </li> </ul> <ul> <li>Determine p-Value     Using an F-distribution table or statistical software with \\( df_1 = df_{Between} = 2 \\) and \\( df_2 = df_{Within} = 12 \\), an F-value of 53.1 is highly significant (p &lt; 0.001).</li> </ul> Automatic Calculation <p>For calculation the p-value and the f-statistics of the ANOVA we can use the 'f_oneway' method of the 'scipy.stats' library: </p> <pre><code>from scipy.stats import f_oneway\n\nf,p = f_oneway(A, B, C)\n\nprint(f\"F-Statistic: {f}\")\nprint(f\"P-Value: {p}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>F-Statistic: 53.09090909090907\nP-Value: 1.0959316602384747e-06\n</code></pre> <ul> <li> <p>Interpretation</p> <p>Since the p-value is less than 0.05, we reject the null hypothesis. There are significant differences in production times among the three machines.</p> </li> </ul> <ul> <li> <p>Post-Hoc Analysis     To identify which specific machines differ we can perform a Tukey HSD Test revealing that:</p> <pre><code>from scipy.stats import tukey_hsd\n\ntukey_results = tukey_hsd(A,B,C)\nprint(tukey_results)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Tukey's HSD Pairwise Group Comparisons (95.0% Confidence Interval)\nComparison  Statistic  p-value  Lower CI  Upper CI\n(0 - 1)     -4.000     0.000    -5.770    -2.230\n(0 - 2)      2.800     0.003     1.030     4.570\n(1 - 0)      4.000     0.000     2.230     5.770\n(1 - 2)      6.800     0.000     5.030     8.570\n(2 - 0)     -2.800     0.003    -4.570    -1.030\n(2 - 1)     -6.800     0.000    -8.570    -5.030\n</code></pre> <ul> <li>Machine A vs. Machine B: Significant difference</li> <li>Machine A vs. Machine C: Significant difference</li> <li>Machine B vs. Machine C: Significant difference</li> </ul> </li> </ul> <ul> <li>Conclusion: All three machines have significantly different production times, with Machine B (<code>Y_mean_A = 17.6</code>) being the slowest and Machine C (<code>Y_mean_A = 10.8</code>) being the fastest.</li> </ul>"},{"location":"statistics/hypothesis/ANOVA/#assumptions-of-one-way-anova","title":"Assumptions of One-Way ANOVA","text":"<p>Before performing One-Way ANOVA, ensure that your data meet the following assumptions:</p> <ul> <li>Independence of Observations: The samples are independent of each other.</li> <li>Normality: The data in each group are approximately normally distributed.</li> <li>Homogeneity of Variances: The variance among the groups should be approximately equal.</li> </ul>"},{"location":"statistics/hypothesis/ANOVA/#task","title":"Task","text":"Task: Student Performance <p>Download the following dataset and load it into your notebook. Therefore use the python package <code>openml</code>. </p> <pre><code># Dataset: https://www.openml.org/search?type=data&amp;status=active&amp;id=43098\nimport openml\n\ndataset = openml.datasets.get_dataset(43098)\ndf ,_ ,_ ,_  = dataset.get_data(dataset_format=\"dataframe\", target=None)\ndf.head()\n</code></pre> <p>It contains the academic performance of 1000 students in different subjects. Answer the following questions using Python:</p> <ol> <li>Are the <code>math.score</code> results of different <code>race.ethnicity</code> groups significantly different (\\(\\alpha = 5\\%\\))?</li> <li>Are the <code>reading.score</code> results of different <code>lunch</code> groups significantly different?</li> </ol> <p>For both questions, proceed as follows:</p> <ul> <li>Perform an ANOVA to answer both questions. Formulate an H<sub>0</sub> and perform:<ul> <li>Manual calculation of the ANOVA</li> <li>Automatic calculation of the ANOVA using <code>scipiy.stats</code>.</li> </ul> </li> <li>Are both results matching? Interpret the results.</li> </ul>"},{"location":"statistics/hypothesis/General/","title":"Hypothesis","text":""},{"location":"statistics/hypothesis/General/#independent-variables","title":"(In)dependent Variables","text":"<p>Let's begin with the basic concepts of independent variables (IVs) and dependent variables (DVs). </p> <ul> <li>The dependent variable is what you're aiming to explain or predict in your study (the target). </li> <li>The independent variables are the factors you believe will influence or cause changes in the dependent variable (the features).</li> </ul> <p>In experimental research, independent variables are what you manipulate or control to observe their effect on the dependent variable. For example, if you're studying the effect of water on a plant, the amount of water is the independent variable, and the size or number of leaves are the dependent variable. </p> Controlable Variables <p>Not all independent variables are under your control; some are observed variables that you believe have an impact on the dependent variable.</p> (Source: Sciencenotes.org)  <p>Let's explore a few examples.</p> Examples: IV/DV Easy <ol> <li> <p>The effect of sleep duration on cognitive function.</p> <ul> <li>IV: Sleep duration\u2014it's what you might manipulate or measure to see its effect.</li> <li>DV: Cognitive function\u2014the outcome you're trying to explain or predict.</li> </ul> </li> <li> <p>The relationship between socioeconomic status and health outcomes.</p> <ul> <li>IV: Socioeconomic status\u2014it potentially influences health outcomes.</li> <li>DV: Health outcomes\u2014the aspect you're studying in relation to socioeconomic status.</li> </ul> </li> <li> <p>Does the amount of screen time affect children's attention spans?</p> <ul> <li>IV: Amount of screen time - this is the variable you might adjust or observe.</li> <li>DV: Children's attention spans - the outcome you're assessing.</li> </ul> </li> </ol> <p>In some cases, distinguishing between the IV and DV can be ambiguous. For instance:</p> Examples: IV/DV Difficult <p>The correlation between stress levels and physical activity.</p> <p>It's not immediately clear which variable influences the other. Does increased stress lead to less physical activity, or does less physical activity contribute to higher stress levels? Depending on your research focus, you might assign stress levels as the IV and physical activity as the DV, or vice versa.</p>"},{"location":"statistics/hypothesis/General/#models-and-modeling","title":"Models and Modeling","text":""},{"location":"statistics/hypothesis/General/#definition","title":"Definition","text":"<p>Models are simplified representations of reality, designed to explain or predict phenomena by highlighting essential features while ignoring less critical details. The real world is incredibly complex, and models help us make sense of it by focusing on key variables.</p> Imagination Examples: Model <p>Consider, for example, modeling what influences a person's academic performance. A simple model might include:</p> <ul> <li>Study Time: More hours spent studying could lead to better performance.</li> <li>Class Attendance: Regular attendance might improve understanding of the material.</li> <li>Access to Resources: Availability of textbooks and learning materials could enhance learning.</li> </ul> <p>Our model might look something like this:</p> \\[ \\text{Academic Performance} = \\beta_1 (\\text{Study Time}) + \\beta_2 (\\text{Class Attendance}) + \\beta_3 (\\text{Access to Resources}) + \\epsilon \\] <p>Here, \\( \\beta_1, \\beta_2, \\beta_3 \\) are coefficients representing the influence of each independent variable, and \\( \\epsilon \\) is the residual or error term. The residual captures all other factors affecting academic performance that aren't included in the model.</p>"},{"location":"statistics/hypothesis/General/#residuals","title":"Residuals","text":"<p>Residuals are a key component in evaluating the effectiveness of a statistical model. They represent the difference between the observed data points and the values predicted by the model</p> Interpretation: Residuals <p>Residuals essentially capturing what the model fails to explain. </p> <p>For example, suppose you're modeling the impact of study time (independent variable) on test scores (dependent variable). After applying your model, you find that some students scored higher or lower than predicted. These differences are the residuals. </p> <ul> <li>Small residuals imply that the model accurately captures the relationship between study time and test scores. </li> <li>On the other hand, large residuals indicate that there are other factors - like prior knowledge or test anxiety - not accounted for in the model. </li> </ul> <p>Analyzing residuals helps you identify shortcomings in your model and areas where it can be refined for better accuracy.</p>"},{"location":"statistics/hypothesis/General/#fitting","title":"Fitting","text":"<p>In modeling, there's a crucial balance between simplicity and accuracy:</p> <ul> <li>Underfitting: A model that's too simple may not capture important patterns in the data, leading to large residuals and poor predictions.</li> <li>Overfitting: A model that's too complex may fit the training data too closely, including the noise, and may not generalize well to new data.</li> <li>Optimal Model: A model with an optimal fit captures the underlying trend without fitting the noise.</li> </ul> (Source: Medium)  <p>The goal is to develop a model that's as simple as possible but still effectively explains the data - a concept known as Occam's Razor in philosophy.</p>"},{"location":"statistics/hypothesis/General/#models-vs-hypothesis","title":"Models vs Hypothesis","text":"<p>Let's touch on hypothesis testing. In statistics, hypothesis testing is essentially about comparing models to see which one better fits the data. A hypothesis might propose that one model is superior to another in explaining a particular phenomenon.</p> <p>For example:</p> <ul> <li>Null Hypothesis (\\( H_0 \\)): There is no effect of study time on exam scores - the simpler model without the study time variable fits the data just as well.</li> <li>Alternative Hypothesis (\\( H_1 \\)): Study time does affect exam scores - the model including study time provides a better fit.</li> </ul> <p>By testing these hypotheses, we're evaluating whether adding a variable (making the model more complex) significantly improves our ability to explain the dependent variable.</p>"},{"location":"statistics/hypothesis/General/#hypothesis_1","title":"Hypothesis","text":"<p>In this section, we'll delve into the concept of hypothesis testing, a cornerstone of statistical analysis. Understanding what a hypothesis is and how to formulate a strong one is essential for designing experiments and interpreting data effectively.</p> (Source: Imgflip Meme Generator)"},{"location":"statistics/hypothesis/General/#what-is-a-hypothesis","title":"What Is a Hypothesis?","text":"<p>Hypothesis: an idea or explanation for something that is based on known facts but has not yet been proved</p> <p>-- Cambridge Dictionary</p> <p>So in other words, a hypothesis is a testable statement that predicts a relationship between variables. It is an educated guess based on prior knowledge and observation, which can be rejected or not through experimentation or further observation. Crucially, a hypothesis must be falsifiable, meaning there must be a possible negative answer to the hypothesis.</p> <p>Hypotheses serve several critical functions in research and data analysis:</p> <ul> <li>Enhance Experimental Design: They provide a clear focus and direction for designing experiments.</li> <li>Promote Critical Thinking: Formulating hypotheses encourages logical reasoning and careful consideration of potential outcomes.</li> <li>Guide Data Analysis: They help determine which statistical tests to use and how to interpret the results.</li> <li>Advance Knowledge: Hypotheses enable the development and refinement of theories, contributing to scientific progress.</li> <li>Test Theories: They allow researchers to challenge existing theories and potentially replace them with more accurate ones.</li> </ul> <p>By converting abstract ideas into specific, testable predictions, hypotheses facilitate meaningful research and discoveries.</p>"},{"location":"statistics/hypothesis/General/#characteristics-of-a-hypothesis","title":"Characteristics of a Hypothesis","text":"<p>A Hypothesis should fullfill some requirements to be considered as strong. Those requirement are:</p> <ul> <li>Clear and Specific: It precisely states the expected relationship between variables.</li> <li>Testable and Falsifiable: It can be supported or refuted through experimentation or observation.</li> <li>Based on Existing Knowledge: It relies on prior research or established theories.</li> <li>Predictive: It makes definite predictions about outcomes.</li> <li>Relevant: It has implications for understanding broader phenomena, not just a single dataset.</li> <li>Directional (when appropriate): It specifies the expected direction of the relationship (e.g., increases, decreases).</li> </ul> Not a Hypothesis <p>\"Technology is changing rapidly.\"</p> <p>Explanation: This is a general observation, not a testable hypothesis. It lacks specificity and does not predict a measurable outcome.</p> Weak Hypothesis <p>\"Eating fruits affects health.\"</p> <p>Explanation: While somewhat testable, it's vague. It doesn't specify which fruits, what aspect of health, or the nature of the effect.</p> Strong Hypothesis <p>\"Adults who eat an apple a day have lower cholesterol levels than those who do not.\"</p> <p>Explanation*: This hypothesis is specific, testable, and based on prior knowledge about the health benefits of apples. It predicts a measurable outcome (cholesterol levels) in a defined group (adults).</p> <p>To explore the concept of hypothesis characteristics we can practice on some examples</p> Task: Hypothesis Characteristics <p>Let's practice by classifying some statements (no/weak/strong hypothesis):</p> <ol> <li>\"Does exercise improve mental health?\"</li> <li>\"Drinking green tea leads to weight loss.\"</li> <li>\"College students who sleep at least 7 hours per night have higher GPAs than those who sleep less.\"</li> <li>\"Reading improves language skills.\"</li> <li>\"Children exposed to bilingual education from an early age will perform better on cognitive flexibility tests than those who are not.\"</li> </ol>"},{"location":"statistics/hypothesis/General/#the-null-hypothesis","title":"The Null Hypothesis","text":"<p>In statistical testing, we often work with two hypotheses:</p> Definition: Null &amp; Alternative Hypothesis <ul> <li>Null Hypothesis (\\( H_0 \\)): Assumes no effect or no difference between groups or variables.</li> <li>Alternative Hypothesis (\\( H_1 \\) or \\( H_a \\)): Proposes that there is an effect or a difference.</li> </ul> Examples: \\( H_0 \\) &amp; \\( H_1 \\) <ul> <li>Null Hypothesis \\( H_0 \\): \"Listening to classical music while studying has no effect on memory retention in high school students.\"</li> <li>Alternative Hypothesis \\( H_1 \\): \"Listening to classical music while studying improves memory retention in high school students.\"</li> </ul> <p>In statistical analyses, we test the null hypothesis to determine whether there is sufficient evidence to reject it in favor of the alternative hypothesis. Rejecting the null hypothesis suggests that the data support the alternative hypothesis.</p> (Source: Imgflip Meme Generator)  <p>But why Focus on the Null Hypothesis?</p> <ul> <li>Statistical Simplicity: It's mathematically simpler to test for no effect than to prove a specific effect.</li> <li>Avoiding Bias: It prevents researchers from seeing effects that aren't there due to expectations.</li> <li>Falsifiability: It's easier to disprove (falsify) a universal negative (no effect) than to prove a universal positive.</li> </ul>"},{"location":"statistics/hypothesis/General/#recap","title":"Recap","text":"<ul> <li>Independent Variables (IVs): Factors you believe influence the outcome.</li> <li>Dependent Variables (DVs): The outcomes you're trying to explain or predict.</li> <li>Models: Simplified representations of reality using equations to explain relationships between variables.</li> <li>Residuals: Differences between the observed data and what the model predicts; they capture unexplained variability.</li> <li>Balance in Modeling: Aim for models that are simple yet sufficiently complex to accurately capture the underlying patterns in the data.</li> <li>Hypothesis Testing: A method of comparing models to determine which one better explains the data.</li> </ul> <p>Understanding how to formulate and test hypotheses is essential for conducting rigorous research and making meaningful contributions to knowledge. A strong hypothesis guides the research process, informs experimental design, and provides a basis for interpreting results. By mastering hypothesis testing, you enhance your ability to analyze data critically and draw valid conclusions.</p>"},{"location":"statistics/hypothesis/Metrics/","title":"Metrics","text":""},{"location":"statistics/hypothesis/Metrics/#p-values","title":"P-Values","text":"<p>P-values are a central measure of modern inferential statistics, often mentioned in research studies and statistical analyses. You might frequently hear statements like \"The p-value is less than 0.05,\" highlighting their importance in determining statistical significance.</p> (Source: xkcd)"},{"location":"statistics/hypothesis/Metrics/#definition","title":"Definition","text":"<p>A p-value is a probability metric that helps us determine the significance of our results. </p> Definition: P-Value <p>The p-Value quantifies the likelihood of obtaining a test statistic at least as extreme as the one observed, assuming the null hypothesis is true. </p> <p>In simpler terms, it answers the question: If there is no real effect or difference, what is the probability of observing the data we have collected?</p> Code <pre><code>import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\n# Generate the data for the normal distribution\nx = np.linspace(-4, 4, 1000)\ny = norm.pdf(x, 0, 1)\n\n# Data for filling the upper 5% region\nx_fill = np.linspace(norm.ppf(0.95, 0, 1), 4, 100)\ny_fill = norm.pdf(x_fill, 0, 1)\n\n# Create DataFrame for Plotly\ndf = pd.DataFrame({'x': x, 'y': y})\ndf_fill = pd.DataFrame({'x': x_fill, 'fill': y_fill})\n\n# Create the plot\nfig = px.line(df, x='x', y='y')\nfig.add_trace(px.line(df_fill, x='x', y='fill').data[0])\n\n# Adjust the plot\nfig.data[0].update(line=dict(color='#00416E', width=2))\nfig.data[1].update(\n    fill='tozeroy', \n    fillcolor='rgba(0, 65, 110, 0.4)',\n    line=dict(width=0),\n)\n\n# Add layout modifications\nfig.update_layout(\n    xaxis_title_text='z',\n    yaxis_title_text='Probability Density',\n    title=dict(\n        text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Null Distribtion vs Alternativ Value &lt;/span&gt;&lt;/b&gt;',\n    ),\n    showlegend=False,\n)\n\n# Add vertical line and annotations\nlimit_down = norm.ppf(0.95, 0, 1)\nfig.add_vline(x=limit_down, line_dash=\"dash\", annotation_text=\"H_A value\", annotation_position=\"top right\", line_color=\"#E87F2B\", annotation_font_color=\"#E87F2B\")\nfig.add_annotation(x=-0, y=0.2, text=\"Null Distribution\", showarrow=False, font=dict(color=\"#00416E\"))\n\n# Show the plot\nfig.show()\n</code></pre> Reminder <ul> <li> <p>Null Hypothesis (H<sub>0</sub>): This is the default assumption that there is no effect or no difference. For example, when testing a new medication, the null hypothesis might state that the medication has no effect on patients compared to a placebo.</p> </li> <li> <p>Alternative Hypothesis (H<sub>1</sub> or H<sub>A</sub>): This hypothesis suggests that there is an effect or a difference. In the medication example, the alternative hypothesis would state that the medication does have an effect on patients.</p> </li> </ul> <p>In statistical testing, we often visualize these hypotheses using distributions:</p> <ul> <li> <p>Null Hypothesis Distribution: This represents the expected distribution of the test statistic if the null hypothesis is true. It is typically derived from theoretical probability distributions (like the normal distribution) and calculated using statistical formulas based on assumptions and degrees of freedom.</p> </li> <li> <p>Alternative Hypothesis Distribution: This represents the distribution if the alternative hypothesis is true. However, in practice, we usually don't have a theoretical distribution for the alternative hypothesis because it's based on the actual effect we're trying to detect, which is unknown.Therefore, we collect data from experiments or studies, resulting in an observed test statistic (like a sample mean or proportion).</p> </li> </ul>"},{"location":"statistics/hypothesis/Metrics/#interpretation","title":"Interpretation","text":"<p>The method for calculating a p-value depends on the statistical test being used (e.g., t-test, chi-squared test, ANOVA). While the computational approaches vary, the interpretation of the p-value remains consistent: the p-value answers the  fundamental question about the probability of observing the data under the null hypothesis.</p> <ul> <li> <p>Low P-Value (e.g., p &lt; 0.05): Indicates that the observed data is unlikely under the null hypothesis. This leads us to consider rejecting the null hypothesis in favor of the alternative hypothesis.     </p> </li> <li> <p>High P-Value (e.g., p &gt; 0.05): Suggests that the observed data is consistent with the null hypothesis. We do not have enough evidence to reject the null hypothesis.     </p> </li> </ul> Note <p>A low p-value does not prove that the alternative hypothesis is true; it merely indicates that the null hypothesis may not fully explain the observed data.</p> Limitations <ul> <li> <p>Cannot Prove Hypotheses: P-values do not provide proof but rather evidence against the null hypothesis.</p> </li> <li> <p>Dependence on Sample Size: Large samples can produce small p-values for trivial effects, while small samples might not detect significant effects.</p> </li> <li> <p>Not Measures of Effect Size: A p-value does not indicate the magnitude of an effect.</p> </li> </ul>"},{"location":"statistics/hypothesis/Metrics/#significance","title":"Significance","text":"<p>To determine statistical significance, we compare the p-value to a predetermined significance level, often denoted as alpha (\\(\\alpha\\)). If the p-value is less than \\(\\alpha\\), we declare the result statistically significant. Common choices for \\(\\alpha\\) are 0.05 or 0.01, representing a 5% or 1% threshold for significance.</p> <p>It's crucial to recognize that the choice of \\(\\alpha\\) is somewhat arbitrary and can influence whether a result is deemed significant. For instance, if we obtain a p-value of 0.03, setting \\(\\alpha\\) at 0.05 would lead us to consider the result significant. However, if we set \\(\\alpha\\) at 0.01, the same p-value would not be considered significant.</p> <p>In essence, p-values help us assess how compatible our data is with the null hypothesis. By establishing a significance threshold, we can make informed decisions about whether our findings are likely due to random chance or if they indicate a genuine effect.</p>"},{"location":"statistics/hypothesis/Metrics/#one-tailed-vs-two-tailed-tests","title":"One-Tailed vs. Two-Tailed Tests","text":"<p>One-Tailed Test: Used when the research hypothesis predicts the direction of the effect (e.g., a new drug is expected to lower blood pressure). The critical region for rejecting the null hypothesis is entirely on one side of the distribution.</p> <ul> <li>Example: Testing if a new study technique increases test scores. The null hypothesis states there is no improvement, and the alternative hypothesis states there is an improvement. Only high test scores (one tail) are considered evidence against the null hypothesis.</li> <li> <p>In a one-tailed test with an alpha level of 0.05, the entire 5% significance level is in one tail.</p> <p></p> </li> </ul> <p>Two-Tailed Test: Used when the research hypothesis does not predict the direction (e.g., the drug affects blood pressure but could either raise or lower it). The critical regions are on both ends of the distribution.</p> <ul> <li>Example: Testing if a new fertilizer affects plant growth. The null hypothesis states it has no effect, while the alternative hypothesis states it has an effect (could be an increase or decrease). Both unusually high and low plant growth measurements (both tails) are evidence against the null hypothesis.</li> <li>In a two-tailed test with the same alpha level, the 5% is split between both tails (2.5% in each).     </li> </ul>"},{"location":"statistics/hypothesis/Metrics/#common-misconceptions","title":"Common Misconceptions","text":"<p>Understanding what p-values do not represent is crucial to avoid misinterpretations.</p> <ol> <li> <p>Misconception: A p-value of 0.02 means there's a 2% chance the effect is present in the population.</p> <ul> <li>Correction: A p-value of 0.02 means there's a 2% probability of observing the test statistic (or something more extreme) assuming the null hypothesis is true. It does not indicate the proportion of the population exhibiting the effect.</li> </ul> </li> <li> <p>Misconception: A p-value of 0.02 means there's a 98% chance that the sample statistic equals the population parameter.</p> <ul> <li>Correction: The p-value does not provide the probability that the sample statistic equals the population parameter. It only assesses the likelihood of the observed data under the null hypothesis.</li> </ul> </li> <li> <p>Misconception: A p-value smaller than the threshold confirms the effect is real.</p> <ul> <li>Correction: A small p-value suggests that the observed data is unlikely under the null hypothesis, but it does not prove the effect is real. Other factors like sample size, data quality, and experimental design also play significant roles.</li> </ul> </li> <li> <p>Misconception: A large p-value means the null hypothesis is true.</p> <ul> <li>Correction: A large p-value indicates that the data is consistent with the null hypothesis, but it doesn't prove that the null hypothesis is true. It might also be due to insufficient sample size or variability in the data.</li> </ul> </li> </ol>"},{"location":"statistics/hypothesis/Metrics/#z-score","title":"Z-Score","text":"<p>Every statistical distribution can be associated with a set of p-values because they can all be interpreted in terms of probability. However, some distributions are more prominent due to their frequent use in statistical analyses. The most significant of these is the normal distribution, also known as the bell curve or Gaussian distribution. Its importance is so profound that it's fundamental in various fields, including statistics, physics, and social sciences.</p> <p>Let's begin by exploring z-scores and the proportion of data within a normal distribution. The normal distribution is symmetrical around the mean, and its standard deviation determines the spread of the data. This property allows us to make precise statements about the proportion of data falling within certain ranges.</p> Example: Heights of Adult Women <p>Suppose the average height of adult women in a country is 165 cm, with a standard deviation of 6.35 cm.</p> <ul> <li> <p>Within 1 Standard Deviation (\u00b11 SD): Heights between 158.65 cm and 171.35 cm (165 cm \u00b1 6.35 cm). Approximately 68.3% of women have heights within this range.</p> </li> <li> <p>Within 2 Standard Deviations (\u00b12 SD): Heights between 152.3 cm and 177.7 cm (165 cm \u00b1 12.7 cm). About 95.5% of women fall within this range.</p> </li> <li> <p>Within 3 Standard Deviations (\u00b13 SD): Heights between 145.95 cm and 184.05 cm (165 cm \u00b1 19.05 cm). Approximately 99.7% of women are within this range.</p> </li> </ul> <p>These percentages are known as the Empirical Rule or the 68-95-99.7 rule and are fundamental properties of the normal distribution. Memorizing these values can greatly enhance your understanding and efficiency when working with normally distributed data.</p> Definition: Empirical Rule \\[ \\begin{aligned}\\Pr(\\mu -1\\sigma \\leq X\\leq \\mu +1\\sigma )&amp;\\approx 68.27\\%\\\\\\Pr(\\mu -2\\sigma \\leq X\\leq \\mu +2\\sigma )&amp;\\approx 95.45\\%\\\\\\Pr(\\mu -3\\sigma \\leq X\\leq \\mu +3\\sigma )&amp;\\approx 99.73\\%\\end{aligned} \\]"},{"location":"statistics/hypothesis/Metrics/#common-p-value-and-z-score-pairs","title":"Common P-Value and Z-Score Pairs","text":"<p>Understanding the relationship between p-values and z-scores is crucial for hypothesis testing. Here are some commonly used pairs that are worth committing to memory.</p> <ul> <li> <p>One-Tailed Test</p> P-Value Z-Score 0.05 1.645 0.01 2.33 0.001 3.09 <p>Example: P-Value = 0.05: Z-Score \u2248 1.645</p> <p>This means there's a 5% chance of observing a test statistic at least as extreme as 1.645 standard deviations above the mean, assuming the null hypothesis is true.</p> </li> <li> <p>Two-Tailed Test</p> P-Value Z-Score 0.05 \u00b11.96 0.01 \u00b12.576 0.001 \u00b13.291 <p>Example: P-Value = 0.05: Z-Score \u2248 \u00b11.96</p> <p>There's a 5% chance of observing a test statistic more than 1.96 standard deviations away from the mean in either direction.</p> </li> </ul> Example: Exam Scores <p>Imagine a standardized test where the average score is 500, with a standard deviation of 100.</p> <p>In a one-tailed test, we're interested in deviations in one direction - for example, scores significantly above the mean. The Critical Z-Score for a one-tailed test at the 5% significance level, the critical z-score is approximately 1.645.</p> <ul> <li>Critical Score Calculation:</li> </ul> \\[ \\begin{aligned} \\text{Critical Score} &amp; =  \\text{Mean} + (Z \\cdot \\text{Standard Deviation})\\\\                       &amp; =   500 + (1.645 \\cdot 100) \\\\                       &amp; =  664.5 \\end{aligned} \\] <ul> <li>Interpretation: Any score above 664.5 is considered statistically significant at the 5% level in a one-tailed test.</li> </ul>"},{"location":"statistics/hypothesis/Metrics/#z-table","title":"z-Table","text":"<p>However, these common p-value and z-score pairs are not always sufficient for every analysis. For more precise probability calculations or for confidence levels not listed in standard tables, you can use z-tables. A z-table, also known as the standard normal distribution table, provides the cumulative probabilities associated with z-scores in a standard normal distribution. By using the z-table, you can determine the probability that a data point falls below a specific z-score or find the z-score corresponding to a particular cumulative probability. This table is essential for various statistical analyses, including hypothesis testing and constructing confidence intervals, as it allows for the translation of z-scores into meaningful probability values. Mastery of the z-table enhances the ability to interpret and apply statistical findings accurately.</p> <p>There are many sources for z-tables around the internet. One well-organized version is available on Wikipedia.</p> <p>The table basically consists of two areas:</p> <ul> <li>z-values (divided into rows and columns): need to be summed up (row + column)</li> <li>Cumulative probabilities: the values in the table</li> </ul> <p>The structure of the table can be read as follows:</p> <p></p> Using the z-table <p>It is not sufficient to know the significance level \\(\\alpha = 0.05\\) for determining the z-value; we also need to know if we are going to perform a one-tailed or two-tailed test. For a one-tailed test, we need to look up the cumulative probability of \\(95\\%\\). For a two-tailed test, we need to split the significance level of \\(5\\%\\) and look for the critical values corresponding to \\(2.5\\%\\) and \\(97.5\\%\\). Since the normal distribution is symmetric around zero, both critical values for the two-tailed test will be the same.</p>"},{"location":"statistics/hypothesis/Metrics/#confidence-interval","title":"Confidence Interval","text":"<p>While z-scores are instrumental in understanding the position of a data point within a distribution, confidence intervals provide a range within which we expect a population parameter (like the mean) to lie, based on our sample data. Integrating the concept of confidence intervals with z-scores enhances our ability to make informed statistical inferences.</p>"},{"location":"statistics/hypothesis/Metrics/#what-is-a-confidence-interval","title":"What is a Confidence Interval?","text":"<p>A confidence interval (CI) is a range of values derived from sample data that is likely to contain the true population parameter. The confidence level, typically expressed as a percentage (e.g., 95%), represents the degree of certainty that the interval captures the parameter.</p> <ul> <li>Confidence Level: The probability that the confidence interval contains the true parameter in repeated sampling.</li> <li>Interval Width: Reflects the precision of the estimate; narrower intervals indicate higher precision.</li> <li>Dependence on Sample Size and Variability: Larger sample sizes and lower variability lead to narrower confidence intervals.</li> </ul>"},{"location":"statistics/hypothesis/Metrics/#calculating-confidence-intervals-using-z-scores","title":"Calculating Confidence Intervals Using Z-Scores","text":"<p>In the context of the normal distribution, z-scores play a crucial role in constructing confidence intervals for the population mean when the population standard deviation is known. Specifically, the z-score determines the number of standard deviations to extend from the sample mean to achieve the desired confidence level.</p> Definition: Confidence Interval <p>When the population standard deviation (\\(\\sigma\\)) is known, the confidence interval for the population mean (\\(\\mu\\)) can be calculated using the following formula:</p> \\[ \\left[\\bar{x} - z_{(1-\\frac{\\alpha}{2})} \\frac{\\sigma}{\\sqrt{n}}; \\quad \\bar{x} + z_{(1-\\frac{\\alpha}{2})} \\frac{\\sigma}{\\sqrt{n}} \\right] \\] <p>with</p> <ul> <li>\\(\\bar{x}\\): Sample mean</li> <li>\\(z\\): Critical value from the standard normal distribution corresponding to the desired confidence level</li> <li>\\(\\sigma\\): Population standard deviation</li> <li>\\(n\\): Sample size</li> </ul> Exmple: Estimating Average Height with Confidence Interval <p>Using the earlier example of adult women\u2019s heights:</p> <ul> <li>Sample Size (\\(n\\)): 100</li> <li>Sample Mean (\\(\\bar{x}\\)): 165 cm</li> <li>Population Standard Deviation (\\(\\sigma\\)): 6.35 cm</li> <li>Confidence Level: 95% \u2192 \\(\\alpha = 0.05\\)</li> </ul> <p>Calculate the confidence interval: </p> <ol> <li>Critical Value (\\(z_{(1-\\frac{\\alpha}{2})}\\)): For a 95% confidence level, \\(z_{(1-\\frac{\\alpha}{2})} \\approx 1.96\\).</li> <li> <p>Standard Error: </p> \\[      \\frac{\\sigma}{\\sqrt{n}}= \\frac{6.35}{\\sqrt{100}} = 0.635 cm  \\] </li> <li> <p>Margin of Error: </p> \\[      z_{(1-\\frac{\\alpha}{2})} \\frac{\\sigma}{\\sqrt{n}} = 1.96 \\times 0.635 \\approx 1.245 cm \\] </li> <li> <p>Confidence Interval: \\(165 \\pm 1.245\\)</p> \\[     \\left[163.755 cm, 166.245 cm\\right] \\] </li> </ol> <p>Interpretation: We are 95% confident that the true average height of adult women in the country lies between 163.755 cm and 166.245 cm.</p>"},{"location":"statistics/hypothesis/Metrics/#factors-affecting-confidence-interval-width","title":"Factors Affecting Confidence Interval Width","text":"<p>Several factors influence the width of a confidence interval:</p> <ol> <li> <p>Sample Size (\\(n\\)):</p> <ul> <li>Larger Sample Size: Leads to a smaller standard error, resulting in a narrower confidence interval.</li> <li>Smaller Sample Size: Increases the standard error, leading to a wider confidence interval.</li> </ul> </li> <li> <p>Variability in Data (\\(\\sigma\\)):</p> <ul> <li>Lower Variability: Decreases the standard error, narrowing the confidence interval.</li> <li>Higher Variability: Increases the standard error, widening the confidence interval.</li> </ul> </li> <li> <p>Confidence Level:</p> <ul> <li>Higher Confidence Level (e.g., 99%): Requires a larger critical value, resulting in a wider interval.</li> <li>Lower Confidence Level (e.g., 90%): Uses a smaller critical value, leading to a narrower interval.</li> </ul> </li> </ol>"},{"location":"statistics/hypothesis/Metrics/#degree-of-freedom","title":"Degree of Freedom","text":"Example: DoF Mean <p>Given are four variables: \\(A\\), \\(B\\), \\(C\\), and \\(D\\) with an average (mean) of \\(50\\). Is it possible to determine the exact values of \\(A\\), \\(B\\), \\(C\\), and \\(D\\)?</p> <p>No, it's not. Knowing only the mean and the number of variables, there are infinitely many combinations of values that satisfy the condition. The mean is calculated as:</p> \\[ \\text{Mean} = \\frac{A + B + C + D}{4} = 50 \\] <p>This equation alone isn't enough to solve for the individual variables because there are infinitely many solutions.</p> <p>Now, suppose the values for \\(A\\), \\(B\\) and \\(C\\) are given.</p> <ul> <li>A = 40</li> <li>B = 55</li> <li>C = 60</li> </ul> <p>Can we now calculate \\(D\\)? Yes, by substituting the values into the variables</p> \\[ 50 = \\frac{40 + 55 + 60 + D}{4} \\] \\[ D = (50 \\cdot 4)-(40+55+60) = 45 \\] <p>Once we know the mean and three of the four variables, the fourth variable is completely determined. It has no freedom to vary independently; it's dependent on the others. So, in this scenario, we have three degrees of freedom. That's because we can freely choose any values for \\(A\\), \\(B\\) and \\(C\\), but \\(D\\) is constrained by the mean.</p> <p>Now, Suppose we have a population where the true mean (\\( \\mu \\)) is known to be \\(50\\). We take a random sample of four observations: \\(A\\), \\(B\\), \\(C\\), and \\(D\\). What are the degrees of freedom in our sample?</p> <p>Answer: We have four degrees of freedom. </p> <p>Even though we know the population mean, the sample mean (\\( \\bar{x} \\)) calculated from A, B, C, and D may not be exactly 50 due to sampling variability. The sample values can vary freely; knowing the population mean does not impose a constraint on the individual sample values. Therefore, all four sample observations have the freedom to vary independently.</p> <p>So, based on this example, the Degree of Freedom (DoF) can be defined as: </p> Definition: Degree of Freedom <p>Degrees of freedom are the number of independent values that a statistical analysis can estimate. You can also think of it as the number of values that are free to vary as you estimate parameters. </p> <p>-- Jim Frost - Degrees of Freedom in Statistics</p> <p>In statistics the degrees of freedom are necessary to provide unbiased estimates of population parameters which is crucial for reliable statistical inference. Furthermore it affects the shape of statistical distributions used in hypothesis testing, influencing critical values and p-values.</p> <p>In following test we will conduct, the degree of freedom will depend on the sample size and can be calculated as: </p> \\[dof = n-1\\]"},{"location":"statistics/hypothesis/Metrics/#errors","title":"Errors","text":"<p>In statistical hypothesis testing, we make decisions based on sample data, which can lead to two types of errors due to randomness and variability.</p> Definition: Types of Error <p>In statistical testing, there are two types of errors: </p> <ol> <li>Type I Error (False Positive): Rejecting the null hypothesis (\\( H_0 \\)) when it is actually true.</li> <li>Type II Error (False Negative): Failing to reject the null hypothesis when it is actually false.</li> </ol> Example: Error Type <p>A factory produces electronic components that must meet specific quality standards before they are shipped to customers. Each component is tested to determine whether it is defective or not.</p> <ul> <li>Null Hypothesis (\\( H_0 \\)): The component is not defective; it meets quality standards.</li> <li>Alternative Hypothesis (\\( H_A \\)): The component is defective; it does not meet quality standards.</li> </ul> <p>Type I Error (False Positive)</p> <p>A component that functions perfectly is tested and, due to a measurement error or overly strict testing criteria, is labeled as defective.</p> <p>Consequence:</p> <ul> <li>Increased Costs: Good components are unnecessarily scrapped or reworked, leading to wasted materials and labor.</li> <li>Reduced Efficiency: Production slows down due to re-inspection and additional quality checks.</li> </ul> <p>Type II Error (False Negative)</p> <p>A component with a subtle flaw passes the quality test and is shipped to customers.</p> <p>Consequence:</p> <ul> <li>Customer Dissatisfaction: Defective products may fail during use, leading to returns and complaints.</li> <li>Reputation Damage: Consistent quality issues can harm the company's reputation.</li> <li>Safety Risks: In critical applications, defective components could cause equipment failure or safety hazards.</li> </ul>"},{"location":"statistics/hypothesis/Metrics/#error-probability","title":"Error Probability","text":"<p>The probability of a Type I Error is equal to the significance level (\\( \\alpha \\)), while on the other side, the probability of the Type II Error is denoted as (\\( \\beta \\)). Reducing the probability of a Type I error (\\( \\alpha \\)) increases the probability of a Type II error (\\( \\beta \\)), and vice versa.</p> Table of Error Types Null Hypothesis (H<sub>0</sub>) is True False Decision about Null Hypothesis (H<sub>0</sub>) Not Reject Correct Decision (True Negative)(Probability = 1\u00a0\u2212\u00a0\u03b1) Type II Error (False Negative)(Probability = \u03b2) Reject Type I Error (False Positive)(Probability = \u03b1) Correct Decision (True Positive)(Probability = 1\u00a0\u2212\u00a0\u03b2) (Source: Graduatetutor)"},{"location":"statistics/hypothesis/Testing/","title":"Testing Principals","text":"<p>Before diving into hypothesis testing, let's briefly revisit the central limit theorem (CLT). The CLT states that the sampling distribution of the sample mean will approximate a normal distribution as the sample size becomes large, regardless of the population's distribution. Because measuring the entire population is often impractical, we rely on samples. Each sample provides an estimate, and repeated sampling yields a distribution of these estimates.</p> Examples: Are Apples as Heavy as Apples <p>Consider a seemingly nonsensical question:</p> <p>\"Are apples as heavy as apples?\"</p> <p>At first glance, this question doesn't make sense. We're comparing apples to apples! However, this example illustrates sampling variability under the null hypothesis.</p> <p> </p> <p>Imagine you have a large bucket of apples, and you want to test whether one group of apples is heavier than another group from the same bucket. Here's how you might proceed:</p> <ol> <li>Sample Group A: Randomly select 10 apples and measure their weights.</li> <li>Sample Group B: Randomly select another 10 apples and measure their weights.</li> <li>Calculate the Mean Weights: Compute the average weight for each group.</li> <li>Compute the Difference: Find the difference between the two average weights.</li> </ol> <p>Under the null hypothesis (no difference in weights), we expect the difference in mean weights to be zero. However, due to sampling variability, the difference is unlikely to be exactly zero. You might find that Group A has a mean weight of 150 grams, while Group B has a mean weight of 152 grams - a difference of 2 grams.</p> <p>Does this mean that apples in Group B are inherently heavier? Probably not. The observed difference is likely due to random variation.</p>"},{"location":"statistics/hypothesis/Testing/#null-alternative-distribution","title":"Null &amp; Alternative Distribution","text":""},{"location":"statistics/hypothesis/Testing/#null-distribution","title":"Null Distribution","text":"<p>When comparing samples from the same population the expected difference is Zero (no difference under the null hypothesis). But due to sampling variability, the observed differences are small. By repeatedly sampling and calculating differences, we can create a null distribution - a distribution of differences expected under the null hypothesis \\( H_0 \\). The null distribution will center around zero and display the variability expected due to random sampling. </p> <p>In statistical hypothesis testing, the null distribution is the probability distribution of the test statistic when the null hypothesis is true.</p> <p>-- Kent W. Staley - An Introduction to the Philosophy of Science</p> <p>To build the null distribution:</p> <ol> <li>Repeat Sampling: Take many pairs of samples from the population.</li> <li>Calculate Differences: For each pair, compute the difference in sample means.</li> <li>Plot the Differences: Create a histogram of these differences.</li> </ol> <p>So if we stick with our <code>age</code> example, and compare samples from the whole population, we will receive a distribution of age differences:</p> Code <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \nadult = fetch_ucirepo(id=2) \n\n# data (as pandas dataframes) \ndata = adult.data.features\n\nimport plotly.express as px\nimport random\nimport numpy as np\nimport pandas as pd\n\nsample_means_dif = []\nfor i in range(10000):\n    sample_means_dif.append(np.mean(random.sample(list(data.age), 20))-np.mean(random.sample(list(data.age), 20)))\n\n\nfig = px.histogram(x=sample_means_dif)\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.8)'))\n\n# Add a horizontal line for the population mean\nfig.add_vline(x=0, line_dash=\"dash\", annotation_text=\"Expected Value: 0\", annotation_position=\"top right\", line_color=\"#E87F2B\")\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Absolute Frequency',\n        xaxis_title_text='Sample Means Difference',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Null Distribution&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Age vs. Age&lt;/span&gt;&lt;span style=\"font-size: 5pt\"&gt; Number of Samples: 10.000 | Data: UCIML Repo: Adult; variable: age&lt;/span&gt; &lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <p>In this case we see, that the distribution looks like we would imagine if the null hypothesis cannot be rejected (is true). </p>"},{"location":"statistics/hypothesis/Testing/#alternative-distribution","title":"Alternative Distribution","text":"<p>The alternative hypothesis posits that there is a genuine difference between groups. Therefore, under the alternative hypothesis, the distribution of differences shifts away from zero. The alternative distribution represents the expected differences if the alternative hypothesis is true.</p> <p>Again we stick with the <code>age</code> example and compare the age differences of mutliple samples for <code>male</code> and <code>female</code> in our dataset:</p> Code <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \nadult = fetch_ucirepo(id=2) \n\n# data (as pandas dataframes) \ndata = adult.data.features\n\nimport plotly.express as px\nimport random\nimport numpy as np\nimport pandas as pd\n\nsample_means_dif = []\nfor i in range(10000):\n    sample_means_dif.append(np.mean(random.sample(list(data[data['sex']==\"Male\"].age), 20))-np.mean(random.sample(list(data[data['sex']==\"Female\"].age), 20)))\n\n\nfig = px.histogram(x=sample_means_dif)\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.8)'))\n\n# Add a horizontal line for the population mean\nfig.add_vline(x=np.mean(sample_means_dif), line_dash=\"dash\", annotation_text=\"Mean Difference: \"+str(np.mean(sample_means_dif)), annotation_position=\"top right\", line_color=\"#E87F2B\")\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Absolute Frequency',\n        xaxis_title_text='Sample Means Difference',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Alternative Distribution&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Age (Male) vs. Age (Female) &lt;/span&gt;&lt;span style=\"font-size: 5pt\"&gt; Number of Samples: 10.000 | Data: UCIML Repo: Adult; variable: age&lt;/span&gt; &lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <p>What we can see now is, that the peak (mean) of the distribution ist not around zero and therefore the null hypothesis is false and can be rejected against an alternative hypothesis. But is this difference significant?</p>"},{"location":"statistics/hypothesis/Testing/#quantifying-differences-between-distributions","title":"Quantifying Differences Between Distributions","text":"<p>To assess whether an observed difference is significant, we need to analyze the center of the two peaks of the null distribution and the alternative distribution. However, to make this comparison meaningful, we need to scale or normalize this difference based on the spread (or width) of the distributions. This normalization accounts for the units and ensures that the comparison is independent of the scale. For example, if we're measuring something in centimeters, both the numerator and denominator will be in the same unit, effectively canceling the units out.</p> Code <pre><code>import plotly.graph_objects as go\nfig = go.Figure()\n\nfig.add_trace(go.Histogram(x=sample_means_dif, marker=dict(color='rgba(0, 65, 110, 0.8)'),name='Null Distribution'))\n\n# Zweite Spur hinzuf\u00fcgen\nfig.add_trace(go.Histogram(x=sample_means_difa, marker=dict(color='rgba(232, 127, 43, 0.8)'),name='Alternative Distribution'))\n\n# Add a horizontal line for the population mean\nfig.add_vline(x=0, line_dash=\"dash\", annotation_text=\"H_0 Center\", annotation_position=\"top left\", line_color=\"#00416E\")\nfig.add_vline(x=np.mean(sample_means_difa), line_dash=\"dash\", annotation_text=\"H_a Center\", annotation_position=\"top right\", line_color=\"#E87F2B\")\n\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Absolute Frequency',\n        xaxis_title_text='Sample Means Difference',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Differences in Distributions&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 5pt\"&gt; Number of Samples: 10.000 | Data: UCIML Repo: Adult; variable: age&lt;/span&gt; &lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <p>It\u2019s important to note that different statistical methods may define this \"width\" differently. So, we shouldn't get too caught up in formal definitions here. The key is to keep things flexible and understand this as a general approach that various statistical tests can adapt in different ways.</p> <p>In essence, what we\u2019re doing is taking the difference between two central points (which we'll call \"signal\") and dividing it by some measure of the distribution\u2019s width (which represents \"noise\"). This idea of comparing signal to noise is at the core of hypothesis testing. Whether we\u2019re dealing with null hypothesis testing or alternative methods, the goal is to quantify how strong the signal is relative to the noise.</p> \\[ \\text{Signal-to-Noise Ratio} = \\frac{\\text{Difference of Means}}{\\text{Measure of Variability}} \\] <p>This concept of comparing signal to noise is the cornerstone of inferential statistics. Regardless of the specific statistical test, the underlying principle is the same:</p> <ul> <li>Assess the Effect: Measure the difference or relationship you're interested in.</li> <li>Account for Variability: Consider the variability in the data.</li> <li>Determine Significance: Use the signal-to-noise ratio to infer whether the effect is statistically significant.</li> </ul> <p>Examples of Statistical Tests:</p> <ul> <li>t-test: Compares the means of two groups relative to the variability within the groups.</li> <li>ANOVA: Analyzes differences among group means in a sample.</li> <li>Regression Analysis: Assesses the relationship between variables, considering residual variability.</li> </ul>"},{"location":"statistics/hypothesis/Ttest/","title":"T-Test","text":""},{"location":"statistics/hypothesis/Ttest/#introduction-to-t-tests","title":"Introduction to T-Tests","text":"<p>The t-test is a fundamental statistical method widely used in various fields to compare the means of two groups and determine if they are statistically different from each other. At its core, the t-test evaluates whether the difference between the means of two groups is significant or if it could have occurred by chance due to variability in the data. This is crucial when testing hypotheses in research studies.</p> Example: Math Score <p>Suppose you're investigating whether a new tutoring program improves students' math test scores compared to a standard curriculum. Here, the two groups are:</p> <ul> <li>Group A: Students using the new tutoring program.</li> <li>Group B: Students following the standard curriculum.</li> </ul> <p>Your alternative hypothesis is that the mean test score of Group A is different from that of Group B. The null hypothesis states that there is no difference in the mean test scores between the two groups.</p> <p>The t-test formula provides a standardized way to measure the difference between group means relative to the variability in the data:</p> Definition: T-Test \\[ t_k = \\frac{\\bar{x}-\\bar{y}}{s/\\sqrt{n}} = \\frac{\\text{Difference of Means}}{\\text{Standard Deviations}} \\] <p>Where:</p> <ul> <li>\\(\\bar{x}\\) = average of group x</li> <li>\\(\\bar{y}\\) = average of group y</li> <li>\\(s\\) = standard error of the arithmetic mean = \\(\\sigma\\sqrt{n}\\)</li> <li>\\(n\\) = number of samples</li> <li>\\(k\\) = degree of freedom</li> </ul> <p>This formula essentially calculates how many standard deviations the difference between the two means is away from zero. A larger absolute value of \\(t\\) indicates a more significant difference between the groups. The degree of freedom is equal to the sample size minus 1:</p> \\[dof = n-1\\]"},{"location":"statistics/hypothesis/Ttest/#t-distribution","title":"T-Distribution","text":"<p>In the following graph you see the probability of a certain t-value occurring given that the null hypothesis is true. This curve is called the t-distribution (or sometimes Student's t-distribution).</p> Code <pre><code>import numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom scipy.stats import norm\n\n# Generate the data for the normal distribution\nx = np.linspace(-4, 4, 1000)\ny = norm.pdf(x, 0, 1)\n\n# Create DataFrame for Plotly\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Create the plot\nfig = px.line(df, x='x', y='y')\n\n# Adjust the plot\nfig.data[0].update(line=dict(color='#00416E', width=2))\n\n# Add layout modifications\nfig.update_layout(\n    xaxis_title_text='t-Value',\n    yaxis_title_text='P(t|H0)',\n    title=dict(\n        text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;t-Distribution&lt;/span&gt;&lt;/b&gt;',\n    ),\n    showlegend=False,\n)\n\n# Show the plot\nfig.show()\n</code></pre> <p>If the null hypothesis is true, we expect the difference between the means to be zero \\(\\bar{x} - \\bar{y} = 0\\), resulting in a t-value of zero. This corresponds to the highest probability and the peak of the distribution. However, due to sampling variability and noise, the means will not be exactly equal (even if the null hypothesis is true), leading to t-values around zero and therefore to the t-distribution.</p> <p>In Python we can use the <code>scipy.stats</code> package to work easily with the t-distribution</p> <p><pre><code>import scipy.stats as stats\n</code></pre> There are different methods available for the t-distribution <code>stats.t</code> which can be very helpful. </p> <pre><code>stats.t.pdf(1.5, df=10) # probability density function\nstats.t.cdf(1.5, df=10) # cumulative distribution function\n</code></pre>"},{"location":"statistics/hypothesis/Ttest/#dependency-on-the-degree-of-freedom","title":"Dependency on the Degree of Freedom","text":"<p>The t-distribution depends on the degree of freedom.</p> <pre><code>stats.t.pdf(x, df=dof)\n</code></pre> <p>The higher the DoF, the more the curve converge to a standard normal distribution. Because the DoF depends on the sample size n, the following rule of thumb can be stated: for a sample size &gt;30 the standard normal distribution can be used for calculating the p-value. </p> Code <pre><code># Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport scipy.stats as stats\n\n# Define the x-axis values (t-values)\nx = np.arange(-5, 5.01, 0.01)\n\n# Define the degrees of freedom you want to plot, including 'infinity' for Normal distribution\ndegrees_of_freedom = [1, 3, 5, 30, np.inf]  # np.inf represents infinite degrees of freedom (Normal distribution)\n\n# Create an empty DataFrame to store the data for all curves\ndf_all = pd.DataFrame()\n\n# Loop through each degree of freedom and compute the t-distribution\nfor df in degrees_of_freedom:\n    if df == np.inf:\n        y = stats.norm.pdf(x)  # Normal distribution for df = infinity\n        df_label = 'df=\u221e (Normal)'\n    else:\n        y = stats.t.pdf(x, df=df)\n        df_label = f'df={df}'\n    df_temp = pd.DataFrame({'x': x, 'y': y, 'df': df_label})\n    df_all = pd.concat([df_all, df_temp])\n\n# Create the plot using Plotly Express\nfig = px.line(df_all, x='x', y='y', color='df', \n            title=\"&lt;b&gt;t-Distribution for Different Degrees of Freedom&lt;/b&gt;\",\n            labels={'x': 't-Value', 'y': 'P(t|H0)', 'df': 'Degrees of Freedom'})\n\n# Update layout\nfig.update_layout(\n    title=dict(\n        text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;t-Distribution for Different Degrees of Freedom&lt;/span&gt;&lt;/b&gt;',\n    ),\n    xaxis_title_text='t-Value',\n    yaxis_title_text='P(t|H0)',\n    showlegend=True,\n)\n\n# Show the plot\nfig.show()\n</code></pre>"},{"location":"statistics/hypothesis/Ttest/#t-table","title":"T-Table","text":"<p>As described in the previous chapter, the p-value represents the cumulative probability of obtaining a certain t-value or more extreme values. </p> <p>One-Tailed Test</p> <pre><code>stats.t.cdf(1.812, df=10)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.95\n</code></pre> <p>Two-Tailed Test</p> <pre><code>stats.t.cdf(2.228, df=10)-stats.t.cdf(-2.228, df=10)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.95\n</code></pre> <p>Graphically, the p-value is the area under the t-distribution starting from a given t-value. To fasten the evaluation of a t-test and avoid the need for constant calculations, precomputed values are available in what are known as t-tables.</p> <p>There are a lot of sources for t-tables around the internet. One very neatly is available on Wikipedia.</p> <p>The table basically consists of four areas: </p> <ul> <li>degree of freedom, </li> <li>significance level \\(\\alpha\\)</li> <li>Information about one oder two sided test</li> <li>t-value</li> </ul> <p>The structure of the table can be read as follows:</p> <p></p> Using the t-table <p>At least three of the four pieces of information must therefore be available in order to use the table. </p> Task: t-table <p>Take a closer look at the above shown t-table and compare it to the two examples shown above. Can you see the connection? </p>"},{"location":"statistics/hypothesis/Ttest/#calculating-the-p-value","title":"Calculating the p-Value","text":"<p>The p-value helps determine the statistical significance of your results. It represents the probability of observing a t-value as extreme as the one calculated, assuming the null hypothesis is true.</p> <p>There are two ways to use the p-value in the T-Test: </p> <p>Calculate Critical t-Value</p> <p>The first approach is to calculate the critical t-value. This value depends on the chosen significance level (\\(\\alpha\\)), the degrees of freedom, and whether the test is one-tailed (e.g., alpha = 5% on one side) or two-tailed (alpha = 5%, meaning 2.5% on each side). Based on these factors, the critical t-values can be determined, for example by using the t-table shown above. </p> <p>Next, the t-value of the sample can be calculated (using the before mentioned formula) and compared to the critical t-values in order to make a statement about the validity of the null hypothesis.</p> Example: Math Score <p>Let's stick with the example from before. Imagine you conduct the tutoring program study and calculate a t-value of \\(2.5\\). Because we are only interested in the fact that the grades get better, we can use a one-tailed test. Our significance level \\(\\alpha = 5\\%\\). The sample size was \\(11\\) and therefore the degree of freedom is \\(10\\). </p> <p>We can determine the critical t-value using a t-table or by using python</p> <pre><code>stats.t.ppf(0.95, df=10)\n</code></pre> &gt;&gt;&gt; Output<pre><code>1.812\n</code></pre> <p></p> <p>Since the sampled t-value of 2.5 is extremer than the critical t-value of 1.8, the null hypothesis can be rejected and you conclude that the tutoring program has a statistically significant effect on test scores.</p> <p> (Source: Memecreator)  </p> <p>Calculate p-Value of the Sample</p> <p>The second approach tackles the problem from the opposite side. In this case, we start with the sample's t-value and calculate the corresponding p-value. If the p-value is below the significance level alpha, we can reject the null hypothesis.</p> Example: Math Score <p>Now we use the second approach and start from the sample t-value. We calcualte the corresponding p-value</p> <pre><code>(1-stats.t.cdf(2.5, df=10))\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.016\n</code></pre> <p>The calculated p-value (\\(1.6\\%\\)) is lower than the significance level (\\(\\alpha = 5\\%\\)) and the null hypothesis can therefore be rejectet. </p> <p></p>"},{"location":"statistics/hypothesis/Ttest/#strategies-to-maximize-the-t-value","title":"Strategies to Maximize the t-value","text":"<p>To increase the likelihood of detecting a true effect, consider the following approaches:</p> <ol> <li> <p>Increase the Difference Between Means (\\(\\bar{x} - \\bar{y}\\)):</p> <ul> <li>Action: Enhance the impact of the treatment or condition.</li> <li>Example: If testing a new drug, use a dosage that is expected to produce a noticeable effect compared to the placebo.</li> </ul> </li> <li> <p>Decrease the Variability (Reduce \\(s\\)):</p> <ul> <li>Action: Control external factors to minimize data dispersion.</li> <li>Example: In an agricultural study measuring crop yield, ensure that soil quality, irrigation, and sunlight are consistent across test plots.</li> </ul> </li> <li> <p>Increase the Sample Size (\\(n\\)):</p> <ul> <li>Action: Collect data from more subjects to reduce the standard error.</li> <li>Example: Survey a larger number of participants in a market research study to obtain more reliable results.</li> </ul> </li> </ol>"},{"location":"statistics/hypothesis/Ttest/#one-sample-t-test","title":"One-Sample T-Test","text":"<p>The one-sample t-test is the simplest form of the t-test family and serves as an excellent introduction to understanding t-tests in general. It is used when you have a single sample and want to determine whether its mean is significantly different from a known or hypothesized population mean. So in this case, we do not have two different samples or groups, but one sample from a population. </p> Definition: One-Sample T-Test <p>The formula for calculating the t-value in a one-sample t-test is:</p> \\[ t = \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\] <p>Where:</p> <ul> <li>\\(\\bar{x}\\) = Sample mean</li> <li>\\(\\mu\\) = Hypothesized population mean (the value you're testing against)</li> <li>\\(s\\) = Sample standard deviation</li> <li>\\(n\\) = Sample size</li> </ul> <p>The degrees of freedom (df) for this test are calculated as \\(df = n - 1\\).</p>"},{"location":"statistics/hypothesis/Ttest/#applying-the-test","title":"Applying the Test","text":"<ol> <li>Set the Hypothesis</li> <li>Collect Data: Measure the stress levels of the sample employees.</li> <li>Calculate the Sample Mean (\\(\\bar{x}\\)): Find the average stress level from your data.</li> <li>Compute the Sample Standard Deviation (s).</li> <li>Calculate the t-value using the formula above.</li> <li>Determine Degrees of Freedom: \\(df = n - 1\\).</li> <li>Find the p-value: Use the t-distribution table or statistical software.</li> <li>Make a Decision: If the p-value is less than your significance level (e.g., 0.05), reject the null hypothesis.</li> </ol> Robustness <p>The t-test is relatively robust to violations of normality with larger sample sizes (n &gt; 30).</p> Example: Thickness Testing <p>A factory produces metal sheets that are supposed to have an average thickness of \\(2.5 mm\\). The quality control team wants to ensure that the production process is meeting this specification. They randomly sample 30 sheets from the production line and measure their thickness.</p> <p>They want to determine if the average thickness of the sampled sheets is statistically different from the target mean of \\(2.5 mm\\).</p> <p>Assumptions: </p> <ul> <li>Significance level \\( \\alpha = 0.05 \\)</li> <li>Two-Tailed Tests: There can be positive or negative deviations </li> </ul> <ul> <li>Set the hypotheses:<ul> <li>Null hypothesis (H<sub>0</sub>): The mean thickness of the sheets is \\(2.5 mm\\) (\u03bc = \\(2.5 mm\\)).</li> <li>Alternative hypothesis (H<sub>1</sub>): The mean thickness of the sheets is not \\(2.5 mm\\) (\u03bc \u2260 \\(2.5 mm\\)).</li> </ul> </li> </ul> <ul> <li>Collect sample data <pre><code># Generate Data\nimport numpy as np\n\n# Simulate a dataset for the example\nnp.random.seed(46)  # for reproducibility\n\n# Given parameters\nsample_size = 30\nsample_mean = 2.45  # as found in the test example\nstd_dev = 0.1  # standard deviation\n\n# Generate random sample data\ndata = np.random.normal(loc=sample_mean, scale=std_dev, size=sample_size)\n</code></pre></li> </ul> Manual Calculation <ul> <li> <p>Calculate the \\(\\bar{x}\\) and \\(s\\):      <pre><code>print(\"Mean:\", np.mean(data))\nprint(\"Standard Deviation:\", np.std(data, ddof=1))\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>Mean: 2.448088453428328\nStandard Deviation: 0.08337699472325459\n</code></pre> <p>The sample of 30 sheets has an average thickness of \\(2.45 mm\\) and a standard deviation of \\(0.08 mm\\).</p> </li> </ul> <ul> <li> <p>Calculate the t-value:</p> \\[ t = \\frac{2.45 - 2.5}{\\frac{0.08}{\\sqrt{30}}} \u2248 -3.41 \\] <pre><code>true_mean = 2.5\nt_statistic = (np.mean(data) - true_mean) / (np.std(data, ddof=1) / np.sqrt(sample_size))\nprint(\"t-statistic:\", t_statistic)\n</code></pre> &gt;&gt;&gt; Output<pre><code>t-statistic: -3.4101882835499286\n</code></pre> </li> </ul> <ul> <li> <p>Determine Degrees of Freedom:</p> <pre><code>dof = sample_size - 1\nprint(\"Degrees of Freedom:\", dof)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Degrees of Freedom: 29\n</code></pre> </li> </ul> <ul> <li> <p>Find the p-value:</p> <pre><code>p_val = stats.t.cdf(t_statistic, df=dof)*2\nprint (\"p-value:\", p_val)\n</code></pre> &gt;&gt;&gt; Output<pre><code>p-value: 0.0019285965194208732\n</code></pre> </li> </ul> Automatic Calculation <p>For calculating the p-value, the t-statistics and the degree of freedom we can use the <code>ttest_1samp</code> method of the <code>scipy.stats</code> library:</p> <pre><code>res = stats.ttest_1samp(data, popmean=true_mean, alternative='two-sided')\nprint(\"t-statistic:\", res.statistic)\nprint(\"p-value:\", res.pvalue)\nprint(\"Degrees of Freedom:\", res.df)\n</code></pre> &gt;&gt;&gt; Output<pre><code>t-statistic: -3.4101882835499286\np-value: 0.0019285965194208732\nDegrees of Freedom: 29\n</code></pre> <ul> <li>Make a Decision     The calculated p-value of the sample data is lower than the signifance level \\( \\alpha = 0.05 \\). Therefore, we reject the null hypothesis.So, there is significant evidence at the 5% level to conclude that the average thickness of the metal sheets is not 2.5 mm. The production process may need to be adjusted to ensure the thickness specification is met.</li> </ul>"},{"location":"statistics/hypothesis/Ttest/#assumptions-of-the-one-sample-t-test","title":"Assumptions of the One-Sample t-test","text":"<p>For the test results to be valid, the following assumptions should be met:</p> <ol> <li>Independence: Observations are independent of one another.</li> <li>Normality: The data should be approximately normally distributed, especially important for small sample sizes.</li> <li>Scale of Measurement: The data are continuous and measured on an interval or ratio scale.</li> </ol> Task: Weight of Euro Coins <p>  Download the following dataset from this page and load it into your notebook.</p> <pre><code># Website: https://jse.amstat.org/v14n2/datasets.aerts.html\n# Dataset: https://jse.amstat.org/datasets/euroweight.dat.txt\n# Description: https://jse.amstat.org/datasets/euroweight.txt\n\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\ndata = pd.read_csv('Daten/euroweight.dat.txt', sep='\\t', header=None, index_col=0, names=['Weight', 'Batch'])\n\n# Display the first few rows\ndata.head()\n</code></pre> <p>It contains information about the weight of a sample of specially issued euro coins. Answer the following questions using Python.  Assume a significance level \\(\\alpha = 5 \\%\\) and a two-sided test:</p> <ol> <li>Determine the average weight and the standard deviation of the sample.</li> <li>Formulate the hypothesis (Null and Alternative)</li> <li>Test the hypothesis that the population mean weight is \\(7.5\\) g.</li> <li>Test the hypothesis that the population mean weight is \\(7.51\\) g and \\(7.52\\) g, respectively. </li> <li>Interpret the results.</li> </ol>"},{"location":"statistics/hypothesis/Ttest/#two-sample-t-test","title":"Two-Sample T-Test","text":"<p>The two-sample t-test, also known as the independent samples t-test, is used to determine whether there is a statistically significant difference between the means of two independent groups. Unlike the one-sample t-test, which compares a sample mean to a known population mean, the two-sample t-test compares the means from two separate groups to see if they come from the same population.</p> Equal vs. Unequal Variances <p>There are two versions of the two-sample t-test:</p> <ul> <li>Student's t-test: Assumes equal variances between the two groups.</li> <li>Welch's t-test: Does not assume equal variances and is more robust when the variances are unequal.</li> </ul> <p>In practice, Welch's t-test is often preferred due to its robustness.</p> Definition: Two-Sample T-Test - Welch's T-Test <p>The formula for calculating the t-value in a two-sample t-test is:</p> \\[ t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\] <p>Where:</p> <ul> <li>\\(\\bar{x}_1\\), \\(\\bar{x}_2\\) = Sample means of group 1 and group 2</li> <li>\\(s_1^2\\), \\(s_2^2\\) = Sample variances of group 1 and group 2</li> <li>\\(n_1\\), \\(n_2\\) = Sample sizes of group 1 and group 2</li> </ul> <p>The degrees of freedom (DoF) for the test can be approximated using the Welch-Satterthwaite equation:</p> \\[ DoF = \\frac{\\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{\\left( \\frac{s_1^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_2^2}{n_2} \\right)^2}{n_2 - 1}} \\]"},{"location":"statistics/hypothesis/Ttest/#applying-the-test_1","title":"Applying the Test","text":"<ol> <li>Set the Hypotheses:<ul> <li>Null hypothesis (H<sub>0</sub>): The means of the two groups are equal (\\(\\mu_1 = \\mu_2\\)).</li> <li>Alternative hypothesis (H<sub>1</sub>): The means of the two groups are not equal (\\(\\mu_1 \\ne \\mu_2\\)).</li> </ul> </li> <li>Collect Data: Obtain independent random samples from both groups.</li> <li>Calculate the Sample Means (\\(\\bar{x}_1\\) and \\(\\bar{x}_2\\)).</li> <li>Compute the Sample Variances (\\(s_1^2\\) and \\(s_2^2\\)).</li> <li>Calculate the t-value using the formula above.</li> <li>Determine Degrees of Freedom using the Welch-Satterthwaite equation.</li> <li>Find the p-value: Use the t-distribution table or statistical software.</li> <li>Make a Decision: If the p-value is less than your significance level (e.g., 0.05), reject the null hypothesis.</li> </ol> Example: Comparing Teaching Methods <p>A researcher wants to determine if two different teaching methods lead to different student performance levels. They randomly assign students to two groups: one uses Method A, and the other uses Method B. After a semester, both groups take the same standardized test.</p> <p>Assumptions:</p> <ul> <li>Significance level \\( \\alpha = 0.05 \\)</li> <li>Two-tailed test: Testing for any difference in means</li> </ul> <ul> <li>Set the hypotheses:<ul> <li>Null hypothesis (H<sub>0</sub>): \\(\\mu_1 = \\mu_2\\) (no difference in mean scores)</li> <li>Alternative hypothesis (H<sub>1</sub>): \\(\\mu_1 \\ne \\mu_2\\) (difference in mean scores)</li> </ul> </li> </ul> <ul> <li> <p>Collect sample data:</p> <pre><code># Generate Data\nimport numpy as np\n\nnp.random.seed(42)  # for reproducibility\n\n# Group sizes\nn1 = 30  # Method A\nn2 = 35  # Method B\n\n# Simulate test scores\nmean1, std1 = 75, 10  # Method A\nmean2, std2 = 80, 12  # Method B\n\nscores1 = np.random.normal(mean1, std1, n1)\nscores2 = np.random.normal(mean2, std2, n2)\n</code></pre> </li> </ul> Manual Calculation <ul> <li> <p>Calculate sample means and variances:</p> <pre><code>mean1 = np.mean(scores1)\nmean2 = np.mean(scores2)\nvar1 = np.var(scores1, ddof=1)\nvar2 = np.var(scores2, ddof=1)\n\nprint(f\"Mean of Method A: {mean1:.2f}\")\nprint(f\"Mean of Method B: {mean2:.2f}\")\nprint(f\"Variance of Method A: {var1:.2f}\")\nprint(f\"Variance of Method B: {var2:.2f}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Mean of Method A: 73.12\nMean of Method B: 78.01\nVariance of Method A: 81.00\nVariance of Method B: 119.45\n</code></pre> </li> <li> <p>Calculate the t-value:</p> \\[ t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} = \\frac{73.12- 78.01}{\\sqrt{\\frac{81}{30} + \\frac{119.45}{35}}} \\approx -1.98 \\] <pre><code>numerator = mean1 - mean2\ndenominator = np.sqrt((var1 / n1) + (var2 / n2))\nt_statistic = numerator / denominator\nprint(f\"t-statistic: {t_statistic:.2f}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>t-statistic: -1.98\n</code></pre> </li> <li> <p>Determine degrees of freedom (df) using Welch's formula:</p> \\[ df = \\frac{\\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{\\left( \\frac{s_1^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_2^2}{n_2} \\right)^2}{n_2 - 1}} \\approx 62.91 \\] <pre><code>s1_squared_over_n1 = var1 / n1\ns2_squared_over_n2 = var2 / n2\n\nnumerator_df = (s1_squared_over_n1 + s2_squared_over_n2) ** 2\ndenominator_df = ((s1_squared_over_n1 ** 2) / (n1 - 1)) + ((s2_squared_over_n2 ** 2) / (n2 - 1))\ndf = numerator_df / denominator_df\nprint(f\"Degrees of Freedom: {df:.2f}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Degrees of Freedom: 62.91\n</code></pre> </li> <li> <p>Find the p-value:</p> <pre><code>from scipy import stats\n\np_value = 2 * stats.t.cdf(-abs(t_statistic), df)\nprint(f\"p-value: {p_value:.4f}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>p-value: 0.0520\n</code></pre> </li> </ul> Automatic Calculation <p>Alternatively, use <code>ttest_ind</code> from <code>scipy.stats</code> with <code>equal_var=False</code> for Welch's t-test (you can use <code>equal_var=True</code> for Student's t-test):</p> <pre><code>from scipy import stats\n\nt_statistic, p_value = stats.ttest_ind(scores1, scores2, equal_var=False)\nprint(f\"t-statistic: {t_statistic:.2f}\")\nprint(f\"p-value: {p_value:.4f}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>t-statistic: -1.98\np-value: 0.0520\n</code></pre> <ul> <li> <p>Make a Decision:</p> <p>The p-value (0.0520) is higher than the significance level (\\(\\alpha = 0.05\\)), so we cannot reject the null hypothesis. There is no significant evidence to suggest a difference in mean test scores between the two teaching methods.</p> </li> </ul>"},{"location":"statistics/hypothesis/Ttest/#assumptions-of-the-two-sample-t-test","title":"Assumptions of the Two-Sample T-Test","text":"<p>For the results of the two-sample t-test to be valid, the following assumptions must be met:</p> <ol> <li>Independence: Observations are independent both within and between groups.</li> <li>Normality: The data in each group are approximately normally distributed.</li> <li>Homogeneity of Variances:<ul> <li>Student's t-test: Assumes equal variances between groups.</li> <li>Welch's t-test: Does not assume equal variances.</li> </ul> </li> <li>Scale of Measurement: The dependent variable is measured on a continuous scale (interval or ratio).</li> </ol> Testing for Equal Variances <p>Before deciding between Student's t-test and Welch's t-test, you can perform an F-test or Levene's Test (robust against non-normal distribution) to assess the equality of variances. </p> <pre><code>from scipy import stats\n\nstat, p_value = stats.levene(scores1, scores2)\nprint(f\"Levene's Test Statistic: {stat:.2f}\")\nprint(f\"p-value: {p_value:.4f}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Levene's Test Statistic: 2.00\np-value: 0.1623\n</code></pre> <p>Since, the p-value is 0.1623, which is greater than the significance level of 0.05, we do not have sufficient evidence to conclude that the variances are different, and therefore we cannot reject the null hypothesis (H<sub>0</sub>: equal variance) </p> <p>So it's appropriate to use Student's t-test assuming equal variances and not the Welch's t-test. which is above the  of 0.05. This suggests that the variances are equal.</p> Task: Firefighter Test Results <p> (Source: Wikipedia | Copyright: Steven Terblanche)  </p> <p>To become a Captain or Lieutenant in the New Haven Fire Department, both a written and oral test must be passed.  Download the following dataset and load it into your notebook. Therefore the python package <code>liac-arff</code> needs to be installed first. </p> <pre><code>#Website: https://www.openml.org/search?type=data&amp;sort=runs&amp;id=42665&amp;status=active\n#Download: https://www.openml.org/data/download/22044446/ricci_processed.arff\n\nimport arff #Installation: pip install liac-arff\nimport pandas as pd\n\n# Load the .arff dataset\nwith open('ricci_processed.arff', 'r') as file:\n    data = arff.load(file)\n\n# Convert data into dataframe\ndf = pd.DataFrame(data['data'], columns=[attr[0] for attr in data['attributes']])\n\n# Show the first 5 rows\ndf.head()\n</code></pre> <p>It contains the results of 118 exams. Answer the following questions using Python:</p> <ol> <li>Are the exam results of Captains significantly different (\\(\\alpha = 5\\%\\)) from those of Lieutenants?</li> <li>Are the exam results of candidates belonging to a minority (<code>Race == H</code> or <code>B</code>) significantly different (\\(\\alpha = 5\\%\\)) from those of candidates who do not belong to a minority?</li> </ol> <p>For both questions, proceed as follows:</p> <ul> <li>The analysis should focus on the total exam scores (attribute <code>Combine</code>).</li> <li>Perform an F-test to ensure that both samples have equal variance.</li> <li>Conduct a two-sided, two-sample t-test (Student or Welch; depending on the F-test results).</li> <li>Interpret the results.</li> </ul>"},{"location":"statistics/probability/CentralLimitTheorem/","title":"Central Limit Theorem","text":"<p>In this section, we'll explore the Central Limit Theorem (CLT), a cornerstone of probability and statistics that works hand in hand with the Law of Large Numbers (LLN) you've previously learned about. The CLT is fascinating because it explains why normal (Gaussian) distributions are so prevalent in statistics, even when the underlying data doesn't seem to fit that mold. </p> (Source: Pinterest)"},{"location":"statistics/probability/CentralLimitTheorem/#definition-interpretation","title":"Definition &amp; Interpretation","text":"Interpretation: Central Limit Theorem <p>The distribution of sample means approaches a normal distribution as the number of samples becomes large, regardless of the shape of the population distribution.</p> <p>But what does this mean?</p> <ul> <li> <p>Population Distribution: This could be any distribution\u2014uniform, skewed, bimodal, or even a highly irregular distribution.</p> <p></p> Code <pre><code>import plotly.express as px\n\nfig = px.histogram(data, x=\"age\")\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.8)'))\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Absolute Frequency',\n        xaxis_title_text='Age',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Age Distribution&lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> </li> <li> <p>Sample Means: If you take multiple samples from this population and calculate their means, those means will form their own distribution.     <pre><code>sample_mean = np.mean(random.sample(list(data.age), 20))\n</code></pre></p> <p></p> Code <pre><code>sample_means = []\nfor i in range(100):\n    sample_means.append(np.mean(random.sample(list(data.age), 20)))\n\nfig = px.histogram(x=sample_means)\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.8)'))\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Absolute Frequency',\n        xaxis_title_text='Sample Mean',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample Mean Distribution&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Sample Size: 20 | Number of Samples: 100&lt;/span&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n\n        ),\n    )\nfig.show()\n</code></pre> </li> <li> <p>Result: As the number of samples increases, the distribution of these sample means will tend toward a normal distribution.     </p> Code <pre><code>sample_means = []\nfor i in range(100000):\n    sample_means.append(np.mean(random.sample(list(data.age), 20)))\n\nfig = px.histogram(x=sample_means)\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.8)'))\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Absolute Frequency',\n        xaxis_title_text='Sample Mean',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample Mean Distribution&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Sample Size: 20 | Number of Samples: 100000&lt;/span&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n\n        ),\n    )\nfig.show()\n</code></pre> </li> </ul> <p>This demonstrates that even if the original data is not normally distributed, the distribution of the sample means will be approximately normal if the sample size is large enough.</p>"},{"location":"statistics/probability/CentralLimitTheorem/#relation-to-the-law-of-large-numbers","title":"Relation to the Law of Large Numbers","text":"<p>The Law of Large Numbers states that as the size of a sample increases, the sample mean will get closer to the population mean. When combined with the CLT, we understand not only that the sample mean converges to the population mean but also that the distribution of those sample means becomes normal.</p>"},{"location":"statistics/probability/CentralLimitTheorem/#why-is-the-central-limit-theorem-important","title":"Why Is the Central Limit Theorem Important?","text":"<p>Simplifies Statistical Analysis</p> <ul> <li>Normality Assumption: Many statistical tests assume normality. Thanks to the CLT, we can often safely make this assumption for sample means or sums.</li> <li> <p>Parameterization: Normal distributions are fully described by their mean and variance, making them easy to work with.</p> <p> (Source: Wikipedia)  </p> </li> </ul> <p>Practical Applications</p> <ul> <li> <p>Confidence Intervals: The CLT allows us to construct confidence intervals around sample means.      (Source: Ainali on Wikipedia)  </p> </li> <li> <p>Hypothesis Testing: Facilitates hypothesis testing by enabling the use of z-scores and t-scores.</p> </li> <li>Quality Control: In manufacturing, the CLT helps in monitoring process variations.</li> </ul>"},{"location":"statistics/probability/CentralLimitTheorem/#recap","title":"Recap","text":"<p>The Central Limit Theorem is a powerful reminder that, regardless of the original data distribution, the process of sampling and aggregation tends to produce a normal distribution. This universality is why we often say, \"All roads lead to Gauss.\" Understanding the CLT not only deepens your grasp of statistical principles but also equips you with the tools to make accurate inferences from data in a wide array of fields.</p>"},{"location":"statistics/probability/CentralLimitTheorem/#tasks","title":"Tasks","text":"Task: Central Limit Theorem <p>We use the biased die example from before: </p> <pre><code>die_biased = [1, 2, 4, 5, 6, 6]\n</code></pre> <p>Work on the following tasks: </p> <ol> <li>Calculate the expected value of the biased die (mean).</li> <li>Visualize the probability of each side in a histrogram.</li> <li>Now we perform some experiments:<ul> <li>Choose one random side (<code>sample_size = 1</code>) of the biased die. Repeat this <code>100.000</code> times and visualize the frequency in a histogram. </li> <li>Compare the result with the visualization of the probability. </li> <li>Now increase the <code>sample_size</code> and inspect the change in histogram. </li> <li>Does the calculated expected value match with the mean value of the normal distribution? </li> </ul> </li> </ol>"},{"location":"statistics/probability/General/","title":"Probability","text":"<p>In this section we trasit from the exploration of descriptive statistics to the domain of inferential statistics. Inferential statistics plays an important role in interpreting data, making predictions, and drawing conclusions about broader populations based on sample data. At the heart of inferential statistics lies the concept of probability - an essential tool for calculating, interpreting, and applying likelihoods to real-world situations. This section delves into the fundamental principles of probability, providing a foundation for understanding the processes behind statistical inference and its practical applications.</p>"},{"location":"statistics/probability/General/#what-is-probability","title":"What is Probability?","text":"<p>We'll begin by defining probability and exploring some common misconceptions. For now, let's start with a basic definition.</p> <p>Probability is the level of possibility of something happening or being true</p> <p>-- Cambridge Dictionary</p> <p>The probability values range from 0 to 1, where </p> <ul> <li>0 indicates impossibility and </li> <li>1 represents certainty. </li> </ul> <p>A key point to note is that the sum of all probabilities for a set of outcomes must equal 1. In other words, the total likelihood of all possible outcomes must add up to certainty.</p> Example: Rolling a Die <p>Imagine you are rolling a standard six-sided die. The die has six possible outcomes: </p> \\[ 1, 2, 3, 4, 5, 6 \\] <p>Each of these outcomes has an equal probability of occurring. The probability of rolling any specific number, say a 1, is 1 out of 6, or </p> \\[ P(X = 1) = 1/6. \\] <p>Now, if we sum the probabilities of all possible outcomes (rolling a 1, 2, 3, 4, 5, or 6), we get:</p> \\[ P(X = 1) + P(X = 2) + \\text{...} + P(X = 6) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = 1 \\] <p>This shows that the total probability of all possible outcomes equals 1, meaning that one of the six outcomes is certain to occur when you roll the die.</p> <p>Let's make this concept more tangible by looking at an example from a common smartphone weather app. Imagine the app shows </p> <p>\"On Tuesday there is a 30% chance of rain.\" This statement can be interpreted in different ways: Does it mean that it will rain for 30 percent of the day? Or that it will rain over 30 percent of the area at a given location? Or is there a 3 in 10 chance it will rain? </p> <p>The correct interpretation, however, is that there is a 3 in 10 chance of rain at any given point during the day. This aligns with how probability works - it's not about how long it will rain or the confidence level in the prediction, but the likelihood of rain occurring at all. We'll explore more examples like this to clarify these ideas as we move forward.</p> <p>Why is probability so important? Probability plays a central role in fields like weather forecasting, actuarial science, and medical research. In medicine, for instance, probability is used to evaluate the effectiveness of treatments and to model the spread of diseases. It's also essential in physics, chemistry, biology, and even machine learning and artificial intelligence, where it helps make predictions based on data.</p> (Source: sandserifcomics)  <p>We rely on probability whenever uncertainty (e.g. randomness) exists about the outcome of an event. For example, determining whether men are generally taller than women requires probability because it isn't universally true that all men are taller than all women. Similarly, probability helps assess the accuracy of medical tests, as even highly reliable tests can produce false positives. In contrast, for questions with known answers, such as the speed of light versus the speed of sound, probability isn't needed.</p>"},{"location":"statistics/probability/General/#random-variables","title":"Random Variables","text":"<p>A random variable differs slightly from traditional mathematical variables. In mathematics, variables can:</p> <ol> <li>Represent values that may vary.</li> <li>Act as unknowns to be solved.</li> </ol> <p>For random variables, the first property holds, but the second does not. Random variables allow us to perform calculations with the outcomes of a random experiment, making it possible to model and work with uncertainty. They are typically represented by capital letters, and their specific values are denoted by lowercase letters.</p> Definition: Random Variable <p>A random variable is a function that assigns numbers to the outcomes of a random process (mapping).</p> <p>For example, consider the result of rolling a die:</p> \\[ X = \\text{The number shown when a fair die is rolled} \\] <pre><code>import random\nrandom.randint(1, 6)\n</code></pre> <p>Or the result of flipping a coin:</p> \\[ X =  \\begin{cases} 1, &amp; \\text{if heads} \\\\ 0, &amp; \\text{if tails} \\end{cases} \\] <pre><code>random.choices([\"Heads\", \"Tails\"])\n</code></pre> <p>In both cases, the random variable maps the outcome of a random process to a numerical value. In the context of a coin flip:</p> <ul> <li>The random process is the coin flip itself.</li> <li>Each experiment refers to an individual flip.</li> <li>An event is the outcome, such as getting heads or tails.</li> <li>The sample space is the set of all possible outcomes (heads or tails).</li> <li>The random variable assigns a numerical value (e.g., 1 for heads, 0 for tails) to each event.</li> </ul> <p>Thus, random variables enable us to quantify the outcomes of random processes, allowing for further analysis and interpretation.</p> Task: Rolling the Dice <p>Now it's your turn to create a random variable. There are several packages in Python that we can use for this purpose. Use the <code>random</code> package we already used in the package management section. </p> <p> (Source: imgflip)  </p> <ol> <li>Now generate your own random number. Use the commands <code>randint</code>, <code>random</code> and <code>choices</code>. A good documentation can be found here</li> <li>Are those numbers really random? Do some research about the <code>random.seed</code> command</li> <li>Now create the following experiments: <ul> <li>Fair Die: Perform a virtual 'rolling of the die' for a fair (normal) die by using the <code>choices</code> command     <pre><code>die_fair = [1, 2, 3, 4, 5, 6]\n</code></pre></li> <li>Biased Die: Now use a biased die with no 3 but two times the side 6     <pre><code>die_biased = [1, 2, 4, 5, 6, 6]\n</code></pre></li> </ul> </li> </ol>"},{"location":"statistics/probability/General/#probability-vs-proportion","title":"Probability vs. Proportion","text":"<p>Two concepts that students frequently mix up in statistics are probability and proportion. Here\u2019s the key distinction:</p> <ul> <li>Probability refers to the likelihood of an event occurring and is based on theoretical outcomes.</li> <li>Proportion reflects how often an event has actually occurred, relying on observed data.</li> </ul> <p>In simpler terms, probability is typically used to discuss the likelihood of future events, while proportion is used to describe the frequency of events that have already happened. The following examples highlight the differences between these two concepts in various situations.</p> Example: Flip a Coin <p>When flipping a fair coin, the probability of it landing on heads is 0.5, or 50%, which is based on theory. However, if we flip the coin 20 times, we can calculate the proportion of times it actually lands on heads. For instance, it might land on tails 40% of the time in those 20 flips. In this case, probability is a theoretical expectation, while proportion is based on real, observable outcomes that we can count.</p> <p></p> Code <pre><code>import random\nimport plotly.express as px\n\nmylist = [\"Heads\", \"Tails\"]\nrandom.seed(23) # Set seed for reproducibility\nflips = random.choices(mylist, k=20)\n\n# Create a histogram\nfig = px.histogram(x=flips, nbins=2)\n\n# Change bar mode\nfig.update_traces(marker=dict(color='#00416E', line=dict(color='#00416E', width=0.5)))\n\n# Set overlay mode\nfig.update_layout(\n    xaxis_title_text='Result',\n    yaxis_title_text='Count',\n    title=dict(\n            text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Experiment: Flipping 20 Coins &lt;/span&gt;&lt;/b&gt;',\n        ),\n    bargap=0.1,\n    showlegend=False,)\n\nfig.show()\n</code></pre> Fun Fact: A Coin Toss is not 50/50 <p>The term \"coin toss\" is often used as a symbol of randomness, but mathematicians have long suspected that even a fair coin has a slight tendency to land more often on one side. To investigate this bias, Ph.D. candidate Franti\u0161ek Barto\u0161 gathered 47 volunteers who flipped coins over multiple weekends, eventually conducting 350,757 tosses. Their findings showed that coins landed with the same side facing upward as before the toss 50.8% of the time, confirming a small but significant bias in coin flips. (Arxiv, derStandard)</p> Task: Roll a Biased Die <p>Let's stick with the example from before and perform further experiments. We use a fair die and a biased die and perform the following task:  </p> <ol> <li>Roll each die <ul> <li>5 </li> <li>50  </li> <li>500 times. </li> </ul> </li> <li>Visualize the results in histogram (one for the fair die, one for the biased die). </li> </ol>"},{"location":"statistics/probability/General/#calculation-of-probability","title":"Calculation of Probability","text":""},{"location":"statistics/probability/General/#prerequisites","title":"Prerequisites","text":"<p>Before we start calculating probability, it's important to note that certain types (scales) of data (see section Attribute Types), such as nominal and ordinal are suitable for probability calculations, while interval and ratio scaled data are not directly valid for such computations. Interval and ratio data must first be converted into a discrete scale (like creating bins in a histogram) before probability can be applied, as their values have infinite precision, making them unsuitable for exact probability calculations.</p> Example: Length of Wooden Beams <p>An example involving the length of wooden beams can illustrate this point. Asking for the probability of a beam being exactly a certain length, down to a microscopic precision, is not a practical question. Instead, engineers or builders focus on the probability of a beam\u2019s length falling within a certain range, such as between 3.0 and 3.5 meters, or even within more precise intervals like 3.2 to 3.25 meters. This approach allows for meaningful analysis while accounting for slight variations in manufacturing or cutting processes.</p> <p>Another key requirement for valid probability calculations is that the data categories must be mutually exclusive. For instance, when flipping a coin, the events \"heads\" and \"tails\" are mutually exclusive since both cannot happen at the same time. Similarly, if a wooden beam is measured to be between 3.5 and 4 m in length, it cannot also be between 4 and 4.5 m.</p> (Source: MakeAMeme)  <p>However, some situations, like getting news from multiple sources, do not have mutually exclusive categories (a person can receive news from both TV and the internet). In such cases, probability cannot be computed unless the question is reformulated with exclusive categories.</p>"},{"location":"statistics/probability/General/#calculation","title":"Calculation","text":"<p>For discrete random variables, each outcome of an experiment can be assigned a probability. The probability of a specific outcome \\(X = x\\) is calculated using the formula:</p> Definition: Probability <p>Probability of a specific outcome</p> \\[ P(X = x) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of possible outcomes}} \\] <pre><code>p_head = 1/2 # favorable outcomes = head / total outcomes = head + tail\n</code></pre> <p>The probability distribution of a discrete random variable shows the likelihood of various outcomes occurring. While this distribution helps us understand the chances of different events, it doesn\u2019t allow us to predict the result of any single experiment. However, if the experiment is repeated many times, the overall pattern becomes clearer, following predictable rules.</p> Example: Flip a Coin <p></p> Code <pre><code>import random\nimport plotly.express as px\n\nmylist = [\"Heads\", \"Tails\"]\nrandom.seed(23) # Set seed for reproducibility\nflips = random.choices(mylist, k=20)\n\n# Create a histogram\nfig = px.histogram(x=flips, nbins=2, histnorm='probability')\n\n# Change bar mode\nfig.update_traces(marker=dict(color='#00416E', line=dict(color='#00416E', width=0.5)))\n\n# Set overlay mode\nfig.update_layout(\n    xaxis_title_text='Result',\n    yaxis_title_text='Probability/Proportion',\n    title=dict(\n            text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Experiment: Flipping 20 Coins &lt;/span&gt;&lt;/b&gt;',\n        ),\n    bargap=0.1,\n\nfor i in range(0, 2):\n    fig.add_shape(\n        type='line',\n        x0=i-0.46,\n        x1=i+0.46,\n        y0=0.5,\n        y1=0.5,\n        line=dict(color='#E87F2B', width=5),\n        xref='x',\n        yref='y',\n        name='Probability' if i == 0 else None,\n        showlegend=(i == 0) \n    )\nfig.show()\n</code></pre> Task: Probability of the Die <p>We will continue with our example of the fair and biased die. </p> <ol> <li>Calculate the probability for each side of the fair/biased die by using the <code>pandas</code> <code>value_counts</code> function</li> <li>Visualize the experiment from before in a histogram and overlay the calculated probabilty. Use the <code>plotly express</code> <code>add_shape</code> function.</li> </ol>"},{"location":"statistics/probability/General/#odds","title":"Odds","text":"<p>When dealing with probabilities, you'll often encounter the term odds. So, what does odds mean? It\u2019s frequently used in everyday language, especially in fields like medicine and gambling. While probability and odds are related, they are not the same thing. This explanation will clarify the meaning of odds and show how they differ from probability, as well as how to convert between them.</p> <p>Odds typically describe the ratio between two possibilities: the chance of something not happening compared to it happening. For example, when you hear \"the odds are five to one,\" this means the odds ratio is 5:1, or 5 divided by 1. In mathematical terms, odds represent the ratio of the probability of an event not happening to the probability of it happening. It can be written as:</p> Definition: Odds \\[ \\text{Odds ratio (r)} = \\frac{p}{1 - p} \\] <p>where \\(p\\) is the probability of the event happening, and \\(1 - p\\) is the probability of the event not happening. To convert odds into probability, you can solve for \\(p\\) using the equation:</p> \\[ p = \\frac{r}{1 + r} \\] <p>where \\(r\\) is the odds ratio. </p> Example: Flip a Coin <p>When flipping a fair coin, there are two possible outcomes: heads or tails. Each outcome has an equal chance of occurring. Understanding the concept of odds in this simple scenario can help clarify the difference between probability and odds.</p> <ul> <li>Probability of getting heads (P): \\( \\frac{1}{2} \\) or 0.5 (50%)</li> <li>Probability of getting tails: \\( 1 - P = \\frac{1}{2} \\) or 0.5 (50%)</li> </ul> <p>Calculating Odds:</p> \\[ \\text{Odds in favor of heads} = \\frac{P(\\text{heads})}{P(\\text{not heads})} = \\frac{0.5}{0.5} = \\frac{1}{1} = 1 \\] <p>This means the odds in favor of getting heads are 1 to 1, often written as 1:1. This indicates an equal chance of getting heads or tails.</p> Task: Odds of the Die <ol> <li>Calculate the odds for the fair die to roll 6</li> <li>Now calculate the same thing for the biased die</li> </ol>"},{"location":"statistics/probability/General/#mass-density-function","title":"Mass &amp; Density Function","text":"<p>In statistics, we often encounter the concepts of probability mass functions (PMF) and probability density functions (PDF). These functions help describe the probabilities of different types of events\u2014whether they are discrete or continuous.</p>"},{"location":"statistics/probability/General/#probability-mass-function-pmf","title":"Probability Mass Function (PMF)","text":"<p>A probability mass function is used to describe probabilities for discrete events. Discrete events are those that occur in distinct, countable states, such as flipping a coin, rolling a die, or drawing a card. For example, if we roll a die, each face (1, 2, 3, 4, 5, or 6) is a discrete event. A PMF assigns probabilities to each possible outcome. In this case, each side of a fair die has a probability of 1/6, and these probabilities are represented in a bar plot or histogram.</p> <pre><code>random.choices([1,2,3,4,5,6])\n</code></pre> <p>Let\u2019s say we have a biased die, and the probability of rolling a 6 is twice as hig than other numbers and there is no 3. </p> <pre><code>random.choices([1,2,3,4,5,6], weights=[1/6, 1/6, 0, 1/6, 1/6, 2/6])\n</code></pre> <p>In this case, the PMF would show different probabilities for each number, but the probabilities are still discrete values\u2014there\u2019s no such thing as rolling a 4.5 on a die.</p> Example: Rolling the Die <ul> <li> <p>Fair Die</p> <p></p> Code <pre><code>import numpy as np\nimport plotly.express as px\nimport pandas as pd\n\nx = [1,2,3,4,5,6]\n\ndf = pd.DataFrame(x, columns=['fair'])\n\nfig = px.histogram(df, x='fair', nbins=6, histnorm='probability density')\n\n# Adjust the plot\nfig.update_layout(\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Rolling a Fair Die&lt;/span&gt;&lt;/b&gt;',\n        ),\n    xaxis_title_text='Number',\n    yaxis_title_text='Probability',\n    bargap=0.1,\n)\n\n# Scale the axis\nfig.update_layout(yaxis_range=[0,1])\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Biased Die</p> <p></p> Code <pre><code>import numpy as np\nimport plotly.express as px\nimport pandas as pd\n\nx = [1,2,4,5,6,6]\n\ndf = pd.DataFrame(x, columns=['unfair'])\n\nfig = px.histogram(df, x='unfair', nbins=6, histnorm='probability density')\n\n# Adjust the plot\nfig.update_layout(\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Rolling a Biased Die&lt;/span&gt;&lt;/b&gt;',\n        ),\n    xaxis_title_text='Number',\n    yaxis_title_text='Probability',\n    bargap=0.1,\n)\n\n# Scale the axis\nfig.update_layout(yaxis_range=[0,1])\n\n# Show the plot\nfig.show()\n</code></pre> </li> </ul> <p>For the Biased Die</p> <p>The probability of rolling a 6 would be:</p> \\[ P(X = 6) = \\frac{2}{6} = 33.3\\% \\] <p>Similarly, the probability of rolling a number greater than 4 would be:</p> \\[ P(X &gt; 4) = \\frac{3}{6} = 50\\% \\] <p>The probability of rolling a number between 1 and 4 is:</p> \\[ P(1 &lt; X &lt; 4) = \\frac{1}{6} = 16.7\\% \\] <p>One key rule of PMFs is that the sum of all probabilities must equal 1. For example, in a deck of cards, the sum of the probabilities for drawing any card must equal 1, since you're certain to draw some card from the deck.</p>"},{"location":"statistics/probability/General/#probability-density-function-pdf","title":"Probability Density Function (PDF)","text":"<p>In contrast, a probability density function is used for continuous events. Continuous events don\u2019t have discrete outcomes; instead, they can take on any value within a range. For example, if we\u2019re measuring the height of a person, we can\u2019t pinpoint an exact value (down to the atom). Instead, we look at the probability of the height falling within a range, such as between 180 cm and 190 cm. Unlike PMFs, PDFs are represented by smooth curves, showing how the probability is distributed over a range of values.</p> <p>With continuous data, we can\u2019t assign a probability to a specific value (such as someone being exactly 165.432 cm tall). Instead, we compute the probability of a value falling within a range using the area under the curve of the PDF. For instance, we might ask, \"What\u2019s the probability that someone\u2019s height is between 180 cm and 190 cm?\" This probability is calculated by integrating the PDF over that range.</p> Example: Height of a Person <p></p> Code <pre><code>import numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom scipy.stats import norm\n\n# Initialization\nlimit_up = 190\nlimit_down = 180\n\n# Generate data for the normal distribution\nx = np.arange(130, 210, 0.1)\ny = norm.pdf(x, 170, 10)\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Create a column for the fill area\ndf['fill'] = np.where((df['x'] &gt;= limit_down) &amp; (df['x'] &lt;= limit_up), df['y'], 0)\n\n\n# Create the plot\nfig = px.line(df, x='x', y='y')\nfig.add_trace(px.line(df, x='x', y='fill').data[0])\n\n# Adjust the plot\nfig.data[0].update(line=dict(color='#00416E', width=2))\n\nfig.data[1].update(\n    fill='tozeroy', \n    fillcolor='rgba(0, 65, 110, 0.4)',\n    line=dict(width=0),)\n\nfig.update_layout(\n    xaxis_title_text='x',\n    yaxis_title_text='Density',\n    title=dict(\n            text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Probability Density Function &lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;\u00b5=170cm, std=10cm&lt;/span&gt; &lt;/b&gt;',\n        ),\n    showlegend=False,\n)\n\nfig.show()\n</code></pre> <p>Calculating the probability, that a person is between 180 and 190 cm:</p> \\[ P(180 \\le X \\le 190) = \\int_{180}^{190} f(X)dx \\] <p>or smaller than 150 cm:</p> \\[ P(X \\le 150) = \\int_{-\\infty}^{150} f(X)dx \\]"},{"location":"statistics/probability/General/#cumulative-distribution-function-cdf","title":"Cumulative Distribution Function (CDF)","text":"<p>A CDF is a function that provides the cumulative probability for a given random variable. In simpler terms, it gives the probability that a random variable will take a value less than or equal to a specific point. </p> <p><pre><code>from scipy.stats import norm\nnorm.cdf(0)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.5\n</code></pre></p> <p>To build a CDF from a PDF, you essentially sum all the probability values of the PDF up to a certain point on the x-axis. Mathematically, this is equivalent to calculating the integral of the PDF for continuous distributions. The CDF at a particular value \\( x \\) gives you the total probability of the random variable being less than or equal to \\( x \\).</p> <p>For example, imagine a probability density function that describes the heights of people in a population. The CDF at a specific height (say 150 cm) tells you the probability of randomly selecting someone who is 150 cm or shorter. As you move further along the x-axis, the CDF will continue to increase until it reaches 1, which represents 100% probability.</p> Example: Height of a Person <p></p> Code <pre><code>import numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom scipy.stats import norm\n\n# Initialization\nlimit_up = 190\nlimit_down = 180\n\n# Generate data for the normal distribution\nx = np.arange(130, 210, 0.1)\ny = norm.cdf(x, 170, 10)\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Create a column for the fill area\ndf['fill'] = np.where((df['x'] &gt;= limit_down) &amp; (df['x'] &lt;= limit_up), df['y'], 0)\n\n\n# Create the plot\nfig = px.line(df, x='x', y='y')\nfig.add_trace(px.line(df, x='x', y='fill').data[0])\n\n# Adjust the plot\nfig.data[0].update(line=dict(color='#00416E', width=2))\n\nfig.data[1].update(\n    fill='tozeroy', \n    fillcolor='rgba(0, 65, 110, 0.4)',\n    line=dict(width=0),)\n\nfig.update_layout(\n    xaxis_title_text='x',\n    yaxis_title_text='Probability',\n    title=dict(\n            text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Cumulated Distribution Function &lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;\u00b5=170cm, std=10cm&lt;/span&gt; &lt;/b&gt;',\n        ),\n    showlegend=False,\n)\n\nfig.show()\n</code></pre> <p>Calculating the probability, that a person is between 180 and 190 cm:</p> \\[ P(180 \\le X \\le 190) = \\int_{180}^{190} f(X)dx = 13.6\\% \\] <p>or smaller than 150 cm:</p> \\[ P(X \\le 150) = \\int_{-\\infty}^{150} f(X)dx = 2.3\\% \\] Code <pre><code>print('Between 180cm and 190cm:', (norm.cdf(190, 170, 10) - norm.cdf(180, 170, 10))*100, '%')\nprint('Smaller than 150cm:', (norm.cdf(150, 170, 10))*100, '%')\n</code></pre> <p>Key Properties of CDFs:</p> <ol> <li>CDFs start at 0: The lowest x-value in the distribution will have a cumulative probability of 0.</li> <li>CDFs increase monotonically: As you move along the x-axis, the CDF always increases or stays the same. It never decreases, since probabilities cannot decrease over time.</li> <li>CDFs approach 1: As the x-values increase and encompass all possible outcomes, the cumulative probability approaches 1, representing 100%.</li> </ol> <p>While PDFs describe the probability density, CDFs are the cumulative sum of those probabilities. One important distinction is that summing all the values of a PDF equals 1 (as it represents the total probability), but summing all the values of a CDF does not necessarily give 1. Instead, the CDF gradually approaches 1 as the x-values increase.</p> <p>CDFs provide a powerful way to compute cumulative probabilities, especially when working with continuous distributions. By understanding the relationship between PDFs and CDFs, we can answer practical questions like \"What is the probability of scoring above a certain value?\" or \"What is the probability of a variable falling within a specific range?\"</p> <p>In practical terms, CDFs allow you to compute probabilities up to a certain value on the x-axis, making them essential tools in statistics, probability theory, and real-world applications like exam scores or analyzing biological data.</p> Task: Density Function <p>Assume the heights of individuals in a certain population follow a normal distribution with a mean of 170 cm and a standard deviation of 10 cm (see examples above).</p> <p>Answer the following questions based on this normal distribution:</p> <ol> <li>What percentage of individuals are taller than 190 cm?</li> <li>What percentage of individuals are between 170 cm and 180 cm tall?</li> <li>Plot the Probability Density Function (PDF) for a normal distribution with a mean of 180 cm and a standard deviation of 5 cm.</li> </ol> <p>For the first two questions use the Cumulative Distribution Function (CDF) to calculate the corresponding probabilities.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/","title":"Law of Large Numbers","text":""},{"location":"statistics/probability/LawOfLargeNumbers/#understanding-the-law","title":"Understanding the Law","text":"<p>The Law of Large Numbers states that as you increase the number of times you repeat an experiment, the average of your sample means will more closely approximate the population mean. In other words, if you conduct the same experiment repeatedly and calculate the mean of each sample, then average those means together, this average will converge toward the true population mean as the number of experiments increases. </p> (Source: QuickMeme)  <p>This is crucial because we often don\u2019t know the true population mean, but by conducting multiple trials, we can estimate it more accurately over time.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/#mathematical-representation","title":"Mathematical Representation","text":"<p>Now, let's explore how this law is expressed mathematically.</p> Definition: Law of Large Numbers \\[ \\lim_{n \\to \\infty} P\\left( \\left| \\bar{x}_n - \\mu \\right| \\geq \\epsilon \\right) = 0 \\] <p>Where:</p> <ul> <li>\\( n \\) is the number of trials (or experiment repetitions) - not the sample size,</li> <li>\\( \\bar{x}_n \\) is the average of the sample means from \\( n \\) trials,</li> <li>\\( \\mu \\) is the true population mean,</li> <li>\\( \\epsilon \\) is an arbitrarily small positive number.</li> </ul> <p>The essence of the formula is that the probability of our average deviating from the true population mean by more than \u03b5 approaches zero as n becomes very large. In simpler terms, the more experiments we conduct, the closer our average gets to the true mean, and the less likely it is to be significantly off.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/#importance-of-the-lln","title":"Importance of the LLN","text":"<p>So, why is the Law of Large Numbers so important? Because it tells us that any single sample or experiment can be heavily influenced by randomness, variability, and noise. This means relying on a single experiment might not give us an accurate estimate of the true population mean. We shouldn't place too much trust in just one set of results.</p> (Source: Imgflip Meme Generator)  <p>However, by repeating the experiment multiple times we can obtain an average that closely approximates the true population mean. Even if we can't measure the population mean directly, averaging multiple samples allows us to estimate it with increasing accuracy.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/#how-large-is-large","title":"How Large is 'Large'?","text":"<p>An important question arises: How large does our number of repetitions (\\(n\\)) need to be to get a reasonable estimate? The law doesn't specify a particular number. Unfortunately, there's no one-size-fits-all answer because it depends on various factors like the nature of the data, experimental conditions, and measurement methods.</p> <p>While we can't name an exact number, we now understand that the Law of Large Numbers operates more qualitatively - it tells us that more repetitions lead to better approximations without specifying exactly how many are needed.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/#an-illustrative-example","title":"An Illustrative Example","text":"<p>Let's consider an example. Suppose we take a fair die, </p> <pre><code>die_fair = [1, 2, 3, 4, 5, 6]\n</code></pre> <p>roll it 50 times (sample size)</p> Code <pre><code>fig = px.scatter(y=fair, x=np.arange(50), labels={'y': 'Die Number', 'x': 'Roll'})\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.6)'))\nfig.update_layout(\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Results of 50 Die Rolls&lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: Fair Die&lt;/span&gt;&lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <p>and calculate the average of the results</p> <p><pre><code>fair_mean = np.mean(fair)\nprint('Mean of 50 rolls of a fair die:', fair_mean)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Mean of 50 rolls of a fair die: 3.64\n</code></pre></p> <p>Then, we repeat this entire experiment 500 times (\\(n\\)). Here's what happens:</p> <ul> <li>Individual Experiment Results: Each experiment's average varies. Sometimes it's close to the expected value of 3.5 (the average roll of a fair die), and other times it's further away. These averages fluctuate and don't necessarily converge to 3.5 on their own.</li> </ul> Code <pre><code>def mean_of_n_rolls(sample_size, number_of_samples):\n    return [np.mean(random.choices([1, 2, 3, 4, 5, 6], k=sample_size)) for _ in range(number_of_samples)]\n\nsample_size = 50\nnumber_of_samples = 500\n\nmeans_result = mean_of_n_rolls(sample_size, number_of_samples)\n\nfig = px.line(x=np.arange(number_of_samples), y=means_result,markers=True)\nfig.update_traces(line=dict(color='rgba(0, 65, 110, 0.6)'))\n\n# Add a vertical line for the population mean\nfig.add_hline(y=3.5, line_dash=\"dash\", annotation_text=\"Expected Value: 3.5\", annotation_position=\"top right\", line_color=\"#E87F2B\")\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Sample Mean',\n        xaxis_title_text='# Experiment',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample Means (Sample Size: 50)&lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: Fair Die&lt;/span&gt;&lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <ul> <li>Cumulative Average: Instead of looking at individual averages, we calculate the cumulative average up to each point. The first point is just the average of the first experiment. The second point is the average of the first two experiments, and so on, up to the 500th point, which is the average of all 500 experiments.</li> </ul> Code <pre><code># Cumulative Average\ncumulative_average = np.cumsum(means_result) / np.arange(1, number_of_samples + 1)\n\n# Plot the cumulative average\nfig = px.line(x=np.arange(number_of_samples), y=cumulative_average,markers=True)\nfig.update_traces(line=dict(color='rgba(0, 65, 110, 0.6)'))\n\n# Add a horizontal line for the population mean\nfig.add_hline(y=3.5, line_dash=\"dash\", annotation_text=\"Expected Value: 3.5\", annotation_position=\"top right\", line_color=\"#E87F2B\")\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Cumulative Average',\n        xaxis_title_text='# Experiment',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Cumulative Average (Sample Size: 50)&lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: Fair Die&lt;/span&gt;&lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <p>What's interesting is how quickly the cumulative average converges to the expected value of 3.5. Even after just a few repetitions, the cumulative average is already quite close to 3.5 - much closer than most individual experiment averages. By the time we reach 500 repetitions, the cumulative average is almost exactly 3.5.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/#recap","title":"Recap","text":"<p>In this session, we learned about the Law of Large Numbers. We've explored what it is, examined its mathematical formulation, and seen a demonstration of how it works. This fundamental principle is crucial for statistical analysis and scientific research, emphasizing the importance of repeated experimentation to obtain accurate estimates of population parameters.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/#tasks","title":"Tasks","text":"Task: Law Of Large Numbers <p>Use the following dataset:  <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \nadult = fetch_ucirepo(id=2)\n# https://archive.ics.uci.edu/dataset/2/adult\n\n# data (as pandas dataframes) \ndata = adult.data.features \n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following tasks: </p> <ol> <li>Calculate the population mean of the variable <code>age</code></li> <li>Take one sample of 100 entries from the variable <code>age</code> and visualize them in a scatter plot</li> <li>Repeat this experiment 500 times and plot the means of each sample in a line chart. Add a horizontal line for the true population mean</li> <li>Visualize the cumulative average in a line chart. Does it approach the expected value? </li> </ol>"},{"location":"statistics/probability/Sampling/","title":"Sampling","text":""},{"location":"statistics/probability/Sampling/#sample-distribution","title":"Sample Distribution","text":"<p>In this explanation, we'll talk about creating sample distributions, an essential topic in inferential statistics. This is important for generating confidence intervals and making inferences about a population from a sample, which is a core goal of inferential statistics.</p> Code <p>For the upcoming example, the following data will be used:      <pre><code>from ucimlrepo import fetch_ucirepo \n\nadult = fetch_ucirepo(id=2) # fetch dataset \ndata = adult.data.features # data (as pandas dataframes) \n</code></pre></p>"},{"location":"statistics/probability/Sampling/#creating-a-data-distribution","title":"Creating a Data Distribution","text":"<p>Let\u2019s say we\u2019re interested in finding out the average age of people. While we know there is a wide range of ages, we want to get a more precise understanding. Statistically, this means determining the population parameter for the age of people. However, it\u2019s not feasible to find out the age of every person in the world. Why? There are simply too many people, living in different regions, and new individuals are born while others pass away. Additionally, we might accidentally include the same person more than once, making the task more complicated.</p> <p>Instead of measuring the entire population, we can take a random sample of people. For example, we could collect the ages of 100 individuals. </p> <pre><code>sample = random.sample(list(data.age), 100)\n</code></pre> <p>One person might be 25 years old, another might be 42 years old, and so on. After gathering the data, we can create a distribution of these ages, often visualized as a histogram, and calculate the sample mean (the average age of the people in our sample).</p> <pre><code>sample_mean = np.mean(sample)\n</code></pre> Code <pre><code>import random \nimport numpy as np\nimport plotly.express as px\nimport pandas as pd\n\n# Pick a random sample of 100 from the data and calculate the mean\nsample = random.sample(list(data.age), 100)\nsample_mean = np.mean(sample)\n\n# Count the frequency of each age in the sample\npd.DataFrame(sample)\ns_counts = pd.DataFrame(sample).value_counts().sort_index()\ns_counts = s_counts.to_frame().reset_index()\n\n# Plot the sample data\nfig = px.bar(x = s_counts[0], y=s_counts['count'])\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.8)'))\n\n# Add a vertical line for the population mean\nfig.add_vline(x=sample_mean, line_dash=\"dash\", annotation_text=\"Mean: \"+str(sample_mean), annotation_position=\"top right\", line_color=\"#E87F2B\")\n\n# Adjust the layout\nfig.update_layout(\n    barmode='overlay',\n    xaxis_title_text='Age',\n    yaxis_title_text='Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Age Distribution - Sample 4 (Mean: '+str(sample_mean)+')&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig.show()\n</code></pre> <p>Now, the sample mean we calculate from our 100 people is just one estimate of the population's average age.</p> Outlook <p>How does this sample mean realate to the actual population mean?  And how confident are we, that this sample mean is really close to the population mean?  Those are questions we will learn to answer in the upcomming sections about confidence intervals and hypothesis testing.</p> <p>However, it\u2019s important to understand that this is just one estimate. If we take another random sample of 100 people, we might get a slightly different average because not all individuals are the same age. So, what can we do? </p> Stewie Repetition GIFfrom Stewie Repetition GIFs <p>Right, we repeat the experiment! We measure another sample of 100 people and calculate a new sample mean. Note, that there can be an overlap between those two samples (one person was part of the first and the second sample). This is called sampling with replacement. </p> <p>We notice that the results are similar to the first sample, but not exactly the same. </p> <p>This difference is known as sampling variability - the natural variation that occurs when taking different samples from the same population. More about this topic will be covered in the  Sampling Variability section. </p> Task: Data Distribution <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \nadult = fetch_ucirepo(id=2)\n# https://archive.ics.uci.edu/dataset/2/adult\n\n# data (as pandas dataframes) \ndata = adult.data.features \n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following tasks: </p> <ol> <li>Pick a random sample of 100 values from the <code>hours-per-week</code> attribute</li> <li>Calculate the mean of this sample</li> <li>Plot the sample distribution as a <code>plotly.express</code> bar chart. Add vertical line (<code>fig.add_vline</code>) to indicate the sample mean</li> <li>Calculate the mean of the population (all the entries in the dataset) and compare it with the sample</li> </ol>"},{"location":"statistics/probability/Sampling/#creating-a-sample-distribution-of-means","title":"Creating a Sample Distribution of Means","text":"<p>We can repeat this sampling process multiple times. Let\u2019s say we take N samples. Each sample gives us a new sample mean, and these means will vary slightly due to the natural differences in people's ages. Now, instead of focusing on individual ages (data distribution), we can create a distribution of sample means - a new distribution that shows how the sample estimates themselves vary.</p> Code <pre><code>import random \nimport numpy as np\nimport plotly.express as px\nimport pandas as pd\n\n# Function to calculate the mean of the sample\ndef calculate_sample_mean(sample_size, n_sample):\n    sample_mean = []\n    for i in range(n_sample):\n        sample = random.sample(list(data.age), sample_size)\n        sample_mean.append(np.mean(sample))\n    return sample_mean\n\n# Initialize variables\nsample_size = 100\nnumber_of_samples = 100\n\n# Calculate the sample means\nx1 = calculate_sample_mean(sample_size, number_of_samples)\n\n# Bin the data\nx1_binned = pd.cut(x1, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\nx1_counts = x1_binned.value_counts().sort_index()\n\n# Trim the data to remove the 0.0 values at the start and end\nfirst_non_zero_index = x1_counts.ne(0.0).idxmax() \nlast_non_zero_index = x1_counts[::-1].ne(0.0).idxmax()\nx1_counts_trimmed = x1_counts.loc[first_non_zero_index:last_non_zero_index]\n\n# Convert the index to string\nx1_counts_trimmed.index = x1_counts_trimmed.index.astype(str)\n\n# Plot the data\nfig = px.bar(x = x1_counts_trimmed.index, y=x1_counts_trimmed)\nfig.update_traces(marker=dict(color='rgba(232, 127, 43, 0.8)'))\n\nfig.update_layout(\n    barmode='overlay',\n    xaxis_title_text='Age Category',\n    yaxis_title_text='Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Age Distribution of Sample Means&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Sample Size: 100 | Number of Samples: 100&lt;/span&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig.show()\n</code></pre> <p>This distribution of sample means gives us valuable insights about the population. For example, if most of the sample means are very close to each other, we can be more confident that our sample estimates are close to the true population mean. However, if the sample means are widely spread out, it indicates that we might need larger or more representative samples to get a more accurate estimate of the population's true average age.</p> Code <pre><code>import random \nimport numpy as np\nimport plotly.express as px\nimport pandas as pd\nimport plotly.graph_objects as go\n\n# Initialize variables\nsample_size = [10, 100, 1000]\nnumber_of_samples = [10, 100, 1000]\n\n# Function to calculate the mean of the sample\ndef calculate_sample_mean(sample_size, n_sample):\n    sample_mean = []\n    for i in range(n_sample):\n        sample = random.sample(list(data.age), sample_size)\n        sample_mean.append(np.mean(sample))\n    return sample_mean\n\nfor i in range(3):\n    # Calculate the sample means\n    x1 = calculate_sample_mean(sample_size[0], number_of_samples[i])\n    x2 = calculate_sample_mean(sample_size[1], number_of_samples[i])\n    x3 = calculate_sample_mean(sample_size[2], number_of_samples[i])\n\n    # Bin the data\n    x1_binned = pd.cut(x1, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\n    x2_binned = pd.cut(x2, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\n    x3_binned = pd.cut(x3, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\n\n    x1_counts = x1_binned.value_counts().sort_index()\n    x2_counts = x2_binned.value_counts().sort_index()\n    x3_counts = x3_binned.value_counts().sort_index()\n\n    x1_counts.index = x1_counts.index.astype(str)\n    x2_counts.index = x2_counts.index.astype(str)\n    x3_counts.index = x3_counts.index.astype(str)\n\n    # Plot the data\n    fig = px.bar(x = x1_counts.index, y=x1_counts)\n    fig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.4)'), name='Sample Size = '+str(sample_size[0]), showlegend=True)\n\n    fig.add_trace(go.Bar(x = x2_counts.index, y=x2_counts, name='Sample Size = '+str(sample_size[1]), marker=dict(color='rgba(255, 0, 0, 0.4)')))\n    fig.add_trace(go.Bar(x = x3_counts.index, y=x3_counts, name='Sample Size = '+str(sample_size[2]), marker=dict(color='rgba(0, 255, 0, 0.4)')))\n\n    fig.update_layout(\n        xaxis_range=[150,300],\n        barmode='overlay',\n        xaxis_title_text='Age Category',\n        yaxis_title_text='Frequency',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Age Distribution of Sample Means&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Number of Samples: ' + str(number_of_samples[i]) + '&lt;/span&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n        ),\n    )\n\n    fig.show()\n</code></pre> Different Types of Sample Distribution <p>While we have been using the mean as an example, the same approach can be applied to other statistics as well. For instance, you could look at the variance. If you're interested in knowing how much age varies across a population, you could calculate the variance or standard deviation for each sample.</p> <p>By repeating the process with multiple samples, you can then create a sample distribution of these variance estimates. This allows you to find the average variance across all the samples, giving you a better understanding of how much variability exists in the ages of the overall population.</p> Task: Sample Distribution <p>Use the the dataset from before and work on the following tasks:</p> <ol> <li>Calculate the sample distribution of the standard deviation for a <code>sample_size = 100</code> and <code>number_of_samples = 100</code> and visualize it in bar chart</li> <li>Calculate the standard deviation of the population and compare the results.</li> <li>Again, visualize the sample distribution of the standard deviation now for <code>sample_size = [10, 100, 1000]</code> and <code>number_of_samples = 100</code></li> </ol>"},{"location":"statistics/probability/Sampling/#sample-distribution-of-differences","title":"Sample Distribution of Differences","text":"<p>Let's shift the focus of our example and explore sample estimate differences instead of just looking at one parameter.  The question we want to explore is: are widowed individuals generally older than people who have never been married? While it seems obvious that widowed individuals are likely to be older, since people tend to marry before becoming widowed, let\u2019s frame this question statistically to better understand the process.</p> <p>We start by imagining two populations: one of widowed individuals and one of never-married individuals. </p> <pre><code>data_widowed = data[data['marital-status'] == 'Widowed'].age\ndata_never_married = data[data['marital-status'] == 'Never-married'].age\n</code></pre> <p>Since it\u2019s not practical to measure the age of every person in both groups, we take a random sample from each. For example, we could sample 100 widowed individuals and 100 people who have never been married, </p> <pre><code>sample_widowed = random.sample(list(data_widowed), 100)\nsample_never_married = random.sample(list(data_never_married), 100)\n</code></pre> <p>and then calculate the average age for each group.</p> <pre><code>sample_widowed_mean = np.mean(sample_widowed)\nsample_never_married_mean = np.mean(sample_never_married)\n</code></pre> <p>Let\u2019s say, in the first sample, the average age of the widowed group is 70 years, and the average age of the never-married group is 45 years. The difference between these averages is 25 years. However, this is just one sample, and the difference might vary slightly if we take another random sample. So, we repeat the process with a second random sample and find that the difference this time is 24 years.</p> Code <pre><code>number_of_samples = 100\nsample_size = 100\n\ndef calculate_sample_mean(sample_size, n_sample, data):\n    sample_mean = []\n    for i in range(n_sample):\n        sample = random.sample(list(data), sample_size)\n        sample_mean.append(np.mean(sample))\n    return sample_mean\n\nx1 = calculate_sample_mean(sample_size, number_of_samples, data_widowed)\nx2 = calculate_sample_mean(sample_size, number_of_samples, data_never_married)\n\nx1_binned = pd.cut(x1, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\nx2_binned = pd.cut(x2, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\n\nx1_counts = x1_binned.value_counts().sort_index()\nx2_counts = x2_binned.value_counts().sort_index()\n\nx1_counts.index = x1_counts.index.astype(str)\nx2_counts.index = x2_counts.index.astype(str)\n\nfig = px.bar(x = x1_counts.index, y=x1_counts)\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.4)'), name='Widowed', showlegend=True)\n\nfig.add_trace(go.Bar(x = x2_counts.index, y=x2_counts, name='Never-Married', marker=dict(color='rgba(255, 0, 0, 0.4)')))\n\nfig.update_layout(\n    xaxis_range=[80,460],\n    barmode='overlay',\n    xaxis_title_text='Age Category',\n    yaxis_title_text='Frequency',\n    title=dict(\n        text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Age Distribution of Sample Means&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Sample Size: 100 | Number of Samples: 100&lt;/span&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age, maritial-statur&lt;/span&gt;&lt;/b&gt;',\n    ),\n)\n\nfig.show()\n</code></pre> <p>By repeating this process across many samples, we can build a distribution of the differences in average age between the widowed and never-married groups. This distribution allows us to see how consistent the differences are across various samples. If the differences are relatively consistent, we can be confident that widowed individuals tend to be older than those who have never been married. If the differences vary widely, we may need larger or more representative samples to draw a reliable conclusion.</p> Code <pre><code>number_of_samples = 100\nsample_size = 100\n\n\ndef calculate_sample_mean_diff(sample_size, n_sample, data1, data2):\n    sample_mean_diff = []\n    for i in range(n_sample):\n        sample1 = random.sample(list(data1), sample_size)\n        sample2 = random.sample(list(data2), sample_size)\n        sample_mean_diff.append(np.mean(sample1)-np.mean(sample2))\n    return sample_mean_diff\n\nx1 = calculate_sample_mean_diff(sample_size, number_of_samples, data_widowed, data_never_married)\n\n\nx1_binned = pd.cut(x1, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\nx1_counts = x1_binned.value_counts().sort_index()\n\nfirst_non_zero_index = x1_counts.ne(0.0).idxmax() \nlast_non_zero_index = x1_counts[::-1].ne(0.0).idxmax()  \nx1_counts_trimmed = x1_counts.loc[first_non_zero_index:last_non_zero_index]\n\nx1_counts_trimmed.index = x1_counts_trimmed.index.astype(str)\n\nfig = px.bar(x = x1_counts_trimmed.index, y=x1_counts_trimmed)\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.4)'))\n\nfig.update_layout(\n    barmode='overlay',\n    xaxis_title_text='Age Category',\n    yaxis_title_text='Frequency',\n    title=dict(\n        text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Age Distribution of Sample Differences (widowed vs. never-married)&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Sample Size: 100 | Number of Samples: 100&lt;/span&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age, maritial-statur&lt;/span&gt;&lt;/b&gt;',\n    ),\n)\n\nfig.show()\n</code></pre> Sign (+/-) of the Estimator <p>It\u2019s also important to note that, just like in other statistical comparisons, the sign of the difference (positive or negative) doesn\u2019t affect the actual result. For instance, if we subtract the average age of the never-married individuals from that of the widowed group, we\u2019ll get a positive difference. But if we subtract the widowed individuals' ages from the never-married individuals' ages, the result will be negative. Statistically, both results show the same difference; it\u2019s just a matter of interpretation.</p>"},{"location":"statistics/probability/Sampling/#random-representative-sampling","title":"Random &amp; Representative Sampling","text":"<p>To give another example, suppose we wanted to compare the ages of people from different regions, say those living in urban areas versus rural areas. Even if there\u2019s no significant age difference between the two groups, each sample might show slight variations due to natural differences in the sample. By taking many samples, we could build a distribution of differences and better understand whether any observed difference is consistent.</p> <p>This also emphasizes the importance of random and representative sampling. If we don\u2019t carefully select our samples, we might get misleading results. For instance, imagine comparing life expectancy in two countries, but we only sample older people in one country and younger people in the other. The resulting data would falsely suggest a significant difference in life expectancy between the countries, when in reality, the sampling was biased. To avoid this, we need to ensure that our samples fairly represent the populations we\u2019re studying.</p>"},{"location":"statistics/probability/Sampling/#sampling-varibility","title":"Sampling Varibility","text":"<p>Variability is a crucial concept in statistics and probability and plays a significant role in scientific research. It\u2019s also one of the main sources of frustration for researchers because it can make results less predictable and more complex. Let\u2019s dive into the idea of sampling variability, where this variability comes from, and why understanding it is so important.</p> Representative Sample (Source: Zieffler, A., &amp; Catalysts for Change. (2019). Statistical Thinking: A simulation approach to uncertainty (4.2th ed.). Minneapolis, MN: Catalyst Press. http://zief0002.github.io/statistical-thinking/) <p>Imagine you have a research question: What is the average age of workers in a factory? To answer this, you decide to measure the age of a few workers. Let's say you randomly select one worker who is 45 years old. Is this the average age of all workers in the factory? Most likely not. So, you keep asking more workers and get different ages like 50, 34, 60, etc. This difference in ages between the workers is an example of sampling variability - the natural differences that occur when taking samples from a population.</p> <p>Even though the overall population (all workers in the factory) has a true average age, you will get slightly different values from each sample you take. This is why relying on just one sample to represent an entire population can lead to inaccurate results. The goal is to reduce this variability by taking more samples and averaging them to get a more reliable estimate of the population\u2019s average age like we did before.</p> Definition: Sampling Variability <p>Using the</p> <ul> <li>same measurement (e.g. mean) on </li> <li>different samples (e.g. age of 100 people) from the </li> <li>same population (e.g. age of all people in the world) can lead to </li> <li>different values (e.g. 47 or 52,...)</li> </ul>"},{"location":"statistics/probability/Sampling/#sources-of-sampling-variability","title":"Sources of Sampling Variability","text":"<ol> <li> <p>Natural Variation: In many fields, especially biology and sociology, natural variation occurs. In our factory example, workers come from different backgrounds and generations, which naturally causes variability in their ages.</p> </li> <li> <p>Measurement Noise: In some cases, the tools or methods used to gather data may introduce variability. For instance, if you\u2019re using a tool with low precision, like a scale that only measures in whole kilograms when you need to measure in grams, you introduce errors in your measurements.</p> </li> <li> <p>Complex Systems: Variability can also come from the complexity of the system being studied. In the factory example, factors like different hiring policies, regional differences, or workforce changes can add to the variation in workers\u2019 ages.</p> </li> <li> <p>Uncontrollable Factors: Some sources of variability, like random or unpredictable events, are outside of the researcher's control. For example, economic conditions might affect the hiring or retirement age of workers, adding unpredictability.</p> </li> </ol>"},{"location":"statistics/probability/Sampling/#dealing-with-sampling-variability","title":"Dealing with Sampling Variability","text":"<p>To reduce the impact of sampling variability, one of the most effective strategies is to take multiple samples. Instead of measuring just a few workers' ages, you would gather information from a larger sample, maybe 100 or more workers, and calculate the average age. The more samples you take, the closer you get to the true average age of the entire population. This is based on the law of large numbers, which states that as you increase the number of samples, the average of those samples will get closer to the population mean.</p> <p>Additionally, statistical tools like confidence intervals can help you understand how close your sample estimate is to the actual population parameter. Confidence intervals provide a range within which the true average age of the factory workers is likely to fall, giving a better sense of the precision of your estimate.</p>"},{"location":"statistics/probability/Sampling/#sampling-variability-vs-reliable-estimates","title":"Sampling Variability vs. Reliable Estimates","text":"<p>So as we mentioned before, we can deal with the sampling variablility by using a larger sample. So we can repeat the experiment from before by randomly selecting a group of different people and calculating the mean of their age. We can do that, by looping through different sample sizes</p> <pre><code>samplesizes = np.arange(5,1000)\nfor sampi in range(len(samplesizes)):\n        sample = random.sample(list(data.age), samplesizes[sampi])\n</code></pre> <p>and for each sample, we calculate the mean age and then plot the result, showing how the mean changes with different sample sizes. Now, let\u2019s consider: what do we expect to happen? Intuitively, as the sample size grows, we\u2019d expect the sample mean to get closer and closer to the population mean, which represents the true average. So, as we increase the sample size, the variation in the sample means should decrease, and the results should start to stabilize around the population mean.</p> Code <pre><code>## Repeat for different sample sizes\nsamplesizes = np.arange(5,1000)\nsamplemeans = np.zeros(len(samplesizes))\n\nfor sampi in range(len(samplesizes)):\n    sample = random.sample(list(data.age), samplesizes[sampi])\n    samplemeans[sampi] = np.mean(sample)\n\nfig = px.line(x=samplesizes, y=samplemeans,markers=True)\nfig.update_traces(line=dict(color='rgba(0, 65, 110, 0.6)'))\n\n# Add a vertical line for the population mean\nfig.add_hline(y=np.mean(data.age), line_dash=\"dash\", annotation_text=\"Mean: \"+str(np.mean(data.age)), annotation_position=\"top right\", line_color=\"#E87F2B\")\n\nfig.update_layout(\n        xaxis_range=[samplesizes.min()-10,samplesizes.max()+10],\n        barmode='overlay',\n        yaxis_title_text='Mean Value',\n        xaxis_title_text='Sample Size',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample Means of Age&lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <p>When we run the experiment, we see that the blue line represents the sample means, and they\u2019re fluctuating quite a bit. The orange line is the true population mean, which is known in advance. While the sample means do start to get closer to the orange line as the sample size increases, they still bounce around quite a lot, even with large samples of up to 1,000 people. This variation is normal for sampling and doesn\u2019t indicate a problem. In fact, it\u2019s expected due to the natural randomness in sample selection. </p> <p>To show something interesting, we take the mean of several sample means (in this case 10)</p> <p><pre><code>print(np.mean(samplemeans[:10]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>35.7\n</code></pre></p> <p>By averaging these sample means together, we get a result that\u2019s much closer to the population mean. </p> <p><pre><code>population_mean = np.mean(data.age)\npopulation_mean\n</code></pre> &gt;&gt;&gt; Output<pre><code>38.6\n</code></pre></p> <p>As we increase the number of sample means we average, the average gets even closer to the true population mean.</p> <p><pre><code>print(np.mean(samplemeans[:100]))\nprint(np.mean(samplemeans[:1000]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>38.3\n38.6\n</code></pre></p> <p>This leads to an important discovery: while individual sample means might not perfectly match the population mean, averaging over multiple sample means brings us much closer to the true value. This concept ties into two key statistical principles - the law of large numbers and the central limit theorem.</p> Task: Sampling Variability <p>Use the the dataset from before and work for the variable <code>hours-per-week</code> on the following tasks:</p> <ol> <li>Calculate the standard deviation for <code>sample_size = np.arange(5,1000)</code></li> <li>Calculate the standard deviation of the population</li> <li>Visualize the results as shown above (incl. standard deviation of the population using <code>fig.add_hline</code> )</li> <li>Calculate the mean of the sample standard deviation for the first <code>10</code>, <code>100</code> and <code>1000</code> samples</li> </ol>"},{"location":"statistics/regression/LinearRegression/","title":"Linear Regression","text":"<p>In many cases, simply characterizing the data is not sufficient. Beyond explaining the data, the goal is often to enable predictions. This chapter introduces the basic approach of linear regression, which allows for approximating bivariate data. The topics covered include linear regression and the coefficient of determination. Regression aims to model the relationships between a dependent variable and one or more independent variables.</p>"},{"location":"statistics/regression/LinearRegression/#motivation","title":"Motivation","text":"<p>To understand the motivation behind linear regression we will start this chapter with an example. Consider a mobile plan that costs \u20ac26, including unlimited SMS, calls, and data within the country. Data roaming costs \u20ac0.84 per MB. The bills for the last year show monthly expenses based on roaming usage.</p> Month Roaming [MB] Bill [\u20ac] Month Roaming [MB] Bill [\u20ac] January 25 47.00 July 125 131.00 February 300 278.00 August 62 78.08 March 258 242.72 September 94 104.96 April 135 139.40 October 381 346.04 May 12 36.08 November 12 36.08 June 0 26.00 December 18 41.12 Code <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\n\n# Create a DataFrame\ndf = pd.DataFrame([(25, 47.00), (300, 278.00), (258, 242.72), (135, 139.40), (12, 36.08), \n                (0, 26.00), (125, 131.00), (62, 78.08), (94, 104.96), \n                (381, 346.04), (12, 36.08), (18, 41.12)], \n                columns=['Roaming', 'Price'])\n\n# Linear Regression\nmodel = LinearRegression()\nmodel.fit(df[['Roaming']], df['Price'])\n\nintercept = model.intercept_\nslope = model.coef_[0]\nr_sq = model.score(df[['Roaming']], df['Price'])\n\n# Generate regression line\ndf['Regression Line'] = intercept + slope * df['Roaming']\n\n# Create Plotly Express figure\nfig = px.scatter(df, x='Roaming', y='Price')\nfig['data'][0]['marker'] = {'color':'red', 'size':10}\n\n# Add regression line\nfig.add_traces(px.line(df, x='Roaming', y='Regression Line').data)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Roaming [MB]',\n    yaxis_title_text='Price [\u20ac]',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Smartphone Bill&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Variables: roaming, price&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> <p>A scatter plot of the data reveals a perfect linear relationship, allowing us to describe the relationship with a linear function:</p> \\[ y = 26 + 0.84 \\cdot x \\] <p>This has several advantages. For one, the bill amount can be explained through fixed and variable costs, specifically showing how the MB usage affects the total cost. Additionally, it allows for predictions of the bill amount for any unobserved amount of MB.</p> <p>However, in reality, most relationships are not perfectly linear. Let's consider two samples, each with variables \\(X\\) and \\(Y\\).</p> \\(X_1\\) \\(Y_1\\) \\(X_2\\) \\(Y_2\\) 0.00 0.23 0.14 2.00 0.12 0.31 0.25 2.41 0.18 0.49 0.18 2.69 0.26 1.11 0.27 3.41 0.40 1.03 0.42 3.43 0.51 1.32 0.50 3.82 0.60 1.58 0.62 4.18 0.68 1.66 0.70 4.36 0.80 1.65 0.79 4.45 0.80 1.85 0.85 4.75 0.99 1.69 1.00 4.69 <p>When analyzing these samples, we find:</p> <ul> <li>Sample 1 has a Pearson correlation coefficient of \\( \\rho_1 = 0.938 \\).</li> <li>Sample 2 has a Pearson correlation coefficient of \\( \\rho_2 = 0.942 \\).</li> </ul> <p>These values are very similar and suggest a strong correlation.</p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul> Code <pre><code>import pandas as pd\nimport plotly.express as px\n\nx1 = [0.00, 0.12, 0.18, 0.26, 0.40, 0.51, 0.60, 0.68, 0.80, 0.80, 0.99]\ny1 = [0.23, 0.31, 0.49, 1.11, 1.03, 1.32, 1.58, 1.66, 1.65, 1.85, 1.69]\n\nx2 = [0.14, 0.25, 0.18, 0.27, 0.42, 0.50, 0.62, 0.70, 0.79, 0.85, 1.00]\ny2 = [2.00, 2.41, 2.69, 3.41, 3.43, 3.82, 4.18, 4.36, 4.45, 4.75, 4.69]\n\ndf = pd.DataFrame({'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2})\n\n\n# Create Plotly Express figure\nfig = px.scatter(df, x='x1', y='y1')\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='x1',\n    yaxis_title_text='y1',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Dataset 1&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n\n\n# Create Plotly Express figure\nfig2 = px.scatter(df, x='x2', y='y2')\n\n# Adjust the plot\nfig2.update_layout(\n    xaxis_title_text='x2',\n    yaxis_title_text='y2',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Dataset 2&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig2.show()\n</code></pre> <p>At first glance, a scatter plot supports this conclusion, but the impression changes when the axes are normalized equally. </p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul> Code <pre><code>import pandas as pd\nimport plotly.express as px\n\nx1 = [0.00, 0.12, 0.18, 0.26, 0.40, 0.51, 0.60, 0.68, 0.80, 0.80, 0.99]\ny1 = [0.23, 0.31, 0.49, 1.11, 1.03, 1.32, 1.58, 1.66, 1.65, 1.85, 1.69]\n\nx2 = [0.14, 0.25, 0.18, 0.27, 0.42, 0.50, 0.62, 0.70, 0.79, 0.85, 1.00]\ny2 = [2.00, 2.41, 2.69, 3.41, 3.43, 3.82, 4.18, 4.36, 4.45, 4.75, 4.69]\n\ndf = pd.DataFrame({'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2})\n\n\n# Create Plotly Express figure\nfig = px.scatter(df, x='x1', y='y1')\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='x1',\n    yaxis_title_text='y1',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Dataset 1&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig.update_layout(yaxis_range=[0,6])\n\n# Show the plot\nfig.show()\nfig.write_html(\"outputpic/regression_scatter_unscale1.html\", full_html=False, include_plotlyjs='cdn')\n\n\n# Create Plotly Express figure\nfig2 = px.scatter(df, x='x2', y='y2')\n\n# Adjust the plot\nfig2.update_layout(\n    xaxis_title_text='x2',\n    yaxis_title_text='y2',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Dataset 2&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig2.update_layout(yaxis_range=[0,6])\n\n# Show the plot\nfig2.show()\n</code></pre> <p>Proper scaling reveals:</p> <ol> <li>For every \\(X\\) value, the corresponding \\(Y\\) value in Sample 2 is consistently larger than in Sample 1.</li> <li>The change in \\(Y\\) is more significant in Sample 2 compared to Sample 1 when \\(X\\) changes.</li> </ol> <p>This phenomenon occurs because we intuitively focus on the overall picture and draw a mental line through the points. The question then arises: how do we determine this line? This leads us into the core of linear regression, where we aim to model the relationship between variables and make informed predictions.</p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul> Code <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\n\nx1 = [0.00, 0.12, 0.18, 0.26, 0.40, 0.51, 0.60, 0.68, 0.80, 0.80, 0.99]\ny1 = [0.23, 0.31, 0.49, 1.11, 1.03, 1.32, 1.58, 1.66, 1.65, 1.85, 1.69]\n\nx2 = [0.14, 0.25, 0.18, 0.27, 0.42, 0.50, 0.62, 0.70, 0.79, 0.85, 1.00]\ny2 = [2.00, 2.41, 2.69, 3.41, 3.43, 3.82, 4.18, 4.36, 4.45, 4.75, 4.69]\n\ndf = pd.DataFrame({'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2})\n\n# Linear Regression Sample 1\nmodel1 = LinearRegression()\nmodel1.fit(df[['x1']], df['y1'])\nintercept1 = model1.intercept_\nslope1 = model1.coef_[0]\ndf['y1_hat'] = intercept1 + slope1 * df['x1']\n\n# Linear Regression Sample 2\nmodel2 = LinearRegression()\nmodel2.fit(df[['x2']], df['y2'])\nintercept2 = model2.intercept_\nslope2 = model2.coef_[0]\ndf['y2_hat'] = intercept2 + slope2 * df['x2']\n\n# Create Plotly Express figure\nfig = px.scatter(df, x='x1', y='y1')\nfig['data'][0]['marker'] = {'color':'red', 'size':10}\n\n# Add regression line\nfig.add_traces(px.line(df, x='x1', y='y1_hat').data)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='x1',\n    yaxis_title_text='y1',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample 1&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig.update_layout(yaxis_range=[0,6])\n\n# Show the plot\nfig.show()\n\n\n# Create Plotly Express figure\nfig2 = px.scatter(df, x='x2', y='y2')\nfig2['data'][0]['marker'] = {'color':'red', 'size':10}\n\n# Add regression line\nfig2.add_traces(px.line(df, x='x2', y='y2_hat').data)\n\n# Adjust the plot\nfig2.update_layout(\n    xaxis_title_text='x2',\n    yaxis_title_text='y2',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample 2&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig2.update_layout(yaxis_range=[0,6])\n\n# Show the plot\nfig2.show()\n</code></pre>"},{"location":"statistics/regression/LinearRegression/#linear-regression_1","title":"Linear Regression","text":"<p>In linear regression, the relationship between variables is not exactly described. To account for this, a random error \\( e_i \\) is added. </p> Definition <p>Approximation of the real values \\( y_i \\)</p> \\[ y_i = \\hat{y}_i + e_i \\] <p>With the linear regression</p> \\[ \\hat{y}_i = a + b \\cdot x_i \\] <p>The goal of linear regression is to find the best fit line by solving a minimization problem. This problem minimizes the sum of the squared residuals, expressed as</p> Definition <p>Minimization problem for fitting the linear regression</p> \\[ \\min_{a,b} \\sum_{i=1}^{n}e_i^2 = \\min_{a,b} \\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2= \\min_{a,b} \\sum_{i=1}^{n}(y_i-a-bx_i)^2 \\] <p>By solving this, the coefficients of the regression line can be determined, with</p> Definition <p>Intercept</p> \\[ a = \\bar{y} - b \\bar{x} \\] <p>Slope</p> \\[ b = \\frac{\\text{cov}(X, Y)}{\\sigma_x^2} \\] <p>The best fit line is the one that minimizes the sum of squared differences between observed and predicted values. These differences, known as residuals, represent the distance between the actual \\( Y \\)-values and the predicted \\( Y \\)-values from the model.</p>"},{"location":"statistics/regression/LinearRegression/#coefficient-of-determination","title":"Coefficient of Determination","text":"<p>To evaluate the goodness of fit of a model, the coefficient of determination \\( R^2 \\) can be used. It is calculated as:</p> Definition <p>Coefficient of Determination</p> \\[ R^2 = \\frac{\\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} = \\rho_{X,Y}^2 \\] <p>The coefficient of determination indicates the proportion of variance explained by the model. In essence, \\( R^2 \\) measures how well the regression model fits the observed data.</p> <p>\\( R^2 \\) can range from zero to one:</p> <ul> <li> <p>\\( R^2 = 0 \\): None of the variance is explained by the model, indicating a poor fit.</p> </li> <li> <p>\\( R^2 = 1 \\): All of the variance is explained by the regression, indicating a perfect fit, which can only occur when the original data points lie exactly on the regression line.</p> </li> </ul>"},{"location":"statistics/regression/LinearRegression/#recap","title":"Recap","text":"<ul> <li>Linear regression attempts to model the relationship between multiple variables.</li> <li>Using the model, further values can be predicted.</li> <li>In linear regression, the squared distance between the raw data points and the regression line is minimized.</li> <li>The coefficient of determination \\(R^2\\) is used to assess the model's goodness of fit.</li> <li>The closer the \\(R^2\\) value is to one, the better the model fits the data.</li> </ul>"},{"location":"statistics/regression/LinearRegression/#tasks","title":"Tasks","text":"Task <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ncars = fetch_ucirepo(id=9) \n# https://archive.ics.uci.edu/dataset/9/auto+mpg\n\n# data (as pandas dataframes) \ndata = cars.data.features\ndata = data.join(cars.data.ids)\n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following task: </p> <ol> <li>Perform a linear regression for the following attribute combinations: <ul> <li>Displacement vs. Weight</li> <li>Displacement vs. Acceleration</li> <li>Acceleration vs. Weight</li> </ul> </li> <li>For all performed regressions calculate the coefficient of determination</li> <li>Write down the formula for all performed regressions </li> </ol>"},{"location":"statistics/univariate/CentralTend/","title":"Measures of Central Tendency","text":"<p>Measures of central tendency characterize a distribution by using an average value. These metrics describe what can be considered a \"typical\" or \"representative\" observation within the data set.</p>"},{"location":"statistics/univariate/CentralTend/#mean","title":"Mean","text":"<p>(for numeric attributes) The arithmetic mean \\( \\bar{x} \\) is the most common and effective numerical measure for describing the average value of a distribution. </p> Definition: Arithmetic Mean \\[ \\bar{x}=\\frac{\\sum_{i=1}^{N}x_i}{N}=\\frac{x_1+x_2+\\dots+x_N}{N} \\] <p>with \\( x_1, x_2, \\dots, x_N \\) representing a set of \\( N \\) values of a metric variable \\( X \\).</p> <p>One disadvantage of the mean is its sensitivity to outliers. Even a small number of extreme values can distort the result.</p> <pre><code>import statistics \n\nstatistics.mean([1,2,1,2,3,4,1,100,1,2,1])\n</code></pre> &gt;&gt;&gt; Output<pre><code>10.73\n</code></pre> <p>To address this issue, the trimmed mean is used. This method involves removing a percentage of values from both the upper and lower ends of the distribution (e.g., 1%). </p> <pre><code>from scipy import stats\n\n#calculate 10% trimmed mean\nstats.trim_mean([1,2,1,2,3,4,1,100,1,2,1], 0.1)\n</code></pre> &gt;&gt;&gt; Output<pre><code>1.889\n</code></pre> <p>However, this approach can lead to a loss of information, especially when a large portion of the data is trimmed.</p> Example: Mean of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the mean and the trimmed mean (20% left and right) of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution </p> \\[ \\begin{eqnarray*} \\bar{x} &amp;=&amp; \\frac{28.3 + 27.2 + 27.4 + 22.7 + 14.3 + 11.9 + 13.8}{14}\\\\     &amp;&amp;+\\frac{19.8 + 9.6 + 21.1 + 20.8 + 19.8 + 25.3 + 22.8}{14} \\\\     &amp;=&amp; 20.34 \\end{eqnarray*} \\] \\[ \\begin{eqnarray*} \\bar{x}_{trim} &amp;=&amp; \\frac{13.8+ 14.3+ 19.8+ 19.8+20.8 }{14}\\\\     &amp;&amp;+\\frac{21.1+ 22.7+ 22.8+ 25.3+ 27.2}{14} \\\\     &amp;=&amp; 20.76 \\end{eqnarray*} \\] <p>The mean temperature is \\(20.34^\\circ C\\) and the trimmed mean is \\(20.76^\\circ C\\) .</p> Code <pre><code>import statistics \nTemp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\nprint('Mean: ' + str(statistics.mean(Temp)))\n\nfrom scipy import stats\nprint('Trimmed Mean: ' + str(stats.trim_mean(Temp, 0.2)))\n</code></pre>"},{"location":"statistics/univariate/CentralTend/#median","title":"Median","text":"<p>(for numeric and ordinal attributes) The median is more robust against outliers, as it splits the distribution into an upper and a lower half. It corresponds to the 50th percentile or the second quartile. </p> <pre><code>import numpy as np\nnp.sort([1,2,1,2,3,4,1,100,1,2,1])\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1,1,1,1,1,2,2,2,3,4,100]\n           \u2191\n</code></pre> <pre><code>statistics.median([1,2,1,2,3,4,1,100,1,2,1])\n</code></pre> &gt;&gt;&gt; Output<pre><code>2\n</code></pre> <p>However, one downside is that it can be more complex to calculate, especially when dealing with large datasets.</p> Definition: Median <p>A data set of \\( N \\) values of an attribute \\( X \\) is sorted in increasing order</p> <ul> <li>If \\( N \\) is odd, the median is the middle value of the ordered set</li> <li>\\( N \\) is even, the median is the two middlemost values and any value in between (average of those two for numeric attribute )</li> </ul> Example: Median of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the median of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution <pre><code>[9.6, 11.9, 13.8, 14.3, 19.8, 19.8, 20.8, 21.1, 22.7, 22.8, 25.3, 27.2, 27.4, 28.3]\n</code></pre></p> \\[ \\bar{x} =\\frac{20.8+21.1}{2} = 20.95\\\\ \\] <p>The median temperature is \\(20.95^\\circ C\\).</p> Code <pre><code>import statistics \nTemp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\nprint('Median: ' + str(statistics.median(Temp)))\n</code></pre>"},{"location":"statistics/univariate/CentralTend/#mode","title":"Mode","text":"<p>(for numeric, ordinal and nominal attributes) The mode is the most frequently occurring value in a distribution. It is also more robust against outliers, making it a useful measure in certain cases where extreme values might distort other central tendency metrics.</p> <pre><code># Calculate the first Mode\nstatistics.mode([1,2,1,2,3,4,1,100,1,2,2])\n</code></pre> &gt;&gt;&gt; Output<pre><code>1\n</code></pre> <pre><code># Calculate all Modes\nstatistics.multimode([1,2,1,2,3,4,1,100,1,2,2])\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1, 2]\n</code></pre> Definition: Mode <ul> <li>Mode is the value that occurs most frequently in a data set.</li> <li>There can be more than one mode (unimodal, bimodal, multimodal)</li> <li>If each data value occurs only once, then there is no mode</li> </ul> Example: Mode of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the mode of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution  The mode of the temperature is \\(19.8^\\circ C\\) because this value appears twice, making it the only value that occurs more than once. The distribution is unimodal.</p> Code <pre><code>import numpy as np\nTemp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n\nfrom scipy import stats\n#https://docs.scipy.org/doc/scipy/reference/stats.html\nprint('Mode: ' + str(stats.mode(Temp))) # returns the first mode (for unimodal)\n\nimport statistics \n#https://docs.python.org/3/library/statistics.html\nprint('Mode: ' + str(statistics.mode(Temp))) # returns the first mode (for unimodal)\nprint('Mode (Multi): ' + str(statistics.multimode(Temp))) # returns a list of all modes (for multimodal)\n</code></pre>"},{"location":"statistics/univariate/CentralTend/#quantile","title":"Quantile","text":"<p>(numeric and ordinal attributes) The \\( q \\)-quantile divides the data into \\( q \\) equal-sized parts. There are \\( q - 1 \\) quantiles (for example, 3 quantiles divide the data into 4 parts). The 2-quantile corresponds to the median. </p> <pre><code>import numpy as np\nnp.sort([1,2,1,2,3,4,1,100,1,2,2])\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1,1,1,1,2,2,2,2,3,4,100] \n</code></pre> <p>The quartiles can be determined graphically using</p> <pre><code>[1,1,1,1,2,2,2,2,3,4,100]\n |----|----|----|----|\n      Q1   Q2   Q3   \n</code></pre> <p>or by using pythons <code>scipy</code> package</p> <pre><code>from scipy import stats\n\nstats.mstats.mquantiles([1,2,1,2,3,4,1,100,1,2,2], prob=[0.25, 0.5, 0.75])\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1. , 2. , 2.8]\n</code></pre> <p>There are different methods for determining quantiles, especially when \\( N \\) is even. As a result, it is possible for different libraries to produce a different result than a manual calculation of quantiles.</p> Definition: Quantile <p>A data set of \\(N\\) values of an attribute \\(X\\) is sorted in increasing order</p> <ul> <li>The \\(k\\)-th \\(q\\)-quantile is the value \\(x\\) where \\(k/q\\) of the data values are less and \\((q-k)/q\\) values are more than \\(x\\) (with \\(0 &lt; k &lt; q\\))</li> <li>If set of numbers are odd, you have to calculate the middle</li> </ul> <p>Most widely used forms</p> <ul> <li>2-quantile = median: Divides the data set in halves</li> <li>4-quantile = quartiles: Three data points split the data into four equal parts</li> <li>100-quantile = percentiles: Divide the data into 100 equal-sized sets</li> </ul> Example: Quantiles of Temperature Distribution <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the quartiles of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution <pre><code>[9.6, 11.9, 13.8, &lt;&lt;14.3&gt;&gt;, 19.8, 19.8, &lt;&lt;20.8&gt;&gt;, &lt;&lt;21.1&gt;&gt;, 22.7, 22.8, &lt;&lt;25.3&gt;&gt;, 27.2, 27.4, 28.3]\n</code></pre></p> \\[ Q_1 = 14.3^\\circ C \\qquad Q_2 = 20.95^\\circ C \\qquad Q_3 = 25.3^\\circ C \\] Code <pre><code>from scipy import stats\n#https://docs.scipy.org/doc/scipy/reference/stats.html\n\nprint('Quantile: ' + str(stats.mstats.mquantiles(Temp, prob=[0.25, 0.5, 0.75])))\n</code></pre>"},{"location":"statistics/univariate/CentralTend/#five-number-sumary-boxplot","title":"Five Number Sumary &amp; Boxplot","text":"<p>A single metric alone is not sufficient to fully characterize a distribution. The Five Number Summary is a useful approach to gain a deeper understanding of a distribution by combining multiple key metrics, offering a more comprehensive view of the data.</p> Definition: Five Number Summary <p>The Five Number Summary consists of the following metrics in this exact order:</p> <p>[Minimum, \\(Q_1\\), Median, \\(Q_3\\), Maximum]</p> <p>A boxplot is a visualization of a distribution and provides a graphical representation of the Five Number Summary. </p> <pre><code>import plotly.express as px\n\nfig = px.box([1,2,1,2,3,4,1,100,1,2,2])\nfig.show()\n</code></pre> <p>The first and third quartiles (\\(Q_1\\) and \\(Q_3\\)) mark the ends of the box, while the interquartile range (IQR) represents the length of the box. The median is depicted as a line within the box. Two lines, known as whiskers, extend from the box to the smallest and largest observations, provided these values are no more than \\(1.5\\times\\)IQR above \\(Q_3\\) or below \\(Q_1\\). Outliers are marked separately.</p> Example: Boxplot of the Temperature <ul> <li> <p>Without Outlier</p> <p></p> Code <pre><code>import plotly.express as px\nimport pandas as pd\n\nTemp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n\n# Generate Box Plot\nfig = px.box(pd.DataFrame(Temp))\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text=' ',\n    yaxis_title_text='Temperature',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Boxplot - no Outlier&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;variable: Temperature&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>With Outlier</p> <p></p> Code <pre><code>import plotly.express as px\nimport pandas as pd\n\nTemp2 = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8, 50, 60]\n\n# Generate Box Plot\nfig = px.box(pd.DataFrame(Temp2))\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text=' ',\n    yaxis_title_text='Temperature',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Boxplot - Outlier&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;variable: Temperature&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> </ul>"},{"location":"statistics/univariate/CentralTend/#recap","title":"Recap","text":"<ul> <li>Measures of central tendency characterize a distribution by using an average value.</li> <li>Common metrics include the mean, median, mode, and quantiles.</li> <li>Individual metrics alone are not sufficient to provide a full understanding.</li> <li>The Five Number Summary combines multiple values to offer a more comprehensive characterization of the distribution.</li> <li>The boxplot is the graphical representation of the Five Number Summary.</li> <li>However, the informative value of the Five Number Summary is limited.</li> <li>It can be misleading to describe a distribution solely through measures of central tendency. </li> </ul> <p>The following distributions have the same mean, median and mode</p> <ul> <li> <p>Without Outlier</p> <p></p> </li> <li> <p>With Outlier</p> <p></p> </li> </ul> Code <pre><code>a = [2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4]\nb = [1,1,1,1,1,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,4,5,5,5,5,5,5,5,5]\n\nimport plotly.express as px\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\n# Distribution a\n# Generate Histogram\nfig = px.histogram(pd.DataFrame(a))\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Value',\n    yaxis_title_text='Absolute Frequency',\n    showlegend=False,\n)\n\n# Show the plot\nfig.show()\n\n# Calculate Measures\nprint('Mean: ' + str(np.mean(a)))\nprint('Median: ' + str(np.median(a)))\nprint('Mode: ' + str(stats.mode(a)))\n\n# Distribution b\n# Generate Histogram\nfig = px.histogram(pd.DataFrame(b))\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Value',\n    yaxis_title_text='Absolute Frequency',\n    showlegend=False,\n)\n\n# Show the plot\nfig.show()\n\n# Calculate Measures\nprint('Mean: ' + str(np.mean(b)))\nprint('Median: ' + str(np.median(b)))\nprint('Mode: ' + str(stats.mode(b)))\n</code></pre>"},{"location":"statistics/univariate/CentralTend/#tasks","title":"Tasks","text":"Task: Measures of Central Tendency <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ncars = fetch_ucirepo(id=9) \n# https://archive.ics.uci.edu/dataset/9/auto+mpg\n\n# data (as pandas dataframes) \ndata = cars.data.features\ndata = data.join(cars.data.ids)\n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following task: </p> <ol> <li>Calculate all useful and meaningful measure of central tendency (mean, median, mode) for the following attributes (think about attribute types)<ul> <li><code>car_name</code></li> <li><code>origin</code></li> <li><code>displacement</code></li> </ul> </li> <li>Calculate the quartiles and plot a Boxplot fo the attribute <code>acceleration</code></li> </ol>"},{"location":"statistics/univariate/Dispersion/","title":"Measure of Dispersion","text":"<p>Measures of dispersion characterize a distribution by indicating how data is spread around an average value. These metrics describe the variability or heterogeneity of the data.</p> Info <p>Some formulas differ between samples and populations (e.g., variance), which may result in slight variations in the calculations. </p> <p> </p>"},{"location":"statistics/univariate/Dispersion/#range","title":"Range","text":"<p>The range, denoted as \\( R \\), is the difference between the largest and smallest value in a dataset. However, in the presence of extremely large or small outliers, the range can provide a distorted view of the data's variability.</p> <pre><code>import numpy as np\nnp.ptp([1,2,1,2,3,4,1,100,1,2,2])\n</code></pre> &gt;&gt;&gt; Output<pre><code>99\n</code></pre> Defintion: Range \\[ R = \\text{max}(X)-\\text{min}(X) \\] <p>with \\( x_1, x_2, \\dots, x_N \\) representing a set of \\( N \\) values of a metric variable \\( X \\). </p> Example: Range of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the range of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution </p> \\[ R = 28.3-9.6 = 18.7 \\] <p>The temperature range is \\(18.7^\\circ C\\).</p> Code <pre><code>Temp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n\nimport numpy as np\nprint('Range:', np.ptp(Temp))\n</code></pre>"},{"location":"statistics/univariate/Dispersion/#interquartile-range","title":"Interquartile Range","text":"<p>The interquartile range (IQR) is the difference between the third and first quartile. </p> <pre><code>from scipy import stats\nstats.iqr([1,2,1,2,3,4,1,100,1,2,2], interpolation = 'nearest')\n</code></pre> &gt;&gt;&gt; Output<pre><code>2\n</code></pre> <p>It describes the spread of the middle 50% of the data, providing a measure of variability that is less sensitive to outliers</p> Definition: Interquartile Range \\[ IQR = Q_3-Q_1 \\] <p>Q1 and Q3 are the first and third quartiles of a dataset with \\( N \\) values of a variable \\( X \\).</p> Example: IQR of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the IQR of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution <pre><code>[9.6, 11.9, 13.8, &lt;&lt;14.3&gt;&gt;, 19.8, 19.8, &lt;&lt;20.8&gt;&gt;, &lt;&lt;21.1&gt;&gt;, 22.7, 22.8, &lt;&lt;25.3&gt;&gt;, 27.2, 27.4, 28.3]\n</code></pre></p> \\[ $Q_1 = 14.3^\\circ C \\qquad Q_2 = 20.95^\\circ C \\qquad Q_3 = 25.3^\\circ C $ \\] \\[ IQR = 25.3 - 14.3 = 11^\\circ C \\] <p>The IQR of the temperature is \\(11^\\circ C\\).</p> Code <pre><code>Temp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n\nfrom scipy import stats\nprint('IQR: ', stats.iqr(Temp, interpolation = 'nearest'))\n</code></pre>"},{"location":"statistics/univariate/Dispersion/#variance","title":"Variance","text":"<p>Variance \\( \\sigma^2 \\) is the mean of the squared deviations from the average. It indicates how spread out a distribution is.</p> <pre><code>import statistics \nprint('Variance: ', statistics.variance([1,2,1,2,3,4,1,100,1,2,2]))\nprint('Population Variance: ', statistics.pvariance([1,2,1,2,3,4,1,100,1,2,2]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>Variance:  875.76\nPopulation Variance:  796.15\n</code></pre> Definition: Variance \\[ \\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N}(x_i-\\bar{x})^2 \\] <p>with \\( x_1, x_2, \\dots, x_N \\) representing a set of \\( N \\) values of a metric variable \\( X \\). This formula applies to the entire population. For samples, it differs slightly, as the division is by \\( N - 1 \\) instead of \\( N \\).</p> Example: Variance of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the variance of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution </p> \\[ \\sigma^2 = \\frac{463.7}{14} = 33.12 \\] <p>The variance of the temperature is \\(33.12^\\circ C^2\\).</p> Code <pre><code>Temp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n\nimport statistics \nprint('Variance: ', statistics.variance(Temp))\nprint('Population Variance: ', statistics.pvariance(Temp))\n</code></pre>"},{"location":"statistics/univariate/Dispersion/#standard-deviation","title":"Standard Deviation","text":"<p>The standard deviation \\( \\sigma \\) describes a \"typical\" deviation from the mean. It indicates how spread out a distribution is.</p> <pre><code>import statistics \nprint('Standard Deviation: ', statistics.stdev([1,2,1,2,3,4,1,100,1,2,2]))\nprint('Population Standard Deviation: ', statistics.pstdev([1,2,1,2,3,4,1,100,1,2,2]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>Standard Deviation:  29.59\nPopulation Standard Deviation:  28.22\n</code></pre> <p>A small \\( \\sigma \\) suggests that the data tends to be close to the mean, while a large \\( \\sigma \\) indicates that the data is spread over a wide range of values.</p> Definition: Standard Deviation \\[ \\sigma=\\sqrt{\\sigma^2} \\] <p>With \\( x_1, x_2, \\dots, x_N \\) representing a set of \\( N \\) values of a metric variable \\( X \\), and \\( \\sigma^2 \\) being the corresponding variance.</p> Example: StD of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the standard deviation of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution </p> \\[ \\sigma^2 = \\frac{463.7}{14} = 33.12 \\] \\[ \\sigma = \\sqrt{\\sigma^2} = \\sqrt{33.12} = 5.76 \\] <p>The temperature values deviate, on average, by \\(5.76^\\circ C\\) from the mean.</p> Code <pre><code>Temp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n\nimport statistics \nprint('Standard Deviation: ', statistics.stdev(Temp))\nprint('Population Standard Deviation: ', statistics.pstdev(Temp))\n</code></pre>"},{"location":"statistics/univariate/Dispersion/#coefficient-of-variation","title":"Coefficient of Variation","text":"<p>We previously encountered the issue that variance and standard deviations of different data series were difficult to compare. The coefficient of variation (\\(c_v\\)) can be used to solve this problem. It is often referred to as the relative standard deviation.</p> <pre><code>stats.variation([1,2,1,2,3,4,1,100,1,2,2])\n</code></pre> &gt;&gt;&gt; Output<pre><code>2.61\n</code></pre> Definition: Coefficient of Variation \\[ c_v = \\frac{\\sigma}{\\bar{x}} \\] <p>With \\( x_1, x_2, \\dots, x_N \\) representing a set of \\( N \\) values of a metric variable \\( X \\), \\( \\sigma \\) being the corresponding standard deviation, and \\( \\bar{x} \\) the mean.</p> Example: Pizza Prices <p>You are given a table of pizza prices in New York listed in various currencies. <pre><code>dollar = [1, 2, 3, 3, 5, 6, 7, 8, 9, 11]\nPesos = [18.81, 37.62, 56.43, 56.43, 94.05, 112.86, 131.67, 150.48, 169.29]\n</code></pre></p> \\[ \\begin{eqnarray*}     \\sigma_{Dollar} = 3.1 &amp;\\qquad &amp; \\sigma_{Pesos} = 58.43\\\\     \\bar{x}_{Dollar} = 5.5 &amp;\\qquad &amp; \\bar{x}_{Pesos} = 103.46\\\\ \\end{eqnarray*} \\] <p>The goal is to calculate the coefficient of variation for both data series.</p> <p>Solution </p> \\[ \\begin{eqnarray*}     c_{v,dollar} = \\frac{3.1}{5.5} = 0.56 &amp;\\qquad&amp; c_{v,pesos} = \\frac{58.43}{103.46} = 0.56 \\end{eqnarray*} \\] Code <pre><code>Dollar = [1,2,3,3,5,6,7,8,9,11]\nPesos = [x * 18.81 for x in Dollar]\n\nfrom scipy import stats\nprint('CoV Dollar: ' + str(stats.variation(Dollar)))\nprint('CoV Pesos: ' + str(stats.variation(Pesos)))\n</code></pre>"},{"location":"statistics/univariate/Dispersion/#recap","title":"Recap","text":"<ul> <li>Measures of dispersion characterize a distribution by describing how data is spread around a central value.</li> <li>The IQR represents the middle 50% of the data.</li> <li>Variance indicates how wide a distribution is.</li> <li>Interpreting variance can be challenging because its units are squared.</li> <li>For this reason, standard deviation is a more suitable measure for interpretation.</li> <li>To better compare standard deviations across datasets, the coefficient of variation is used.</li> <li>There are different formulas for variance depending on whether the entire population or a sample is being analyzed</li> </ul>"},{"location":"statistics/univariate/Dispersion/#tasks","title":"Tasks","text":"Task: Measures of Dispersion <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ncars = fetch_ucirepo(id=9) \n# https://archive.ics.uci.edu/dataset/9/auto+mpg\n\n# data (as pandas dataframes) \ndata = cars.data.features\ndata = data.join(cars.data.ids)\n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following task: </p> <ol> <li>For the attribute <code>acceleration</code> calculate the following measures (use the sample formula - not population):<ul> <li>Range</li> <li>IQR --&gt; compare to the boxplot from the section Measures of Central Tendency</li> <li>Variance</li> <li>Standard Deviation</li> <li>CV</li> </ul> </li> </ol> Task: Weight of Euro Coins <p>  Download the following dataset from this page and load it into your notebook.</p> <pre><code># Website: https://jse.amstat.org/v14n2/datasets.aerts.html\n# Dataset: https://jse.amstat.org/datasets/euroweight.dat.txt\n# Description: https://jse.amstat.org/datasets/euroweight.txt\n\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\ndata = pd.read_csv('Daten/euroweight.dat.txt', sep='\\t', header=None, index_col=0, names=['Weight', 'Batch'])\n\n# Display the first few rows\ndata.head()\n</code></pre> <p>As the Head of Quality Control at the European Central Bank (ECB), you are responsible, among other duties, for the quality management of 1-Euro coins. Consequently, you have tasked an employee with selecting a random sample of 2,000 coins. (Dataset: 'euro.csv', Unit: grams)</p> <ol> <li>Calculate the average weight of the coins.</li> <li>Determine the corresponding standard deviation and interpret its significance.</li> <li>Create a histogram. Ensure that all axes are labeled and the chart is properly titled.</li> </ol>"},{"location":"statistics/univariate/Frequency/","title":"Frequency Distribution","text":"<p>A list \\( X \\) consists of \\( n \\) elements \\( x_1, \\dots, x_n \\). Within this list, \\( X \\) contains \\( k \\) distinct values (\\( a_1, \\dots, a_k \\)). The frequency refers to how often a specific value \\( a_k \\) appears in \\( X \\). </p> <pre><code>drinks = ['small', 'small', 'small', 'medium', 'medium', 'medium', 'large']\n</code></pre> <p>In this example, </p> <ul> <li>\\( X \\): <code>drinks</code></li> <li>\\( n \\): <code>7</code></li> <li>\\( x_1, \\dots, x_n \\): <code>['small', 'small', 'small', 'medium', 'medium', 'medium', 'large']</code></li> <li>\\( k \\): 3</li> <li>\\( a_1, \\dots, a_k \\): <code>['small', 'medium', 'large']</code></li> </ul> <p>In the case of a nominal scale, \\( k \\) is equal to the number of categories, with \\( k \\) typically much smaller than \\( n \\). For a metric scale, there are often only a few identical values, meaning \\( k \\) is approximately equal to \\( n \\).</p> <p>The representation of frequencies can be done in the form of a table or a graphical format. When a frequency distribution is depicted as a bar chart, it is referred to as a histogram. </p> <pre><code>import plotly.express as px\n\ndf = px.data.tips()\nfig = px.histogram(df, x=\"total_bill\")\nfig.show()\n</code></pre> <p>It is important that the data remains the focal point and is presented as accurately and objectively as possible, avoiding distortions such as 3D effects or shadows. Titles, axis labels, legends, the data source, and the time of data collection should always be clearly indicated.</p> Definition: Frequency <p>Absolute Frequency of the value \\( a_j \\)</p> \\[ h(a_j) = h_j \\] <p>Relative Frequency of the value \\( a_j \\)</p> \\[ f(a_j) = f_j = \\frac{h_j}{n} \\] <p>Absolute Frequency Distribution: \\( h_1, \\dots, h_k \\)</p> <p>Relative Frequency Distribution: \\( f_1, \\dots, f_k \\)</p> Code <p>For the upcoming analysis, the following data will be used:      <pre><code># Import Libraries\nimport pandas as pd\n\n# Import Data\ndata = pd.read_csv('https://raw.githubusercontent.com/JeffSackmann/tennis_atp/master/atp_matches_2023.csv')\n</code></pre></p>"},{"location":"statistics/univariate/Frequency/#nominal-scale","title":"Nominal Scale","text":"<p>For nominally scaled variables, the values correspond to the possible categories. The internal order of these categories is not relevant in the substantive analysis.</p> Example: Graphical Representation of Nominal Variables <ul> <li> <p></p> Code <pre><code>import plotly.express as px\n\n# Generate Histogram\nfig = px.histogram(\n    data, \n    x=\"surface\",\n)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Surface',\n    yaxis_title_text='Absolute Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Nominal Variable: Histogram&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: surface&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p></p> Code <pre><code>import plotly.express as px\n\n# Generate Pie Chart\nfig = px.pie(\n    data,\n    names=\"surface\",\n)\n\n# Adjust the plot\nfig.update_layout(\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Nominal Variable: Pie Chart&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: surface&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\nfig.update_traces(textposition='outside', textinfo='percent+label')\n\n# Show the plot\nfig.show()\n</code></pre> </li> </ul>"},{"location":"statistics/univariate/Frequency/#ordinal-scale","title":"Ordinal Scale","text":"<p>For ordinally scaled variables, the values also correspond to the possible categories. However, the internal order of these categories is relevant in the substantive analysis. The values should always be presented in either ascending or descending order. In order to tell <code>Python</code> the correct order, we need to define it first</p> <pre><code>round_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']\n</code></pre> <p>Afterwards we can use this order in the histogram <pre><code>fig = px.histogram(\n                data, \n                x=\"round\",\n                category_orders={\"round\": round_order}\n                )\n</code></pre></p> <p>and for the calculation of the corsstable <pre><code>data['round'] = pd.Categorical(data['round'], categories=round_order, ordered=True)\n</code></pre></p> Example: Graphical Representation of Ordinal Variables <ul> <li> <p>Histogram WITHOUT Order</p> <p></p> Code <pre><code>import plotly.express as px\n\n# Generate Histogram\nfig = px.histogram(\n    data, \n    x=\"round\",\n)\n# Adjust the plot\nfig.update_layout(\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Ordinal Variable: NO Order&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: round&lt;/span&gt;&lt;/b&gt;',\n        ),\n    xaxis_title_text='Round',\n    yaxis_title_text='Absolute Frequency',\n)\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Histogram WITH Order</p> <p></p> Code <pre><code>import plotly.express as px\n\n# Define the order of the ordinal variable\ndata_ord = data.copy()\nround_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']  # Define order of the rounds\n\n# HISTOGRAM sorted\n# Generate Histogram\nfig = px.histogram(\n    data_ord, \n    x=\"round\",\n    category_orders={\"round\": round_order[::-1]},\n)\n\n# Adjust the plot\nfig.update_layout(\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Ordinal Variable: WITH Order&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: round&lt;/span&gt;&lt;/b&gt;',\n        ),\n    xaxis_title_text='Round',\n    yaxis_title_text='Absolute Frequency',\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Table WITHOUT Order</p> round Absoulte Frequency Relative Frequency [%] F 68 2.278 QF 256 8.57 R128 416 13.93 R16 512 17.15 R32 880 29.47 R64 432 14.47 RR 286 9.58 SF 136 4.55 Code <pre><code># FREQUENCY TABLE unsorted\n# Generate table with absolute and relative frequencies\nabsolutefrequency_ord_unsort = pd.crosstab(index=data['round'], columns='Absoulte Frequency')\nrelativefrequency_ord_unsort = pd.crosstab(index=data['round'], columns='Relative Frequency [%]',normalize=True)*100\n\n# Combine the tables\nfrequencytable_ord_unsort = pd.concat([absolutefrequency_ord_unsort, relativefrequency_ord_unsort], axis=1).reset_index()\nfrequencytable_ord_unsort.columns.name = None\n\n# Show table\nprint(frequencytable_ord_unsort)\n</code></pre> </li> <li> <p>Table WITH Order</p> Round Absoulte Frequency Relative Frequency [%] F 68 2.28 SF 136 4.55 QF 256 8.57 R16 512 17.15 R32 880 29.47 R64 432 14.47 R128 416 13.93 RR 286 9.58 Code <pre><code># FREQUENCY TABLE sorte\nround_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']  # Define order of the roundsd\ndata['round'] = pd.Categorical(data['round'], categories=round_order, ordered=True)  # Define order of the rounds\n\n# Generate table with absolute and relative frequencies\nabsolutefrequency_ord_sort = pd.crosstab(index=data['round'], columns='Absoulte Frequency')\nrelativefrequency_ord_sort = pd.crosstab(index=data['round'], columns='Relative Frequency [%]',normalize=True)*100\n\n# Combine the tables\nfrequencytable_ord_sort = pd.concat([absolutefrequency_ord_sort, relativefrequency_ord_sort], axis=1).reset_index()\nfrequencytable_ord_sort.columns.name = None\n\n# Show table\nprint(frequencytable_ord_sort)\n</code></pre> </li> </ul> <p>In the case of ordinally scaled variables, a cumulative absolute or relative frequency can also be calculated. The cumulative absolute frequency indicates how often a reference value (or category) has not been exceeded. The cumulative relative frequency is this number divided by the total number of observations.</p> <p>To calculate the cumulative relative frequency in the histogram, we need to add the lines:  <pre><code>fig = px.histogram(\n                data, \n                x=\"round\",\n                category_orders={\"round\": round_order[::-1]},\n                cumulative=True,\n                histnorm=\"percent\"\n                )\n</code></pre> To do the same for the crosstab we need to add:</p> <pre><code>freq_rel_cum = pd.crosstab(\n                index=data['round'], \n                columns='Relative Frequency',\n                normalize=True\n                ).cumsum()\n</code></pre> Example: Cumulative Frequency of Ordinal Variables <ul> <li> <p>Histogram (Abolute, Cumulative)</p> <p></p> Code <pre><code>import plotly.express as px\n\n# HISTOGRAM sorted Cumulative Absolute\nround_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']  # Define order of the roundsd\ndata['round'] = pd.Categorical(data['round'], categories=round_order, ordered=True)  # Define order of the rounds\n\n# Generate Histogram\nfig = px.histogram(\n    data, \n    x=\"round\",\n    category_orders={\"round\": round_order[::-1]},\n    cumulative=True,\n)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Round',\n    yaxis_title_text='Absolute Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Ordinal Variable: Cumulated&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: round&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Histogram (Relative, Cumulative)</p> <p></p> Code <pre><code>import plotly.express as px\n# HISTOGRAM sorted cumulated Relative\nround_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']  # Define order of the roundsd\ndata['round'] = pd.Categorical(data['round'], categories=round_order, ordered=True)  # Define order of the rounds\n\n# Generate Histogram\nfig = px.histogram(\n    data, \n    x=\"round\",\n    category_orders={\"round\": round_order[::-1]},\n    cumulative=True,\n    histnorm=\"percent\"\n)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Round',\n    yaxis_title_text='Relative Frequency [%]',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Ordinal Variable: Cumulated&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: round&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Table (Abolute, Cumulative)</p> Round Absoulte Frequency Absoulte Frequency Cumulated F 68 68 SF 136 204 QF 256 460 R16 512 972 R32 880 1852 R64 432 2284 R128 416 2700 RR 286 2986 Code <pre><code># FREQUENCY TABLE sorted cumulated absolute\nround_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']  # Define order of the roundsd\ndata['round'] = pd.Categorical(data['round'], categories=round_order, ordered=True)  # Define order of the rounds\n\n# Generate table with absolute and relative frequencies\nabsolutefrequency_ord_sort = pd.crosstab(index=data['round'], columns='Absoulte Frequency')\nrelativefrequency_ord_sort = pd.crosstab(index=data['round'], columns='Relative Frequency [%]',normalize=True)*100\n\n# Combine the tables\nfrequencytable_ord_sort = pd.concat([absolutefrequency_ord_sort, relativefrequency_ord_sort], axis=1).reset_index()\nfrequencytable_ord_sort.columns.name = None\n\nfrequencytable_ord_sort['Absoulte Frequency Cumulated'] = frequencytable_ord_sort['Absoulte Frequency'].cumsum()\nfrequencytable_ord_sort.drop(columns='Relative Frequency [%]', inplace=True)\nprint(frequencytable_ord_sort)\n</code></pre> </li> <li> <p>Table (Relative, Cumulative)</p> Round Relative Frequency [%] Relative Frequency Cumulated F 2.28 2.28 SF 4.55 6.83 QF 8.57 15.41 R16 17.15 32.55 R32 29.47 62.02 R64 14.47 76.49 R128 13.93 90.42 RR 9.58 100 Code <pre><code># FREQUENCY TABLE sorted cumulated relative\nround_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']  # Define order of the roundsd\ndata['round'] = pd.Categorical(data['round'], categories=round_order, ordered=True)  # Define order of the rounds\n\n# Generate table with absolute and relative frequencies\nabsolutefrequency_ord_sort = pd.crosstab(index=data['round'], columns='Absoulte Frequency')\nrelativefrequency_ord_sort = pd.crosstab(index=data['round'], columns='Relative Frequency [%]',normalize=True)*100\n\n# Combine the tables\nfrequencytable_ord_sort = pd.concat([absolutefrequency_ord_sort, relativefrequency_ord_sort], axis=1).reset_index()\nfrequencytable_ord_sort.columns.name = None\n\nfrequencytable_ord_sort['Relative Frequency Cumulated '] = frequencytable_ord_sort['Relative Frequency [%]'].cumsum()\nfrequencytable_ord_sort.drop(columns='Absoulte Frequency', inplace=True)\nprint(frequencytable_ord_sort)\n</code></pre> </li> </ul>"},{"location":"statistics/univariate/Frequency/#numeric-scale","title":"Numeric Scale","text":"<p>When the number of values \\( k \\) for a metrically scaled variable is small, it can be presented in the same way as an ordinal scale. However, when \\( k \\) is large, the representation can become cluttered and lose clarity.</p> Example: Few and Many Numeric Values <ul> <li> <p>Numeric Variable with Few of Values</p> <p></p> Code <pre><code>import plotly.express as px\n# HISTOGRAM Small Number of Values\n# Generate Histogram\nfig = px.histogram(\n    data, \n    x=\"draw_size\",\n)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Draw Size',\n    yaxis_title_text='Absolute Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Small Number of Values&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: draw_size&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Numeric Variable with Many Values</p> <p></p> Code <pre><code>import plotly.express as px\n# HISTOGRAM Large Number of Values\n# Generate Histogram\nfig = px.bar(\n    data['winner_rank_points'].value_counts().reset_index(), \n    x='winner_rank_points', \n    y='count'\n    )\n\n# Adjust the width of the bars\nfig.update_traces(width=50)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Winner Rank Points',\n    yaxis_title_text='Absolute Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Large Number of Values&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: winner_rank_points&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> </ul> <p>In such cases, categories (intervals or bins) should be created to reduce the number of displayed values, making the data easier to interpret. This can be done automatically e.g. by <code>px.histogram()</code> or manually:</p> <pre><code>data['points_cat'] = pd.cut(\n                        data['winner_rank_points'], \n                        bins=range(0,int(data['winner_rank_points'].max()),100), \n                        right=False)\n</code></pre> Example: Numeric Attribute Binning <ul> <li> <p>Automatic Binning</p> <p></p> Code <pre><code>import plotly.express as px\n\n# HISTOGRAM Large Number of Values\n# Generate Histogram\nfig = px.histogram(\n    data, \n    x=\"winner_rank_points\",\n)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Winner Rank Points',\n    yaxis_title_text='Absolute Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Automatic Binning&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: winner_rank_points&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Manual Binning</p> <p></p> Code <pre><code>import plotly.express as px\n# Binning of the Data\ndata['points_cat'] = pd.cut(data['winner_rank_points'], bins=range(0,int(data['winner_rank_points'].max()),100), right=False) # 100 Bins between 0 and the maximum value of winner_rank_points\n# data_num['points_cat'] = pd.cut(data_num['winner_rank_points'], bins=[0, 60, 120, 180, 240, 300, 360, 420, 480, 540, 600, 660, 720, 780, 840, 900, 960, 1020, 1080, 1140, 1200], right=False) # Custom Bins\n\n# Count the values in each bin\npoints_cat_count = data['points_cat'].value_counts().sort_index()\npoints_cat_count.index = points_cat_count.index.astype(str)\n\n# Generate Bar Chart\nfig = px.bar(\n    points_cat_count,\n    )\n\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Winner Rank Points',\n    yaxis_title_text='Absolute Frequency',\n    showlegend=False,\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Manual Binning&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: winner_rank_points&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> </ul> <p>Tables and charts are well-suited for providing an overview of the data. However, in some cases, it is beneficial to further condense the information within the data to reduce complexity. Nevertheless, care must be taken not to oversimplify, as this could lead to misleading interpretations. There are several key metrics available for further reducing complexity. These are typically divided into measures of central tendency and measures of dispersion.</p>"},{"location":"statistics/univariate/Frequency/#recap","title":"Recap","text":"<ul> <li>Data should always be the focus, with an unbiased representation.</li> <li>Frequencies indicate how often a particular value occurs.</li> <li>Relative frequency (%) is the absolute frequency divided by the total number of observations.</li> <li>The form of representation depends on the scale level of the variable.</li> <li>In general, both tables and charts can be used.</li> <li>Cumulative frequencies show how often a reference value has not been exceeded.</li> </ul>"},{"location":"statistics/univariate/Frequency/#tasks","title":"Tasks","text":"Task: Frequency Distribution <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ncars = fetch_ucirepo(id=9) \n# https://archive.ics.uci.edu/dataset/9/auto+mpg\n\n# data (as pandas dataframes) \ndata = cars.data.features\ndata = data.join(cars.data.ids)\n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following task: </p> <ol> <li>Analyse the Dataset<ul> <li>Look at the website of the dataset and get familiar </li> </ul> </li> <li>Generate the following plot (think about attribute types, title, labeling of the axes)<ul> <li>Histogram | Absolute Frequency | Variable: <code>origin</code></li> <li>Bar Chart | Absoulte Frequency | no binning | Variable: <code>weight</code></li> <li>Histogram | Absoulte Frequency | automatic binning | Variable: <code>weight</code> </li> <li>Histogram | Relative Frequency | cumulated | Variable: <code>hoursepower</code></li> <li>Pie Chart | Relative Frequency | Variable: <code>cylinders</code></li> </ul> </li> </ol>"},{"location":"yolo/","title":"Introduction","text":"Under Construction <p>The <code>Computer Vision</code> section is currently under construction.  </p> <p>Please check back later for updates.</p> <p></p> Project Setup <p>This chapter serves as an introduction to the topic of computer vision. We'll explore various tasks, demonstrating their use with code snippets. Even though this is just an introductory chapter and you might not grasp all the details yet, we encourage you to run the code on your own computer.</p> <p>To follow along, we recommend setting up a new project folder with a Jupyter notebook. Additionally, create a new virtual environment  and activate it. Install the required packages:</p> <pre><code>pip install ultralytics opencv-python pytesseract face-recognition\n</code></pre> <p>Your project structure should look like this:</p> <pre><code>\ud83d\udcc1 vision_intro/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc4 input.jpg\n\u2514\u2500\u2500 \ud83d\udcc4 cv_intro.ipynb\n</code></pre> <p>Computer Vision is a field of artificial intelligence that enables machines to interpret and understand the visual world. By using digital images from cameras and videos along with deep learning models, machines can accurately identify and classify objects - and then react to what they \"see.\"</p> <p>In this introduction, we'll delve into the basics of computer vision, its challenges, and how it's interconnected with other fields. Let's embark on this visual journey together!</p>"},{"location":"yolo/#what-is-computer-vision","title":"What Is Computer Vision?","text":"<p>Before diving into computer vision, let's briefly touch upon artificial intelligence (AI). AI is a broad field aiming to create systems capable of performing tasks that typically require human intelligence. As one of the pioneers of AI, John McCarthy, described it:</p> <p>\"An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves.\"</p> <p>-- John McCarthy</p> <p>Artificial Intelligence is a multidisciplinary field divided into several subfields, each contributing to simulating intelligent behavior in machines. These include:</p> <ul> <li>Machine Learning</li> <li>Natural Language Processing</li> <li>Robotics</li> <li>Computer Graphics</li> <li>Computer Vision</li> </ul> <p>These subfields are interconnected; advancements in one often benefit the others. For instance, computer vision is essential in robotics for environment perception and in natural language processing for image captioning.</p> <p>But now we still want to know: What is computer vision exactly?</p> <p>At its core, computer vision seeks to automate tasks that the human visual system can do. It involves techniques for acquiring, processing, analyzing, and understanding images to produce numerical or symbolic information.</p> Biological Vision (Source: Ai   Miquel Perello Nieto on Wikipedia)  Interesting Fact <p>Did you know, that over 50% of the processing in the human brain is devoted directly or indirectly to visual information (Source: MIT News)</p> <p>In other words, computer vision transforms visual data into meaningful information. Now, let's explore some typical computer vision  tasks and see how they come to life through examples you can try yourself!</p>"},{"location":"yolo/#typical-computer-vision-tasks","title":"Typical Computer Vision Tasks","text":""},{"location":"yolo/#classification","title":"Classification","text":"<p>Assigning objects within an image to predefined categories or classes.</p> Example: Classification <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> &gt;&gt;&gt; Output<pre><code>1 person, 1 dog\n</code></pre> </li> </ul> Code <pre><code>from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load an official model\n\n# Predict with the model\nresults = model(\"dog.jpg\")  # predict on an image\n</code></pre>"},{"location":"yolo/#localization","title":"Localization","text":"<p>Determining the exact location of an object within an image.</p>"},{"location":"yolo/#detection","title":"Detection","text":"<p>Identifying and locating multiple objects within an image, effectively combining classification and localization.</p> Example: Detection <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load an official model\n\n# Predict with the model\nresults = model(\"dog.jpg\", save=True)  # predict on an image\nresults[0].show()  # display the image\n</code></pre>"},{"location":"yolo/#segmentation","title":"Segmentation","text":"<p>Precisely delineating the pixels that belong to an object, separating it from the background.</p> Example: Segmentation <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-seg.pt\")  # load an official model\n\n# Predict with the model\nresults = model(\"dog.jpg\", save=True)  # predict on an image\nresults[0].show()  # display the image\n</code></pre>"},{"location":"yolo/#tracking","title":"Tracking","text":"<p>Monitoring the movement of objects over time in videos or live streams, analyzing factors like velocity and relative position.</p> Example: Tracking <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>from collections import defaultdict\nimport cv2\nimport numpy as np\n\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Open the video file\nvideo_path = \"street2.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Store the track history\ntrack_history = defaultdict(lambda: [])\n\nvideo = cv2.VideoWriter(\"output.mp4\", 0, 25, (960,540))\n\n# Loop through the video frames\nwhile cap.isOpened():\n    # Read a frame from the video\n    success, frame = cap.read()\n\n    if success:\n        # Run YOLO11 tracking on the frame, persisting tracks between frames\n        results = model.track(frame, persist=True, classes=[2])\n\n        # Get the boxes and track IDs\n        boxes = results[0].boxes.xywh.cpu()\n        track_ids = results[0].boxes.id.int().cpu().tolist()\n\n        # Visualize the results on the frame\n        annotated_frame = results[0].plot()\n\n        # Plot the tracks\n        for box, track_id in zip(boxes, track_ids):\n            x, y, w, h = box\n            track = track_history[track_id]\n            track.append((float(x), float(y)))  # x, y center point\n            if len(track) &gt; 30:  # retain 90 tracks for 90 frames\n                track.pop(0)\n\n            # Draw the tracking lines\n            points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n            cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n\n        # Display the annotated frame\n        cv2.imshow(\"YOLO11 Tracking\", annotated_frame)\n        video.write(annotated_frame)\n        # Break the loop if 'q' is pressed\n        if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n            break\n    else:\n        # Break the loop if the end of the video is reached\n        break\n\n# Release the video capture object and close the display window\ncap.release()\ncv2.destroyAllWindows()\nvideo.release()\n</code></pre>"},{"location":"yolo/#optical-character-recognition","title":"Optical Character Recognition","text":"<p>Recognizing and extracting printed or handwritten text from images, enabling machines to read and process written information.</p> Example: OCR <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> &gt;&gt;&gt; Output<pre><code>LAN DOR.\n\nCHAPTER IL\n\nBIRTH AND PARENTAGE\u2014SCHOOL \u2014 COLLEGE.\n(1775 \u20141794.)\n\nFew men have ever impressed their peers so much, or the\ngeneral public so little, as Watrer Savage Lanpor. Of\nall celebrated authors, he has hitherto been one of the\nleast popular. Nevertheless he is among the most strik-\ning figures in the history of English literature ; striking\nalike by his character and his powers. Personally, Landor\nexercised the spell of genius upon every one who came\nnear him. His gifts, attainments, impetuosities, his\noriginality, his force, his charm, were all of the same\nconspicuous and imposing kind. Not to know what is\nto be known of so remarkable a man is evidently to be a\nloser. Not to be familiar with the works of so noble\n</code></pre> </li> </ul> Code <pre><code># Need to install tesseract on your PC https://www.nutrient.io/blog/how-to-use-tesseract-ocr-in-python/\nfrom PIL import Image\nimport pytesseract\n\nprint(pytesseract.image_to_string(Image.open('scan.png')))\n</code></pre>"},{"location":"yolo/#facial-recognition","title":"Facial Recognition","text":"<p>Identifying individuals based on their facial features and recognizing various facial expressions.</p> Example: Facial Recognition <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> <p> (Source: Library of Congress on Unsplash)</p> Code <pre><code># You need to install cmake on your PC first\n# https://github.com/ageitgey/face_recognition?tab=readme-ov-file\n\nimport face_recognition\nimport cv2\nimport numpy as np\n\n# Load a sample picture and learn how to recognize it.\nobama_image = face_recognition.load_image_file(\"obama.jpg\")\nobama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n\n# Load a sample picture and learn how to recognize it.\ntrump_image = face_recognition.load_image_file(\"trump.jpg\")\ntrump_face_encoding = face_recognition.face_encodings(trump_image)[0]\n\n# Load a second sample picture and learn how to recognize it.\nbiden_image = face_recognition.load_image_file(\"biden.jpg\")\nbiden_face_encoding = face_recognition.face_encodings(biden_image)[0]\n\n# Create arrays of known face encodings and their names\nknown_face_encodings = [\n    obama_face_encoding,\n    trump_face_encoding,\n    biden_face_encoding\n]\nknown_face_names = [\n    \"Barack Obama\",\n    \"Donald Trump\",\n    \"Joe Biden\"\n]\n\n# Initialize some variables\nface_locations = []\nface_encodings = []\nface_names = []\nprocess_this_frame = True\n\nrgb_small_frame = face_recognition.load_image_file(\"trump2.jpg\")\n\nface_locations = face_recognition.face_locations(rgb_small_frame)\nface_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n\nface_names = []\nfor face_encoding in face_encodings:\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n    name = \"Unknown\"\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = known_face_names[best_match_index]\n\n    face_names.append(name)\n\n# Display the results\nfor (top, right, bottom, left), name in zip(face_locations, face_names):\n\n    # Draw a box around the face\n    cv2.rectangle(rgb_small_frame, (left, top), (right, bottom), (0, 0, 255), 2)\n\n    # Draw a label with a name below the face\n    cv2.rectangle(rgb_small_frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n    font = cv2.FONT_HERSHEY_DUPLEX\n    cv2.putText(rgb_small_frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n\n# Display the resulting image\ncv2.imshow('Video', cv2.cvtColor(rgb_small_frame, cv2.COLOR_BGR2RGB))\ncv2.waitKey(0)\ncv2.destroyAllWindows()\ncv2.imwrite('trump_out.jpg', cv2.cvtColor(rgb_small_frame, cv2.COLOR_BGR2RGB))\n</code></pre>"},{"location":"yolo/#pose-estimation","title":"Pose Estimation","text":"<p>Determining the position and orientation of an object or person relative to a reference point or coordinate system.</p> Example: Pose Estimation <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load an official model\n\n# Predict with the model\nresults = model(\"dog.jpg\", save=True)  # predict on an image\nresults[0].show()  # display the image\n</code></pre> <p>These tasks represent the core of computer vision, each contributing to its wide-ranging real-world applications. From enabling machines to read and understand handwritten documents to enhancing interactive gaming experiences through accurate motion tracking, the advancements in computer vision are transforming industries and everyday life.</p> <p>And: as you can see, they're not just theoretical concepts - you can try them out yourself !</p>"},{"location":"yolo/#applications","title":"Applications","text":"<p>Computer vision has a wide range of applications across various industries.</p> Possible Applications for Computer Vision RoboticsAutonomous VehiclesMedicalQuality ControlRetailFacial Recognition <p>Robots use computer vision to navigate and interact with their environment.</p> <p> </p> <p>Self-driving cars rely heavily on computer vision to perceive the road and make driving decisions.</p> <p> </p> <p>Computer vision aids in medical imaging for diagnostics and treatment planning.</p> <p> (Source: Newe A, Ganslandt T on Wikipedia) </p> <p>Automated inspection systems detect defects in manufacturing processes.</p> <p> (Source: elunic) </p> <p>Augmented reality shopping experiences enhance customer engagement.</p> <p> (Source: SNAP INC via Forbes) </p> <p>Used for security and authentication purposes.</p> <p> (Source: Sylenius on Wikipedia) </p>"},{"location":"yolo/#how-can-machines-see","title":"How can Machines \"See\"?","text":"<p>When we look at the world, our eyes receive light reflected from objects. Similarly, cameras capture light to create images. </p>          Camera sensor prinicpal (Source: Neg)       <p>However, interpreting these images to understand the scene involves complex algorithms that can discern patterns, shapes, and colors. This process involves several steps:</p> <ol> <li>Image Acquisition: Capturing the visual data using cameras or sensors.</li> <li>Preprocessing: Enhancing image quality and correcting distortions.</li> <li>Feature Extraction: Identifying edges, textures, and other significant parts of the image.</li> <li>High-Level Processing: Recognizing objects, understanding scenes, and making decisions.</li> </ol>"},{"location":"yolo/#challenges-in-computer-vision","title":"Challenges in Computer Vision","text":"<p>Despite the advancements, computer vision faces several challenges. Let's explore them.</p> <ul> <li> <p>Inverse Problem </p> <p>One of the fundamental challenges in computer vision is the inverse problem: Reconstructing a 3D scene from a 2D image is challenging because multiple 3D scenes can produce the same 2D projection.</p> </li> <li> <p> (Source: mosso on Wikipedia)  </p> </li> <li> <p> (Source: Palazzi et al at Computer.org)  </p> </li> <li> <p>Variability Due to Viewpoint </p> <p>An object can look vastly different from various angles. For example, a car viewed from the front, side, or top presents different shapes and features, complicating recognition tasks.</p> </li> <li> <p>Deformation </p> <p>Non-rigid objects, like clothing or human bodies, can change shape, making it challenging to maintain consistent recognition.</p> </li> <li> <p> </p> </li> <li> <p> </p> </li> <li> <p>Occlusion </p> <p>Objects in images often block parts of other objects. Detecting partially visible objects requires algorithms to infer the hidden parts.</p> </li> <li> <p>Illumination </p> <p>Lighting conditions can alter the appearance of objects. An apple under bright sunlight looks different from one under indoor lighting.</p> </li> <li> <p> (Source: Flocutus)  </p> </li> <li> <p> (Source: Osi on Wikipedia)  </p> </li> <li> <p>Motion Blur </p> <p>Movement during image capture can blur images, obscuring details necessary for recognition.</p> </li> <li> <p>Optical Illusions </p> <p>Our perception can be deceived by optical illusions, where our brain interprets images differently from the actual measurements.</p> </li> <li> <p> </p> </li> <li> <p> (Source: Nizar Massouh on ResearchGate)  </p> </li> <li> <p>Intra Class Variation </p> <p>Objects within the same category can look very different.Chairs come in numerous designs\u2014armchairs, stools, recliners\u2014but they all serve the same function. Recognizing all variations as \"chairs\" is challenging for computer vision systems.</p> </li> <li> <p>Number of Categories </p> <p>There are thousands of object categories, each with its own variations. Building systems that can recognize all of them requires extensive data and sophisticated algorithms.</p> </li> <li> <p> (Source: Cees Snoek on ResearchGate)  </p> </li> </ul> <p>By understanding these challenges, you're better equipped to appreciate the complexities involved in teaching machines to see.</p> <p>Congratulations! You've taken your first steps into the world of computer vision. Feel free to experiment with the code examples provided and explore further. In the next chapters, we'll delve deeper into specific algorithms and techniques.</p> <p>See you in the next chapter! \ud83d\udc4b</p>"},{"location":"yolo/approaches/","title":"Approaches in CV","text":"Running the Code <p>In this chapter, we'll explore different approaches in computer vision. We'll delve into traditional methods and then move on to deep learning techniques. We'll provide code snippets along the way, so feel free to run them on your own machine!</p> <p>To get started, ensure you have the necessary packages installed. If you haven't already, set up a new project folder with a Jupyter notebook and activate a new virtual environment. Then, install the required packages:</p> <pre><code>pip install torch torchvision matplotlib ultralytics opencv-python scikit-image\n</code></pre> <p>Your project structure should look like this:</p> <pre><code>\ud83d\udcc1 vision_approaches/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc4 input.jpg\n\u2514\u2500\u2500 \ud83d\udcc4 cv_approaches.ipynb\n</code></pre> <p>To tackle the complex problems in computer vision, various approaches have been developed over the years. In this chapter, we'll journey from traditional methods to the cutting-edge deep learning techniques that are revolutionizing the field.</p>"},{"location":"yolo/approaches/#traditional-approaches-in-object-detection","title":"Traditional Approaches in Object Detection","text":"<p>Before the rise of deep learning, object detection relied heavily on handcrafted features and traditional machine learning techniques. Let's explore some of these foundational methods.</p>"},{"location":"yolo/approaches/#haar-cascades","title":"Haar Cascades","text":"<p>Introduced by Viola and Jones in 2001, Haar-like features were used for rapid face detection by scanning an image at multiple scales and positions. Haar Cascades work by training a model with positive and negative images, where the positive images contain the object to detect (e.g., faces), and the negative images do not. </p> Process <ul> <li>The model scans the entire image at different sizes (scales) and positions.</li> <li>It uses simple patterns (called Haar-like features) to look for areas that resemble the object.</li> <li>By combining these simple features, the model can decide whether the object is present in a particular area of the image.</li> </ul> <ul> <li> Advantage <ul> <li>Real-Time Detection: Haar Cascades offer fast detection speeds, making them suitable for real-time applications.</li> <li>Low Computational Resources: They require relatively low computational power compared to more complex algorithms.</li> </ul> </li> <li> Disadvantage <ul> <li>Limited Accuracy: They may produce a high rate of false positives and are less accurate in detecting objects under varying lighting and orientations.</li> <li>Rigid Training Process: Training Haar Cascades requires a large amount of positive and negative images, and they are not easily adaptable to new object classes without retraining.</li> </ul> </li> </ul> <p>There are already available pretrained models which we can try right away. Let's try them out!</p> Example: Haar Cascades <p> </p> Code <pre><code>#Source: https://medium.com/analytics-vidhya/haar-cascades-explained-38210e57970d\n\n#-- Load Packages\nimport cv2\nfrom skimage import data\n\n#-- Load Image and Convert to RGB\nimage = data.astronaut()\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert to RGB\n\n#-- Load Haar Cascades\nf_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\ne_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n\n#-- Detect Faces and Eyes\nfaces = f_cascade.detectMultiScale(image, 1.3, 5)\nfor (x,y,w,h) in faces:\n    img = cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n    roi_color = img[y:y+h, x:x+w]\n    eyes = e_cascade.detectMultiScale(roi_color)\n    for (ex,ey,ew,eh) in eyes:\n        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n\n#-- Display and Save Image\ncv2.imwrite('output_haar_cascade.jpg',image)\ncv2.imshow('Haar Cascade',image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"yolo/approaches/#hog","title":"HOG","text":"<p>Proposed for human detection, Histogram of Oriented Gradients (HOG) features capture edge orientations and are combined with Support Vector Machines (SVMs) for classification.</p> Process <ul> <li>Edge Direction Detection: HOG focuses on the edges within an image. It looks at the direction in which the edges or lines in the image are pointing. Think of it as analyzing the outline or shape of objects by observing where the edges go.</li> <li>Counting Edge Directions Locally: The image is divided into small regions or cells. In each of these small areas, HOG counts how many edges point in each direction. It's like keeping track of the directions of all the tiny lines within that small patch.</li> <li>Creating a Histogram: For each cell, HOG creates a histogram (a simple bar chart) that represents the number of edges pointing in various directions. This captures the local shape information.</li> <li>Combining Information: These histograms from all the cells are combined to form a detailed description of the entire image's shape and structure.</li> <li>Object Detection with SVM: This combined information is then fed into a machine learning algorithm called a Support Vector Machine (SVM). The SVM uses this data to classify the image - for example, determining whether a human is present in the image or not.</li> </ul> <p>In essence, HOG helps computers understand and recognize objects by analyzing the directions of edges in an image, much like how we might recognize a shape by its outline.</p> <ul> <li> Advantage <ul> <li>Effective Feature Representation: HOG descriptors are robust in capturing shape and appearance information, improving object detection performance.</li> <li>Invariance to Illumination and Geometric Transformations: They are relatively invariant to changes in illumination and small geometric transformations.</li> </ul> </li> <li> Disadvantage <ul> <li>Computationally Intensive: Calculating HOG features can be time-consuming, which may not be ideal for real-time applications.</li> <li>Not Deep Learning-Based: HOG relies on manual feature extraction and may not capture complex patterns as effectively as deep learning methods.</li> </ul> </li> </ul> <p>Let's see it in action.</p> Example: HOG <p> </p> Code <pre><code>#Source: https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_hog.html\n\n#-- Load Packages\nimport cv2\nfrom skimage.feature import hog\nfrom skimage import data, exposure\n\n#-- Load Image\nimage = data.astronaut()\n\n#-- Compute HOG\nfd, hog_image = hog(\n    image,\n    orientations=8,\n    pixels_per_cell=(16, 16),\n    cells_per_block=(1, 1),\n    visualize=True,\n    channel_axis=-1,\n)\n# Rescale histogram for better display\nhog_image= exposure.rescale_intensity(hog_image, in_range=(0, 10))\nhog_image = cv2.normalize(hog_image, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n\n#-- Display and Save Image\ncv2.imwrite('output_hog.jpg',hog_image)\ncv2.imshow('HOG',hog_image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre> Task: Haar Cascades &amp; HOG <p>Now it's your turn! Try running the code from above for </p> <ul> <li>Haar Cascade  </li> <li>HOG </li> </ul> <p>Can you detect faces and eyes in a different image? Replace <code>data.astronaut()</code> with your own image and see how it works!</p> <p>If you're interested in learning more about Haar Cascades and HOG, check out this comparative article</p>"},{"location":"yolo/approaches/#deep-learning-approaches","title":"Deep Learning Approaches","text":"<p>As datasets grew and computational power increased, deep learning methods began to outperform traditional techniques, especially in complex tasks like object detection.</p> Convolutional Neural Network Architecture (Source: Geeksforgeeks.org) <p>Deep learning models, particularly Convolutional Neural Networks (CNNs), automatically learn hierarchical feature representations from images, eliminating the need for manual feature extraction. In this section, we'll explore deep learning approaches to object detection, focusing on both multi-stage and one-stage detectors.</p> Info <p>Deep learning models can learn complex patterns directly from data. This ability has significantly improved the performance of object detection systems, making them more accurate and robust.</p>"},{"location":"yolo/approaches/#multi-stage-object-detection","title":"Multi-Stage Object Detection","text":"<p>Multi-stage object detection approaches the detection problem in sequential steps, typically starting with region proposals followed by classification. This methodology emerged from the need to combine localization and classification effectively. Let's explore some key models in this category.</p>"},{"location":"yolo/approaches/#r-cnn","title":"R-CNN","text":"<p>R-CNN, introduced in 2014, was a breakthrough in applying deep learning to object detection. It follows a three-step process:</p> Process <ol> <li>Region Proposal: The algorithm starts by scanning the image to find areas that might contain an object. It creates a bunch of boxes (called region proposals) that potentially surround objects in the image. Think of it as highlighting all the spots where something interesting might be.</li> <li>Feature Extraction: Each proposed region is warped to a fixed size and passed through a pre-trained CNN (often AlexNet at the time) to extract a feature vector.</li> <li>Classification: The extracted features are fed into Support Vector Machines (SVMs) to classify the presence of objects in each region. A separate regression model refines the bounding box coordinates.</li> </ol> R-CNN Architecture (Source: Geeksforgeeks.org) <ul> <li> Advantage <ul> <li>Improved Accuracy: Leveraging CNNs for feature extraction significantly enhances detection accuracy over traditional methods.</li> <li>Modular Design: Allows the use of different region proposal methods and classifiers, providing flexibility in the pipeline.</li> </ul> </li> <li> Disadvantage <ul> <li>Slow Processing Time: Each region proposal is processed individually through the CNN, leading to long training and inference times.</li> <li>High Storage Requirements: Requires a large amount of storage for caching features extracted from region proposals.</li> </ul> </li> </ul>"},{"location":"yolo/approaches/#fast-r-cnn","title":"Fast R-CNN","text":"<p>Fast R-CNN, published in 2015, addressed several inefficiencies of R-CNN while maintaining its accuracy. Key improvements include:</p> Process <ol> <li>Single CNN Pass: Instead of running the CNN thousands of times on each region proposal, Fast R-CNN processes the entire image once to create a feature map, then projects the region proposals onto this map.</li> <li>RoI Pooling: This layer transforms regions of different sizes into fixed-size feature vectors efficiently, enabling end-to-end training.</li> <li>Multi-task Learning: The network simultaneously predicts object class probabilities and bounding box coordinates, eliminating the need for separate SVM classifiers</li> </ol> Fast R-CNN Architecture (Source: Geeksforgeeks.org) <ul> <li> Advantage <ul> <li>Faster Training and Inference: Reduces computation by sharing convolutions across proposals, making it significantly faster than R-CNN.</li> <li>End-to-End Training: Simplifies the training process by allowing the entire network to be trained jointly.</li> </ul> </li> <li> Disadvantage <ul> <li>Dependency on External Proposals: Still relies on external region proposal algorithms like Selective Search, which can be slow.</li> <li>Memory Intensive: Processing large images with many proposals can consume substantial memory resources.</li> </ul> </li> </ul>"},{"location":"yolo/approaches/#faster-r-cnn","title":"Faster R-CNN","text":"<p>Faster R-CNN, also from 2015, introduced the Region Proposal Network (RPN), making the entire object detection pipeline trainable end-to-end. This architecture consists of two main networks:</p> Process <ol> <li>Region Proposal Network (RPN):<ul> <li>Slides a small network over the CNN feature map</li> <li>At each location, predicts multiple potential object regions using anchor boxes</li> <li>Outputs \"objectness\" scores and box coordinates for each proposal</li> </ul> </li> <li>Detection Network:<ul> <li>Similar to Fast R-CNN</li> <li>Uses RoI Pooling on proposals from RPN</li> <li>Outputs final classifications and refined box coordinates</li> </ul> </li> </ol> <p>Both the RPN (which proposes potential object regions) and the part of the network that classifies these regions use the same underlying data from the image. In other words, they share the same convolutional features extracted from the image. This means the heavy processing of the image is done only once, and both tasks use this shared information.</p> Faster R-CNN Architecture (Source: Geeksforgeeks.org) <ul> <li> Advantage <ul> <li>Significant Speed Improvement: Eliminates the need for external proposal algorithms, making detection faster.</li> <li>High Accuracy: Maintains high detection accuracy while improving computational efficiency.</li> </ul> </li> <li> Disadvantage <ul> <li>Complex Architecture: The addition of the RPN adds complexity, making the model harder to implement and tune.</li> <li>Hardware Requirements: May require powerful GPUs to achieve real-time performance due to computational demands.</li> </ul> </li> </ul> Example: Object Detection with Faster R-CNN <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>import torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load a pre-trained Faster R-CNN model\nmodel = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nmodel.eval()\n\n# Transform the input image\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n# Load and transform the image\nimage = Image.open('dog.jpg')\nimage_tensor = transform(image)\n\n# Perform object detection\nwith torch.no_grad():\n    outputs = model([image_tensor])\n\n# Visualize the results\nlabels = outputs[0]['labels'].numpy()\nscores = outputs[0]['scores'].detach().numpy()\nboxes = outputs[0]['boxes'].detach().numpy()\n\n# COCO dataset label names (for mapping label IDs to names)\nCOCO_INSTANCE_CATEGORY_NAMES = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A',\n    'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n    'N/A', 'N/A', 'toilet', 'N/A', 'TV', 'laptop', 'mouse', 'remote', 'keyboard',\n    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\n# Plot the image with bounding boxes and labels\nfig, ax = plt.subplots(1)\nax.imshow(image)\nax.axis('off')  # Turn off the axes\nfor idx, box in enumerate(boxes):\n    if scores[idx] &gt; 0.8:\n        xmin, ymin, xmax, ymax = box\n        width, height = xmax - xmin, ymax - ymin\n        rect = plt.Rectangle((xmin, ymin), width, height, fill=False, color='red', linewidth=2)\n        ax.add_patch(rect)\n        # Add label with score\n        label_text = f\"{COCO_INSTANCE_CATEGORY_NAMES[labels[idx]]}: {scores[idx]:.2f}\"\n        ax.text(xmin, ymin - 5, label_text, color='red', fontsize=10, backgroundcolor='white')\nplt.tight_layout()\nplt.show()\n</code></pre> Task: Faster R-CNN <p>Again it's your turn! Try running the code from above. Perform following tasks: </p> <ul> <li>Can you detect faces and eyes in a different image? Replace <code>data.astronaut()</code> with your own image and see how it works!</li> <li>What is the <code>COCO_INSTANCE_CATEGORY_NAMES</code> variable? Do some research.</li> </ul>"},{"location":"yolo/approaches/#one-stage-object-detection","title":"One-Stage Object Detection","text":"<p>One-stage detectors aim to predict object classes and bounding boxes directly from image pixels in a single network forward pass, without the region proposal step. This approach trades some accuracy for significant speed improvements.</p>"},{"location":"yolo/approaches/#ssd","title":"SSD","text":"<p>SSD, introduced in 2016, is a one-stage object detection model that performs object localization and classification in a single forward pass of the network, using default boxes of different scales and aspect ratios.</p> Process <ul> <li>One-Step Detection: SSD performs object detection in a single pass through the neural network. This means it looks at the image once and simultaneously figures out where objects are (localization) and what they are (classification). There's no separate step for proposing regions where objects might be.</li> <li>Default Boxes of Various Sizes: SSD uses a set of predefined boxes called default boxes or anchor boxes. These boxes come in different sizes and shapes (scales and aspect ratios) and are spread out across the image at various locations. The network checks each of these boxes to see if there's an object inside and determines the object's class (like a car, person, or dog).</li> </ul> SSD Architecture (Source: Medium) <ul> <li> Advantage <ul> <li>Real-Time Detection: Capable of high-speed detection suitable for real-time applications.</li> <li>Simplified Architecture: Combines localization and classification tasks into one network, simplifying deployment.</li> </ul> </li> <li> Disadvantage <ul> <li>Difficulty with Small Objects: Tends to have lower accuracy in detecting small objects compared to two-stage detectors.</li> <li>Trade-Off Between Speed and Accuracy: While faster, it may not achieve the same accuracy levels as more complex models like Faster R-CNN.</li> </ul> </li> </ul> Example: Object Detection with SSD <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>import torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load a pre-trained SSD model\nmodel = models.detection.ssd300_vgg16(pretrained=True)\nmodel.eval()\n\n# Transform the input image\ntransform = transforms.Compose([\n    transforms.Resize((300, 300)),\n    transforms.ToTensor(),\n])\n\n# Load and transform the image\nimage = Image.open('dog.jpg')\noriginal_width, original_height = image.size\nimage_tensor = transform(image)\n\n# Perform object detection\nwith torch.no_grad():\n    outputs = model([image_tensor])\n\n# Extract results\nlabels = outputs[0]['labels'].numpy()\nscores = outputs[0]['scores'].detach().numpy()\nboxes = outputs[0]['boxes'].detach().numpy()\n\n# Scale boxes back to original image size\nboxes[:, [0, 2]] *= original_width / 300  # Scale x-coordinates\nboxes[:, [1, 3]] *= original_height / 300  # Scale y-coordinates\n\n# Plot the image with bounding boxes and labels\nfig, ax = plt.subplots(1)\nax.imshow(image)\nax.axis('off')  # Turn off the axes\nfig.subplots_adjust(left=0, right=1, top=1, bottom=0)  # Remove padding/margins\nfor idx, box in enumerate(boxes):\n    if scores[idx] &gt; 0.5:\n        xmin, ymin, xmax, ymax = box\n        width, height = xmax - xmin, ymax - ymin\n        rect = plt.Rectangle((xmin, ymin), width, height, fill=False, color='red', linewidth=2)\n        ax.add_patch(rect)\n        # Add label with score\n        label_text = f\"{COCO_INSTANCE_CATEGORY_NAMES[labels[idx]]}: {scores[idx]:.2f}\"\n        ax.text(xmin, ymin - 5, label_text, color='red', fontsize=10, backgroundcolor='white')\n\n# Save the plot to remove additional padding (optional)\nplt.savefig(\"output_ssd.png\", bbox_inches='tight', pad_inches=0, dpi=300)\nplt.show()\n</code></pre> Task: SSD <p>Now try to perform a SSD. Perform following tasks: </p> <ul> <li>Can you detect obects in a different image? Replace with your own image and see how it works! </li> <li>Also try adjusting the confidence threshold in the code to see how it affects the detection results. (Hint: take a look at <code>scores</code>)</li> </ul>"},{"location":"yolo/approaches/#yolo","title":"YOLO","text":"<p>YOLO (You Only Look Once), introduced by Redmon et al., is an object detection algorithm that treats detection as a regression problem, directly predicting bounding boxes and class probabilities from full images in one evaluation, enabling end-to-end real-time detection. Since we will focus on YOLO in the next couple sections, we will not further discuss the process here. </p> <ul> <li> Advantage <ul> <li>Extremely Fast Performance: Designed for real-time detection with high frame rates.</li> <li>Unified Model Architecture: Simplifies the detection pipeline by using a single neural network.</li> </ul> </li> <li> Disadvantage <ul> <li>Struggles with Small Objects: May miss small objects or closely packed objects due to spatial constraints in its grid system.</li> <li>Localization Errors: Can be less accurate in localizing objects precisely compared to two-stage detectors.</li> </ul> </li> </ul> Example: Object Detection with YOLOv8 <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>from ultralytics import YOLO\n\n# Load a pre-trained YOLOv8 model\nmodel = YOLO(\"yolov8n.pt\")\n\n# Perform object detection on an image\nresults = model(\"input.jpg\", save=True, conf=0.5)\n\n# Display the results\nresults[0].show()\n</code></pre>"},{"location":"yolo/approaches/#comparison","title":"Comparison","text":"Method Speed Accuracy Complexity Suitable For R-CNN Slow High High Research, small datasets Fast R-CNN Moderate High Moderate Applications requiring accuracy Faster R-CNN Moderate Very High Moderate Balanced speed and accuracy SSD Fast Moderate-High Low Real-time applications YOLO Very Fast High Low Real-time applications <p> Key Takeaways:</p> <ul> <li>Multi-Stage Detectors: Offer higher accuracy but at the cost of speed and computational complexity.</li> <li>One-Stage Detectors: Provide faster inference suitable for real-time applications, with a trade-off in accuracy.</li> </ul>"},{"location":"yolo/approaches/#conclusion","title":"Conclusion","text":"<p>You've journeyed through the evolution of object detection methods, from traditional techniques like Haar Cascades and HOG to advanced deep learning models like Faster R-CNN, SSD, and YOLO.</p> <p>Object detection has evolved significantly, with deep learning methods pushing the boundaries of what's possible. Multi-stage detectors like Faster R-CNN provide high accuracy but can be computationally intensive. One-stage detectors like SSD and YOLO offer faster detection suitable for real-time applications.</p> \ud83c\udf89 Congratulations <p>You've gained an understanding of deep learning approaches in object detection!</p> <p>Data without context is noise! (With Zoom) byu/Anxious_City_7864 indatascience</p>"},{"location":"yolo/approaches/#whats-next","title":"What's Next?","text":"<p>In the upcoming sections, we'll delve deeper into YOLO, explore training custom models, and discuss real-world applications.</p> <p>References:</p> <ul> <li>Liu, W. et al. (2016). \"SSD: Single Shot MultiBox Detector\". European Conference on Computer Vision (ECCV).</li> <li>Girshick, R. (2015). \"Fast R-CNN\". IEEE International Conference on Computer Vision (ICCV).</li> <li>Ren, S. et al. (2016). \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\". IEEE Transactions on Pattern Analysis and Machine Intelligence.</li> <li>Redmon, J. et al. (2016). \"You Only Look Once: Unified, Real-Time Object Detection\". IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</li> </ul> <p>Further Reading:</p> <ul> <li>Understanding SSD MultiBox\u2014Real-Time Object Detection In Deep Learning</li> <li>R-CNN, Fast R-CNN, Faster R-CNN, YOLO \u2014 Object Detection Algorithms</li> <li>YOLOv8 vs SSD: Choosing the Right Object Detection Model</li> </ul>"},{"location":"yolo/yolo/","title":"Getting Started with YOLO","text":""},{"location":"yolo/yolo/#introduction","title":"Introduction","text":"<p>Remember the object detection approaches we covered earlier? YOLO revolutionized this field when it was introduced in 2015. Instead of using complex pipelines or scanning an image multiple times, YOLO takes a refreshingly simple approach: it looks at the image just once (hence the name \"You Only Look Once\") to detect all objects.</p> Historical Context <p>When YOLO was first released, object detection systems were complex multi-stage pipelines. The original paper titled \"You Only Look Once: Unified, Real-Time Object Detection\" by Joseph Redmon et al. introduced a radically different approach that would change the field forever.</p>"},{"location":"yolo/yolo/#the-yolo-approach","title":"The YOLO Approach","text":"<p>Let's break down how YOLO works in simple terms:</p> <ol> <li>Grid Division: YOLO first divides your image into a grid (say 13x13).</li> <li>Grid Cells Predictions: Each cell in the grid is responsible for predicting objects centered in that cell. Each cell predicts a certain number of bounding boxes and confidence scores for those boxes. A confidence score reflects how confident the model is that the box contains an object and also how accurate it thinks the box is.</li> <li>Bounding Box Parameters: Each bounding box has five predictions: <code>x</code>, <code>y</code>, <code>w</code>, <code>h</code>, and a confidence score. (<code>x</code>, <code>y</code>) coordinates represent the center of the box relative to the bounds of the grid cell. Width (<code>w</code>) and height (<code>h</code>) are predicted relative to the whole image. Finally, the confidence score represents the likelihood that the box contains an object and how accurate the box is.</li> <li>Class Predictions: In addition to predicting bounding boxes, each cell also predicts class probabilities. These probabilities are conditioned on the grid cell containing an object. </li> <li>Combining Predictions: The bounding box predictions and class predictions are combined to create a complete detection. If a grid cell is confident that it contains an object, and if the predicted class score is high, then it\u2019s a strong detection.</li> <li>Non-Max Suppression: Since YOLO predicts multiple boxes for each grid cell, it uses a technique called non-max suppression</li> </ol> <p>Here's a visualization of how YOLO divides an image and makes predictions:</p> YOLO Grid System (Source: Jonathan Hui on Medium) Task: YOLO Approach <p>Watch the following video about YOLO and answer the questions below:</p> <p></p> <p>Basics: </p> <ol> <li>In basic object detection, what two main things does YOLO need to determine about an object?</li> <li>What happens when no object is detected in a grid cell?</li> <li>True or False: YOLO must always use a 4x4 grid to divide images.</li> <li>Which is faster at detecting objects: YOLO or older methods like R-CNN?</li> <li>What are the components of YOLO's 7-dimensional output vector for a single grid cell prediction?</li> </ol>"},{"location":"yolo/yolo/#advantages-and-disadvantages","title":"Advantages and Disadvantages","text":"<p>Let's examine what makes YOLO special and where it might not be the best choice:</p> <ul> <li> Advantage <ul> <li>Speed: YOLO is blazingly fast, capable of processing images in real-time</li> <li>Accuracy: Despite its speed, it maintains good detection accuracy</li> <li>Global Context: By looking at the entire image at once, YOLO understands context better than sliding window approaches</li> <li>Generalization: YOLO learns generalizable representations of objects</li> </ul> </li> <li> Disadvantage <ul> <li>Small Objects: YOLO can struggle with detecting small objects, especially in groups</li> <li>Unusual Aspects: Objects in unusual aspects ratios or configurations might be missed</li> <li>Precision: While fast, it might not be as precise as two-stage detectors for some applications</li> </ul> </li> </ul>"},{"location":"yolo/yolo/#yolo-versions","title":"YOLO Versions","text":"<p>The YOLO family has evolved significantly since its introduction in 2016. Each version brought important improvements:</p>"},{"location":"yolo/yolo/#yolov1-v3-2016-2018","title":"YOLOv1-v3 (2016-2018)","text":"<ul> <li>YOLOv1: First version, introduced the grid-based approach</li> <li>YOLOv2/YOLO9000: Added anchor boxes, batch normalization</li> <li>YOLOv3: Implemented feature pyramid networks, better backbone (Darknet-53)</li> </ul>"},{"location":"yolo/yolo/#yolov4-v5-2020-2021","title":"YOLOv4-v5 (2020-2021)","text":"<ul> <li>YOLOv4: Introduced Mosaic augmentation, CSPNet backbone</li> <li>YOLOv5: Brought PyTorch implementation, improved training methods</li> </ul>"},{"location":"yolo/yolo/#latest-versions-2022-2024","title":"Latest Versions (2022-2024)","text":"<ul> <li>YOLOv6: Released by Meituan, optimized for industrial applications</li> <li>YOLOv7: Improved architecture design and training strategies</li> <li>YOLOv8: Ultralytics' flagship model with multi-task capabilities</li> <li>YOLOv9: Introduced revolutionary new features</li> <li>YOLOv10: Enhanced previous versions' capabilities</li> <li>YOLOv11: Latest iteration with significant improvements</li> </ul> YOLO Version Comparison (Source: Ultralytics)"},{"location":"yolo/yolo/#installation-and-setup","title":"Installation and Setup","text":"<p>Getting started with YOLO is straightforward. You can use the Ultralytics implementation of YOLOv11, which offers a user-friendly API and excellent documentation.</p> <ol> <li> <p>First, create a new project folder and virtual environment and activate it:</p> <pre><code>\ud83d\udcc1 computer_vision/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 pics/\n\u2514\u2500\u2500 \ud83d\udcc4 your_files.ipynb\n</code></pre> </li> <li> <p>Install the required packages: <pre><code>pip install ultralytics\n</code></pre></p> </li> <li> <p>Verify your installation: <pre><code>from ultralytics import YOLO\nmodel = YOLO('yolov8n.pt')  # load a pretrained model\n</code></pre></p> </li> </ol> Installation Tips for Pro's <p>If you're using a GPU, make sure you have the correct CUDA version installed</p>"},{"location":"yolo/yolo/#reading-the-docs","title":"Reading the Docs","text":"(Source: Imgflip Meme Generator)  <p>The Ultralytics documentation is your best friend when working with YOLO. Here are the key resources you should bookmark:</p> <ol> <li>Official Ultralytics Docs</li> <li>YOLO Quickstart Guide</li> <li>YOLO11 Tasks</li> </ol> Documentation Best Practices <ul> <li>Keep the API reference handy for specific function documentation</li> <li>Check the examples section for common use cases</li> <li>Join the Ultralytics Discord community for help and updates</li> </ul>"},{"location":"yolo/yolo/#whats-next","title":"What's Next?","text":"<p>In the upcoming sections, we'll dive deeper into:</p> <ul> <li>Working with pretrained models</li> <li>Analyzing images and videos</li> <li>Preparing custom datasets</li> <li>Training YOLO on your own data</li> </ul>"},{"location":"yolo/image/detection/","title":"Detection","text":"<p>After learning about computer vision in general and how YOLO works, we can start using YOLO for our purposes. We will see, how much we can do with little code. </p>"},{"location":"yolo/image/detection/#project-setup","title":"Project Setup","text":"<p>We start with the project structure from before and create a new jupyter notebook <code>yolo_detect.ipynb</code> <pre><code>\ud83d\udcc1 computer_vision/\n    \u251c\u2500\u2500 \ud83d\udcc1 .venv/\n    \u251c\u2500\u2500 \ud83d\udcc1 pics/\n    \u2514\u2500\u2500 \ud83d\udcc4 yolo_detect.ipynb\n</code></pre> Make sure the virtual environment (here <code>.venv</code>) is selected and all needed packages (<code>ultralytics</code>) are installed. </p> <p>Now we can download our test pictures, extract the ZIP file and save it in the <code>\ud83d\udcc1 pics/</code> folder.</p> <p>Test Pictures </p> <p>Now we are all set for our first detection \ud83c\udf89</p> <p>We will start with the picture <code>pic2.jpg</code> which is quite challenging due to motion blur. </p>"},{"location":"yolo/image/detection/#inference","title":"Inference","text":"Inference <p>In machine learning, inference refers to the process of using a trained model to make predictions or decisions on new, unseen data. It's the production phase where the model applies what it learned during training to analyze new inputs and generate outputs, like when a trained image recognition model identifies objects in a new photo.</p>"},{"location":"yolo/image/detection/#pretrained-models","title":"Pretrained Models","text":"<p>Fortunately, so that we don't have to start from scratch, there are already pre-trained models from YOLO that we can use. These pre-trained models have been trained with the help of a lot of data (for detection e.g. on the COCO dataset) and are now available to us without any further effort. For example for detection: </p> Model size<sup>(pixels) mAP<sup>val50-95 Speed<sup>CPU ONNX(ms) Speed<sup>T4 TensorRT10(ms) params<sup>(M) FLOPs<sup>(B) YOLO11n 640 39.5 56.1 \u00b1 0.8 1.5 \u00b1 0.0 2.6 6.5 YOLO11s 640 47.0 90.0 \u00b1 1.2 2.5 \u00b1 0.0 9.4 21.5 YOLO11m 640 51.5 183.2 \u00b1 2.0 4.7 \u00b1 0.1 20.1 68.0 YOLO11l 640 53.4 238.6 \u00b1 1.4 6.2 \u00b1 0.1 25.3 86.9 YOLO11x 640 54.7 462.8 \u00b1 6.7 11.3 \u00b1 0.2 56.9 194.9 Available pretrained YOLO models for detection (Source: Ultralytics)"},{"location":"yolo/image/detection/#running-the-detection","title":"Running the Detection","text":"<p>Based on these pretrained models, to detect objects in an image with YOLO, only a few lines of code are required:</p> <p><pre><code># Import required librarys\nfrom ultralytics import YOLO\n\n# Define the path to the source picture\npicpath = \"pics/pic2.jpg\"\n\n# Load a pretrained YOLO11 Model (Size: Nano)\nmodel_det = YOLO(\"yolo11n.pt\")\n\n# Apply the model to our source picture\nresults = model_det(picpath)\n</code></pre> &gt;&gt;&gt; Output<pre><code>image 1/1 c:\\path\\to\\pics\\pic2.jpg: 448x640 6 persons, 1 car, 5 motorcycles, 1 traffic light, 1 stop sign, 40.3ms\nSpeed: 2.0ms preprocess, 40.3ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n</code></pre></p> <p>And that's it! Your performed your first detection. </p> (Source: Imgflip Meme Generator)"},{"location":"yolo/image/detection/#analyzing-the-output","title":"Analyzing the Output","text":"<p>Now we can take a closer look at the different parts of the output:</p> <ul> <li> <p>Information about the image:</p> <ul> <li> <p><code>image 1/1 c:\\path\\to\\pics\\pic2.jpg: 448x640</code></p> <p>This includes the path to the image and the image size YOLO uses for the detection (this is not the original image size)</p> </li> <li> <p><code>shape (1, 3, 448, 640)</code></p> <p>batch size (number of pictures): <code>1</code></p> <p>picture channels: <code>3</code> (RGB)</p> <p>picture hight: <code>448</code></p> <p>picture width: <code>640</code></p> </li> </ul> </li> <li> <p>Detected objects: </p> <ul> <li><code>6 persons, 1 car, 5 motorcycles, 1 traffic light, 1 stop sign</code></li> </ul> </li> <li> <p>Speed Metrics:</p> <ul> <li><code>Speed: 2.0ms preprocess, 40.3ms inference, 0.0ms postprocess per image</code></li> </ul> </li> </ul> <p>But are those all of the results? Where are those objects in the image? How sure are we, that there are six persons? We need to go deeper into the results. </p>"},{"location":"yolo/image/detection/#understanding-the-results","title":"Understanding the results","text":"<p>The before seen output of the detection is just a brief overview. All the information is stored in <code>results</code></p> <pre><code># Get the first (and only) image's results\nresult = results[0]\n\nprint(result)\n</code></pre> Task: Analyze the Results <p>Take a look at the <code>result</code> and answer the following questions (don't forget to use google and the docs)</p> <ul> <li>What is the original shape of the image?</li> <li>How many different classes are available and what are they?</li> <li>Why is <code>keypoints</code>, <code>masks</code> and <code>obb</code> <code>None</code>? What do you think?</li> </ul> <p>Now go deeper and analyze <code>result.boxes</code></p> <ul> <li>What are the detected classes? How do they respond to the labels (person, car,...)</li> <li>How sure is YOLO about the detected objects? (hint: the right wording is confidence)</li> <li>What are the differences between <code>xywh</code>, <code>xywhn</code>, <code>xyxy</code> and <code>xyxyn</code></li> <li><code>data</code> is the collection of what? </li> <li>How are <code>data</code> and <code>shape</code> are connected? </li> </ul> Multiple Images <p>You can also pass multiple images at once. To access the results for a specific image, you need to choose one from the results list.</p> <pre><code>results = model_det(['pics/pic1.jpg','pics/pic1.jpg'])\nfirst_pic_result = results[0]\nsecond_pic_result = results[1]\n</code></pre> <p>Now, that we are familiar with the results, we can write a little function to print a more detailed overview:</p> <pre><code>def print_overview(result): \n    # Print object count\n    print(f\"Detected {len(result.boxes)} objects\")\n\n    # Examine each detection\n    for box in result.boxes:\n        # Get class name\n        class_id = int(box.cls)\n        class_name = model_det.names[class_id]\n\n        # Get confidence\n        confidence = float(box.conf)\n\n        # Get coordinates (x1, y1, x2, y2 format)\n        x1, y1, x2, y2 = box.xyxy[0].tolist()\n\n        print(f\"\\nDetection:\")\n        print(f\"- Class: {class_name}\")\n        print(f\"- Confidence: {confidence:.2f}\")\n        print(f\"- Coordinates: ({x1:.1f}, {y1:.1f}) to ({x2:.1f}, {y2:.1f})\")\n\nprint_overview(result)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Detected 14 objects\n\nDetection:\n- Class: motorcycle\n- Confidence: 0.81\n- Coordinates: (2177.1, 3142.4) to (3490.8, 4281.8)\n\nDetection:\n- Class: motorcycle\n- Confidence: 0.75\n- Coordinates: (0.0, 3574.5) to (755.0, 4952.4)\n\n...\n</code></pre> <p>So far we have everything we need to detect objects in images and work with the results. </p> <p>But sometimes a picture is worth a thousand words!</p>"},{"location":"yolo/image/detection/#visualize-the-results","title":"Visualize the Results","text":"<p>In some cases, the 'simple' extraction of the results is not sufficient and a visualization is needed.</p> <p>For visualizing the results there are two options available: showing and saving the picture. </p>"},{"location":"yolo/image/detection/#showing-the-resulting-image","title":"Showing the Resulting Image","text":"<p>YOLO makes it really easy to show the resulting image. With the method <code>.show()</code> the result can be visualized: </p> <pre><code>result.show()\n</code></pre> <p>This should result in the above shown image. </p>"},{"location":"yolo/image/detection/#saving-the-resulting-image","title":"Saving the Resulting Image","text":"<p>There are multiple ways to save the resulting image. </p> Saving Images Option 1: YOLOOption 2: openCV <p>YOLO offers various visualization arguments, which can be used directly in the inference command. One of them is <code>save=True</code> <pre><code>results = model_det(picpath, save=True)\n</code></pre></p> <p>Unless other specified, the file will be saved in a new folder <code>\ud83d\udcc1 runs/detect/predict/</code>. Use the argument <code>save_dir='your/custom/path'</code> to specify a different folder.</p> <p>When you install <code>ultralytics</code> the package <code>opencv-python</code> will be installed automatically as dependency. <code>openCV</code> is a powerful tool for image and video processing and can also be used for saving the image</p> <p><pre><code>annotated_image = result.plot()\n\n# Display using OpenCV\nimport cv2\n\n# Save the image\ncv2.imwrite(\"output_detection.jpg\", annotated_image)\n</code></pre> Unless other specified, the file will be saved directly into the working directory <code>\ud83d\udcc1 computer_vision</code>. </p>"},{"location":"yolo/image/detection/#see-the-magic-happen","title":"See the Magic Happen","text":"<p>If you are interessted in seeing, what the model is seeing and doing, you can use another visualization argument  <pre><code>results = model_det(picpath, visualize=True)\n</code></pre></p> <p>The resulting pictures in the folder <code>\ud83d\udcc1 runs/detect/predict/pic2</code> are a 'Intermediate Features Visualization'</p> <ul> <li>This parameter saves visualizations of the intermediate feature maps or activations from the YOLO model.</li> <li>These feature maps show what parts of the image the model focuses on during different stages of the neural network.</li> <li>These outputs help in debugging and understanding the model's inner workings.</li> </ul>"},{"location":"yolo/image/detection/#inference-arguments","title":"Inference Arguments","text":"<p>After we worked on the output side of the Yolo, it's now time to focus more on the input side. Besides those visualization arguments we have used before, there are numerous inference arguments, which can be handed over to the model. An overview can be found in the documentation</p> Task: Inference Arguments <ol> <li>Confidence<ul> <li>Run a detection with confidence threshold 0.5.</li> <li>Save this image as <code>high_conf.jpg</code></li> <li>Now try confidence 0.25, save as <code>low_conf.jpg</code></li> <li>Compare both images - what differences do you notice?</li> <li>What is the default value? </li> </ul> </li> <li>Classes<ul> <li>Limit your detection to just detect <code>motorcycle</code> and <code>car</code></li> <li>Save the results as <code>class_limit.jpg</code></li> </ul> </li> <li>Adjusting the output path<ul> <li>Use the two inference arguments <code>project</code> and <code>name</code> to adjust the output folder to <code>\ud83d\udcc1 output_pics/detection_pics/</code>. </li> </ul> </li> <li>Adjust the visual outcome<ul> <li>Run a detection for our <code>pic2.jpg</code> image and adjust:<ul> <li>no label should be shown</li> <li>no confidence should be shown</li> <li>line width of the boxes should be 3  </li> </ul> </li> </ul> </li> </ol>"},{"location":"yolo/image/detection/#further-adjustments","title":"Further Adjustments","text":"<p>As we have discussed before, YOLO comes in different model sizes, trading speed for accuracy.</p> Task: Model Comparison <p>For each model size determine the following characteristics by running a detection on the <code>pic2.jpg</code>:</p> <ol> <li>Time the detection speed</li> <li>Count detected objects</li> <li>Compare confidence scores</li> <li>Create a table with your findings:</li> </ol> Model Detection Time Objects Found Avg Confidence nano small medium large extra large \ud83c\udf89 Congratulations <p>You are now able to perform object detection on images and work with the results!    Detection Fail (Source: Visualizing Object Detection Features on Springer Nature Link)  </p>"},{"location":"yolo/image/kp-extraction/","title":"Keypoint Extraction","text":"<p>The next step in computer vision is the extraction of keypoints in images. YOLO also has a solution for this and its own models.  YOLO can detect and track specific points of interest in images, which is particularly useful for pose estimation, facial landmark detection, and custom keypoint tracking.</p>"},{"location":"yolo/image/kp-extraction/#project-setup","title":"Project Setup","text":"<p>We'll continue with the project structure from before and create a new jupyter notebook <code>yolo_keypoint.ipynb</code>:</p> <pre><code>\ud83d\udcc1 computer_vision/\n    \u251c\u2500\u2500 \ud83d\udcc1 .venv/\n    \u251c\u2500\u2500 \ud83d\udcc1 pics/\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_detect.ipynb\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_segment.ipynb\n    \u2514\u2500\u2500 \ud83d\udcc4 yolo_keypoints.ipynb\n</code></pre> <p>Ensure your virtual environment is active, and the required packages (<code>ultralytics</code>) are installed.</p> <p>We will start with the picture <code>pic1.jpg</code> which we already downloaded before. </p>"},{"location":"yolo/image/kp-extraction/#yolo-keypoint-detection","title":"YOLO Keypoint Detection","text":"<p>YOLO's keypoint detection is primarily used for human pose estimation, where it can identify specific points on objects or bodies. This is particularly useful for:</p> <ul> <li>Human pose estimation</li> <li>Facial landmark detection</li> <li>Custom keypoint tracking</li> <li>Sports motion analysis</li> <li>Animal pose tracking</li> </ul> <p>Therefore, the pre-trained models provided by YOLO are only trained for humans and do not recognize anything else. We can also train models ourselves to recognize various types of key points besides humans. We will look at this topic a little later. </p>"},{"location":"yolo/image/kp-extraction/#inference","title":"Inference","text":"Pretrained Models <p>Again, just like with detection and segmentation, YOLO provides pre-trained models specifically for keypoint detection. These models have been trained on various datasets to recognize different types of keypoints.</p>"},{"location":"yolo/image/kp-extraction/#running-keypoint-detection","title":"Running Keypoint Detection","text":"<p>The code for keypoint detection and the results are similar to other YOLO tasks - we just need to use a keypoint-specific model:</p> <pre><code># Import required libraries\nfrom ultralytics import YOLO\n\n# Define the path to the source picture\npicpath = \"pics/pic1.jpg\"\n\n# Load a pretrained YOLO11 Keypoint Model (Size: Nano)\nmodel_kp = YOLO(\"yolo11n-pose.pt\") # (1)!\n\n# Apply the model to our source picture\nresults = model_kp(picpath)\n</code></pre> <ol> <li>As with detection and segmentation, there are also different models for keypoint extraction: <code>YOLO11n-pose</code>, <code>YOLO11s-pose</code>, <code>YOLO11m-pose</code>, <code>YOLO11l-pose</code>, <code>YOLO11x-pose</code></li> </ol> &gt;&gt;&gt; Output<pre><code>image 1/1 c:\\path\\to\\pics\\pic_keypoints.jpg: 384x640 10 persons, 63.4ms\nSpeed: 3.0ms preprocess, 63.4ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n</code></pre>"},{"location":"yolo/image/kp-extraction/#visualizing-keypoint-results","title":"Visualizing Keypoint Results","text":"<p>Like inference, the visualization of keypoints is equivalent to segmentation and detection. You can customize how the keypoint results are displayed. For more control over the visualization:</p> <pre><code>fname = \"output_kp.jpg\"\n\nresult.plot(\n    show = True,        # Display the plot immediately\n\n    save = True,        # Save the plotted image to a file\n    filename = fname,    # Specify the filename for the saved image\n\n    conf=True,           # Show confidence scores\n    line_width=2,        # Line thickness for connections\n    font_size=14,        # Font size for labels\n    boxes=True,          # Show bounding boxes\n    labels=True          # Show keypoint labels\n)\n</code></pre>"},{"location":"yolo/image/kp-extraction/#inference-arguments","title":"Inference Arguments","text":"<p>Many of the same inference arguments from detection and segmentation also work with keypoint extraction, plus some additional ones specific to masks. Therefore check the documentation.</p> Task: Perform a Keypoint Extraction <p>Perform a keypoint extraction on the above shown image <code>pic1.jpg</code>. Take a closer look at the results and answer these questions:</p> <ol> <li>How do the results differ from those of the detection/segmentation?</li> <li>Take a look at the names attribute. How many different classes can be detected?</li> <li>How many poses are detected?</li> <li>What parts does the keypoints object consist of?</li> <li>Whats the difference between conf, data and xy?</li> <li>Do some research about which keypoint represents what body part. </li> <li>Now visualize the results by saving the image. </li> </ol> \ud83c\udf89 Congratulations <p>You've learned how to use YOLO for image processing! Try applying these concepts to your own projects and explore more advanced applications.</p> <p> (Source: Reddit)  </p> <p>Stay tuned for more advanced techniques and use cases in the next chapters!</p>"},{"location":"yolo/image/segmentation/","title":"Segmentation","text":"<p>After learning how to detect objects in images, we can now go one step further: Instead of just detecting where objects are located using bounding boxes, we can identify exactly which pixels belong to each object. This is called segmentation.</p>"},{"location":"yolo/image/segmentation/#project-setup","title":"Project Setup","text":"<p>We'll continue with the project structure from before and create a new jupyter notebook <code>yolo_segment.ipynb</code>: <pre><code>\ud83d\udcc1 computer_vision/\n    \u251c\u2500\u2500 \ud83d\udcc1 .venv/\n    \u251c\u2500\u2500 \ud83d\udcc1 pics/\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_detect.ipynb\n    \u2514\u2500\u2500 \ud83d\udcc4 yolo_segment.ipynb\n</code></pre> Make sure your virtual environment is still active. We'll use the same test pictures as before.</p> <p>We'll start with the picture <code>pic2.jpg</code> that we used for detection:</p>"},{"location":"yolo/image/segmentation/#inference","title":"Inference","text":"Pretrained Models <p>Just like with detection, YOLO provides pre-trained models specifically for segmentation. These models have been trained on the COCO dataset but with segmentation masks instead of just bounding boxes.</p>"},{"location":"yolo/image/segmentation/#running-the-segmentation","title":"Running the Segmentation","text":"<p>The code for segmentation is very similar to detection - we just need to use a segmentation model instead:</p> <pre><code># Import required libraries\nfrom ultralytics import YOLO\n\n# Define the path to the source picture\npicpath = \"pics/pic2.jpg\"\n\n# Load a pretrained YOLO11 Segmentation Model (Size: Nano)\nmodel_seg = YOLO(\"yolo11n-seg.pt\") # (1)!\n\n# Apply the model to our source picture\nresults = model_seg(picpath)\n</code></pre> <ol> <li>As with detection, there are also different models for segmentation: <code>YOLO11n-seg</code>, <code>YOLO11s-seg</code>, <code>YOLO11m-seg</code>, <code>YOLO11l-seg</code>, <code>YOLO11x-seg</code></li> </ol> &gt;&gt;&gt; Output<pre><code>image 1/1 c:\\path\\to\\pics\\pic2.jpg: 448x640 4 persons, 1 car, 5 motorcycles, 3 traffic lights, 1 stop sign, 52.7ms\nSpeed: 2.0ms preprocess, 52.7ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n</code></pre> <p>The output looks similar to detection, but behind the scenes YOLO has created detailed segmentation masks for each object!</p> Task: Analyze the Segmentation Results <p>Now it's time to analyze the segmentation results and compare them to the results from our detection. Take a closer look at the <code>results</code> and answer the following questions:</p> <ol> <li>What's the difference between the detection results and the segmentation results now? </li> <li>What's the difference between <code>boxes</code> and <code>masks</code>? What information is stored in these variables?</li> <li>What's the shape of a mask and what does each dimension represent?</li> <li>How are the coordinates in masks different from bounding boxes?</li> <li>Visualize the results by saving the resulting image. </li> </ol>"},{"location":"yolo/image/segmentation/#visualizing-segmentation-results","title":"Visualizing Segmentation Results","text":"<p>A graphical representation of the results can also be useful for segmentation. For this, the same commands are available as for detection. </p> <p>Another visualization option is the <code>result.plot</code> command. With this, you can customize how the segmentation (or detection) results are displayed to better suit your analysis or presentation needs, allowing you to highlight specific features like bounding boxes, segmentation masks, confidence scores, or class labels.</p> <pre><code>fname = \"output_segmentation.jpg\"\n\nresult.plot(\n    show = True,        # Display the plot immediately\n\n    save = True,        # Save the plotted image to a file\n    filename = fname,   # Specify the filename for the saved image\n\n    boxes = True,       # Include bounding boxes around detected objects\n    masks = True,       # Overlay segmentation masks on the image\n    conf = False,       # Do not display confidence scores for the predictions\n    labels = True,      # Display class labels for each detected object\n)\n</code></pre>"},{"location":"yolo/image/segmentation/#inference-arguments","title":"Inference Arguments","text":"<p>Many of the same inference arguments from detection also work with segmentation, plus some additional ones specific to masks. Therefore check the documentation.</p> Task: Segmentation Practice <p>Try these exercises to better understand image segmentation:</p> <ol> <li>Mask Quality (Inference Argument)<ul> <li>Run segmentation with <code>retina_masks=True</code></li> <li>Compare the output with default masks</li> <li>What differences do you notice in quality and speed?</li> </ul> </li> <li>Compare different model sizes (nano vs. small vs. medium) for segmentation</li> <li>Experiment with different confidence thresholds</li> <li>Try segmenting different types of images</li> </ol> \ud83c\udf89 Congratulations <p>You've learned the basics of image segmentation! Try applying these concepts to your own projects and explore more advanced techniques.</p>"},{"location":"yolo/train/acquisition/","title":"Data Acquisition","text":"<pre><code>graph LR\n  A[Data Acquisition]:::active --&gt; B[Annotation];\n  B --&gt; C[Training];\n  C --&gt; D[Inference];\n  click A \"../acquisition\" _self\n  click B \"../annotation\" _self\n  click C \"../training\" _self\n  click D \"../inference\" _self\n  classDef active fill:#950f42</code></pre> <p>After learning about different computer vision tasks with YOLO, you might want to train your own model for specific use cases. The first step in training a custom YOLO model is acquiring a suitable dataset. A well-curated and diverse dataset is key to achieving high performance and generalization in computer vision tasks.This chapter will guide you through various methods of collecting training data.</p>"},{"location":"yolo/train/acquisition/#project-setup","title":"Project Setup","text":"<p>We'll start a new project for training our custom YOLO model: <pre><code>\ud83d\udcc1 yolo_training/\n    \u251c\u2500\u2500 \ud83d\udcc1 .venv/\n    \u251c\u2500\u2500 \ud83d\udcc1 data/\n    \u2514\u2500\u2500 \ud83d\udcc4 data_acquisition.ipynb\n</code></pre></p>"},{"location":"yolo/train/acquisition/#the-need-for-data","title":"The Need for Data","text":"<p>Training an effective YOLO model requires a substantial amount of data. Here's why:</p> <ul> <li>Better Generalization: A diverse dataset helps the model learn features that generalize to new, unseen data.</li> <li>Handling Variability: Capturing different lighting conditions, perspectives, and object appearances ensures robust performance.</li> <li>Avoiding Overfitting: A small dataset can cause the model to memorize specific examples rather than learning general patterns, leading to overfitting.</li> </ul> <p>The amount of data needed depends on several factors like Task Complexity, Required Accuracy, Object Variation and Background Variation</p> Task Complexity Recommended Images Simple (e.g., logo detection) 100 - 2,000 Moderate (e.g., car types) 2,000 - 10,000 Complex (e.g., defect detection) 5,000 - 20,000+ <p>Building a large dataset can be a challenging task, but there are several strategies to gather the required data efficiently.</p>"},{"location":"yolo/train/acquisition/#automatic-image-collection","title":"Automatic Image Collection","text":"<p>Web scraping can be used to download large amounts of images for training datasets. Python libraries like <code>requests</code> and <code>BeautifulSoup</code> are common tools for this purpose. An even more comfortable way is to use an API of a search engine like Bing , Google  or DuckDuckGo . The <code>ImageEngine</code> package can be used to search all three search engines at ones. </p> <pre><code>pip install ImageEngine\n</code></pre> <p>You can run a search  with just a couple of lines: </p> <pre><code>from ImageEngine import searchDDG       # Only search DDG\nfrom ImageEngine import searchBing      # Only search Bing\nfrom ImageEngine import searchGoogle    # Only search Google\nfrom ImageEngine import searchWeb       # Search all three engines\n\n# Search images from DuckDuckGo\n# \"Homer Simpson\" is the search string and \"homer\" is the directory where images will be stored\nsearchDDG(term=\"Homer Simpson\", path=\"data/homer\", max_images=5)\n# Search images from Bing\nsearchBing(term=\"Marge Simpson \", path=\"data/marge\", max_images=5)\n# Search images from Google\nsearchGoogle(term=\"Bart Simpson\", path=\"data/bart\", max_images=5)\n# Search images from all three engines\nsearchWeb(term=\"Lisa Simpson\", path=\"data/lisa\", max_images=5)\n</code></pre>"},{"location":"yolo/train/acquisition/#data-cleaning-tips","title":"Data Cleaning Tips","text":"<p>After downloading, it's important to clean your dataset by going through the following steps:</p> Dataset Cleaning Checklist <ul> <li> Check copyright restrictions</li> <li> Remove corrupted images</li> <li> Remove duplicates</li> <li> Verify image quality</li> <li> Check image relevance</li> <li> Ensure consistent format (not allways needed)</li> </ul> <p>Those steps can be done manually by looking through the pictures. Finding duplicates and corrupted images can also be automated by using this custom function</p> Remove Corrupted Images and Duplicates <pre><code>import os\nfrom PIL import Image\nimport hashlib\n\ndef clean_dataset_by_subfolder(directory):\n    \"\"\"\n    Cleans all subfolders in 'directory' by removing corrupted files and\n    duplicates *only* within each subfolder.\n\n    Args:\n        directory (str): Path to a top-level directory that contains subfolders.\n    \"\"\"\n    # 1) Loop through all entries in the top-level directory\n    for entry in os.listdir(directory):\n        subpath = os.path.join(directory, entry)\n\n        # Check if the entry is a directory (i.e., a subfolder)\n        if os.path.isdir(subpath):\n            print(f\"\\n--- Checking subfolder: {subpath} ---\")\n\n            # Create a new hash dictionary for each subfolder\n            hash_dict = {}\n\n            # 2) Recursively walk through all files in the subfolder\n            for root, dirs, files in os.walk(subpath):\n                for filename in files:\n                    filepath = os.path.join(root, filename)\n\n                    try:\n                        # Attempt to open the image file\n                        with Image.open(filepath) as img:\n                            # Compute the MD5 hash of the image bytes\n                            img_hash = hashlib.md5(img.tobytes()).hexdigest()\n\n                            # Check if this hash already exists (i.e., a duplicate)\n                            if img_hash in hash_dict:\n                                print(f\"Removing duplicate: {filepath}\")\n                                os.remove(filepath)\n                            else:\n                                # If not a duplicate, store it in the dictionary\n                                hash_dict[img_hash] = filepath\n\n                    except Exception as e:\n                        # If there's any error (e.g., corrupted file), remove it\n                        print(f\"Removing corrupted file {filepath}: {e}\")\n                        os.remove(filepath)\n\n# Example call, if 'dataset/homer' contains multiple subfolders\nclean_dataset_by_subfolder(\"data\")\n</code></pre> Task: Download Images <p>Now it's your turn. We want to collect images of different Euro notes. </p> <ul> <li>Try to use the <code>ImageEngine</code> package to download 100 suitable images of a 5\u20ac note and save them into the folder <code>data/five</code>.</li> <li>Go through the data cleaning checklist (you can use the function from above)</li> </ul> <p> </p>"},{"location":"yolo/train/acquisition/#video-frame-extraction","title":"Video Frame Extraction","text":"<p>Another effective way to collect a vast amount of images is by extracting and saving each single frame from a video . This video can be a live stream from the webcam or a saved video from your hard drive. There are some benefits of using video data:</p> <ul> <li>Efficiency: Videos can capture many frames in one recording session, saving time compared to capturing individual photos.</li> <li>Diverse Scenarios: Recording videos in various environments ensures that frames capture different conditions and perspectives.</li> </ul> Recording Guidelines <p>When recording video for training data:</p> <ul> <li>Capture different angles</li> <li>Vary lighting conditions</li> <li>Include different backgrounds</li> <li>Move around the object</li> <li>Vary object positions</li> </ul>"},{"location":"yolo/train/acquisition/#frame-extraction","title":"Frame Extraction","text":"<p>We already introduced OpenCV in the previous chapter. We can use this package to access the video (saved or webacm) and instead of showing the image, we can save it as an image in a folder</p> <pre><code>cv2.imwrite(f'data/video/frame_{frameNr}.jpg', frame)\n</code></pre> Unique File Name <p><code>frameNr</code> is simply a frame counter. It starts at 0 before entering the loop and increments by 1 every time a frame is successfully read from the video. This counter is used to give each extracted frame a unique filename (e.g., <code>frame_0.jpg</code>, <code>frame_1.jpg</code>, etc.).</p> Task: Frame Extraction <p>Now we continue from before and try to record a video of a Euro note.</p> <ul> <li>Record a video of a 10\u20ac note and save each 10th frames seperately in the folder <code>data/ten</code>. In the end it should be at least 100 images.</li> <li>Keep the recording guidelines in your mind.</li> </ul> <p> </p>"},{"location":"yolo/train/acquisition/#best-practices","title":"Best Practices","text":"<p>When collecting your dataset:</p> <ol> <li> <p>Diversity</p> <ul> <li>Vary lighting conditions</li> <li>Include different backgrounds</li> <li>Capture different angles</li> </ul> </li> <li> <p>Quality Control</p> <ul> <li>Check image resolution</li> <li>Remove blurry images</li> <li>Ensure correct labeling</li> <li>Verify class balance</li> </ul> </li> <li> <p>Organization</p> <ul> <li>Use clear folder structure</li> <li>Maintain consistent naming</li> </ul> </li> </ol>"},{"location":"yolo/train/acquisition/#whats-next","title":"What's Next?","text":"<p>After collecting the data, the next step is to annotate it with labels or bounding boxes for training the model. Continue to the next section, Image Annotation, to learn how to prepare your dataset for training!</p>"},{"location":"yolo/train/annotation/","title":"Image Annotation","text":"<pre><code>graph LR\n  A[Data Acquisition]:::active --&gt; B[Annotation];\n  B:::active --&gt; C[Training];\n  C --&gt; D[Inference];\n  click A \"../acquisition\" _self\n  click B \"../annotation\" _self\n  click C \"../training\" _self\n  click D \"../inference\" _self\n  classDef active fill:#950f42</code></pre>"},{"location":"yolo/train/inference/","title":"Inference","text":"<pre><code>graph LR\n  A[Data Acquisition]:::active --&gt; B[Annotation];\n  B:::active --&gt; C[Training];\n  C:::active --&gt; D[Inference]:::active;\n  click A \"../acquisition\" _self\n  click B \"../annotation\" _self\n  click C \"../training\" _self\n  click D \"../inference\" _self\n  classDef active fill:#950f42</code></pre>"},{"location":"yolo/train/training/","title":"Training","text":"<pre><code>graph LR\n  A[Data Acquisition]:::active --&gt; B[Annotation];\n  B:::active --&gt; C[Training];\n  C:::active --&gt; D[Inference];\n  click A \"../acquisition\" _self\n  click B \"../annotation\" _self\n  click C \"../training\" _self\n  click D \"../inference\" _self\n  classDef active fill:#950f42</code></pre>"},{"location":"yolo/video/","title":"Analysing Videos","text":"(Source: Medium)  <p>Moving from static images to videos, YOLO demonstrates its real power. With its high-speed analysis, YOLO can process each video frame individually and provide real-time insights. Whether detecting objects, segmenting images, or estimating poses, the procedure mirrors that used for static images but adds the dimension of handling sequential frames.</p>"},{"location":"yolo/video/#project-setup","title":"Project Setup","text":"<p>To start working with video analysis, we'll extend our previous project structure and create a new Jupyter notebook <code>yolo_video.ipynb</code>: <pre><code>\ud83d\udcc1 computer_vision/\n    \u251c\u2500\u2500 \ud83d\udcc1 .venv/\n    \u251c\u2500\u2500 \ud83d\udcc1 pics/\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_detect.ipynb\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_segment.ipynb\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_keypoints.ipynb\n    \u2514\u2500\u2500 \ud83d\udcc4 yolo_video.ipynb\n</code></pre> Ensure your virtual environment (<code>.venv</code>) is active and that all necessary packages, including <code>ultralytics</code> and <code>opencv-python</code>, are installed.</p> OpenCV Installation <p>OpenCV is one of the few Python packages that have different names when installed and integrated into our notebook. The installation is carried out using:</p> <pre><code>pip install opencv-python\n</code></pre>"},{"location":"yolo/video/#capture-video-stream","title":"Capture Video Stream","text":"<p>To analyze video data, whether from a webcam or a saved file, we leverage the Python package OpenCV (<code>cv2</code>). Let's begin with a program to access your web camera and display its live feed.</p>"},{"location":"yolo/video/#step-1-import-the-library","title":"Step 1: Import the Library","text":"<p>To use OpenCV, start by importing the library:</p> <p><pre><code>import cv2\n</code></pre> This statement includes the OpenCV library in our program, giving us access to its methods and properties.</p>"},{"location":"yolo/video/#step-2-create-a-videocapture-object","title":"Step 2: Create a VideoCapture Object","text":"<p>In OpenCV, the <code>VideoCapture()</code> method allows us to capture the video stream from our webcam:</p> <p><pre><code>cap = cv2.VideoCapture(0)\n</code></pre> The argument <code>0</code> refers to the first camera connected to the device. If additional cameras are connected, you can use <code>1</code>, <code>2</code>, etc.</p> Saved Video <p>You can also open videos from your hard drive or from a website like Youtube . Simply enter the path to your video instead of <code>0</code>. Don't forget to read the docs.</p>"},{"location":"yolo/video/#step-3-read-frames","title":"Step 3: Read Frames","text":"<p>The <code>read()</code> method of the <code>VideoCapture</code> object retrieves each frame from the video stream:</p> <pre><code>ret, frame = cap.read()\n</code></pre> <ul> <li><code>ret</code>: Boolean indicating if the frame was captured successfully.</li> <li><code>frame</code>: The captured frame as a NumPy array.</li> </ul>"},{"location":"yolo/video/#step-4-display-frames","title":"Step 4: Display Frames","text":"<p>To display the captured frames in a window, use the <code>imshow()</code> method:</p> <p><pre><code>cv2.imshow('Capturing Video', frame)\n</code></pre> The first argument is the window title, and the second argument is the frame to display.</p>"},{"location":"yolo/video/#step-5-loop-and-exit","title":"Step 5: Loop and Exit","text":"<p>To continuously capture frames, use a <code>while</code> loop and break it based on user input. Use <code>cv2.waitKey()</code> to listen for key presses:</p> <p><pre><code>if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n    break\n</code></pre> This stops the loop when the <code>q</code> key is pressed.</p>"},{"location":"yolo/video/#step-6-release-resources","title":"Step 6: Release Resources","text":"<p>Release the video stream and close any OpenCV windows:</p> <pre><code>cap.release() # release the resource\ncv2.destroyAllWindows()     # closes all OpenCV windows\n</code></pre> Release Resources <p>Releasing resources at the end is crucial to avoid issues. If the resource (e.g., webcam) is not released, it may remain locked, preventing further connections. This issue can occur if an error interrupts your code, skipping the release command. In such cases, manually execute the release method before attempting to use the webcam again. Alternatively, restarting the kernel can also resolve the issue:</p>"},{"location":"yolo/video/#complete-program","title":"Complete Program","text":"<p>Here\u2019s the complete program to access your webcam and show the live feed. </p> <pre><code>import cv2\n\n# Define the video source (0 for webcam or path to a video file)\nvideo_source = 0  # Use \"video.mp4\" for a saved video\ncap = cv2.VideoCapture(video_source)\n\n# Process the video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Display the  frame\n    cv2.imshow(\"Video Analysis\", frame)\n\n    # Exit when 'q' is pressed\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\n# Release resources\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"yolo/video/#inference","title":"Inference","text":"<p>YOLO processes each video frame independently, making it suitable for real-time applications like detection, segmentation, or keypoint extraction. To use any of these vision algorithms, simply analyze each frame in sequence and display the annotated frame.</p> Task: Analyze a Video <ol> <li>Try to access the webcam and display the live video</li> <li>Use the YOLOv8 Nano model to perform a:<ul> <li>Detection</li> <li>Segmentation</li> <li>Pose Estimation</li> <li>Tracking </li> </ul> </li> <li>Visualize and save results for further analysis.</li> </ol>"},{"location":"yolo/video/solutions/","title":"YOLO Solutions","text":"Level up your computer vision skills with YOLO solutions! \ud83d\udd0d (Source: Ultralytics)  Ultralytics Solutions <p>This chapter introduces Ultralytics Solutions, a collection of  ready-to-use applications built on top of YOLO models. These solutions make it easier to implement  common computer vision tasks without extensive customization.</p> <p>We'll explore practical applications and create our first object counting system.</p>"},{"location":"yolo/video/solutions/#project-setup","title":"Project Setup","text":"<p>If you've followed the previous chapters, your project structure should look like this:</p> <pre><code>\ud83d\udcc1 computer_vision/\n    \u251c\u2500\u2500 \ud83d\udcc1 .venv/\n    \u251c\u2500\u2500 \ud83d\udcc1 pics/\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_detect.ipynb\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_segment.ipynb\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_keypoints.ipynb\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_video.ipynb\n    \u2514\u2500\u2500 \ud83d\udcc4 yolo_object_counting.ipynb\n</code></pre> <p>For this section, we added a new Jupyter notebook named <code>yolo_object_counting.ipynb</code> within the project to follow along.</p>"},{"location":"yolo/video/solutions/#object-counting","title":"Object Counting","text":"<p>One of the most practical applications of computer vision is counting objects in specific regions. This could be:</p> <ul> <li>Counting vehicles in traffic lanes</li> <li>Monitoring people in store sections</li> <li>Tracking inventory movement</li> <li>Analyzing crowd density in areas</li> </ul> <p>Let's implement a basic object counting system using YOLO and explore how to customize it for different scenarios.</p>"},{"location":"yolo/video/solutions/#basic-setup","title":"Basic Setup","text":"<p>Before we start counting, we need to define where we want to count objects. Therefore we need to know the frame size of our webcam or video. </p> <pre><code>import cv2\nfrom ultralytics import solutions\n\n# Define video source (0 for webcam)\nvideo_source = 0  # Change to video path for file\ncap = cv2.VideoCapture(video_source)\n\n# Get video properties\nw, h, fps = (int(cap.get(x)) for x in (\n    cv2.CAP_PROP_FRAME_WIDTH, \n    cv2.CAP_PROP_FRAME_HEIGHT, \n    cv2.CAP_PROP_FPS\n))\n\nprint(f\"Video properties: {w}x{h} @ {fps}fps\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Video properties: 640x480 @ 30fps\n</code></pre>"},{"location":"yolo/video/solutions/#define-region-of-interest","title":"Define Region of Interest","text":"<p>Now that we know the frame size, we can define a 'region of interest'. We start with a generic rectangle which has a distance of 20 pixels to all sides.</p> <pre><code># Define counting region (rectangle)\nregion_points = [\n    (20, h-20),     # Bottom left\n    (w-20, h-20),   # Bottom right\n    (w-20, 20),     # Top right\n    (20, 20)        # Top left\n]\n</code></pre>"},{"location":"yolo/video/solutions/#initialize-counter","title":"Initialize Counter","text":"<p>Now we'll set up the YOLO-based counter:</p> <pre><code># Initialize RegionCounter\ncounter = solutions.RegionCounter(\n    show=False,            # Show visualization\n    region=region_points,  # Our defined region\n    model=\"yolo11n.pt\",    # Use nano model for speed\n    classes=[0]            # Only count persons (class 0)\n)\n</code></pre>"},{"location":"yolo/video/solutions/#process-video","title":"Process Video","text":"<p>Let's create the main processing loop:</p> <pre><code>while cap.isOpened():\n    success, frame = cap.read()\n    if not success:\n        break\n\n    # Process frame and count objects\n    annotated_frame = counter.count(frame)\n\n    # Display results\n    cv2.imshow(\"Object Counting\", annotated_frame)\n\n    # Press 'q' to quit\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> What Counts to the Region? <p>If you run the code, you will see that YOLO counts the people in the region of interest. Only objects whose center point of the bounding box lies within the region are counted.</p>"},{"location":"yolo/video/solutions/#save-results","title":"Save Results","text":"<p>To save your counting results for later analysis you need to add the following lines</p> <ul> <li>Before the Main Loop:      <pre><code># Create video writer\noutput_path = 'counting_results.mp4'\nwriter = cv2.VideoWriter(\n    output_path,\n    cv2.VideoWriter_fourcc(*'mp4v'),\n    fps,\n    (w, h)\n)\n</code></pre></li> <li>In the Main Loop:     <pre><code>writer.write(annotated_frame)\n</code></pre></li> <li>After the Main Loop     <pre><code>writer.release()\n</code></pre></li> </ul> Experiment with Different Settings <p>Perform an object counting in region with the above code by accessing your webcam.  Try these modifications to enhance your counter:</p> <ol> <li> <p>Change the size of the region.</p> </li> <li> <p>Change the counting region shape: <pre><code># Triangle region\nregion_points = [\n    (w//2, 100),    # Top\n    (50, h-100),    # Bottom left\n    (w-50, h-100)   # Bottom right\n]\n</code></pre></p> </li> <li> <p>Count different objects: <pre><code># Count multiple classes\ncounter = solutions.RegionCounter(\n    classes=[0, 2, 3]  # Person, car, motorcycle\n)\n</code></pre></p> </li> <li>Save the annotated video with object counts for later analysis.</li> </ol> <p>What other modifications could make this more useful for your needs?</p>"},{"location":"yolo/video/solutions/#recap","title":"Recap","text":"<p>In this chapter, we've learned how to:</p> <ul> <li>Set up a basic object counting system</li> <li>Define custom regions of interest</li> <li>Process video streams in real-time</li> <li>Save and analyze counting results</li> </ul> <p>Next, we'll explore how we can train our own models. </p> (Source: Medium)"}]}