{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"blog/","title":"Projects &amp; Blog","text":"<p>Want to know what we\u2019re working on when we\u2019re not teaching? In these short posts, we share our latest projects, along with the tools we're developing.  Along the way, we sprinkle in some tips and lessons learned that will hopefully help YOU in your development journey. </p>"},{"location":"blog/2025/08/18/landslides/","title":"Master Thesis - Landslide Pipeline","text":"<p>TL;DR: Landslide Pipeline is a tool that lets you select a place and time and run a trained Machine Learning model to identify landslides using satellite images. It is designed to also be accessible for non-developers.  </p> <p> </p> Info <p>This project was completed as a Master's thesis supervised by Jakob Klotz. The post was written by the student, Armin Muzaferovic.</p>","tags":["Master Thesis","Landslides","GUI","Machine Learning"]},{"location":"blog/2025/08/18/landslides/#what-is-landslide-pipeline","title":"What is Landslide Pipeline?","text":"<p>In short, the repo contains all the necessary code to select a time and place using a GUI to then be able to detect landslides with the selected parameters. The U-Net model in the background was trained using the landslide4sense baseline model and the provided dataset. GUI was done using CustomTKinter as well as TkinterMap.</p>","tags":["Master Thesis","Landslides","GUI","Machine Learning"]},{"location":"blog/2025/08/18/landslides/#how-does-it-work","title":"How does it work?","text":"<p>When selecting the area on the provided map in the GUI, a request is sent to the ESA Copernicus API with the corresponding parameters. In the query to the API, 12 Layers of the Sentinel2-L1C Satellite and the Copernicus30 DEM Data are requested. After receiving the data and calculating the slope from the digital  elevation model, the data is formatted for use with the machine learning model. After user confirmation, the data for each pixel is evaluated to check if there is a landslide detected or not.</p>","tags":["Master Thesis","Landslides","GUI","Machine Learning"]},{"location":"blog/2025/08/18/landslides/#why-landslide-pipeline","title":"Why Landslide Pipeline?","text":"<p>While there are already great models for landslide detection, the next step is to make it easier to generate training data or to check whether a specific  region has experienced a landslide. The goal of this implementation was to  simplify future research in the field of machine learning with satellite data,  making it easier to use available datasets. For example, city administrations  could use this tool to check their areas for possible landslide risks.</p>","tags":["Master Thesis","Landslides","GUI","Machine Learning"]},{"location":"blog/2025/08/18/landslides/#how-to-use-it","title":"How to use it?","text":"<p>Simply visit GitHub - muzaferovarmin/LandslideMA and either download the precompiled executable file for your OS or clone the repo and build the Python  code yourself! After that, use your free Copernicus Dataspace account (which can be created here) to generate your API credentials to use and you are ready to go!</p>","tags":["Master Thesis","Landslides","GUI","Machine Learning"]},{"location":"blog/2025/08/08/md-snakeoil/","title":"A exercise in Python packaging: <code>md-snakeoil</code>","text":"<p>TL;DR: Have you ever tried to read code in a document, only to find it messy and hard to follow? I often write guides and lecture materials in Markdown, but keeping all the code examples tidy can be a real challenge. That\u2019s why I created  md-snakeoil, a tool that automatically cleans up Python code, making  everything look neat. </p> <p> </p>","tags":["Project Showcase","Python Packaging","PyPI"]},{"location":"blog/2025/08/08/md-snakeoil/#about-the-project","title":"About the project","text":"<p>md-snakeoil helps you keep your code examples looking their best. If you  write notes, tutorials or documentation that includes Python code, this tool  will make sure all your code blocks are consistently styled and easy to read\u2014no more messy formatting! It was written purely in Python   and can be easily installed with your favorite package manager.</p> Package Manager Command <code>pip</code> <code>pip install md-snakeoil</code> <code>pipx</code> (as tool) <code>pipx install md-snakeoil</code> <code>uv</code> (as package) <code>uv add md-snakeoil</code> <code>uv</code> (as tool) <code>uv tool install md-snakeoil</code>","tags":["Project Showcase","Python Packaging","PyPI"]},{"location":"blog/2025/08/08/md-snakeoil/#before-and-after","title":"Before and After","text":"<p>Here\u2019s a quick visual example of what md-snakeoil can do:</p> <p> </p> <p> Left: Before using <code>md-snakeoil</code> \u00a0 | \u00a0 Right: After using <code>md-snakeoil</code> </p> <p>The import statements get sorted, unnecessary new lines and whitespace is  removed. Although this might be an exaggerated example, it illustrates the kind of improvements md-snakeoil can make.</p> <p>Whole directories can be processed in one go (a single command), making it easy to maintain consistency across large projects. </p> Info <p>With md-snakeoil installed, use</p> <pre><code>snakeoil path/to/directory\n</code></pre> <p>to process all files in the directory.</p>","tags":["Project Showcase","Python Packaging","PyPI"]},{"location":"blog/2025/08/08/md-snakeoil/#why-did-i-build-it","title":"Why did I build it?","text":"<p>I write a lot of technical documentation in Markdown which includes a lot of  code examples. Keeping a consistent style across these code blocks can be a challenge, especially when working with multiple files or collaborating with others. I wanted a tool that could take care of that. Since there was no  existing solution that met my needs, I simply wrote one myself.</p>","tags":["Project Showcase","Python Packaging","PyPI"]},{"location":"blog/2025/08/08/md-snakeoil/#what-did-i-learn","title":"What did I learn?","text":"<p>The development process taught me a lot about Python packaging, such as  structuring and packaging the project with <code>uv</code>  (see the previous post on <code>uv</code>). I gained more experience setting  up automated tests and managing releases using GitHub Actions. Additionally,  through code contributions from others, the project has grown and improved!</p> <p>All that left me with a blueprint for</p> <ul> <li>A workflow to publish the package on PyPI</li> <li>Documentation for users and contributors</li> <li>Automated testing</li> </ul> Tip <p>Feel free to skim through the codebase and copy bits and pieces useful for  your own projects.</p>","tags":["Project Showcase","Python Packaging","PyPI"]},{"location":"blog/2025/08/08/md-snakeoil/#try-it-out","title":"Try it out!","text":"<p>If you\u2019re curious, you can check out md-snakeoil on GitHub or PyPI.</p> <ul> <li> GitHub Repository</li> <li> PyPI Package</li> </ul> <p>Want to know more about the development process or the tools involved to build something similar? Feel free to reach out! </p>","tags":["Project Showcase","Python Packaging","PyPI"]},{"location":"blog/2025/08/07/uv/","title":"<code>uv</code>: My favourite Python package manager","text":"<p>TL;DR: <code>uv</code> simplifies the whole development  experience. I use it daily to manage my Python projects and it has become an  essential part of my workflow. A brief overview why <code>uv</code> is great...</p>","tags":["Package Management","Python Development"]},{"location":"blog/2025/08/07/uv/#what-is-uv","title":"What is <code>uv</code>?","text":"<p>It lets you manage your Python packages. But not just that, with <code>uv</code> you  can also manage your Python installations. On top of that, it is way faster than other package managers like <code>pip</code>.</p>","tags":["Package Management","Python Development"]},{"location":"blog/2025/08/07/uv/#why-uv","title":"Why <code>uv</code>?","text":"<p>Setting up a new development environment has never been easier. On a new machine, I can simply install <code>uv</code> which is just a single command. Having <code>uv</code> installed, run:</p> <pre><code>uv python list\n</code></pre> <p>... to list all available Python versions. With another command, for example:</p> <pre><code>uv python install 3.13\n</code></pre> <p>... I can install the latest Python version. So with just 2 commands, I have a working Python development environment on my  machine. There's no need to install Python manually anymore. </p> Tip <p>With</p> <pre><code>mkdir my_project &amp;&amp; cd my_project\nuv init  #(1)!\nuv sync  #(2)!\n</code></pre> <p>you can scaffold a new Python project and set up the virtual environment.  You can start developing right away.</p> <ol> <li><code>uv init</code> creates a new project with a default configuration.</li> <li><code>uv sync</code> sets up the virtual environment.</li> </ol>","tags":["Package Management","Python Development"]},{"location":"blog/2025/08/07/uv/#another-example","title":"Another example","text":"<p>Most of the time, programming is a team sport. Working with others on the same codebase can be challenging, especially when it comes to managing dependencies. If you set up your project with <code>uv</code>, another developer can easily install your project, it's as simple as:</p> <pre><code>uv sync\n</code></pre> Info <p><code>uv sync</code> reads the <code>pyproject.toml</code> and <code>uv.lock</code> files to install the  required dependencies and sets up the virtual environment. If the required Python version is not installed, <code>uv</code> will automatically install it.</p>","tags":["Package Management","Python Development"]},{"location":"blog/2025/08/07/uv/#wrap-up","title":"Wrap up","text":"<p>Although this introductory post is just a brief overview, I hope it gives you  an idea of how <code>uv</code> can simplify your Python development workflow. In  subsequent posts, I will dive deeper into specific features and use cases of  <code>uv</code>. Until then, you can take a look at a couple of projects that use <code>uv</code>:</p> <ul> <li>This site you're reading right now is built with <code>uv</code> - code.</li> <li>Two Python packages I develop, first and second</li> <li>An app to assess rainfall-triggered landslide risks - here.</li> </ul>","tags":["Package Management","Python Development"]},{"location":"data-science/","title":"Home","text":"<p>Welcome to the <code>Data Science Course</code>! </p> <p>This course is designed to provide you with a comprehensive introduction to the field of data science. It is structured into four blocks, each focusing on a different aspect. All concepts and techniques are demonstrated with code examples!</p>"},{"location":"data-science/#course-overview","title":"Course overview","text":"<ol> <li>Basics: Introduction to various terms (data science, machine learning, etc.)     and data basics such as attribute types.</li> <li>Data Preparation &amp; Preprocessing: Data cleaning, integration, and     transformation.</li> <li>Supervised vs. Unsupervised Learning: Exploration of both terms, and     coverage of various algorithms.</li> <li>Data Science in Practice: Step-by-step guide to a real-world data science     project.</li> </ol>"},{"location":"data-science/#tools","title":"Tools","text":"<p>As always, we use Python and these great packages </p>"},{"location":"data-science/#sneak-peek","title":"Sneak peek","text":"<p>Here is a sneak peek of selected topics we cover in this course:</p> <ul> <li> <p> Decision trees</p> <p>What are decision trees? How do they work?</p> </li> <li> <p> </p> </li> <li> <p> </p> </li> <li> <p> Recommender system</p> <p>With a large Spotify data set, we build a recommender system. At the end you will be able to recommend songs.</p> </li> <li> <p> Elbow method</p> <p>We introduce the elbow method and how it can help us to (for example) refine the recommender system.</p> </li> <li> <p> </p> </li> <li> <p> </p> </li> <li> <p> Data Science in Practice</p> <p>We present a step-by-step guide to a real-world data science project. The project applies concepts, algorithms and techniques introduced up to this point.</p> <p>... oh, and along the way we cover a different type of transformer.</p> </li> </ul>"},{"location":"data-science/#expected-outcome","title":"Expected outcome","text":"<p>By the end of this course, you will have a solid understanding of the data science field. Additionally, you will be capable of preprocessing real-world data, selecting and applying appropriate algorithms to solve practical problems.</p> Let's get started! \ud83d\ude80"},{"location":"data-science/algorithms/","title":"Introduction","text":"<p>With extensive data preparation knowledge, we can tackle the next big part of the course: algorithms. An algorithm is a</p> <p>a set of mathematical instructions or rules that, especially if given to a computer, will help to calculate an answer to a problem.</p> <p>Cambridge Dictionary</p> <p>In data science/machine learning, algorithms are used to solve problems, such as modelling data to make predictions for unseen data, or clustering data to find patterns.</p> <p>The consecutive chapters will introduce you to common algorithms, like linear and logistic regression, decision trees and k-means clustering. We will explore the theory as well as practical examples. First, we establish two main concepts in machine learning: supervised and unsupervised learning.</p>"},{"location":"data-science/algorithms/#supervised-learning","title":"Supervised Learning","text":"<p>Supervised learning is a type of machine learning where algorithms learn from labeled training data to make predictions on new, unseen data. The term \"supervised\" comes from the idea that the algorithm is guided by a \"supervisor\" (the labeled data) that provides the correct answers during training.</p> <p>In supervised learning, each training example consists of:</p> <ul> <li>Input features (\\(X\\)): The characteristics or attributes we use to make     predictions</li> <li>Target variable (\\(y\\)): The correct output we want to predict</li> </ul> <p>The algorithm learns the relationship between inputs (\\(X\\)) and outputs (\\(y\\)), creating a model that can then (hopefully!) generalize to new data.</p>"},{"location":"data-science/algorithms/#example","title":"Example","text":"<p>Assume we want to predict apartment prices (\\(y\\)) based on apartment size and number of rooms (\\(X\\)):</p> <pre><code>from sklearn.linear_model import LinearRegression\n\n# apartment data [m\u00b2, rooms]\nX = [\n    [75, 3],\n    [120, 4],\n    [50, 2],\n]\n# apartment prices\ny = [500_000, 675_000, 425_000]  # (1)!\n\n# use linear regression to predict apartment prices\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# predict price for a new apartment with 150m\u00b2 and 5 rooms\nnew_apartment = [[150, 5]]\npredicted_price = model.predict(new_apartment)\n</code></pre> <ol> <li>Underscores can be used as visual separators in numeric literals to improve     readability. They have no effect on the value of the number. For example,     <code>500_000</code> is the same as <code>500000</code>.</li> </ol> <p>For each new observation, we can use the trained model to predict the price. The apartment with 150m\u00b2 and 5 rooms has a predicted price of <code>775000</code>.</p> Info <p>Whether this estimate is actually close to reality depends on the quality of the model and its underlying data. Later, we will discuss how to measure a model's quality.</p>"},{"location":"data-science/algorithms/#classification-vs-regression","title":"Classification vs. Regression","text":"<p>Supervised learning encapsulates both classification and regression tasks.</p> <pre><code>graph LR\n  A[Supervised Learning] --&gt; B[Classification];\n  A --&gt; C[Regression];</code></pre>"},{"location":"data-science/algorithms/#classification","title":"Classification","text":"<p>Classification problems involve predicting discrete categories or labels. The output is always one of a fixed set of classes. For instance, in binary classification, the model decides between two possibilities.</p> <p>For example, the Portuguese retail bank data can be used to predict whether a customer would subscribe to a term deposit. The target variable is binary: yes or no.</p> <p>On the other hand, multiclass classification handles three or more categories (like classifying animals in photos  dog, cat, dolphin, tiger, elephant, etc.).</p>"},{"location":"data-science/algorithms/#regression","title":"Regression","text":"<p>Regression problems, on the other hand, predict continuous numerical values. Instead of categorizing input into classes, regression models estimate a numerical value along a continuous spectrum. These models work by finding patterns in the data to estimate a mathematical function that best describes the relationship between input features and the target variable.</p> <p>For instance the example, predicting the price of an apartment based on its size and the number of rooms is a regression task.</p>"},{"location":"data-science/algorithms/#examples","title":"Examples","text":"<ul> <li> <p>Classification</p> <p>Predicting a categorical target variable:</p> <ul> <li>Spam or not spam</li> <li>Fraudulent or legitimate transaction</li> <li>Medical diagnosis (disease or no disease)</li> <li>Sentiment analysis of text (positive, negative, neutral)</li> <li>Image classification (cat, dog, dolphin, etc.)</li> <li>...</li> </ul> </li> <li> <p>Regression</p> <p>Predicting a continuous target variable:</p> <ul> <li>Apartment prices (like in the example above)</li> <li>Temperature</li> <li>Sales revenue</li> <li>...</li> </ul> </li> </ul> Info <p>No matter if you're dealing with a classification or regression task, the key to successful supervised learning lies in having high-quality labeled data and selecting appropriate features (variables) that have predictive power for the target variable.</p>"},{"location":"data-science/algorithms/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Contrary, unsupervised learning deals with unlabeled data to discover hidden patterns and structures. Unlike supervised learning, there is no \"supervisor\" providing correct answers. The algorithm tries to find meaningful patterns on its own.</p> <p>In unsupervised learning, we solely have:</p> <ul> <li>Input features (\\(X\\)): The characteristics or attributes of the data</li> </ul> <p>The algorithm's task is to find groupings, reduce complexity or reveal underlying structures in the data.</p>"},{"location":"data-science/algorithms/#example_1","title":"Example","text":"<p>Let's say we want to segment customers based on their shopping behavior:</p> <pre><code>from sklearn.cluster import KMeans\n\n# customer data [annual_spending, avg_basket_size]\nX = [[1200, 50], [5000, 150], [800, 30], [4500, 140], [1000, 45]]\n\n# use k-means to find customer segments\nmodel = KMeans(n_clusters=2, random_state=42)  # (1)!\nsegments = model.fit_predict(X)\n\nprint(segments)\n</code></pre> <ol> <li>Setting the <code>random_state</code> parameter ensures that you always get the same     results when executing the code repeatedly. Reproducibility is discussed     more in-depth in upcoming chapters.</li> </ol> &gt;&gt;&gt; Output<pre><code>[1 0 1 0 1]\n</code></pre> <p>The variable <code>segments</code> contains the cluster assignments for each customer. The cluster assignment is simply an <code>int</code> indicating which group the customer belongs to. In this example, we have two clusters with the first customer (<code>[1200, 50]</code>) belonging to cluster 1 and the second customer (<code>[5000, 150]</code>) to cluster 0 and so on.</p> <p>The following plot visualizes the input data as scatter plot colored by the cluster assignments:</p>          Similar data points are grouped to a cluster. Cluster 0 in the left          corner represents the first customer segment and cluster 1 in the right         corner the second.      <p>The algorithm will group similar customers together without being told what these groups should be, it discovers the patterns based on attributes.</p>"},{"location":"data-science/algorithms/#clustering-dimensionality-reduction","title":"Clustering &amp; Dimensionality Reduction","text":"<p>Unsupervised learning can be further divided into two main categories:</p> <pre><code>graph LR\n  A[Unsupervised Learning] --&gt; B[Clustering];\n  A --&gt; C[Dimensionality Reduction];</code></pre>"},{"location":"data-science/algorithms/#clustering","title":"Clustering","text":"<p>Clustering algorithms group similar data points together based on their features. The goal is to find cluster/groups in the data without any prior knowledge of the groups just like in the previous customer segmentation example.</p>"},{"location":"data-science/algorithms/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction techniques aim to reduce the number of input features while preserving the most important information. This can help to simplify complex data, speed up algorithms and improve model performance.</p>"},{"location":"data-science/algorithms/#examples_1","title":"Examples","text":"<ul> <li> <p>Clustering</p> <p>Clustering/grouping of similar data points:</p> <ul> <li>Customer segmentation in marketing (like in the example above)</li> <li>Anomaly detection</li> <li>Product recommendations</li> <li>...</li> </ul> </li> <li> <p>Dimensionality Reduction</p> <p>Reducing data complexity:</p> <ul> <li>Feature extraction from high-dimensional data</li> <li>Visualization of complex datasets</li> <li>Noise reduction in signals</li> <li>...</li> </ul> </li> </ul> Info <p>While unsupervised learning offers powerful ways to explore and understand data, its results can be harder to evaluate since there are no \"correct\" answers to compare against. The value of the results often depends on how meaningful the discovered patterns are for the specific application.</p> Domain knowledge <p>No matter if you're dealing with supervised or unsupervised learning, domain knowledge is crucial. Understanding the data and the problem you're trying to solve will help you select the right algorithms, features and interpret the results.</p>"},{"location":"data-science/algorithms/#recap","title":"Recap","text":"<p>This chapter introduced two fundamental concepts in machine learning, supervised and unsupervised learning:</p> Concept Data Task Goal Supervised Learning Labeled (\\(X\\), \\(y\\)) Regression Predict continuous values Classification Predict categories Unsupervised Learning Unlabeled (\\(X\\)) Clustering Group similar data Dimensionality Reduction Reduce data complexity <p>The following chapters will cover algorithms for each task with theory and practical examples.</p>"},{"location":"data-science/algorithms/supervised/classification/","title":"Classification","text":""},{"location":"data-science/algorithms/supervised/classification/#logistic-regression","title":"Logistic Regression","text":"<p>While linear regression helps us predict continuous values, other real-world problems require predicting categorical outcomes: Will a customer subscribe to a term deposit? Is an email spam? Is a transaction fraudulent? Logistic regression addresses these binary classification problems by extending the concepts we learned in linear regression to predict probabilities between 0 and 1.</p> <p>We will cover the theory and apply logistic regression to the breast cancer dataset to predict whether a tumor is malignant or benign.</p>"},{"location":"data-science/algorithms/supervised/classification/#theory","title":"Theory","text":"Info <p>The theoretical part is adapted from:</p> <p>Daniel Jurafsky and James H. Martin. 2025. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models<sup>1</sup></p>"},{"location":"data-science/algorithms/supervised/classification/#deja-vu-linear-regression","title":"Deja vu: Linear regression","text":"<p>Just like in linear regression, we have a set of features \\(x_1, x_2, ..., x_n\\) describing an outcome \\(y\\). But instead of predicting a continuous value, \\(y\\) is binary: 0 or 1.</p> <p>Similar to linear regression, logistic regression uses a linear combination of the features to predict the outcome. I.e., each feature is assigned a weight, and a bias term is added at the end.</p> Linear combination \\[ z = b_1 \\cdot x_1 + b_2 \\cdot x_2 + ... + b_n \\cdot x_n + a \\] <p>with \\(a\\) being the bias term and \\(b_1, b_2, ..., b_n\\) the weights. \"The resulting single number \\(z\\) expresses the weighted sum of the evidence for the class.\" (Jurafsky &amp; Martin, 2025 p. 79) Bias, weights and the intercept are all real numbers.</p> <p>So far, logistic regression is the same as linear regression with the sole difference that in linear regression we referred to the bias \\(a\\) as the intercept, and the weights \\(b_1, b_2, ..., b_n\\) as coefficients or slope.</p> <p>However, \\(z\\) is not the final prediction, since it can take real values and in fact ranges from \\(-\\infty\\) to \\(+\\infty\\). Thus, \\(z\\) needs to be transformed to a probability between 0 and 1. This is where the sigmoid function comes into play.</p>"},{"location":"data-science/algorithms/supervised/classification/#the-sigmoid-function","title":"The sigmoid function","text":"<p>Unlike linear regression, which outputs unbounded values, logistic regression uses the sigmoid (or logistic) function to transform \\(z\\) into a probability</p> Sigmoid function \\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\] <p>The sigmoid function takes the real number \\(z\\) and transforms it to the range (0,1).</p>          An illustration of the sigmoid function often referred to as          logistic function. Thus, the name logistic regression.      <p>For given input features \\(x_1, x_2, ..., x_n\\), we can calculate the linear combination \\(z\\) and then apply the sigmoid function to get the probability of the outcome. To compute the probability of the outcome being 1  \\(P(y=1|x)\\), for example if an email is spam, we have to set a decision boundary.</p> Decision boundary <p>If \\(\\sigma(z) \\gt 0.5\\), we predict \\(y=1\\), otherwise \\(y=0\\).</p> <p>For instance, if the probability of an email being spam is 0.7, we predict that the email is spam \\((0.7 \\gt 0.5)\\). With a probability of 0.4, we predict that the email is not spam \\((0.4 \\le 0.5)\\).</p>"},{"location":"data-science/algorithms/supervised/classification/#the-optimization-problem","title":"The optimization problem","text":"<p>But how do we find the best parameter combination (weights and bias) for our logistic regression model? Unlike linear regression, which uses ordinary least squares, logistic regression typically uses Maximum Likelihood Estimation (MLE), i.e., the best parameters (weights and bias) that maximize the likelihood of the observed data.</p> Lo and behold, even more math... <p>For optimization purposes we use the negative log-likelihood as our loss function:</p> Negative log-likelihood \\[ J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m [y_i\\log(\\sigma(z_i)) + (1-y_i)\\log(1-\\sigma(z_i))] \\] <p>with:</p> <ul> <li>\\(m\\) as the number of training examples</li> <li>\\(y_i\\) being the the actual class (0 or 1)</li> <li>\\(\\sigma(z_i)\\) is the predicted probability using the sigmoid function</li> <li>\\(\\theta\\) represents all parameters (weights and bias)</li> </ul> Tip <p>Intuitively speaking, the loss function penalizes the model for making wrong predictions. If the model predicts a probability of 0.9 for a spam email, and the email is actually spam (\\(y=1\\)), the loss is small. On the other hand, if the model predicts a probability of 0.1 for a spam email, and the email is spam (\\(y=1\\)), the loss will be high.</p> <p>The weights are gradually adjusted to minimize the loss. Think of it like turning knobs slowly until we get better predictions.</p> <p>Gradually adjusting these knobs to minimize the loss is referred to as gradient descent.</p> <p>Conveniently, <code>scikit-learn</code> provides a logistic regression implementation that takes care of the optimization for us. Finally, we look at a practical example to see logistic regression in action.</p>"},{"location":"data-science/algorithms/supervised/classification/#example","title":"Example","text":"<p>Let's apply logistic regression to the breast cancer dataset, a classic binary classification problem where we need to predict whether a tumor is malignant or benign based on various features.</p> <p>With class labels \\(y\\) being 0 (malignant) or 1 (benign), we can use logistic regression to predict the probability of a tumor being benign. The features were calculated from digitized images of a breast mass.</p> Info <p>See the UCI Machine Learning Repository for more information on the data set.<sup>2</sup></p>"},{"location":"data-science/algorithms/supervised/classification/#load-the-data","title":"Load the data","text":"<p>Conveniently, <code>scikit-learn</code> provides a couple of data sets for both regression and classification tasks. One of them is the breast cancer dataset.</p> <pre><code>from sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\n\n# count the number of malignant and benign tumors\nprint(y.value_counts())\n</code></pre> &gt;&gt;&gt; Output<pre><code>target\n1    357\n0    212\n</code></pre> <p>In total, the data contains 569 samples with 357 benign and 212 malignant tumors.</p> Tip <p>Just like in the previous chapter, the data is divided into <code>X</code>, containing the attributes and <code>y</code> holding the corresponding labels. Having attributes and labels separated, makes life a bit easier when training and testing the model.</p> Number of features <p>Investigate the <code>DataFrame</code> <code>X</code> to answer the below quiz question.</p> # <p>How many features (attributes) does the breast cancer dataset have?</p> 302932 <p><code>X.shape</code> reveals that we are dealing with 30 features.</p>"},{"location":"data-science/algorithms/supervised/classification/#split-the-data","title":"Split the data","text":"<p>Before training our model, we want to split our data into two parts. Just like in the previous chapter, we perform a 80/20 split, i.e., we use 80% to train the model and evaluate it on the remaining 20%.</p> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, shuffle=True\n)\n</code></pre> Tip <p>If you need a refresh on the parameters used in <code>train_test_split()</code> revisit, the Split the data section from the previous chapter.</p>"},{"location":"data-science/algorithms/supervised/classification/#train-the-model","title":"Train the model","text":"<p>Now that we have our training data, we can train the logistic regression model.</p> <pre><code>from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(random_state=42, max_iter=5_000)  # (1)!\nmodel.fit(X_train, y_train)\n</code></pre> <ol> <li>The <code>random_state</code> parameter ensures reproducibility, while <code>max_iter</code>     specifies the maximum number of iterations taken for the solver to     converge (i.e., solving the optimization problem to find the best     parameter combination).</li> </ol> <p><code>model=LogisticRegression(...)</code> creates an instance of the logistic regression model. Only after calling the <code>fit()</code> method, the <code>model</code> is actually trained. Since we separated attributes and labels into <code>X_train</code> and <code>y_train</code> respectively, we can directly call the method without any further data handling.</p>"},{"location":"data-science/algorithms/supervised/classification/#weights-and-bias","title":"Weights and bias","text":"<p>With a trained model at hand, we can look at the weights \\((b_1, b_2, ..., b_n)\\) and bias \\((a)\\).</p> <pre><code>print(f\"Model weights: {model.coef_}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Model weights: [[ 0.98208299  0.22519686 -0.36688444  0.0262268 ... ]]\n</code></pre> <p>The <code>coef_</code> attribute contains the weight for each feature. As discussed, the weights are real numbers.</p> You might not have the exact same results <p>Your model weights might differ slightly from the ones shown above. This is completely normal and happens because:</p> <p>Numerical precision: The default optimization solver (<code>\"lbfgs\"</code>) behind <code>LogisticRegression</code> encounters tiny hardware-specific variations. The underlying libraries handle floating-point arithmetic differently across hardware platforms. During the iterative optimization, these tiny rounding differences accumulate, causing the solver to converge to slightly different solutions.</p> <p> These small differences don't affect your model's predictions or accuracy.</p> <p>Now, it's your turn to look at the bias.</p> Model bias <ol> <li>Open the <code>scikit-learn</code> docs on the     <code>LogisticRegression</code>     class.</li> <li>Find out how to access the bias term of the model.</li> <li>Simply print the bias term of the model.</li> </ol> <p> Remember, the bias is often referred to as intercept.</p>"},{"location":"data-science/algorithms/supervised/classification/#predictions","title":"Predictions","text":"<p>Since, the main purpose of a machine learning model is to make predictions, we will do just that.</p> <p>Predicting, is as simple as using the <code>predict()</code> method. We will use the patient measurements of the test set - <code>X_test</code>.</p> <pre><code>y_pred = model.predict(X_test)\n\n# first 5 predictions\nprint(y_pred[:5])\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1 0 0 1 1]\n</code></pre> <p>Congratulations, you just build a machine learning model to predict breast cancer. But how good is the model? To conclude the chapter, we will briefly evaluate the model's performance.</p>"},{"location":"data-science/algorithms/supervised/classification/#evaluate-the-model","title":"Evaluate the model","text":"<p>Surely, we could just manually compare the predictions (<code>y_pred</code>) with the actual labels (<code>y_test</code>) and evaluate how often the model was correct. Or instead, we can leverage another method called <code>score()</code>.</p> <pre><code>score = model.score(X_test, y_test)\n</code></pre> <p>First, the <code>score()</code> method takes <code>X_test</code> and makes the corresponding predictions and programmatically compares the predictions with the actual labels <code>y_test</code>. <code>score()</code> returns the accuracy  the proportion of correctly classified instances.</p> <pre><code>print(f\"Model accuracy: {round(score, 4)}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Model accuracy: 0.9561\n</code></pre> <p>In our case, the model correctly classified 95.61% of the test set. In other words, in 95.61% of instances, the model was able to correctly predict if a tumor is malignant or benign.</p> Tip <p>As the test set (both attributes and labels) were never used to train the model, the accuracy is a good indicator of how well the model generalizes to unseen data.</p>"},{"location":"data-science/algorithms/supervised/classification/#recap","title":"Recap","text":"<p>We covered logistic regression, a popular algorithm for binary classification.</p> <p>Upon discussing the theory, we discovered similarities to linear regression in regard to the linear combination of features. With the help of the sigmoid function, we transformed the linear combination into probabilities between 0 and 1.</p> <p>Subsequently, we trained a logistic regression model on the breast cancer data to predict whether a tumor is malignant or benign. To evaluate the model we split the data and finally calculated the accuracy.</p> Info <p>In subsequent chapters we will explore more sophisticated ways to split data and evaluate models.</p> <p>Next up, we will dive into algorithms, like decision trees and random forest, that can handle both regression and classification problems.</p> <ol> <li> <p>3rd edition. Online manuscript released January 12, 2025. https://web.stanford.edu/~jurafsky/slp3 \u21a9</p> </li> <li> <p>Wolberg, W., Mangasarian, O., Street, N., &amp; Street, W. (1993). Breast Cancer Wisconsin (Diagnostic) [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5DW2B.\u00a0\u21a9</p> </li> </ol>"},{"location":"data-science/algorithms/supervised/regression/","title":"Regression","text":""},{"location":"data-science/algorithms/supervised/regression/#linear-regression","title":"Linear Regression","text":"<p>In machine learning, we often want to predict continuous numerical values, like house prices, temperatures or sales figures. Linear regression also knows as Ordinary Least Squares (OLS) provides a foundational approach to this problem by modeling the relationship between input variables and a target variable using a straight line.</p> <p>This chapter introduces linear regression through a hands-on example. You'll learn to:</p> <ul> <li>Build and train a linear regression model</li> <li>Interpret model parameters (intercept and coefficients)</li> <li>Make predictions with new data</li> <li>Evaluate model performance using the coefficient of determination (\\(R^2\\))</li> <li>Get familiar with the <code>scikit-learn</code> workflow to train and evaluate models</li> </ul> Info <p>This chapter adapts and expands upon:</p> <ul> <li>scikit-learn: Ordinary Least Squares and Ridge Regression</li> <li>scikit-learn: Linear Models </li> <li>scikit-learn: Metrics and scoring: quantifying the quality of predictions</li> </ul>"},{"location":"data-science/algorithms/supervised/regression/#theory","title":"Theory","text":"<p>Linear regression, also known as Ordinary Least Squares (OLS), models the relationship between a continuous target variable \\(y\\) and one or more input variables \\(X\\). The goal is to find the best linear function that predicts \\(\\hat{y}\\) from \\(X\\).</p> Linear combination \\[ \\hat{y} = w_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n \\] <p>where:</p> <ul> <li>\\(w_0\\) is the intercept (bias term)</li> <li>\\(w_1, w_2, ..., w_n\\) are the coefficients (weights)</li> <li>\\(x_1, x_2, ..., x_n\\) are the input features</li> </ul> <p>The term \"Ordinary Least Squares\" refers to the optimization objective, finding the weights \\(w_0, w_1, ..., w_n\\) that minimize the sum of squared differences called residuals between the actual values \\(y\\) and predicted values \\(\\hat{y}\\).</p> Cost function \\[ \\text{min} \\quad \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\] <p>where \\(n\\) is the number of observations.</p> <p>This minimization ensures that our model makes the smallest possible errors on average when predicting the training data. Let's look at an example.</p>"},{"location":"data-science/algorithms/supervised/regression/#example","title":"Example","text":"<p><code>scikit-learn</code> provides a couple of data sets for download. To fit a linear regression on a real-world example, we choose the California housing data set. More information about the California Housing data set can be found here.</p> Info <p>Data reference:</p> <p>Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33:291-297, 1997</p> <p>Our objective is to model the target variable \\(y\\) using input variables \\(X\\). In this case, \\(y\\) corresponds to the median house value, expressed in hundreds of thousands of dollars ($100,000). Below figure shows all houses in California colored by their median value \\(y\\).</p>          California median house values (in $100,000): Higher values are          concentrated along the coast and in major urban centers such as          San Francisco and Los Angeles."},{"location":"data-science/algorithms/supervised/regression/#load-the-data","title":"Load the data","text":"<p>Start by loading the data:</p> <pre><code>from sklearn.datasets import fetch_california_housing\n\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\nprint(X.head())\n</code></pre> <p>Conveniently, by setting <code>return_X_y=True</code>, the function splits the input variables \\(X\\) and the target \\(y\\). Note, \\(X\\) contains multiple input variables such as such as house age HouseAge and average bedrooms AveBedrms. However, some are not needed.</p> Question <p>The data frame <code>X</code> contains the variables <code>\"Latitude\"</code> and <code>\"Longitude\"</code>, remove them from the data frame. Tip: Consult the pandas documentation.</p>"},{"location":"data-science/algorithms/supervised/regression/#split-the-data","title":"Split the data","text":"<p>Before training our OLS model, we need to split our data into two distinct sets:</p> <ul> <li>Training set: Used to fit the model and learn the optimal weights (80% of     data)</li> <li>Test set: Used to evaluate the model's performance on unseen data (20% of     data)</li> </ul> <p>Think of this like preparing for an exam: you study from practice problems (training set) and then test your knowledge with new questions (test set). This separation allows us to assess whether our model can accurately predict house prices it hasn't seen before, rather than just memorizing the training data.</p> Info <p>The 80/20 split is a common convention, but not a strict rule. Depending on the data set size, other split ratios might be a better fit.</p> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, shuffle=True\n)\n</code></pre> <p>Let's break the code snippet down:</p> <ol> <li><code>train_test_split()</code> takes the complete data set (<code>X</code> and <code>y</code>) as input</li> <li>Splits off 20% for testing (<code>test_size=0.2</code>)</li> <li>Randomly shuffles the data (<code>shuffle=True</code>) to remove any inherent     ordering</li> <li>Set a seed (<code>random_state=42</code>) which ensures the same outcome every     time the code snippet is executed. Since the shuffle operation is     stochastic, we aim for reproducibility.</li> </ol> Why shuffle? <p>Some data sets may have inherent order (e.g., the houses could be sorted by location). Shuffling ensures that both training and test sets are representative of the entire data distribution.</p> <p>After splitting, we put our test data (<code>X_test</code> and <code>y_test</code>) aside and use it at the very end to measure the model's performance.</p>"},{"location":"data-science/algorithms/supervised/regression/#intuition","title":"Intuition","text":"<p>For the first OLS model, we use a single input variable \\(X\\) as it allows us to easily visualize and interpret the results. The choice falls on the median income at the house location, referred to as MedInc. Visualize the target and input variable in a scatter plot:</p> <pre><code>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.scatter(X_train[\"MedInc\"], y_train, color=\"#009485\")\nax.set(\n    xlabel=\"Input Variable (MedInc)\",\n    ylabel=\"Target (House Value)\",\n    title=\"Train set\",\n)\nplt.show()\n</code></pre> <ul> <li> <p>Scatter Plot</p> <p>Looking at the scatter plot, you might intuitively imagine drawing a straight line through the points that best captures the trend. This intuition is exactly what OLS does mathematically, it finds the optimal line that minimizes the distance between the line and all data points. </p> </li> <li> <p> </p> </li> <li> <p> </p> </li> <li> <p>Best-Fit Line</p> <p>The OLS model finds the line that minimizes the sum of squared residuals, the vertical distances between each point and the line. Recall from the theory section that this is exactly what the cost function measures:</p> \\[ \\text{min} \\quad \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\] </li> </ul>"},{"location":"data-science/algorithms/supervised/regression/#train-the-model","title":"Train the model","text":"<p>Our next step is to train an OLS model to automatically find this \"best-fit\" line. Remember, since we have one input variable, the linear combination simplifies to:</p> \\[ \\hat{y} = w_0 + w_1 \\cdot x_1 \\] <p>where:</p> <ul> <li>\\(\\hat{y}\\) is the predicted house price</li> <li>\\(w_0\\) is the intercept (baseline house price)</li> <li>\\(w_1\\) is the coefficient for the input variable MedInc (\\(x_1\\))</li> </ul> <p>Start by importing the linear regression model from the package.</p> <pre><code>from sklearn.linear_model import LinearRegression\n\n# create an instance\nmodel = LinearRegression()\n</code></pre> <p>At this point, the model is not trained, however that can be easily done using the <code>fit()</code> method. Remember, to use the training set</p> <pre><code>model.fit(X=X_train[[\"MedInc\"]], y=y_train)\n</code></pre>"},{"location":"data-science/algorithms/supervised/regression/#intercept-and-coefficient","title":"Intercept and coefficient","text":"<p>After training, we can inspect the model's learned parameters. The intercept and coefficient that define the best-fit line:</p> <pre><code>print(f\"Intercept (w\u2080): {round(model.intercept_, 4)}\")\nprint(f\"Coefficient (w\u2081): {round(model.coef_[0], 4)}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Intercept (w\u2080): 0.4446\nCoefficient (w\u2081): 0.4193\n</code></pre> <p>These values tell us that our linear model is:</p> \\[ \\hat{y} = 0.4446 + 0.4193 \\cdot x_1 \\] <p>Interpretation:</p> <ul> <li>Intercept (0.4446): The baseline house value (when MedInc is zero)     is around $44,460</li> <li>Coefficient (0.4193): For each unit increase in MedInc, the house value     increases by ~ $41,930</li> </ul>"},{"location":"data-science/algorithms/supervised/regression/#predictions","title":"Predictions","text":"<p>Now that the model is trained, we can predict house prices for new observations. Let's predict the price \\(\\hat{y}\\) for a house in an area where MedInc is <code>3.5</code>:</p> <pre><code>import pandas as pd\n\nnew_house = pd.DataFrame({\"MedInc\": [3.5]})\n# predict returns a numpy array\nnew_price = model.predict(new_house)\n# access first and only element\nprint(round(new_price[0], 4))\n</code></pre> &gt;&gt;&gt; Output<pre><code>1.9123\n</code></pre> <p>The model predicts a house value of approximately $191,230.</p>"},{"location":"data-science/algorithms/supervised/regression/#manual-validation","title":"Manual validation","text":"<p>We can verify this prediction using our linear equation. Substituting \\(x_1 = 3.5\\):</p> \\[ \\begin{align} \\hat{y} &amp;= 0.4446 + 0.4193 \\cdot 3.5 \\\\ &amp;= 1.9122 \\end{align} \\] <p>This matches our model's prediction!</p> Practice: Make your own prediction <p>Calculate the predicted house price for an area where MedInc is <code>5.0</code>.</p> <ol> <li>Use <code>model.predict()</code> to get the prediction.</li> <li>Validate it by hand using the linear equation.</li> <li>Do the results match?</li> </ol>"},{"location":"data-science/algorithms/supervised/regression/#evaluate-the-model","title":"Evaluate the model","text":"<p>Now we can make predictions, but we don't know how accurate they actually are. We need to quantify the model's performance to determine if it generalizes well to new, unseen data.</p> <p>Remember we set aside our test set earlier? This is where we use it. By evaluating on data the model hasn't seen during training, we get an honest assessment of its predictive power.</p> <p>To measure the model's performance, we'll use the coefficient of determination.</p>"},{"location":"data-science/algorithms/supervised/regression/#coefficient-of-determination","title":"Coefficient of determination","text":"Info <p>This section focuses on the definition implemented by <code>scikit-learn</code>.</p> <p>The coefficient of determination, known as the \\(R^2\\) score, measures the proportion of variance in the target variable that is explained by the model.</p> \\(R^2\\) Score \\[ R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} \\] <p>where:</p> <ul> <li>\\(y_i\\) are the actual values</li> <li>\\(\\hat{y}_i\\) are the predicted values</li> <li>\\(\\bar{y}\\) is the mean of actual values</li> </ul> <p>Interpretation:</p> <ul> <li>\\(R^2 = 1\\): Perfect predictions (model explains all variance)</li> <li>\\(R^2 = 0\\): Model performs no better than simply predicting the mean</li> <li>\\(R^2 &lt; 0\\): Model performs worse than predicting the mean</li> </ul> <p>Let's calculate the \\(R^2\\) score for our model on the test set:</p> <pre><code>from sklearn.metrics import r2_score\n\n# make predictions on test set\ny_pred = model.predict(X_test[[\"MedInc\"]])\n\n# calculate R\u00b2 score\nr2 = r2_score(y_true=y_test, y_pred=y_pred)\nprint(f\"R\u00b2 Score: {round(r2, 4)}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>R\u00b2 Score: 0.4589\n</code></pre> Understanding \\(R^2\\) <p>An \\(R^2\\) score of 0.4589 means the model explains 45.89% of the variance in house prices using only median income. While this is informative, it's not great. It suggests that other factors (location, house size, etc.) significantly influence house prices.</p> Find a better model <p>Can you improve the \\(R^2\\) score? Fit new models and experiment with the following:</p> <p>Model variations:</p> <ul> <li>Use different individual input variables (e.g., HouseAge, AveRooms,     AveBedrms)</li> <li>Use a combination of multiple input variables</li> <li>Compare single-variable vs. multi-variable models</li> </ul> <p>Data preparation:</p> <ul> <li>Adjust the train-test split ratio</li> <li>Remember to use <code>random_state</code> for reproducibility</li> </ul> <p>Analysis:</p> <ul> <li>Calculate and compare \\(R^2\\) scores for each model</li> <li>Inspect the intercept and coefficients for multi-variable models</li> <li>Make predictions with your best-performing model</li> <li>Manually verify one prediction using the linear equation</li> </ul> <p>Which combination gives you the highest \\(R^2\\) score? What does this tell you about which features are most important for predicting house prices?</p>"},{"location":"data-science/algorithms/supervised/regression/#detour-model-workflow","title":"Detour: Model workflow","text":"<p>The workflow you practiced here forms the foundation for all supervised learning algorithms in <code>scikit-learn</code>:</p> <pre><code># 1. Split your data\nX_train, X_test, y_train, y_test = train_test_split(X, y, ...)\n\n# 2. Import and instantiate a model\nmodel = SomeModel()\n\n# 3. Train the model\nmodel.fit(X_train, y_train)\n\n# 4. Make predictions\ny_pred = model.predict(X_test)\n\n# 5. Evaluate performance\nscore = model.score(y_test, y_pred)\n</code></pre> <p>This consistent pattern applies to all upcoming chapters, whether you're building regression or classification models.</p>"},{"location":"data-science/algorithms/supervised/regression/#recap","title":"Recap","text":"<p>In this chapter, you learned the fundamentals of linear regression through a practical example. The key takeaways:</p> <ul> <li>Linear regression models the relationship between input variables and a     target variable using a linear combination. Find the best-fit line by     minimizing the sum of squared residuals.</li> <li>\\(R^2\\) score quantifies how well the model explains variance in the     target variable</li> <li><code>scikit-learn</code> workflow allows to easily train and evaluate model</li> </ul>"},{"location":"data-science/algorithms/supervised/tree-based/cart/","title":"Decision Tree","text":"<p>So far we have covered linear regression and logistic regression which are limited to linear relationships. In contrast, decision trees are non-linear models able to capture complex relationships in the data. They are easy to interpret and visualize, making them a popular choice for many applications.</p> <p>Moreover, decision trees can be used for both regression and classification!</p> <p>In this chapter, we will explore the theory behind decision trees followed by practical examples. As always we will use <code>scikit-learn</code> for hands-on experience.</p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#basic-intuition","title":"Basic intuition","text":"<p>Although you might not know it, you're already familiar with decision trees. Imagine, you're planning a skiing trip and need to decide whether to go skiing or not. You might ask yourself:</p> <pre><code>graph TD\n    A[Can I get to a skiing resort?] --&gt;|Yes| B[Is there any snow?]\n    A --&gt;|No| C[No skiing \ud83d\ude25]\n    B --&gt;|Yes| D[Go skiing \u26f7]\n    B --&gt;|No| E[No skiing \ud83d\ude25]\n\n    style A fill:#1e2129,stroke:#ffffff\n    style B fill:#1e2129,stroke:#ffffff\n    style D fill:#009485,stroke:#ffffff\n    style C fill:#e92063,stroke:#ffffff\n    style E fill:#e92063,stroke:#ffffff</code></pre> <p>Depending on the answers, you can decide whether to go skiing or not.</p> <p>A decision tree resembles a flowchart where each internal node represents a decision based on a feature (e.g., Is there any snow?), each branch represents the outcome of that decision, and each leaf node represents a final prediction (either a class label for classification or a continuous value for regression).</p> <p>To get a better understanding of the terms node, branch and leaf, consider the illustration of a (rotated) tree.</p> The same decision tree with literal illustrations of          node, branch and leaf.      <p>In the skiing example, the nodes are the questions you ask yourself. With branches being a simple binary split (the answers to the question). The leaf nodes are the final predictions, in our case whether to go skiing.</p> # <p>Given the skiing decision tree, what kind of supervised learning task is this?</p> Multi-class classification (not previously covered)RegressionBinary classificationIt's not a supervised learning task, since the target is missing. <p>The leaves predict two different labels, namely \"Go skiing\" and \"No skiing\" which is a classic binary classification task.</p> Excited for some theory?"},{"location":"data-science/algorithms/supervised/tree-based/cart/#theory","title":"Theory","text":"Info <p>This theoretical section on decision trees follows: Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning<sup>1</sup></p> <p>We focus on a particular algorithm called CART (=Classification And Regression Trees). The theoretical foundations of CART were developed by: Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone. 1984. Classification and Regression Trees<sup>2</sup></p> <p>When building a decision tree a couple of questions arise:</p> <ul> <li> <p> Question</p> <ol> <li>How do we pick the right feature for a split?</li> <li>What's the decision criteria at each node?</li> <li>How large do we grow the tree?</li> </ol> </li> <li> <p> Intuition</p> <ol> <li>Which questions do we ask? Why did we ask \"Can I get to a skiing resort?\"     and \"Is there any snow?\"?</li> <li>It does not have to be a simple yes/no question. It can be a threshold for     continuous values as well. E.g., \"Is there more than 10cm of fresh     snow?\" But how do we choose the threshold?</li> <li>How many questions do we ask? Why only 2 and not more?</li> </ol> </li> </ul> <p>With these questions in mind, let's dive into the theory of decision trees in order to tackle them.</p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#greedy-optimization","title":"Greedy optimization","text":"<p>As a decision tree is a supervised learning algorithm, the goal is to predict the target variable \\(y\\) with a set of features \\(x_1, x_2, ..., x_n\\).</p> <p>With the data at hand, the CART algorithm finds the optimal tree structure that minimizes the prediction error. In turn, the optimal tree structure depends on the chosen splits.</p> Info <p>A split in CART is a binary decision rule that divides the dataset into two subsets based on a specific feature and threshold.</p> <p>Imagine if we extend our skiing example with the split \"Is there more than 10cm of fresh snow?\". The split divides the data into two subsets: one where observations have more than 10cm of fresh snow and another where observations don't. With amount of fresh snow being the feature and 10cm the threshold.</p> <p>However, given large data sets, there are simply too many splitting possibilities to consider at once. Hence, the tree is grown in a greedy fashion.</p> <p>The greedy optimization starts with a single root node splitting the data into two partitions and adds additional nodes one at a time. At each step, the algorithm chooses a split using exhaustive search. The best split is determined by a criterion. Remember, that decision trees can deal with regression and classification problems. Hence, the criterion differs for the two tasks.</p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#regression","title":"Regression","text":"<p>For regression trees, the best split (feature threshold combination) at each node is determined by minimizing the residual sum-of-squares error (RSS), defined as:</p> Residual sum-of-squares (RSS) \\[ RSS = \\sum_{i \\in t_L} (y_i - \\bar{y}_L)^2 + \\sum_{i \\in t_R} (y_i -             \\bar{y}_R)^2 \\] <p>where \\(t_L\\) and \\(t_R\\) are the left and right child nodes after the split, and \\(\\bar{y}_L\\) and \\(\\bar{y}_R\\) are the mean target values in the respective nodes.</p> <p>The algorithm searches through all possible splits to find the one that minimizes this RSS criterion.</p> Info <p>Since each split separates the input data into two partitions, the prediction is the mean of the target variable \\(y\\) in the respective partition.</p> <p>Hence, intuitively speaking, we do not optimize the entire tree at once but rather optimize each split locally.</p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#classification","title":"Classification","text":"<p>For classification tasks, the best split at each node is determined by minimizing the Gini impurity.</p> Gini impurity <p>For a node \\(t\\) with \\(K\\) classes, the Gini impurity is defined as:</p> \\[ Gini(t) = \\sum_{k=1}^K p_{k}(1-p_{k}) = 1 - \\sum_{k=1}^K p_{k}^2 \\] <p>where \\(p_k\\) is the proportion of class \\(k\\) observations.</p> <p>The Gini impurity (sometimes referred to as Gini index) encourages leaf nodes where the majority of observations belong to a single class.</p> Info <p>The prediction at each leaf node is the majority class among the training observations in that node.</p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#tldr","title":"TLDR","text":"<p>No matter the task (regression or classification), with a greedy optimization strategy, the CART algorithm searches for the best split using an exhaustive search at each node to ultimately minimize the prediction error. Thus answering the first two questions, a (How do we pick the right feature for a split?) and b (What's the decision criteria at each node?).</p> <p>A CART can be seen as a piecewise-constant model, as it partitions the feature space into regions and assigns a constant prediction (either the mean of a continuous value or a label) to each region.</p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#tree-size","title":"Tree size","text":"<p>Lastly, we answer question, c (How large do we grow the tree?). Put differently, when should we stop adding nodes?</p> <p>First, the tree is grown as large as possible until a stopping criterion is met. This criterion can be the maximum tree depth or a minimum number of observations per leaf. Second, the tree is pruned back. Pruning is the process of removing nodes that do not improve the model's performance. It balances the RSS error or Gini impurity against model complexity.</p> Info <p>If you want to dive deeper into tree pruning, we recommend reading page 665 of Bishop's book Pattern Recognition and Machine Learning<sup>1</sup></p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#advantages-and-limitations","title":"Advantages and Limitations","text":"<p>Decision trees offer several significant advantages, but they also have their limitations:</p> <ul> <li> <p> Advantages</p> <ul> <li>Easy to interpret and visualize</li> <li>Can capture non-linear relationships</li> </ul> </li> <li> <p> Limitations</p> <ul> <li>Prone to overfitting, i.e., building a model that perfectly fits the     training data but fails to generalize on new (unseen) data.</li> <li>Sensitive to data, i.e., small changes in the data can lead to     significantly different trees.</li> </ul> </li> </ul>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#examples","title":"Examples","text":"<p>As mentioned earlier, we will use <code>scikit-learn</code> for hands-on experience. <code>scikit-learn</code> contains an implementation of the CART algorithm discussed.<sup>3</sup></p> <p>Functionalities around decision trees are all part of the <code>tree</code> module in <code>scikit-learn</code>.</p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#regression_1","title":"Regression","text":"<p>First, we start with a regression task. We will use the California housing data to predict house prices using a decision tree regressor.</p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#load-data","title":"Load data","text":"<p>Load the data and split it into training and test sets. If you need a refresh on training and test splits, visit the Split the data section of the previous chapter.</p> <pre><code>from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, shuffle=True\n)\n</code></pre> <p>As always, a seed is set for reproducibility (<code>random_state=42</code>). It can be any integer, you can simply pick any number.</p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#fit-and-evaluate-the-model","title":"Fit and evaluate the model","text":"<p>Next, we load the class <code>DecisionTreeRegressor</code> from the <code>tree</code> module.</p> <pre><code>from sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor(random_state=784)\n</code></pre> <p>Again, we set a seed, as the tree's construction involves randomness.</p> <p>To fit and evaluate the model:</p> <pre><code>model.fit(X_train, y_train)\nscore = model.score(X_test, y_test)\nprint(f\"Model performance (R\u00b2): {round(score, 2)}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Model performance (R\u00b2): 0.61\n</code></pre> <p>The <code>score()</code> method returns the coefficient of determination \\(R^2\\). You should be already familiar with \\(R^2\\), as it was first introduced in the Regression chapter to evaluate the fit of a linear regression.</p> <p>The decision tree model achieved an \\(R^2\\) of 0.61 on the test set, which leaves room for improvement.</p> Info <p>On a side note: Although we fitted a decision tree on <code>16512</code> observations, the process of actually training the model is quite fast!</p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#plot-the-tree","title":"Plot the tree","text":""},{"location":"data-science/algorithms/supervised/tree-based/cart/#its-a-mess","title":"It's a mess...","text":"<p>As discussed, one main advantage of decision trees is their interpretability. We can easily visualize the tree using the <code>plot_tree</code> function.</p> Tip <p>This is the first time that we discourage you from running the code snippet below. Soon you will know why.</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\n\nplot_tree(model)\nplt.show()  # use matplotlib to show the plot\n</code></pre> Yes, that's the actual tree.  <p>Though we can't read any of the information present, the plot hints at a huge tree. Due to its complexity, the model does not add much value to the understanding of the data (it's simply not interpretable).</p> <p>Actually visualizing this particular tree takes some time, hence we discouraged you from executing the code.</p> <p>But why do we get such a huge tree? By default, the CART implementation in <code>scikit-learn</code> grows the tree as large as possible and does not prune it.</p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#to-fix","title":"... to fix","text":"<p>To prevent the tree from growing too large, we can set two parameters.</p> <pre><code>from sklearn.tree import DecisionTreeRegressor\n\n# set max_depth and min_samples_leaf\nmodel = DecisionTreeRegressor(\n    random_state=784, max_depth=2, min_samples_leaf=15\n)\n\n# fit the model again\nmodel.fit(X_train, y_train)\n</code></pre> <p>The <code>max_depth</code> parameter limits the depth of the tree, while <code>min_samples_leaf</code> sets the minimum number of samples (observations) required to be in a leaf node. Both prevent the tree from growing too large.</p> Info <p>Remember, we want to prevent overfitting. By setting these parameters, we control the complexity of the tree and thus reduce the risk of overfitting. Additionally, it results in a smaller tree which is easier to interpret.</p> <p>Let's plot the pruned tree.</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\n\nplot_tree(\n    model,\n    filled=True,  # (1)!\n    feature_names=X.columns,  # (2)!\n    proportion=True,  # (3)!\n)\nplt.show()\n</code></pre> <ol> <li><code>filled=True</code> colors nodes according to prediction values. A     stronger color indicating a higher value.</li> <li>The parameter <code>feature_names</code> is used to label the features in the tree.</li> <li><code>proportion=True</code> displays the proportion of samples in each node.</li> </ol> Info <p>Generally, it is always good practice to consult the documentation, if you are unsure about the usage of a function/class.</p> <p>Regarding <code>plot_tree()</code>, you might find some useful information in the docs that can help you customize the plot to your liking. So don't shy away from reading the documentation!</p>          The tree is in a stark contrast to the one we had before; it is way          smaller.      Tip <p>The nodes are quite easy to read:</p> <p>Starting with the root node, the feature <code>MedInc</code> performs the first split. If the median income is less than 5.086, we follow the left branch else the right branch. The resulting <code>squared_error</code> of the split is shown as well. At the root node, the <code>squared_error</code> (sum of the squared differences between the actual values and the predicted value) is 1.337. The lower the <code>squared_error</code>, the better the split. A \"perfect split\" would result in a <code>squared_error</code> of 0.</p> <p>The root node splits the data into two subsets, the left branch results in a subest containing 79.3% of the training data and the right branch 20.7%. Compared to the root node, both additional splits lead to a decrease of the <code>squared_error</code> and thus increase the predictive power. After two more splits, we reach the leaf nodes. Each leaf node contains a value, the final prediction.</p> <p>Now we have a pruned tree, which reduced the risk of overfitting. However, at the cost of model performance. The \\(R^2\\) decreased from 0.61 to 0.42 which might indicate that such a simple tree might not capture the complexity of the data well.</p> Now get to the point! <p>In practice, you have to find the right parameters to balance model complexity and performance. Unfortunately, there is no one-size-fits-all solution. You have to tune the parameters based on the data and the task at hand.</p> Parameter tuning <p>Try some different combinations of <code>max_depth</code> and <code>min_samples_leaf</code>. Use the same train test split, we defined earlier.</p> <ol> <li>Manually change the values.</li> <li>Fit the model.</li> <li>Evaluate the model.</li> <li>Plot the model.</li> <li>Repeat! </li> </ol> <p>Can you get an \\(R^2\\) higher than <code>0.7</code>?</p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#classification_1","title":"Classification","text":"<p>Next, we switch to a classification task. We will re-use the breast cancer data set introduced in the previous Classification chapter.</p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#load-data_1","title":"Load data","text":"<pre><code>from sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, shuffle=True\n)\n</code></pre>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#fit-and-evaluate-the-model_1","title":"Fit and evaluate the model","text":"<p>For classification trees, <code>scikit-learn</code> provides the class <code>DecisionTreeClassifier</code>.</p> <pre><code>from sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(\n    # again, set max_depth and min_samples_leaf to prevent growing a huge tree\n    random_state=784,\n    max_depth=7,\n    min_samples_leaf=5,\n)\n</code></pre> Fit and evaluate the model <p>Now it is your time to fit and evaluate the model. Although, you have never used an instance of <code>DecisionClassifier</code> before, you can use the same methods as with other models in <code>scikit-learn</code>. Simply refer to the previous regression example.</p> <ol> <li>Fit the model on <code>X_train</code> and <code>y_train</code>.</li> <li>Evaluate the model on <code>X_test</code> and <code>y_test</code>.</li> <li>Print the model's performance.</li> <li>Plot the tree.</li> </ol> <p>Lastly answer following quiz question to evaluate your result.</p> # <p>What is the model's accuracy (rounded to 2 decimal places)?</p> 92.98%94.74%90.35% <p> Correct! The mean accuracy is 94.74% which is a bit lower than the 95.61% from the logistic regression. </p>"},{"location":"data-science/algorithms/supervised/tree-based/cart/#recap","title":"Recap","text":"<p>We comprehensively explored decision trees, focusing on the CART algorithm. The theory section illuminated its core mechanisms, while practical examples demonstrated building and evaluating decision trees for regression and classification tasks. Key takeaways include:</p> <ul> <li>Algorithm insights into tree construction</li> <li>Practical implementation skills</li> <li>Understanding of decision trees' interpretability and overfitting risks</li> </ul> <p>Next, we'll extend our knowledge to Random Forests, an ensemble method combining multiple decision trees to enhance predictive performance.</p> <ol> <li> <p>Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. Link \u21a9\u21a9</p> </li> <li> <p>Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone. Classification and Regression Trees. Chapman and Hall/CRC, 1984. https://doi.org/10.1201/9781315139470 \u21a9</p> </li> <li> <p><code>scikit-learn</code> documentation: Decision Trees \u21a9</p> </li> </ol>"},{"location":"data-science/algorithms/supervised/tree-based/forest/","title":"Random Forest","text":"<p>While decision trees are easy to interpret, they have several drawbacks: they are prone to overfitting and are sensitive to slight changes in the data.</p> <p>Random forest is an ensemble method that addresses these drawbacks at the cost of slightly reduced interpretability. At its core, a random forest is simply a collection of decision trees. Since we have already extensively discussed the CART (Classification and Regression Trees) algorithm, we can dive right in.</p>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#the-basics","title":"The basics","text":"Info <p>Random forests were introduced by Leo Breiman in 2001. The following section closely follows the original paper.</p> <p>Breiman, L. Random Forests. Machine Learning 45, 5\u201332 (2001). https://doi.org/10.1023/A:1010933404324</p> <p>A random forest combines multiple decision trees to create an ensemble model. The idea is to grow multiple trees and average their predictions. Thus, resulting in a more robust model that improves generalization and reduces overfitting.</p> <p>The randomness in a random forest stems from two techniques:</p> <ol> <li>Bootstrap sampling</li> <li>Random feature selection</li> </ol>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#bootstrap-sampling","title":"Bootstrap sampling","text":"<p>The first technique is known as bootstrap sampling. Given a training set of size \\(N\\), we draw \\(N\\) samples with replacement. This means that some samples may be repeated, while others may not be included at all. This results in a new training set of the same size as the original, but with some samples missing and others duplicated.</p> <p>Each tree is fit on a different bootstrap sample. Intuitively speaking, this means that each tree sees a slightly different \"version\" of the training data.</p>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#random-feature-selection","title":"Random feature selection","text":"<p>The second technique is random feature selection. Remember, that a CART is grown by selecting the best split at each node. This is done by considering all features. Contrary when growing trees for a random forest, we only consider a random subset of features at each split.</p>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#putting-it-all-together","title":"Putting it all together","text":"<p>Each tree in a random forest is fit on a bootstrap sample and uses a random subset of features at each split. In case of regression, the predictions of all trees are simply averaged. In case of classification, the majority vote is taken. The majority vote in a random forest classification means that the class predicted most frequently by the individual trees is selected as the final prediction.</p> <p>No matter the task, classification or regression: it was observed that introducing randomness in the tree-growing process improves the model performance.</p> Info <p>Contrary to the classic CART, random forests do not constrain the tree growth. I.e., trees are fully grown and not pruned.</p>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#examples","title":"Examples","text":"<p>With a basic understanding of random forests we take a look at some examples. As always, we'll use our favorite machine learning package <code>scikit-learn</code> (at least that of the author ).</p> <p>In order to focus on the random forest implementation and its parameters, we'll reuse the California housing data (for regression) and the breast cancer data (for classification). Both were utilized in the decision tree examples.</p>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#regression","title":"Regression","text":"<p>Let's start with building a random forest to predict California housing prices.</p>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#load-data","title":"Load data","text":"<p>As usual, we load the data and split it into a training and test set in order to evaluate the model later on.</p> <pre><code>from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, shuffle=True\n)\n</code></pre>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#fit-the-model","title":"Fit the model","text":"<p>Just like with decision trees, <code>scikit-learn</code> provides two separate classes for regression and classification, namely <code>RandomForestRegressor</code> and <code>RandomForestClassifier</code>. Both are part of the <code>ensemble</code> module.</p> <pre><code>from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(random_state=784)  # (1)!\nmodel.fit(X_train, y_train)\n</code></pre> <ol> <li>As a random forest is well random , we set the <code>random_state</code>     to ensure the reproducibility of our results.</li> </ol> <p>Depending on your setup, the fitting process might take a couple of seconds.</p>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#evaluate-the-model","title":"Evaluate the model","text":"<pre><code>score = model.score(X_test, y_test)\nprint(f\"Model performance (R\u00b2): {round(score, 2)}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Model performance (R\u00b2): 0.81\n</code></pre> Info <p>Remember, that the <code>score()</code> method of a decision tree regressor (<code>DecisionTreeRegressor</code>) returned the coefficient of determination \\(R^2\\). The same applies to random forests regressors.</p> <p>Compared to a single tree with an \\(R^2\\) of 0.61, the random forest performs considerably better with an \\(R^2\\) of 0.81. You can re-visit the according section here.</p> How many trees are in the forest? <p>Consult the <code>scikit-learn</code> docs to find out how many trees are in the forest by default. Use the following question for self-assessment.</p> # <p>How many trees form a forest by default?</p> None, you have to pass it as argument.10001, the forest defaults to a single decision tree.100 <p>The parameter <code>n_estimators</code> defaults to 100 trees.</p> Info <p>If you want to get closer to the original definition of a random forest regressor by Breiman, you have to set the <code>max_features</code> parameter. Specifically, with \\(m\\) features, the number of features considered at each split should be \\(\\frac{m}{3}\\) for regression.</p> <pre><code>RandomForestRegressor(max_features=len(X_train.columns) // 3, random_state=784)\n</code></pre> <p>By default, <code>scikit-learn</code> considers \\(m\\) features for each split.</p> Tip <p>If you're unsure how to set parameters of a model (such as <code>max_features</code>), stick to the defaults. <code>scikit-learn</code> provides sensible defaults that work well. In later chapters, we will explore methods to automatically tune these hyperparameters.</p>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#classification","title":"Classification","text":"<p>Next, we switch to a classification task.</p> Question <p>Load the breast cancer data, fit and evaluate a random forest.</p> <ol> <li>Load the data and split it into a training and test set.</li> <li>Load the appropriate random forest class.</li> <li>Fit the model.</li> <li>Evaluate the model on the test set.</li> </ol> <p>Hint: This and the previous chapter should provide all necessary information, to solve the tasks.</p>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#inspecting-the-forest","title":"Inspecting the forest","text":"<p>We can even inspect all individual trees of our ensemble forest. Simply access the attribute <code>estimators_</code> of your fitted model.</p> <pre><code>print(model.estimators_)  # (1)!\n</code></pre> <ol> <li>Assuming, you named the forest from the above task <code>model</code>.</li> </ol> &gt;&gt;&gt; Output<pre><code>[\n    DecisionTreeClassifier(max_features=1.0, random_state=1877362837), \n    DecisionTreeClassifier(max_features=1.0, random_state=1395144809)\n    ...\n]\n</code></pre> <p><code>estimators_</code> is a list of individual tree instances. If you're dealing with a <code>RandomForestRegressor</code>, <code>estimators_</code> is a list of <code>DecisionTreeRegressor</code>.</p> <p>In most cases, you won't need to inspect the individual trees. Nevertheless, we can utilize this information to solidify our understanding of random forests.</p>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#stronger-together","title":"Stronger together","text":"<p>We fit a random forest classifier on a synthetic data set to literally illustrate the different trees. First, we generate the data.</p> <pre><code>from sklearn.datasets import make_classification\n\nX, y = make_classification(random_state=42, n_clusters_per_class=1)\n</code></pre> <p>Next, we initialize and fit a random forest classifier.</p> <pre><code>classifier = RandomForestClassifier(\n    random_state=42, n_estimators=4, max_depth=3\n)\nclassifier.fit(X, y)\n</code></pre> <p>Note, that we set the number of trees to <code>4</code>. We keep the number small as we visualize them later on. The <code>max_depth</code> parameter limits the depth of each tree to <code>3</code>. This is done to perform pruning and thus keep the trees simple and easier to plot.</p> <p>Finally, we visualize all trees. We access the trees via the <code>estimators_</code> attribute and plot them using the familiar <code>plot_tree()</code> function. Everything else is just plot customization.</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\n\nfig = plt.figure(figsize=(20, 12))\nfor index, tree in enumerate(classifier.estimators_, 1):\n    plt.subplot(2, 2, index)\n    plot_tree(\n        tree,\n        label=\"root\",\n        class_names=True,\n        filled=True,\n        fontsize=14,\n    )\n    plt.title(f\"Decision Tree {index}\", fontsize=25)\n\nplt.tight_layout()\nplt.show()\n</code></pre> All four individual trees of this particular forest.      <p>Although there is a lot of information cramped inside one figure, at first glance it is obvious that all four trees are different. Each of them differs in splits (feature and threshold), number of nodes and predictions.</p> <p>Each one of these trees on their own might not generalize well, hence they are often referred to as weak learners. However, when combined, they form a \"strong\" model. That's the essence of an ensemble method!</p>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#feature-importance","title":"Feature importance","text":"<p>One of the most powerful attribute of random forests is their ability to assess feature importance: measuring how much each input variable contributes to predicting the target variable.</p> <p>Remember that trees are fitted on a bootstrap training set. Since some samples are left out during this process, we can use these to measure the importance of each feature. These unused observations are called \"out-of-bag\" (OOB) samples. For each feature, the OOB samples are randomly permuted (shuffled) and the increase in prediction error is measured. Features that lead to larger increases in error when permuted are considered more important.</p> <p>Let's examine feature importance using the breast cancer dataset:</p> <pre><code>X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X, y)\n\nprint(rf.feature_importances_)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[0.03484323 0.01522515 0.06799034 0.06046164 0.00795845 0.01159704\n ...]\n</code></pre> Info <p>To keep the example concise, we did not perform a train test split.</p> <p>Feature importance values are a <code>list</code> of <code>float</code>s. Each value corresponds to a feature in the order they were passed to the model. The values are normalized and sum to <code>1.0</code>. A higher value indicates that the feature contributes more to making correct predictions.</p> <p>Feature importance can help with:</p> <ol> <li>Feature selection: Identifying which features are most relevant for     predictions</li> <li>Model interpretation: Understanding which features drive the model's     decisions</li> <li>Data collection: Guiding future data collection efforts by highlighting     important measurements</li> </ol> Visualize the feature importance <p>Generate a bar plot to visualize the feature importance. Use any package of your choice. For convenience, you can use the following code snippet to get started.</p> <pre><code>import pandas as pd\n\nfeature_importance = pd.DataFrame(\n    {\"feature\": X.columns, \"importance\": rf.feature_importances_}\n)\n</code></pre> <p>Don't worry about styling the plot!</p> <p>A possible solution is provided below.</p>"},{"location":"data-science/algorithms/supervised/tree-based/forest/#recap","title":"Recap","text":"<p>Random forests improve upon single decision trees by combining multiple trees into an ensemble model. Through bootstrap sampling and random feature selection, they address the main drawbacks of decision trees - overfitting and sensitivity to data changes. While slightly less interpretable than single trees, random forests provide better generalization, more robust predictions, and useful insights through feature importance measures.</p> <p>With <code>scikit-learn</code>, you are now able to build a random forest for regression and classification tasks. You have also learned how to inspect individual trees and assess feature importance.</p>"},{"location":"data-science/algorithms/unsupervised/clustering/","title":"Clustering","text":"<p>In this section, we will start to explore unsupervised learning, where we work with data that isn't accompanied by labels. One of the primary techniques within this realm is clustering, which aims to uncover patterns or structures in the data by grouping similar data points together. A popular method for achieving this is k-means clustering, which aims to identify clusters of similar observations.</p>"},{"location":"data-science/algorithms/unsupervised/clustering/#k-means","title":"K-means","text":"<p>K-means was briefly introduced in the Introduction to Supervised vs. Unsupervised Learning and used to segment customers based on their annual spending and average basket size.</p>          An exemplary application of k-means clustering to segment customers.      <p>The algorithm groups similar data points together based on their attributes without being told what these groups should be.</p> <p>To get a better understanding of k-means, we will explore the theory behind it and employ the algorithm to cluster data from Spotify and a semiconductor manufacturer.</p>"},{"location":"data-science/algorithms/unsupervised/clustering/#theory","title":"Theory","text":"Info <p>The theoretical part is adapted from: Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning<sup>1</sup></p> <p>Assume a set of features \\(x_1, x_2, ..., x_n\\). K-means partitions the data into \\(K\\) number of clusters. Each cluster is represented by \\(\\mu_k\\), which can be seen as the center of a cluster \\(k\\).</p> <p>Intuitively speaking, the goal is to assign each data point \\(x_n\\) to the cluster with the closest center \\(\\mu_k\\).</p>"},{"location":"data-science/algorithms/unsupervised/clustering/#the-objective","title":"The objective","text":"<p>Since, the optimal assignment of data points to specific clusters is not known, the objective is to minimize the sum of squared distances between data points and their assigned cluster centers. This is known as the distortion measure:</p> Distortion measure \\[ J = \\sum_{n=1}^{N} \\sum_{k=1}^{K} r_{nk} \\lVert x_n - \\mu_k \\rVert^2 \\] <p>where:</p> <ul> <li>\\(N\\) is the number of data points,</li> <li>\\(K\\) being the number of clusters,</li> <li>\\(r_{nk}\\) is a binary indicator of whether data point \\(x_n\\) is assigned to     cluster \\(k\\),</li> <li>\\(\\mu_k\\) representing the cluster center.</li> </ul> <p>In short, we want to find the optimal \\(r_{nk}\\) and \\(\\mu_k\\) that minimize the distortion measure \\(J\\).</p> <p>\\(J\\) is minimized in an iterative process. First, we initialize \\(\\mu_k\\) with some random values. Then we alternate between two steps:</p> <ol> <li>Assignment step: Keep \\(\\mu_k\\) fixed. Minimize \\(J\\) with respect to     \\(r_{nk}\\). This is done by assigning each data point to the closest     cluster center.</li> <li>Update step: Keep \\(r_{nk}\\) fixed. Minimize \\(J\\) with respect to     \\(\\mu_k\\). This is done by updating the cluster centers to the mean of the     data points assigned to the cluster.</li> </ol> <p>Step 1 can be seen as re-assigning the data points to clusters, while step 2 re-computes the cluster centers.</p> Info <p>Since \\(\\mu_k\\) is the mean of the data points assigned to cluster \\(k\\), we speak of the k-means algorithm.</p> <p>The optimization of \\(J\\) is guaranteed to converge, but it might not find the global minimum. The final solution depends on the initial cluster centers.</p> Get a better understanding <p>To improve your understanding of the k-means algorithm, either watch the following video or visit the interactive visualization. Both variants illustrate the iterative process of k-means.</p> Option 1:  VideoOption 2:  Website <p> </p> <p>Visit the site clustering-visualizer.web.app/kmeans. Use mouse clicks to draw data points. Click on \"START\".</p> <p>The web app illustrates the iterative algorithm. You can watch the data points being assigned to clusters and the update of cluster centers which are denoted in the app as \\(C_1, C_2, ... , C_N\\).</p>"},{"location":"data-science/algorithms/unsupervised/clustering/#elbow-method","title":"Elbow method","text":"<p>So far we have not discussed the number of clusters \\(K\\) in depth. Since the algorithm requires the number of clusters as an input, it is crucial to choose \\(K\\) wisely.</p> <p>One common approach to determine the optimal number of clusters is the elbow method. The idea is to plot the distortion measure \\(J\\) (inertia) for different values of \\(K\\). The plot will show a sharp decrease in \\(J\\) as \\(K\\) increases. The optimal number of clusters is the point where the decrease flattens out, resembling an elbow.</p>          Illustration of the elbow method.      <p>We will apply both k-means and the elbow method in the following examples.</p>"},{"location":"data-science/algorithms/unsupervised/clustering/#examples","title":"Examples","text":"<p>With the theory out of the way, we can now apply k-means to real-world data. First, we build a recommendation system for Spotify tracks and then move on to clustering semiconductor data.</p>"},{"location":"data-science/algorithms/unsupervised/clustering/#recommendation-system","title":"Recommendation system","text":"<p>If you're using a music streaming service, you're familiar with listening to playlist. At the end of a playlist, the service recommends you similar songs based on the previous songs.</p> <p>We will build such a recommendation system (a rudimentary one) with k-means. The goal is to cluster songs based on their audio features and recommend similar songs to the user.</p> <p>To build our own recommendation system, we will use a modified Spotify dataset.</p> Info <p>The original data can be found on Kaggle.</p> <p>The modified data we are using, contains songs from 2024 up until now (time of writing: January 31, 2025).</p> Download and read data <ol> <li>Download the data set.</li> <li>Read it with <code>pandas</code> and for convenience assign it to a variable called     <code>data</code>. Then you will be able to use the following code snippets more     easily.</li> <li>Print the first rows of <code>data</code>.</li> </ol> <p>Download Spotify tracks </p> <p>With the data set loaded, we pick the following audio features for clustering:</p> <pre><code>features = [\n    \"danceability\",\n    \"energy\",\n    \"loudness\",\n    \"speechiness\",\n    \"acousticness\",\n    \"instrumentalness\",\n    \"liveness\",\n    \"valence\",\n    \"tempo\",\n]\n\n# subset data\nX = data[features]\n</code></pre> Have a look at the data <ol> <li>Look at the first couple of rows of the <code>DataFrame</code> <code>X</code>.</li> <li>Check for potential missing values.</li> </ol> <p>Hint: If you need a refresh on missing values, visit the Data preprocessing chapter.</p> <p>You might have noticed that all features are numerical. In fact, k-means requires numerical data.</p> Danger <p>K-means clustering relies on Euclidean distances, which only make sense for numerical data.</p> <p> Never use k-means for categorical data, even if you encode the categories as numbers or labels. Distances between categorical values are not meaningful!</p> <p>For clustering categorical data, use specialized algorithms like k-modes or other appropriate methods.</p> <p>Let's have another look at the data:</p> <pre><code>print(X.describe())\n</code></pre> &gt;&gt;&gt; Output<pre><code>       danceability        energy  ...       valence         tempo\ncount  11320.000000  11320.000000  ...  11320.000000  11320.000000\nmean       0.683081      0.660006  ...      0.525337    122.571478\nstd        0.134193      0.162655  ...      0.222797     27.628201\nmin        0.093900      0.001740  ...      0.000010     46.999000\n25%        0.597000      0.560000  ...      0.355000     99.987000\n50%        0.701000      0.675000  ...      0.526000    121.974000\n75%        0.780000      0.777000  ...      0.696000    140.056000\nmax        0.988000      0.998000  ...      0.989000    236.089000\n</code></pre> <p>These basic statistics reveal that the features have different scales. For example, compare <code>tempo</code> and <code>danceability</code>. Tempo ranges from <code>46</code> to <code>236</code>, while danceability ranges from <code>0.0939</code> to <code>0.988</code>.</p> <p>Thus, we apply a Z-Score normalization to all features (to have a mean of <code>0</code> and a standard deviation of <code>1</code>). This prevents k-means to disproportionately weigh features like <code>tempo</code> and ensures each feature contributes equally to the distance calculations.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()  # Z-Score normalization\nX = scaler.fit_transform(X)\n</code></pre>"},{"location":"data-science/algorithms/unsupervised/clustering/#apply-k-means","title":"Apply k-means","text":"<p>The application of k-means is straightforward:</p> <pre><code>from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=5, random_state=42)\ncluster_indices = kmeans.fit_predict(X)\nprint(cluster_indices)\n</code></pre> &gt;&gt;&gt; Output<pre><code>array([4, 0, 3, ..., 1, 1, 2], dtype=int32)\n</code></pre> <p>The <code>n_clusters</code> parameter specifies the number of clusters. We set it to <code>5</code> for now. The <code>random_state</code> parameter ensures reproducibility. Using the <code>fit_predict()</code> method, we obtain the cluster indices for each data point. In this case, these indices range from <code>0</code> to <code>4</code>. I.e., the first track belongs to cluster <code>4</code>, the second to cluster <code>0</code>, and so on.</p> <p>... but wait, how do we know if <code>5</code> is the right number of clusters? This is where the elbow method comes into play. </p>"},{"location":"data-science/algorithms/unsupervised/clustering/#elbow-method_1","title":"Elbow method","text":"<p>With the attribute <code>inertia_</code>, we can access the distortion measure \\(J\\). From the k-means docs:</p> <p><code>inertia_</code>:</p> <p>Sum of squared distances of samples to their closest cluster center,...</p> <p>KMeans docs</p> <p>In a loop we fit the k-means algorithm for different numbers of clusters \\(K\\) and store the corresponding distortion measure (<code>inertia_</code>). Then we plot the results.</p> <p>We define a function to apply the elbow method:</p> <pre><code>def elbow_method(X, max_clusters=15):\n    inertia = []\n    K = range(1, max_clusters + 1)\n\n    for k in K:\n        model = KMeans(n_clusters=k, random_state=42)\n        model.fit(X)\n        inertia.append(model.inertia_)\n\n    # for convenience store in a DataFrame\n    distortions = pd.DataFrame(\n        {\"k (number of cluster)\": K, \"inertia (J)\": inertia}\n    )\n\n    return distortions\n</code></pre> <p>By default, the function <code>elbow_method()</code> tries values for \\(K\\) from <code>1</code> to <code>15</code> and stores the corresponding distortion measure in a <code>DataFrame</code>.</p> Apply the elbow method <ol> <li> <p>Apply the <code>elbow_method()</code> on our scaled data <code>X</code>.</p> </li> <li> <p>Create a line plot with the number of clusters (K) on the x-axis and the     distortion measure on the y-axis.</p> <p>Hint: Use the <code>plot()</code> method of the resulting <code>DataFrame</code>.</p> </li> </ol> <p>Expand the below section to see a plot as possible solution.</p> Expand to see the plot <p>          Elbow method applied to the Spotify data set.      </p>"},{"location":"data-science/algorithms/unsupervised/clustering/#choice-paralysis","title":"Choice paralysis","text":"<p>Like in our example, it is not always obvious how many clusters to pick, because the \"elbow\" can sometimes be subtle or ambiguous. Ideally, you choose the point where the distortion/inertia sharply decreases and then levels off, forming an elbow-like bend in the plot.</p> <p>In this example, possible candidates for the number of clusters \\(K\\) are <code>5</code>, <code>6</code> or <code>7</code>. As we have to make a choice, we choose <code>6</code> clusters. Now, we have to simply fit the k-means algorithm with <code>n_clusters=6</code>.</p> <pre><code>kmeans = KMeans(n_clusters=6, random_state=42)\ncluster_indices = kmeans.fit_predict(X)\n</code></pre>"},{"location":"data-science/algorithms/unsupervised/clustering/#make-recommendations","title":"Make recommendationsNow to the fun part!","text":"<p>The goal of this exercise is to recommend a song based on a previous track. The idea is to pick a song as recommendation that is in the same cluster as the previous one. To do so, we can use the <code>cluster_indices</code> to recommend similar songs.</p> <p>Since the <code>cluster_indices</code> are in the same order as our initial <code>data</code>, we can simply assign them as a new column.</p> <pre><code>data[\"cluster\"] = cluster_indices\nprint(data.head())\n</code></pre> &gt;&gt;&gt; Output<pre><code>               spotify_id                name                artists  ... valence    tempo  cluster\n0  2plbrEY59IikOBgBGLjaoe    Die With A Smile  Lady Gaga, Bruno Mars  ...   0.535  157.969        4\n1  3sK8wGT43QFpWrvNQsrQya                DtMF              Bad Bunny  ...   0.032  136.020        0\n2  4wJ5Qq0jBN4ajy7ouZIV1c                APT.       ROS\u00c9, Bruno Mars  ...   0.939  149.027        2\n3  2lTm559tuIvatlT1u0JYG2   BAILE INoLVIDABLE              Bad Bunny  ...   0.219  119.387        0\n4  6dOtVTDdiauQNBQEDOtlAB  BIRDS OF A FEATHER          Billie Eilish  ...   0.438  104.978        4\n</code></pre> <p>Now, that we assigned a cluster to all <code>11320</code> tracks, we can easily recommend a song based on a given <code>spotify_id</code> (the unique identifier of a song on the platform).</p> <p>Use the below functions to see your recommender system in action. Don't worry about the details of these functions.</p> <pre><code>def print_track_info(track):\n    name, artists, cluster = track[\"name\"], track[\"artists\"], track[\"cluster\"]\n    print(\n        f\"Track name: {name}\\nArtist: {artists}\\nCluster index: {cluster}\\n{'-' * 20}\"\n    )\n\n\ndef recommend_track(spotify_id, data):\n    \"\"\"Get a recommendation for a given spotify_id.\"\"\"\n\n    # info about previous track\n    previous_track = data[data[\"spotify_id\"] == spotify_id].squeeze()\n    # get the cluster index of the previous track\n    cluster_index = previous_track[\"cluster\"]\n\n    print(\"Your previous track:\\n\")\n    print_track_info(previous_track)\n\n    # pull all tracks from the cluster\n    recommendation = data[data[\"cluster\"] == cluster_index]\n    # exclude the previous track\n    # (we do not want to recommend the same track again)\n    recommendation = recommendation[recommendation[\"spotify_id\"] != spotify_id]\n\n    # pull a random sample from the cluster\n    recommendation = recommendation.sample(1).squeeze()\n\n    print(\"Your recommendation:\\n\")\n    print_track_info(recommendation)\n</code></pre> <p>Pick any <code>spotify_id</code> from the data set (<code>data</code>) to start recommending songs. <code>recommend_track()</code> will pick a song that is in the same cluster.</p> <pre><code>recommend_track(\"5ehXToeJ8Tgc4wMhY42Oul\", data)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Your previous track:\n\nTrack name: Wake Me Up (feat. Justice)\nArtist: The Weeknd, Justice\nCluster index: 4\n--------------------\nYour recommendation:\n\nTrack name: Aarre\nArtist: Ultra Bra\nCluster index: 4\n--------------------\n</code></pre> Recommend more songs <p>Every time you run the <code>recommend_track()</code> function, you will get a new recommendation. Try it out!</p> <ol> <li>Pick another <code>spotify_id</code> and recommend a song.</li> <li>Repeat the process a couple of times.</li> </ol>"},{"location":"data-science/algorithms/unsupervised/clustering/#are-the-recommendations-good","title":"Are the recommendations good?","text":"<p>As you've tried the recommender system a couple of times, you might have wondered if the recommendations are actually good?! </p> <p>Simply put, you have to be the judge if we were actually able to cluster similar songs together and build a good recommendation system.</p> <p>In this application, it's quite intuitive: If you as a user like the recommendations and keep listening to the recommended songs, the system is successful.</p> Info <p>When talking about supervised tasks, we were able to measure the performance of our models. However, in unsupervised learning, like clustering, we do not have labels to compare our results to. Thus, evaluating the performance of unsupervised learning methods is challenging.</p> <p>In practice, you have to rely on domain knowledge to interpret the results and assess the quality of the model.</p>"},{"location":"data-science/algorithms/unsupervised/clustering/#semiconductor-data","title":"Semiconductor data","text":"<p>K-means is not only useful for recommendation systems, but also for anomaly detection. The idea is to form clusters which in turn can be used to detect the outliers/anomalies.</p> Info <p>The data is adapted from the UCI Machine Learning Repository.<sup>2</sup></p> <p>In this example, you will apply k-means to semiconductor data.</p> Download and read data <ol> <li>Download the below data set.</li> <li>Read it with <code>pandas</code>.</li> <li>Have a look at the data.</li> </ol> <p>Download semiconductor data </p> <p>Each row in the data set</p> <p>represents a single production entity with associated measured features [...]</p> <p>UCI Machine Learning Repository</p> Apply k-means <p>Solve the following tasks to apply k-means to the semiconductor data:</p> <ol> <li>Are there any missing values in the data?</li> <li>Deal with potential missing values; choose any suitable strategy. We     recommend to utilize the     <code>SimpleImputer</code>     with your chosen strategy. The application of the <code>SimpleImputer</code> should     be straightforward as it implements the methods you already know, e.g.,     <code>fit_transform()</code>.</li> <li>Do you need to scale the features? If so, apply a <code>StandardScaler</code>.</li> <li>Use the elbow method to determine the number of clusters.</li> <li>Fit the k-means algorithm with the optimal number of clusters.</li> </ol> <p>Hint: You can reuse the functions and code snippets from the Spotify example.</p> Info <p>If you have solved the above tasks, you might wonder how to interpret your clustering results. Moreover, how can you detect potential anomalies?</p> <p>Again, it all depends on domain knowledge. If you're a expert in the semiconductor industry you might be able to tell if the clusters make sense and if there are any anomalies in the data. Otherwise, interpretation can be quite challenging.</p>"},{"location":"data-science/algorithms/unsupervised/clustering/#recap","title":"Recap","text":"<p>In this chapter, we introduced k-means clustering. We covered the theory followed by two practical examples: building a recommendation system for Spotify tracks and clustering semiconductor data.</p> <p>We employed the elbow method to determine the optimal number of clusters and discussed the challenges of evaluating clustering results.</p> <p>In the upcoming chapter, we introduce another unsupervised method, namely Principal Component Analysis (PCA) to reduce the dimensionality of data. PCA can be useful in various ways:</p> <ul> <li>reducing the computational complexity of algorithms</li> <li>visualizing high-dimensional data in a 2D or 3D space</li> </ul> <p>Especially the latter can aid in interpreting the results of clustering algorithms.</p> <ol> <li> <p>Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. Link \u21a9</p> </li> <li> <p>McCann, M. &amp; Johnston, A. (2008). SECOM [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C54305 \u21a9</p> </li> </ol>"},{"location":"data-science/algorithms/unsupervised/dim-reduction/","title":"Dimensionality Reduction","text":""},{"location":"data-science/algorithms/unsupervised/dim-reduction/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>In data science and machine learning, we often encounter data sets with hundreds or even thousands of features. We speak of high-dimensional data sets. While these features may contain valuable information, working with such high-dimensional data can be computationally expensive, prone to overfitting, and difficult to visualize. This is where another unsupervised method, dimensionality reduction comes in \u2014 a technique used to simplify data sets, while retaining much of the critical information.</p> <p>One of the most widely used methods for dimensionality reduction is Principal Component Analysis (PCA). PCA transforms a high-dimensional (= lots of features) data set into a smaller set of features (components). In practice, PCA can reduce hundreds of features down to just 2 or 3 features, making PCA an ideal tool for visualization, preprocessing, and feature extraction.</p> <p>In this section, we will explain the inner workings of PCA and apply it to the semiconductor data set.</p>"},{"location":"data-science/algorithms/unsupervised/dim-reduction/#what-is-pca","title":"What is PCA?","text":"<p>PCA is a linear transformation technique that identifies the directions (also called principal components) in which the data varies the most. These principal components capture as much variance as possible. PCA has a variety of applications, such as:</p> <ul> <li>Data visualization: Plot a dimensionality reduced data set in 2D.</li> <li>Preprocessing: Removing noise or redundant features while retaining the     essential patterns in data.</li> <li>Feature engineering: Summarizing high-dimensional data into a smaller set     of meaningful features.</li> </ul>"},{"location":"data-science/algorithms/unsupervised/dim-reduction/#how-does-it-work","title":"How does it work?","text":"<p>PCA follows these essential steps:</p> <ol> <li> <p>Compute the covariance matrix: PCA captures relationships between     features by calculating the covariance between them.</p> Info <p>Think of the covariance matrix as the \"spread\" of the data. PCA looks at the interaction  the correlation of features with each other. Visit the correlation chapter in the statistics course to learn more about covariance.</p> </li> <li> <p>Eigen decomposition: Identify the eigenvalues and eigenvectors of the     covariance matrix. The eigenvectors represent the directions of the     principal components, while the eigenvalues represent the amount of     variance captured by each component.</p> Info <p>If you want to know more about eigenvalues and eigenvectors, check out this site.</p> </li> <li> <p>Rank components: Components are ranked by their eigenvalues. The first     principal component captures the most variance, the second captures the     next-most, and so on.</p> </li> <li> <p>Transform the data: Project the original data onto the top principal     components to reduce its dimensionality.</p> </li> </ol>"},{"location":"data-science/algorithms/unsupervised/dim-reduction/#the-mathematical-objective","title":"The mathematical objective","text":"<p>Let\u2019s assume we have a data set \\(X\\) with \\(p\\) features (dimensions). We aim to transform \\(X\\) into a new matrix \\(Z\\) with \\(k\\) features such that \\(k &lt; p\\), while retaining as much variance as possible.</p> <p>The transformation (described previously under point 4) is defined as:</p> PCA transformation \\[ Z = X W \\] <p>Where:</p> <ul> <li>\\(Z\\) is the transformed data set in the lower-dimensional space,</li> <li>\\(W\\) is a matrix whose columns are the top \\(k\\) eigenvectors of the     covariance matrix of \\(X\\).</li> </ul> Tip <p>Dimensionality reduction helps in combating the curse of dimensionality, a phenomenon where the performance of algorithms deteriorates with an increase in the number of features. Algorithms like clustering often struggle to find meaningful patterns when working with a high-dimensional data set.</p>"},{"location":"data-science/algorithms/unsupervised/dim-reduction/#example","title":"Example","text":"<p>It\u2019s time to apply PCA to real-world data. We'll revisit the semiconductor data set that we used in the previous clustering chapter. The first goal is to use PCA to reduce the data set's dimensions and visualize them.</p>"},{"location":"data-science/algorithms/unsupervised/dim-reduction/#prepare-the-data","title":"Prepare the data","text":"<p>First, we load the data set. If you haven\u2019t already downloaded it, you can grab it below:</p> <p>Download semiconductor data </p> Load the data <ol> <li>Load the <code>csv</code> file and assign it to a variable called <code>data</code>.</li> </ol> <p>Before applying PCA, let\u2019s make sure we deal with potential problems such as missing values.</p> <pre><code># fill missing values with the mean\ndata = data.fillna(data.mean())\n</code></pre> <p>Next, scale the features to standardize the data set to ensure that all features contribute equally to the analysis, preventing features with larger numerical ranges from dominating the principal components.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()  # Z-Score\nscaled_data = scaler.fit_transform(data)\n</code></pre>"},{"location":"data-science/algorithms/unsupervised/dim-reduction/#apply-pca","title":"Apply PCA","text":"<p>We now apply PCA to reduce the dimensions. First, we fit the PCA model on the <code>scaled_data</code>:</p> <pre><code>from sklearn.decomposition import PCA\n\npca = PCA(n_components=2, random_state=42)  # (1)!\ncomponents = pca.fit_transform(scaled_data)\n</code></pre> <ol> <li>Although the above definition of PCA is deterministic, the actual     implementation can be stochastic (depending on the solver used). Since     <code>svd_solver</code> is set to <code>\"auto\"</code> by default, the results can vary     slightly. Long story short, setting <code>random_state</code> ensures reproducibility     in all cases.</li> </ol> <p><code>n_components=2</code> specifies that we want to reduce the data set to 2 dimensions.</p> Check the shape <ol> <li>What is the shape of the <code>components</code> array?</li> </ol>"},{"location":"data-science/algorithms/unsupervised/dim-reduction/#visualize","title":"Visualize","text":"<p>We can easily visualize the low-dimensional data set using a scatter plot:</p> <pre><code>import matplotlib.pyplot as plt\n\ncomponents = pd.DataFrame(components, columns=[\"PC1\", \"PC2\"])\ncomponents.plot(kind=\"scatter\", x=\"PC1\", y=\"PC2\", alpha=0.5)  # (1)!\nplt.show()\n</code></pre> <ol> <li>The <code>alpha</code> parameter controls the transparency of the points. A value of     <code>0.5</code> makes the points semi-transparent.</li> </ol>          PCA visualized: The semiconductor data set reduced to 2 dimensions.         With principal component 1 on the x-axis and principal component 2 on         the y-axis.      <p>To quickly recap so far: We were able to reduce the semiconductor data set from <code>590</code> features to just <code>2</code>.</p>"},{"location":"data-science/algorithms/unsupervised/dim-reduction/#plot-interpretation","title":"Plot interpretation","text":"<p>The scatter plot shows the data set in a 2D space with each observation as a point. Additionally, we can observe clusters. Since, principal components are ranked by the amount of variance they capture, the first component (PC1) is \"more important\" than the second component (PC2).</p> <p>Therefore, differences along the x-axis (PC1) are more significant than differences along the y-axis (PC2). As we are interested in potential anomalies in semiconductor products, we can detect some observations that might be well worth some further investigation:</p>          Potential anomalies in the semiconductor data set.      <p>A majority of the data points are clustered in the upper left corner. Contrary, these single observations with a high difference on the x-axis (PC1) might be anomalies (annotated by these arrows). Although, samples within the encircled area have their differences on the y-axis (PC2), they are still worth investigating.</p> Re-apply PCA on unscaled data <p>What would happen if you apply PCA to the unscaled data?</p> <ol> <li>Create a new PCA instance with <code>n_components=2</code>.</li> <li>Fit the PCA model on the <code>data</code> (unscaled) and transform it.</li> <li>Visualize the new components in a 2D scatter plot.</li> <li>Compare the results with the previous PCA visualization.</li> </ol> Tip <p>PCA is sensitive to the scale of the data. Thus, the scaled data nicely separates the clusters, while the unscaled data does not. So be sure to pick the right preprocessing steps for your data.</p>"},{"location":"data-science/algorithms/unsupervised/dim-reduction/#explained-variance","title":"Explained variance","text":"<p>When evaluating a PCA model, it is crucial to understand how much variance is captured by each principal component. Simply access the <code>explained_variance_ratio_</code> attribute:</p> <pre><code>print(pca.explained_variance_ratio_)\n</code></pre> <p>Regarding, the <code>pca</code> fitted on the scaled data, the output is:</p> &gt;&gt;&gt; Output<pre><code>[0.06319909 0.03672407]\n</code></pre> <p>The first principal component captures approximately <code>6.32%</code> of the variance, while the second component captures <code>3.67%</code>. Together, the two components capture roughly <code>10%</code> of the variance.</p> Tip <p>Put simply, our two principal components capture <code>10%</code> of the variance of the original <code>590</code> features which is not that great. </p> <p>Unfortunately, when dealing with real world data, results may not be as promising as expected. In this case, we might need to consider more components to capture a higher percentage of the variance.</p> Choosing the number of components <p>It is essential to choose the right number of components. For example, you could use the components as features for another machine learning model, hence you want to retain as much information as possible.</p> <p>However, the choice of how many components to keep is subjective. A common approach is to retain enough components to explain 90-95% of the variance.</p> Number of components to exceed 95% variance <p>Using the scaled semiconductor dataset:</p> <ol> <li>Create a PCA model to analyze the variance in the data</li> <li>Determine the minimum number of principal components needed to explain at     least 95% of the total variance</li> </ol> <p>Solution approaches:</p> <ul> <li>You can use the <code>explained_variance_ratio_</code> attribute, OR</li> <li>There is an alternative approach that requires only 3 lines of code maximum     (hint: google and check the PCA documentation)</li> </ul> <p>Use the following quiz question to evaluate your answer.</p> # <p>How many components are necessary to explain at least 95% of variance?</p> 146590589 <p>To explain at least 95% of variance, you need 146 components.</p>"},{"location":"data-science/algorithms/unsupervised/dim-reduction/#bonus-pca-k-means-combined","title":"Bonus: PCA &amp; k-means combined","text":"<p>In the previous chapter, we applied k-means clustering to the semiconductor data set. Now, we can combine PCA and k-means to cluster the data set in a lower-dimensional space. This approach can help us plot the clusters in a 2D space.</p> <p>With the <code>elbow_method()</code> from the previous chapter we provide an end-to-end solution.</p> <code>elbow_method()</code> <pre><code>def elbow_method(X, max_clusters=15):\n    inertia = []\n    K = range(1, max_clusters + 1)\n\n    for k in K:\n        model = KMeans(n_clusters=k, random_state=42)\n        model.fit(X)\n        inertia.append(model.inertia_)\n\n    # for convenience store in a DataFrame\n    distortions = pd.DataFrame(\n        {\"k (number of cluster)\": K, \"inertia (J)\": inertia}\n    )\n\n    return distortions\n</code></pre> Read and run the code <p>Be sure to read, run and comprehend the code below.</p> PCA &amp; k-means combined<pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\ndata = pd.read_csv(\"semiconductor.csv\")\n\n# data preprocessing\ndata = data.fillna(data.mean())\nscaler = StandardScaler()  # Z-Score\nscaled_data = scaler.fit_transform(data)\n\n# apply PCA\npca = PCA(n_components=2, random_state=42)\ncomponents = pca.fit_transform(scaled_data)\n\n# apply k-means on the components\ndistortions = elbow_method(components)\ndistortions.plot(\n    x=\"k (number of cluster)\",\n    y=\"inertia (J)\",\n    marker=\"o\",\n    title=\"Elbow method on 2 principal components\",\n)\nplt.show()\n\n# optimal number of clusters=5\nmodel = KMeans(n_clusters=5, random_state=42)\ncluster_indices = model.fit_predict(components)\n\n# plot the clusters in 2D\ncomponents = pd.DataFrame(components, columns=[\"PC1\", \"PC2\"])\ncomponents[\"cluster\"] = cluster_indices\n# convert to categorical for plotting\ncomponents[\"cluster\"] = components[\"cluster\"].astype(\"category\")\n\ncomponents.plot(\n    kind=\"scatter\",\n    x=\"PC1\",\n    y=\"PC2\",\n    c=\"cluster\",  # color by cluster\n    cmap=\"tab10\",  # color map\n    alpha=0.85,\n    title=\"PCA &amp; k-means:\\nClustered semiconductor data\",\n)\nplt.show()\n</code></pre> <p>To summarize, we applied the same preprocessing steps, reduced the data to 2 dimensions using PCA. Afterward, we called the elbow method on the 2 components to determine the optimal number of clusters. Then we applied k-means with <code>n_clusters=5</code>. Finally, we plot the 2 components and color the observations according to their corresponding clusters. Have a look at the resulting plots.</p> Clustered componentsElbow method <p>          Clustered semiconductor data in a 2D space using PCA and k-means.      </p> <p>The plot shows the semiconductor data set clustered into 5 groups. Each color represents a different cluster. The clusters are well separated in the 2D space.</p> <p>          Elbow method applied on 2 principal components.      </p> <p>The plot shows the distortion (inertia) for different numbers of clusters. This time around, we can distinctly see an elbow at <code>k=5</code> clusters. </p>"},{"location":"data-science/algorithms/unsupervised/dim-reduction/#recap","title":"Recap","text":"<p>In this chapter, we concluded the Supervised vs. Unsupervised Learning portion of this course and introduced Principal Component Analysis (PCA), a linear technique for dimensionality reduction.</p> <p>We discussed the inner workings of PCA and applied it to the semiconductor data set, where we could identify potential anomalies in the data. We also visualized the data set in a 2D space, making it easier to interpret and analyze. Lastly, a combination of PCA and k-means revealed distinct clusters in the semiconductor data set.</p>"},{"location":"data-science/basics/intro/","title":"Introduction","text":""},{"location":"data-science/basics/intro/#data-science-vs-machine-learning","title":"Data Science vs. Machine Learning","text":"<p>The terms data science and machine learning are often used interchangeably. Let's explore them to get a better understanding of this course's content.</p>  Data Science Machine Learning <p>Data Science is an interdisciplinary field that combines statistics, programming and domain knowledge to extract insights from data. As a data scientist, you could work in vastly different domains, from healthcare and finance to manufacturing and entertainment. The core skills remain the same, but the questions you answer and the data you work with vary greatly.</p> <p>Machine Learning (ML) is a subset of Data Science that focuses on building algorithms that learn patterns from data to make predictions or decisions.</p> The primary focus of this course is the data science workflow, from          setting up your computer to data preparation, exploring different          machine learning algorithms to model evaluation."},{"location":"data-science/basics/intro/#what-to-expect","title":"What to Expect","text":"<p>Before diving into examples and workflows, let's set realistic expectations.</p> <p>Data science is fundamentally about understanding and insight, not perfection. You won't find models that are 100% accurate and that's okay - it's not the goal. Instead, data science helps us:</p> <ul> <li>Uncover patterns in complex data that humans can't easily spot</li> <li>Make informed decisions based on evidence rather than intuition alone</li> <li>Quantify uncertainty by understanding where and why models make mistakes</li> <li>Provide actionable insights that drive business or research value</li> </ul>"},{"location":"data-science/basics/intro/#examples","title":"Examples","text":"<p>Chances are you've already used services built by data scientists today:</p> <ul> <li> Dynamic Pricing: Airlines and concert platforms     adjust prices based on demand, time and user behavior</li> <li> Recommendation Systems: Netflix suggests movies based on     your viewing history; Instagram curates your feed</li> <li> Spam Detection: Your email provider filters unwanted     messages automatically</li> </ul> <p>In this course, we'll build models for tasks like:</p> <ul> <li> Price Prediction: Estimating house prices based on     features like size and location</li> <li> Medical Diagnosis: Classifying tumors as malignant or     benign</li> <li> Anomaly Detection: Identifying faulty products in     manufacturing data</li> </ul>"},{"location":"data-science/basics/intro/#building-blocks","title":"Building blocks","text":"<p>A typical data science project includes several stages, from collecting raw data to deploying models in production. This course focuses on the core workflow:</p> <pre><code>flowchart TD\n    A[Data Collection] --&gt; B[Data Preparation]\n    B --&gt; C[Data Preprocessing]\n    C --&gt; D[Modeling]\n    D --&gt; E[Evaluation]\n    E --&gt; F[Deployment]</code></pre> Stage What You'll Learn Data Preparation Inspect, clean and structure datasets Data Preprocessing Transform features (encoding, scaling, etc., ) Modeling Train different machine learning algorithms Evaluation Measure performance and interpret results Iterative Process <p>Data science is rarely linear. You\u2019ll repeatedly cycle through collecting data, preparing it, training models and evaluating results. Each evaluation highlights new issues (e.g., missing data or unrealistic assumptions) that send you back to earlier stages to improve your approach.</p> <p>Throughout the course, we'll use hands-on Python examples. By the end, you'll apply these skills to a complete project from start to finish.</p> <p>Let's start by setting up your computer for the data science journey.</p>"},{"location":"data-science/basics/setup/","title":"Setup","text":"<p>To get started, we setup the programming environment. Follow these couple of steps to get ready, no prerequisites needed.</p>"},{"location":"data-science/basics/setup/#visual-studio-code","title":"Visual Studio Code","text":"<p>First, install a code editor. We urge you to instal Visual Studio Code (VS Code) a free and open-source editor developed by Microsoft .</p> <p>If you don't have Visual Studio Code already installed, download it from their website: https://code.visualstudio.com/.</p>"},{"location":"data-science/basics/setup/#profile","title":"Profile","text":"<p>To quickstart your VS Code setup, download our profile that includes essential plugins and convenient settings tailored for data science work.</p> <p>VS Code Profile </p>"},{"location":"data-science/basics/setup/#included-extensions","title":"Included Extensions","text":"<p>The profile comes with the following essential extensions:</p> <ul> <li>Python - Core Python language support</li> <li>Python Debugger - Debug your Python code</li> <li>Jupyter - Work with Jupyter Notebooks directly in VS Code</li> </ul> <p>Additionally, stylistic plugins are included for a more pleasant coding experience and auto-save is enabled by default so you never lose your work. </p>"},{"location":"data-science/basics/setup/#uv","title":"<code>uv</code>","text":"<p>From the Python course you should already be familiar with the package manager <code>pip</code>. That background will help you quickly understand <code>uv</code>, a modern tool that not only replaces <code>pip</code> for package management but also handles Python installations.</p> <p>Why the switch? While <code>pip</code> remains widely used and important to understand, this course aims to prepare you for modern real-world projects. <code>uv</code> has become a popular, state-of-the-art tool in modern Python development and learning it now will give you a competitive advantage.</p> No prior Python install necessary <p>A key benefit of <code>uv</code> is that you don\u2019t need to install Python manually.</p>"},{"location":"data-science/basics/setup/#install-uv","title":"Install <code>uv</code>","text":"Windows MacOS /  Linux <p>Open Windows Powershell. Visit the <code>uv</code> documentation under under \"Standalone installer\" link. Make sure the Windows tab is selected.</p> <p>Return to PowerShell and paste the installer command shown in the docs.</p> <p></p> <p>On macOS or Linux, open Terminal. Visit the <code>uv</code> documentation under \"Standalone installer\", link. Make sure the macOS or Linux tab is selected.</p> <p>Return to your terminal and paste the installer command.</p> <p>Press Enter to execute the command</p> <p>Regardless of your operating system, upon completion you should see something like:</p> <pre><code>Downloading uv\n    uv\n    uvx\n    uvw\neverything's installed!\n</code></pre> <p>You can now close the Terminal ( macOS /  Linux) or PowerShell ( Windows).</p> Info <p>The following steps are OS-agnostic; they are the same for Windows, macOS and Linux.</p>"},{"location":"data-science/basics/setup/#1-create-a-project","title":"1. Create a project","text":"<p>Now, we will cover a typical workflow to set up and initialize a new project.</p> Info <p>A project is a folder that contains all scripts, configuration and data files that belong together. Everything for the project lives in that folder.</p> <p>Create a new folder named <code>data-science</code> in an easy-to-find location you\u2019ll use throughout this course.</p> <p>Open VS Code. Go to File \u2192 Open Folder\u2026, select the <code>data-science</code> folder. VS Code will open a new window.</p> Tip <p>For more on navigating VS Code, see the Python course chapter: link</p>"},{"location":"data-science/basics/setup/#2-initialize-the-project","title":"2. Initialize the project","text":"<p>In VS Code, open the integrated terminal (via Terminal \u2192 New Terminal).</p> <pre><code>uv init --vcs none  # (1)!\n</code></pre> <ol> <li>With the <code>--vcs</code> flag a version control system can be specified.     By default <code>--vcs git</code> is set, which initializes a git repository. Since     git is not within the scope of this project, we set <code>--vcs</code> to none.</li> </ol> <p>This initializes the project. <code>uv</code> creates a few files in your folder. Your workspace should look like this:</p> <p>With the project structure:</p> <pre><code>\ud83d\udcc1 data-science/\n\u251c\u2500\u2500 \ud83d\udcc4 .python-version\n\u251c\u2500\u2500 \ud83d\udc0d main.py\n\u251c\u2500\u2500 \ud83d\udcc4 pyproject.toml\n\u251c\u2500\u2500 \ud83d\udcc4 README.md\n</code></pre>"},{"location":"data-science/basics/setup/#explore-the-new-files","title":"Explore the new files","text":"<p>Click through these new files:</p> <ul> <li><code>.python-version</code> Contains the Python version used by your virtual     environment.</li> <li><code>main.py</code> An entry script to verify the setup (we\u2019ll revisit this later).</li> <li><code>pyproject.toml</code> Project metadata such as name and version.</li> <li><code>README.md</code> An empty README for a project description; you can ignore it for     now.</li> </ul>"},{"location":"data-science/basics/setup/#3-virtual-environment","title":"3. Virtual Environment","text":"<p>With an initialized project we can easily set up a virtual environment. To do so simply run:</p> <pre><code>uv sync\n</code></pre> Virtual Environments? <p>If you need a refresh on virtual environments, what they do and their purpose, read through the corresponding section in the Python course: link</p>"},{"location":"data-science/basics/setup/#what-happens-during-uv-sync","title":"What happens during <code>uv sync</code>?","text":"<p>When you run <code>uv sync</code>, three things happen automatically:</p> <ol> <li> <p>Python installation: <code>uv</code> checks the <code>.python-version</code> file and installs     the specified Python version if it's not already available on your     machine.</p> </li> <li> <p>Virtual environment: A <code>.venv</code> folder is created at the root of your     project, containing an isolated Python environment for your project.</p> </li> <li> <p>Dependency locking: A <code>uv.lock</code> file is generated. This file pins all     package versions used in your project, ensuring anyone else can faithfully     recreate the exact same environment.</p> </li> </ol> No manual edits <p>Since the <code>uv.lock</code> is auto-generated, never ever manually edit this file!</p>"},{"location":"data-science/basics/setup/#test-your-setup","title":"Test your setup","text":"<p>Let's verify everything works by running the <code>main.py</code> script that was created during initialization:</p> <pre><code>uv run main.py\n</code></pre> <p>If you have a similar output, you've successfully created your first project. </p> &gt;&gt;&gt; Output<pre><code>Hello from data-science!\n</code></pre> No activation needed <p>Notice that the <code>run</code> command automatically invokes the project's virtual environment, meaning you do not have to activate the environment beforehand. In practice that means you create your scripts and simply execute them without an activated environment.</p>"},{"location":"data-science/basics/setup/#4-packages","title":"4. Packages","text":"<p>Since, we will be working with a couple of different packages, we have to discuss commands for installing and removing packages.</p> Again, no activation needed <p>Once again, you don't have to activate your environment to install and remove packages. With <code>uv</code>, you can manage dependencies directly from any terminal in your project folder, the virtual environment is \"handled\" automatically in the background.</p> <p>To install packages use the <code>add</code> command:</p> <pre><code>uv add &lt;package-name&gt;\n</code></pre> <p>replace <code>&lt;package-name&gt;</code> for example with <code>pandas</code>:</p> <pre><code>uv add pandas\n</code></pre> <p>After a successful installation, take some time to open the <code>pyproject.toml</code> file. Under dependencies you should find the <code>pandas</code> package.</p> pyproject.toml<pre><code>[project]\nname = \"data-science\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.14\"\ndependencies = [\n    \"pandas&gt;=3.0.0\",\n]\n</code></pre> <p>The content of <code>uv.lock</code> was changed as well, the file contains more info on the installed packages such as <code>pandas</code> and its dependencies as well (i.e., <code>numpy</code>, <code>python-dateutil</code>, <code>six</code> and <code>tzdata</code>).</p> Share a project <p>If you share your project, be sure to include the files <code>.python-version</code>, <code>pyproject.toml</code> and <code>uv.lock</code>. These allow for a recreation of your virtual environment.</p> <p>Let's remove the package with the <code>remove</code> command:</p> <pre><code>uv remove pandas\n</code></pre> <p>Again, you can check both <code>pyproject.toml</code> and <code>uv.lock</code> which are automatically updated accordingly.</p> Get a script running <ol> <li> <p>Create a new script called <code>plot.py</code></p> </li> <li> <p>Paste following example (taken from     matplotlib docs)     within your script:</p> plot.py<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import PathPatch\nfrom matplotlib.path import Path\n\nN = 400\nt = np.linspace(0, 2 * np.pi, N)\nr = 0.5 + np.cos(t)\nx, y = r * np.cos(t), r * np.sin(t)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"k\")\nax.set(aspect=1)\nplt.show()\n</code></pre> </li> <li> <p>Determine necessary packages to get this script running and install them     with <code>uv</code>.</p> </li> <li> <p>Lastly, the script with <code>uv</code>.</p> </li> </ol>"},{"location":"data-science/basics/setup/#python-scripts-or-jupyter-notebooks","title":"Python Scripts or Jupyter Notebooks?","text":"<p>For this course, you can work with Python scripts (<code>.py</code> files) and/or Jupyter Notebooks (<code>.ipynb</code> files). Both are supported in VS Code and each has its strengths.</p> <ul> <li> <p> Python Scripts</p> <p> Advantages</p> <ul> <li>Better for production code and reusability</li> <li>Easier version control and collaboration</li> <li>Runs faster without cell-by-cell overhead</li> <li>Cleaner debugging with standard tools</li> </ul> <p> Disadvantages</p> <ul> <li>Less interactive during exploration</li> <li>Need to rerun entire script for changes</li> <li>Harder to visualize intermediate results</li> </ul> </li> <li> <p> Jupyter Notebooks</p> <p> Advantages</p> <ul> <li>Great for exploration and prototyping</li> <li>Inline visualizations</li> <li>Combines documentation and code</li> <li>Easier to share findings with non-programmers</li> </ul> <p> Disadvantages</p> <ul> <li>Can become messy with non-linear execution</li> <li>Harder to maintain as projects grow</li> <li>More challenging for version control</li> <li>Not ideal for reusability</li> </ul> </li> </ul> Our recommendation <p>Many data scientists use both: notebooks for exploration, scripts for production. Simply experiment with both. For quick prototyping lean towards a  Jupyter Notebook. For more refined code switch to  Python scripts.</p>"},{"location":"data-science/basics/setup/#wrap-up","title":"Wrap-Up","text":"<p>You've successfully set up your development environment! Throughout this course, you'll create multiple projects using the workflow covered in sections 1-4. Don't worry about memorizing every step\u2014just refer back to this page when needed.</p> <p>For quick reference, here's a cheat sheet:</p> Cheat Sheet - Project Setup <ol> <li>Create a new folder for your project</li> <li>Open the folder in VS Code</li> <li>In the terminal, run:     <pre><code>uv init --vcs none\nuv sync\n</code></pre></li> <li>Install packages as needed:     <pre><code>uv add &lt;package-name&gt;\n</code></pre></li> <li>Run your code:     <pre><code>uv run &lt;script-name&gt;.py\n</code></pre></li> </ol> <p>Need help? Run <code>uv --help</code> for more commands and options.</p>"},{"location":"data-science/data/basics/","title":"Data Basics","text":"<p>This chapter kicks off the foundational building blocks of a data science pipeline. We start by taking a closer look at data itself. Understanding different attribute types is crucial for choosing appropriate visualizations, preprocessing techniques and machine learning algorithms.</p> Create a new project <ol> <li>For this chapter create a new project. Revisit the     wrap-up section from the setup guide.</li> <li>Install the packages <code>seaborn</code> and <code>pandas</code></li> </ol>"},{"location":"data-science/data/basics/#tabular-data","title":"Tabular Data","text":"<p>Throughout this course, we will primarily work with tabular data, simply think of spreadsheets. Tabular data is organized in a rectangular format with:</p> <ul> <li>Rows: Individual observations or samples (e.g., one student)</li> <li>Columns: Attributes or features describing each observation (e.g., name,     age, average grade)</li> </ul> Name Age Average Grade Claudia 19 1.45 Stefan 22 3.4 Max 20 2.12 <p>Each row represents one student, while each column contains a specific attribute about that student.</p> <p>Understanding the structure of tabular data is essential because most machine learning algorithms expect data in this format. Now let's explore what types of information each column can contain.</p>"},{"location":"data-science/data/basics/#attribute-types","title":"Attribute Types","text":"<p>Not all data is created equal. The type of data in each column determines what operations we can perform and which visualizations make sense. We distinguish between two main categories: numerical and categorical data.</p>"},{"location":"data-science/data/basics/#numerical-quantitative","title":"Numerical (Quantitative)","text":"<p>Numerical data represents measurable quantities, i.e., values you can perform mathematical operations on.</p> <pre><code>import pandas as pd\n\ntemperatures = pd.Series([22.5, 18.3, 25.1, 19.8, 23.4])\n\nprint(f\"Average temperature: {temperatures.mean()}\u00b0C\")\nprint(f\"Maximum temperature: {temperatures.max()}\u00b0C\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Average temperature: 21.82\u00b0C\nMaximum temperature: 25.1\u00b0C\n</code></pre> <p>Numerical data comes in two types:</p> <p>Continuous: Can take any value within a range, including decimals. Examples include temperature (22.5\u00b0C), body mass (3750.5g) or height (1.75m).</p> <p>Discrete: Can only take specific, countable values, typically integers. Examples include number of students (5) or age (22).</p> Tip <p>A simple rule of thumb: If you can meaningfully have fractional values, it's continuous. If counting whole units makes more sense, it's discrete.</p>"},{"location":"data-science/data/basics/#categorical-qualitative","title":"Categorical (Qualitative)","text":"<p>Categorical data represents qualities or characteristics that place observations into groups or categories.</p> <pre><code>colors = pd.Series([\"red\", \"blue\", \"green\", \"red\", \"yellow\"])\n\nprint(f\"Unique colors: {colors.nunique()}\")\nprint(f\"Most common: {colors.mode().squeeze()}\")  # (1)!\n</code></pre> <ol> <li>The <code>mode()</code> method returns a <code>pd.Series</code> with a single value, hence we     <code>squeeze()</code> the value.</li> </ol> &gt;&gt;&gt; Output<pre><code>Unique colors: 4\nMost common: red\n</code></pre> <p>Categorical data can be further divided into two types:</p>"},{"location":"data-science/data/basics/#nominal","title":"Nominal","text":"<p>Nominal data has no inherent order, the categories are just different names or labels. Examples include colors or country names.</p>"},{"location":"data-science/data/basics/#ordinal","title":"Ordinal","text":"<p>Ordinal data has a meaningful order or ranking between categories, but the distance between categories isn't necessarily equal. Examples include t-shirt sizes (XS, S, M, L, XL) or education levels (High School, Bachelor's, Master's, PhD).</p> <p>Now that we understand different data types, let's see them in action with real data.</p>"},{"location":"data-science/data/basics/#penguins","title":"Penguins","text":"<p>We'll use the Palmer Penguins dataset, which contains measurements of three penguin species observed on islands in the Palmer Archipelago, Antarctica.</p>          The Palmer Penguins dataset contains three species: Adelie, Chinstrap          and Gentoo penguins, each with distinct physical characteristics.      Info <p>The Palmer Penguins dataset was collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER.<sup>1</sup> It's become a popular dataset for education.</p>"},{"location":"data-science/data/basics/#loading-the-data","title":"Loading the Data","text":"<p>Let's load the penguins dataset and explore its structure.</p> <pre><code>import seaborn as sns\n\npenguins = sns.load_dataset(\"penguins\")\nprint(penguins.head())\n</code></pre> # <p>How many rows and columns has the <code>penguin</code> dataset?</p> 4 rows and 7 columns5 rows and 7 columns5 rows and 8 columns344 rows and 7 columns <p>The data set has 344 rows (penguins) and 7 columns (features). Use <code>data.shape</code> to quickly get the datasets dimensions.</p> Identify attribute types <p>Looking at the dataset, can you identify which attributes are:</p> <ul> <li>Numerical?</li> <li>Categorical?</li> </ul>"},{"location":"data-science/data/basics/#numerical-attributes","title":"Numerical attributes","text":"<p>The dataset contains several numerical measurements. Let's focus on <code>\"body_mass_g\"</code> as our primary example. Easily get basic statistics with the <code>describe()</code> method:</p> <pre><code>print(penguins[\"body_mass_g\"].describe())\n</code></pre> &gt;&gt;&gt; Output<pre><code>count     342.000000\nmean     4201.754386\nstd       801.954536\nmin      2700.000000\n25%      3550.000000\n50%      4050.000000\n75%      4750.000000\nmax      6300.000000\nName: body_mass_g, dtype: float64\n</code></pre> <p>The mean body mass is roughly 4200g (about 4.2kg or 9.3 pounds), with values ranging from 2700g to 6300g. This variation is quite substantial, the heaviest penguins are more than twice the weight of the lightest ones! The standard deviation of 802g indicates considerable variability in penguin sizes.</p> Missing values <p>You might wonder why the count is 342. There are two missing values within <code>\"body_mass_g\"</code>, resulting again in 344 penguins.</p> <p>For now, we don't worry about missing values as pandas excludes them when applying methods such as the <code>describe()</code> method above. The subsequent chapters will dive into missing values.</p>"},{"location":"data-science/data/basics/#categorical-attributes","title":"Categorical attributes","text":"<p>For categorical attributes, let's examine <code>\"sex\"</code>. Just like with numerical attributes, we can apply the <code>describe()</code> method.</p> <pre><code>print(penguins[\"sex\"].describe())\n</code></pre> &gt;&gt;&gt; Output<pre><code>count      333\nunique       2\ntop       Male\nfreq       168\n</code></pre> <p>Notice how pandas automatically infers the data type and calculates appropriate metrics. Unlike numerical data, calculating mean, min or max would be meaningless for categorical data.</p>"},{"location":"data-science/data/basics/#visualizing-different-attribute-types","title":"Visualizing different attribute types","text":"<p>A key component of data science is visualization, which helps us understand patterns and distributions in our data. Different attribute types require different visualization approaches.</p>"},{"location":"data-science/data/basics/#numerical","title":"Numerical","text":"<p>For numerical attributes like <code>\"body_mass_g\"</code>, we can create a boxplot which shows the median, quartiles and outliers.</p> Plotting with pandas <p>Both <code>pandas.DataFrame</code> and <code>pandas.Series</code> objects have a built-in <code>plot()</code> method that provides quick access to various plot types. Check out the documentation for DataFrame.plot() and Series.plot() to see which plots are supported via the <code>kind</code> argument.</p> <pre><code>import matplotlib.pyplot as plt\n\npenguins[\"body_mass_g\"].plot(kind=\"box\")\nplt.show()\n</code></pre> <p>For numerical data, other suitable plots include histograms (<code>kind=\"hist\"</code>) for showing distribution patterns, or scatter plots (<code>kind=\"scatter\"</code>) for revealing relationships between two numerical variables (like <code>\"flipper_length_mm\"</code> vs. <code>\"body_mass_g\"</code>).</p>"},{"location":"data-science/data/basics/#categorical","title":"Categorical","text":"<p>For categorical data like penguin <code>\"sex\"</code>, a bar chart or pie chart displays the frequency of each category.</p> <pre><code># first, we need the counts of each category (male, female)\ncounts = penguins[\"sex\"].value_counts()\ncounts.plot(kind=\"pie\")\nplt.show()\n</code></pre> <p>The visualization reveals that male and female penguins are nearly equally distributed in the dataset.</p> Choosing the right plot for categorical data <p>While pie charts work well for showing proportions, bar charts are often preferred when comparing more than 3-4 categories or when precise comparison of values is important. Try <code>kind=\"bar\"</code> to see the difference!</p>"},{"location":"data-science/data/basics/#exercises","title":"Exercises","text":"Exercise 1: Explore bill length <ol> <li>Calculate basic statistics for <code>\"bill_length_mm\"</code></li> <li>Create a histogram to visualize its distribution</li> </ol> <p>What's the median bill length? Do you notice any patterns?</p> Exercise 2: Island distribution <ol> <li>Count how many penguins were observed on each island</li> <li>Create a bar chart showing the distribution</li> </ol> <p>Which island has the most penguin observations?</p>"},{"location":"data-science/data/basics/#recap","title":"Recap","text":"<p>In this chapter, we established the foundation for understanding data:</p> <ul> <li>Tabular data organizes information in rows (observations) and columns     (attributes/features)</li> <li>Numerical data represents measurable quantities (continuous or discrete)</li> <li>Categorical data represents groups or categories (nominal or ordinal)</li> <li>Different data types require different visualization approaches</li> </ul>"},{"location":"data-science/data/basics/#further-reading","title":"Further Reading","text":"<p>Expand your knowledge with these related topics:</p> <ul> <li>Plotting Guide: Learn to configure     plots, add styling, titles and customize visualizations</li> <li>Distributions: Dive deeper     into statistical distributions and advanced visualization techniques</li> <li>Pandas Documentation:     Comprehensive guide to data manipulation with pandas</li> </ul> <ol> <li> <p>Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi: 10.5281/zenodo.3960218.\u00a0\u21a9</p> </li> </ol>"},{"location":"data-science/data/preparation/","title":"Data Preparation","text":""},{"location":"data-science/data/preparation/#preface","title":"Preface","text":"Info <p>Starting with this chapter, we will work with adapted data from:</p> <p>S. Moro, P. Cortez and P. Rita (2014). A Data-Driven Approach to Predict the Success of Bank Telemarketing<sup>1</sup></p> <p>The publicly available dataset is from a Portuguese retail bank and houses information on direct marketing campaigns (phone calls). Bank customers were contacted and asked to subscribe to a term deposit. Using this practical example, we will explore the realms of:</p> <ul> <li> <p>Data merging</p> </li> <li> <p>Data cleaning</p> </li> <li> <p>Data transformation</p> </li> <li> <p>Machine learning (with selected algorithms)</p> </li> <li> <p>Comparison of model performance</p> </li> <li> <p>Model persistence (practical guide on how to save and load machine learning     models)</p> <p>Eventually, you will end up with a model that predicts whether a customer will subscribe to a term deposit or not.</p> </li> </ul>"},{"location":"data-science/data/preparation/#obtaining-the-data","title":"Obtaining the data","text":"Set up a project <p>Once again, set up a new project with <code>uv</code>. Call this one <code>bank_marketing</code>. We will perform all steps from data merging to saving the model in this project.</p> <p>If you are having trouble setting up a virtual environment, please refer to the <code>uv</code> wrap-up section.</p> <p>Let's dive right in and download both files:</p> <p>Bank Marketing  Bank Marketing Social Features </p> <p>Place the files in a new folder called <code>data</code>. Your project now should look like this:</p> <pre><code>\ud83d\udcc1 bank_marketing/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 data/\n\u251c\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank.tsv\n\u2514\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank-social.csv\n</code></pre> Open the files <p>Before we start, simply open the files with a text editor. Scroll through both files and read a couple of rows to get acquainted with the data.</p>"},{"location":"data-science/data/preparation/#read-the-files","title":"Read the files","text":"<p>Since we are obviously dealing with two rather large files, we opt to read them with <code>Python</code> . At the end of this section we end up with a single (clean!) data set.</p> Info <p>Conveniently, in our case the data was already collected, saving us hours and hours of work. Thus, we can focus on the data preparation step. Since data is commonly obtained from different sources and in various different formats, both data sets we have at hand (<code>bank.tsv</code> and <code>bank-social.csv</code>) will mimic theses scenarios.</p> <p>To start, we are using <code>pandas</code> for reading and manipulating data. If you haven't already, install the package within your environment. Assuming your Jupyter Notebook or script is located at the project's root, we start by reading the first file  <code>bank.tsv</code>.</p> <pre><code>import pandas as pd\n\ndata = pd.read_csv(\"data/bank.tsv\", sep=\"\\t\")\n</code></pre> <p>Although, we can use a simple single-liner to read the file, there are a couple of things to break down:</p> <ol> <li>We are dealing with a tab-separated file, meaning values within the file are     separated by a tab character (<code>\\t</code>). The fact that we are dealing with a     tab-separated file is indicated by the file extension <code>.tsv</code> and the space     surrounding the values within the file.</li> <li>Although we do not have a <code>csv</code> file at hand, <code>pandas</code> is versatile enough     to handle different separators. Thus, we can utilize the     <code>pd.read_csv()</code> function to read the file. Tip: All sorts of     text files can be usually read with <code>pd.read_csv()</code>.</li> <li>Lastly, the <code>sep</code> parameter is set to <code>\\t</code> to indicate the tab separation.</li> </ol> <p>Let's read the second file  <code>bank-social.csv</code>.</p> # <p>Open the file <code>bank-social.csv</code> with your text editor. Which separator is used in the file?</p> : (colon)None, it is not a valid csv file.; (semicolon), (comma) <p>Values are separated by a semicolon. </p> Read the second file <p>Simply read the second file (<code>bank-social.csv</code>) with <code>pd.read_csv()</code> and specify the appropriate separator. Store the <code>DataFrame</code> in a variable called <code>data_social</code>.</p>"},{"location":"data-science/data/preparation/#duplicated-data","title":"Duplicated data","text":"<p>Now, with both files in memory, let's examine them closer in order to perform a merge.</p> <code>print(data.head())</code><code>print(data_social.head())</code> id age default housing ... cons.conf.idx euribor3m nr.employed y 1 30 no yes ... -46.2 1.313 5099.1 no 2 39 no no ... -36.4 4.855 5191.0 no 3 25 no yes ... -41.8 4.962 5228.1 no 4 38 no unknown ... -41.8 4.959 5228.1 no 5 47 no yes ... -42.0 4.191 5195.8 no <p>The rows represent customers and the columns are features of the customers. The column <code>y</code> indicates whether a customer subscribed to a term deposit or not. Customers are uniquely identified by the <code>id</code> column. Later on, we will have a closer look at the attributes when modelling the data.</p> id job marital education 2178 technician married professional.course 861 blue-collar single professional.course 3020 technician married professional.course 2129 self-employed married basic.9y 3201 blue-collar married basic.9y <p>Again, each row represents a customer (uniquely identified with <code>id</code>). The remaining columns <code>job</code>, <code>marital</code>, and <code>education</code> are social attributes.</p> <p>Let's examine the shape of both <code>DataFrame</code>s as well.</p> <pre><code>print(\n    f\"Shape of data: {data.shape}; Shape of data_social: {data_social.shape}\"\n)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Shape of data: (4530, 18); Shape of data_social: (4304, 4)\n</code></pre> <p>The output indicates that <code>data</code> contains more observations (customers) than <code>data_social</code>. However, first and foremost it is good practice to check for duplicated data.</p> <pre><code># check for duplicated rows\nprint(data.duplicated().sum())\n</code></pre> &gt;&gt;&gt; Output<pre><code>np.int64(411)\n</code></pre> <p><code>data</code> contains <code>411</code> duplicated rows. These can be removed easily:</p> <pre><code>data = data.drop_duplicates()\n</code></pre> Check for duplicates <p>Check for duplicates in <code>data_social</code> and remove them if necessary.</p> # <p>How many duplicates are present in <code>data_social</code>?</p> None3760411376 <p><code>data_social</code> has 376 duplicated rows. </p> A note on <code>pd.DataFrame.drop_duplicates()</code> <p>By default, the method <code>pd.DataFrame.drop_duplicates()</code> removes all duplicated rows. However, you can pass an argument to <code>subset</code> in order to remove duplicates based on specific columns. For example, if we want to drop duplicates based on the <code>id</code> column, we can do so by:</p> <pre><code>data_social = data_social.drop_duplicates(subset=[\"id\"])\n</code></pre> <p><code>subset</code> can also be all list of multiple columns.</p>"},{"location":"data-science/data/preparation/#merge-methods","title":"Merge methods","text":"<p>To combine both data sets we will use the <code>pd.DataFrame.merge()</code> method to</p> <p>Merge DataFrame or named Series objects with a database-style join</p> <p>pandas <code>merge()</code>docs</p> <p>Looking at the <code>how</code> parameter we are presented with 5 (!) different options to perform a merge. The most common ones are:</p> <ul> <li><code>\"left\"</code></li> <li><code>\"right\"</code></li> <li><code>\"inner\"</code></li> <li><code>\"outer\"</code></li> </ul> <p>In order to be able to choose the appropriate method, we need to break them down:</p> <p></p> <ul> <li>Left join: The resulting <code>DataFrame</code> will contain all rows from the left     <code>DataFrame</code> (data 1) and the matched rows from the right <code>DataFrame</code> (data     2).</li> <li>Right join: The resulting <code>DataFrame</code> will contain all rows from the     right <code>DataFrame</code> (data 2) and the matched rows from the left <code>DataFrame</code>     (data 1).</li> <li>Inner join: The resulting <code>DataFrame</code> will contain only the rows that     have matching values in both <code>DataFrame</code>s.</li> <li>Outer join: The resulting <code>DataFrame</code> will contain all rows from both     <code>DataFrame</code>s.</li> </ul>"},{"location":"data-science/data/preparation/#perform-merges","title":"Perform merges","text":"<p>To get further acquainted with merge methods, we simply perform them all. But first, we need to pick a column which uniquely identifies a row (customer) in both data sets. Conveniently, we have the <code>id</code> column. Regardless of the merge we perform, the parameter <code>on</code> requires a column to match the rows (in our case we set <code>on=\"id\"</code>).</p> <pre><code>left_join = data.merge(data_social, on=\"id\", how=\"left\")\nright_join = data.merge(data_social, on=\"id\", how=\"right\")\ninner_join = data.merge(data_social, on=\"id\", how=\"inner\")\nouter_join = data.merge(data_social, on=\"id\", how=\"outer\")\n</code></pre>"},{"location":"data-science/data/preparation/#a-closer-look","title":"A closer look","text":"<p>To comprehend on of these merges, have a look at the resulting shape and the identifiers the <code>DataFrame</code> contains. Let's examine the <code>right_join</code>:</p> <pre><code>equal_nrows = len(data_social) == len(right_join)\nprint(f\"Merged data has the same number of rows as data_social: {equal_nrows}\")\n\nall_ids_present = data_social[\"id\"].isin(right_join[\"id\"]).all()\nprint(f\"All identifiers from data_social are present? {all_ids_present}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Merged data has the same number of rows as data_social: True\nAll identifiers from data_social are present? True\n</code></pre> <p>A breakdown of the code snippet:</p> <ol> <li> <p><code>equal_nrows</code> indicates that all rows from <code>data_social</code> are present in     <code>right_join</code>.</p> Info <p><code>len(data_social)</code> is equivalent to <code>data_social.shape[0]</code>.</p> </li> <li> <p>To verify that <code>right_join</code> contains all identifiers of <code>data_social</code>, we     make use of the <code>pd.Series.isin()</code> method. This method checks     whether each element of a <code>Series</code> is contained in another <code>Series</code>.</p> Info <p><code>pd.Series.all()</code> returns <code>True</code> if all elements in the <code>Series</code> are <code>True</code>.</p> </li> </ol> Counter check <p>Extend, the previous code snippet:</p> <ol> <li>Do the number of rows from <code>data</code> and <code>right_join</code> match?</li> <li>Are all identifiers from <code>data</code> present in <code>right_join</code>?</li> </ol> <p>Our examinations should conclude that <code>right_join</code> contains all rows/customer data from <code>data_social</code> and solely the matching records from <code>data</code>.</p> Examine remaining merges <p>Look at the shapes of the remaining merges (<code>left_join</code>, <code>inner_join</code>, <code>outer_join</code>) to get a better understanding of the different merge methods and its results.</p> <p>Compare the shape of each merge with the shapes of <code>data</code> and <code>data_social</code>.</p>"},{"location":"data-science/data/preparation/#final-merge-application","title":"Final merge (application)","text":"<p>With a solid understanding of different merge methods, we revisit our initial task to join both data sets. But how can we choose the appropriate method? The short answer; it depends on your data and task at hand. There is no method that fits all scenarios.</p> SQL Explained byu/UnorthodoxPrimitive inProgrammerHumor <p>Since we are eventually fitting a machine learning model on the data, we are interested in customer data that is present in both data sets. I.e., we want to prevent the introduction of additional missing values that would result from an outer join. Furthermore, a left join would leave us with missing attributes from <code>social_data</code>. The same applies to a right join, just vice versa.</p> <p>Long story short, we opt for an <code>\"inner\"</code> merge (or join) which leaves us with only the customers that are present in both data sets. The final merge is as simple as:</p> <pre><code>data_merged = data.merge(data_social, on=\"id\", how=\"inner\")\n</code></pre> <p>Let's examine the shape of the merged data set.</p> <pre><code>print(f\"Shape of data_merged: {data_merged.shape}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Shape of data_merged: (3928, 21)\n</code></pre> <p>We end up with <code>3928</code> customers that are present in both data sets. Lastly, we can write the merged data set to a new file. Let's use a common format  <code>csv</code> with the default <code>,</code> as separator.</p> <pre><code>data_merged.to_csv(\"data/bank-merged.csv\", index=False)\n</code></pre> <p>With <code>index=False</code>, we do not</p> <p>Write row names (index).</p> <p>pandas <code>to_csv()</code> docs</p> <p></p>          Congratulations! \ud83c\udf89 You have finally completed your quest to merge the         data."},{"location":"data-science/data/preparation/#recap","title":"Recap","text":"<p>Using the bank marketing data, we have seen how to find and remove duplicated data, explored different merge methods and ended up with a single data set.</p> <p>In the next chapter, we will explore this data further, look for missing values and perform some basic data transformations.</p> <ol> <li> <p>Decision Support Systems, Volume 62, June 2014, Pages 22-31: https://doi.org/10.1016/j.dss.2014.03.001 \u21a9</p> </li> </ol>"},{"location":"data-science/data/preprocessing/","title":"Data Preprocessing","text":"Continue your data preprocessing quest! \ud83e\uddd9\u200d\u2642\ufe0f  <code>scikit-learn</code> <p>This chapter introduces the package <code>scikit-learn</code>, the swiss-army knife for data preprocessing, transformation and machine learning.</p> <p>We will continue to work with the Portuguese retail bank data set<sup>1</sup> and preprocess it further. Alongside we start to explore <code>scikit-learn</code>'s functionalities.</p> <p>Check-out the excellent scikit-learn documentation.</p>"},{"location":"data-science/data/preprocessing/#prerequisites","title":"Prerequisites","text":"<p>If you have followed the previous chapter closely, your project structure looks like this:</p> <pre><code>\ud83d\udcc1 bank_marketing/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 data/\n\u251c\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank.tsv\n\u251c\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank-merged.csv\n\u2514\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank-social.csv\n</code></pre> <p>With <code>bank-merged.csv</code> being the <code>\"inner\"</code> join of <code>bank.csv</code> and <code>social.csv</code>, minus all duplicated customer data.</p> <p>If you are missing the file <code>bank-merged.csv</code>, we strongly recommend you to go back and complete the previous chapter. For the sake of completeness, we provide a distilled version of the code from Data preparation:</p> Merge the data sets <pre><code># Steps from the Data preparation chapter\nimport pandas as pd\n\ndata = pd.read_csv(\"bank.tsv\", sep=\"\\t\")\ndata_social = pd.read_csv(\"bank-social.csv\", sep=\";\")\n\ndata = data.drop_duplicates()\ndata_social = data_social.drop_duplicates()\n\ndata_merged = data.merge(data_social, on=\"id\", how=\"inner\")\ndata_merged.to_csv(\"data/bank-merged.csv\", index=False)\n</code></pre> <p> Merged data  </p> <p>Again, we urge you to use a virtual environment which by now, should be second nature anyway.</p> Info <p>To follow along, create a new script or Jupyter notebook within your project.</p>"},{"location":"data-science/data/preprocessing/#missing-values","title":"Missing values","text":"<p>After dropping duplicates and merging the data, the next step is to check for missing values. First, we read the data.</p> <pre><code>import pandas as pd\n\ndata = pd.read_csv(\"data/bank-merged.csv\")\n</code></pre> <p>We chain a couple of methods to count the missing values in each column.</p> <pre><code>print(data.isna().sum())\n</code></pre> <p>The <code>isna()</code> method checks each element and whether it is a missing value or not. The result is a <code>DataFrame</code> with boolean values of the same shape as the initial <code>DataFrame</code> (in our case <code>data</code>), with <code>True</code> being a missing value. With the chaining of <code>sum()</code> we simply sum the <code>True</code> values (missing values) for each column.</p> <p>A truncated version of the output is shown below:</p> Column Missing Values id 0 age 0 default 0 housing 0 ... ... job 0 marital 0 education 0 ... ... <p>It seems like the columns have no missing values. To sum missing values of the whole <code>DataFrame</code>, we can chain another <code>sum()</code>.</p> <pre><code>print(data.isna().sum().sum())\n</code></pre> <p>The output once more indicates that the whole data set has <code>0</code> missing values. So far so good, but this is not the end of the story (who saw that coming \ud83e\udd2f).</p> <p></p>          Plot twist...      <p>Although, it seems like we don't have to bother with missing values, they are simply a bit more hidden.</p>"},{"location":"data-science/data/preprocessing/#missing-values-in-disguise","title":"Missing values in disguise","text":"<p><code>pandas</code> considers types like <code>None</code> or <code>np.nan</code> as missing. However in practice, missing values are encoded in various ways. For instance, strings like <code>\"NA\"</code> or integers like <code>-999</code> are used. Consequently, we can't detect these ways of encoding with simply calling <code>isna()</code>.</p> <p>Since we have to manually detect these encoded missing values, it is essential to have a good understanding of the data. Let's get more familiarized with the data.</p> <p>Visit the UCI Machine Learning Repository here which also hosts the data set and some additional information. Interestingly, the section Dataset Information states:</p> <p>Has Missing Values?</p> <p>No</p> <p>Although that might be technical correct (the data contains no empty values), we have to dig deeper.</p> Detect the encoding of missing values <p>Open the UCI Machine Learning Repository. Look at the Variables Table. How are the missing values encoded in the data set?</p> <p>Use the following quiz question to validate your answer.</p> <p>Remember, the bigger picture  by getting more familiar with the data, we can train a better fitting model to predict the target variable <code>y</code> (subscribed to term deposit or not).</p> # <p>How are missing values encoded in this specific data set?</p> \"unknown\"-1\"NA\"999 <p>The label <code>\"unknown\"</code> is used for missing values. Have a closer look at nominal attributes (e.g., occupation) or ordinal attributes (e.g., education).</p>"},{"location":"data-science/data/preprocessing/#missing-values-uncovered","title":"Missing values uncovered","text":"<p>Now that we uncovered the encoding of missing values, we replace them with <code>None</code> to properly detect them and handle them more easily.</p> Replace encoding with <code>None</code> <p>Since, you've detected the particular encoding of missing values, replace them with <code>None</code> across the whole data frame.</p> <p>Use the <code>DataFrame.replace()</code> method and read the docs, especially the Examples section for usage guidance.</p> <p>After solving the question, we (again) sum up the missing values per column.</p> <pre><code>print(data.isna().sum())\n</code></pre> <p>A truncated version of the output:</p> Column Missing Values id 0 age 0 default 760 housing 97 ... ... job 35 marital 11 education 161 ... ... <p>At first glance, a lot of columns contain missing values. Let's calculate the ratio to get a better feeling.</p> <pre><code>count_missing = data.isna().sum()\nn_rows = len(data)\n\nmissing_ratio = (count_missing / n_rows) * 100\nprint(missing_ratio.round(2))\n</code></pre> Column Missing Values (%) id 0.00 age 0.00 default 19.35 housing 2.47 ... ... job 0.89 marital 0.28 education 4.10 ... ... <p>Compared to the initial observation where we found <code>0</code> missing values across the whole data set, it's a stark contrast.</p> <p>Looking at the attribute default, nearly a fifth of the observations are missing (19.35 %). Other attributes contain less missing values, yet we still need to handle them. Therefore, we explore different strategies to deal with missing values.</p> Info <p>Though it might not seem much, being able to detect these missing values will prove invaluable in the future.</p> <p>By identifying and properly handling these gaps, we might be able to train a better fitting model as unaddressed missing values can lead to biased predictions. Most importantly, most algorithms can't handle missing values at all.</p>"},{"location":"data-science/data/preprocessing/#sources","title":"Sources","text":"<p>We have extensively covered how to detect missing values but have not talked about their possible origins.</p> <p>The reasons for missing values can be manifold:</p> <ul> <li>Data collection issues<ul> <li>Non-responses in a survey</li> <li>Equipment failures</li> <li>Simple human errors when entering data</li> </ul> </li> <li>Technical challenges<ul> <li>Preprocessing errors (i.e., merging data sets from multiple sources)</li> </ul> </li> <li>Intentional omissions<ul> <li>Privacy concerns or legal restrictions</li> </ul> </li> </ul> <p>... or the information is simply not available.</p>"},{"location":"data-science/data/preprocessing/#drop-columnsrows","title":"Drop columns/rows","text":"<p>One simple way to handle missing values is to drop (i.e. remove) the respective columns which contain any missing values.</p> <pre><code>data_dropped = data.dropna(axis=1)\n</code></pre> <p><code>axis=1</code> specified the columns to be dropped.</p> <p>To comprehend the impact of this operation, we calculate the number of columns that were removed.</p> <pre><code>print(data.shape[1] - data_dropped.shape[1])\n</code></pre> <p>This operation removed <code>6</code> out of <code>21</code> columns/attributes.</p> Remove rows with missing values <p>Contrary, we can leave all columns and instead drop the rows containing missing values.</p> <ol> <li>Use the     <code>DataFrame.dropna()</code>     method to remove rows with missing values.</li> <li>Calculate the number of rows that were removed.</li> </ol>"},{"location":"data-science/data/preprocessing/#set-a-threshold","title":"Set a threshold","text":"<p>Instead of dropping all rows/columns with gaps, we can set a threshold to only drop columns/rows with a certain amount of missing values.</p> <p>To specify a threshold, make use of the <code>thresh</code> parameter, which takes an <code>int</code> value of non-missing values that a column/row must have, to not be dropped.</p> <p>As an example, we would like to remove all columns holding more than 10 % missing values.</p> <pre><code>import math\n\nthreshold = 0.1  # 10 % threshold\nthreshold = (1 - threshold) * len(data)\nprint(threshold)\n\nthreshold = math.ceil(threshold)  # (1)!\nprint(threshold)\n\ndata_dropped_threshold = data.dropna(axis=1, thresh=threshold)\ndiff = data.shape[1] - data_dropped_threshold.shape[1]\nprint(f\"Number of columns dropped: {diff}\")\n</code></pre> <ol> <li>The <code>math.ceil()</code> function is used to round up the threshold value     to the next integer.</li> </ol> &gt;&gt;&gt; Output<pre><code>3535.2000000000003\n3536\nNumber of columns dropped: 1\n</code></pre> <p>A single column was dropped and therefore exceeded the 10 % threshold of missing values.</p> <p>Depending on the data at hand, dropping rows or columns might be a valid option, if you're dealing with a small number of missing values. However, in other cases these operations might lead to a significant loss of information. Since, we are dealing with a substantial amount of missing values, we are looking for more sophisticated ways to handle them.</p>"},{"location":"data-science/data/preprocessing/#imputation-techniques","title":"Imputation techniques","text":"<p>What about filling in the missing values? The process of replacing missing values is called imputation.</p> <p></p>      Data imputation  <p>There are various imputation techniques available, each with its own advantages and disadvantages.</p>"},{"location":"data-science/data/preprocessing/#fill-manually","title":"Fill manually","text":"<p>Of course, there is always the option to fill the values manually which could be time-consuming and infeasible for large data sets.</p>"},{"location":"data-science/data/preprocessing/#global-constant","title":"Global constant","text":"<p>The simplest way to impute missing values is to replace them with a global constant, i.e., filling gaps across all columns with the same value.</p> <pre><code>data_filled = data.fillna(\"no\")\n</code></pre> <p>This method is straightforward and easy to implement. However, there are some drawbacks:</p> <ul> <li>how to choose the global constant?</li> <li>introduces further challenges with mixed attributes (i.e., nominal/ordinal     and numerical attributes)</li> </ul>"},{"location":"data-science/data/preprocessing/#central-tendency","title":"Central tendency","text":"<p>Another common approach is to replace missing values with the mean, median, or mode of the respective column.</p> <p>Fill a nominal attribute with the mode:</p> <pre><code>job_mode = data[\"job\"].mode()\nprint(job_mode)\n\ndata[\"job_filled\"] = data[\"job\"].fillna(job_mode[0])\n</code></pre> &gt;&gt;&gt; Output<pre><code>0    admin.\n</code></pre> <p>Fill a numerical attribute with the mean:</p> <pre><code>age_mean = data[\"age\"].mean()\nprint(age_mean)\n\ndata[\"age_filled\"] = data[\"age\"].fillna(age_mean)\n</code></pre> &gt;&gt;&gt; Output<pre><code>np.float64(40.1433299389002)\n</code></pre> Info <p>Since the bank data does not contain any numerical attribute with missing values, the above code snippet assumed gaps in age. As there are none, the operation did not change the data.</p>"},{"location":"data-science/data/preprocessing/#machine-learning","title":"Machine Learning","text":"<p>Lastly, we can use machine learning algorithms to predict the missing values. The idea is to estimate the missing values based on the other attributes. Linear regression, k-nearest neighbors, or decision trees are common choices.</p> Info <p>As we have not covered machine learning yet, we won't get into the details. But feel free to return to this section. Especially, this scikit-learn comparison of imputation techniques (including k-nearest neighbors) is a good starting point for further exploration.</p>"},{"location":"data-science/data/preprocessing/#transformation","title":"Transformation","text":"<p>Step by step, we are getting closer to actually training a machine learning model. Beforehand, we introduce data transformations that are commonly applied to improve the fit of the model.</p> <p>For starters, install the <code>scikit-learn</code> package within your activated environment.</p> <pre><code>pip install scikit-learn\n</code></pre> <p></p> <code>scikit-learn</code> the swiss-army knife for data          preprocessing and machine learning in Python.      <p>From now on, we will heavily use <code>scikit-learn</code>'s functionalities.</p>"},{"location":"data-science/data/preprocessing/#discretize-numerical-attributes","title":"Discretize numerical attributes","text":"<p>When dealing with noisy data, it is often beneficial to discretize numerical (continuous) attributes.</p> Noise in data <p>Noise is a random error or variance in a measured variable. It is meaningless information that can distort the data.</p> <p>Noise can be identified using basic statistical methods and visualization techniques like boxplots or scatter plots.</p> <p>The process of discretizing is called binning. I.e., the continuous data is separated into intervals (bins). Bins can generally lead to a smoothing effect which in turn reduce the noise.</p> <p>As an example, we pick the attribute age and visualize it with a boxplot.</p> Create a static boxplot <p>To create a static version of the boxplot, perfect for a quick overview:</p> <pre><code>import matplotlib.pyplot as plt\n\ndata[\"age\"].plot(kind=\"box\")  # (1)!\nplt.show()\n</code></pre> <ol> <li>The <code>plot()</code> method uses <code>matplotlib</code> as backend.</li> </ol> <p> </p> <p>Since, age contains outliers, we discretize the attribute age into five bins with the same width.</p> <pre><code>from sklearn.preprocessing import KBinsDiscretizer\n\nbins = KBinsDiscretizer(n_bins=5, strategy=\"uniform\", encode=\"ordinal\")\nbins.fit(data[[\"age\"]])\nage_binned = bins.transform(data[[\"age\"]])  # (1)!\n</code></pre> <ol> <li>The additional square brackets in <code>data[[\"age\"]]</code> are used to     select the column age as a <code>DataFrame</code> (instead of a <code>Series</code>). This is     necessary for the <code>transform()</code> method as a two-dimensional input     is required.</li> </ol> <p>The above snippet returns 5 bins with a width of 14 years. Inspect the bin edges with:</p> <pre><code>print(bins.bin_edges_)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[array([18., 32., 46., 60., 74., 88.])]\n</code></pre> <p>Though the actual binning is just two three lines of code, we have a couple of things to dissect.</p> Working with <code>scikit-learn</code> <p>Although the package is named <code>scikit-learn</code>, it is imported as <code>import sklearn</code>. Package names on PyPI (Python Package Index) can be different from the import name.</p> <p><code>scikit-learn</code> frequently uses classes (e.g., <code>KBinsDiscretizer</code>) to represent different models and preprocessing techniques. Two important methods that many of these classes implement are <code>fit</code> and <code>transform</code>.</p> <ul> <li> <p><code>fit(X)</code>: This method is used to learn the parameters from the data     (referred to as <code>X</code>).</p> </li> <li> <p><code>transform(X)</code>: This method is used to apply the learned parameters     to the data  <code>X</code>.</p> </li> </ul> <p>Put simply, think about the <code>fit(X)</code> method as scikit-learn takes a look at the data and learns from it. The <code>transform(X)</code> method then transfers this knowledge and applies it to the data.</p> <p>The <code>fit_transform()</code> method combines both of these steps in one.</p> <p>Alternatively, use <code>strategy=\"quantile\"</code> to bin the data based on quantiles and thus create bins with the same number of observations.</p> <pre><code>bins = KBinsDiscretizer(n_bins=5, strategy=\"quantile\", encode=\"ordinal\")\nage_binned = bins.fit_transform(data[[\"age\"]])\n\nprint(bins.bin_edges_)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[array([18., 31., 36., 41., 50., 88.])]\n</code></pre> <p>No matter the strategy <code>\"uniform\"</code> or <code>\"quantile\"</code>, a matrix is returned with the</p> <p>bin identifier encoded as an integer value.</p> <p><code>KBinsDiscretizer</code> docs</p>"},{"location":"data-science/data/preprocessing/#normalization","title":"Normalization","text":"<p>Normalization is a common preprocessing step to scale the data to a standard range, which can improve the performance and training stability of machine learning models. Two popular normalization techniques are Min-Max normalization and Z-Score normalization.</p>"},{"location":"data-science/data/preprocessing/#min-max-normalization","title":"Min-Max Normalization","text":"<p>Min-Max normalization scales the data to a fixed range, usually [0, 1].</p> Definition: Min-Max Normalization \\[ X' = \\frac{X - X_{min}}{X_{max} - X_{min}} \\] <p>where \\(X\\) is the original value, \\(X_{min}\\) is the minimum value of the feature, and \\(X_{max}\\) is the maximum value of the feature.</p> <p>This technique is useful when you want to ensure that all features have the same scale without distorting differences in the ranges of values.</p> <p>To illustrate the normalization, we use the attribute euribor3m (3 month Euribor rate).</p> <p>Euribor is short for Euro Interbank Offered Rate. The Euribor rates are based on the average interest rates at which a large panel of European banks borrow funds from one another.</p> <p>euribor-rates.eu</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\n\nprint(f\"X_min: {data['euribor3m'].min()}, X_max: {data['euribor3m'].max()}\")\n\nscaler = MinMaxScaler()\nscaler.fit(data[[\"euribor3m\"]])\nscaled = scaler.transform(data[[\"euribor3m\"]])\n\nprint(scaled)\nprint(f\"Min: {scaled.min()}, Max: {scaled.max()}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>X_min: 0.635, X_max: 4.97\n\n[[0.15640138]\n [0.97347174]\n [0.99815456]\n ...\n [0.16585928]\n [0.99907728]\n [0.80392157]]\n\nMin: 0.0, Max: 1.0\n</code></pre> Normalization of new data <p>Assume new data is added:</p> <pre><code>new_data = pd.DataFrame({\"euribor3m\": [0.5, 5.0, 2.5]})\n</code></pre> <p>We would like to transform these three new interest rates using the Min Max normalization. Remember that the <code>MinMaxScaler</code> was already fitted on the original data with \\(X_{min}=0.635\\) and \\(X_{max}=4.97\\).</p> <p>Answer the following quiz question. Look at the formula again and try to answer the question without executing code.</p> # <p>What happens if you call <code>transform(new_data)</code>?</p> An error is raised, since the new data has not been fitted.The new data is normalized.The normalization works, but the range [0, 1] is not preserved. <p>Since the newly added Euribor rates of 0.5 and 5.0, are lower or higher than the previous minimum and maximum respectively, the normalization will not preserve the range [0, 1], i.e. resulting in the normalized values:</p> <pre><code>[[-0.03114187], [1.00692042], [0.43021915]]\n</code></pre>"},{"location":"data-science/data/preprocessing/#z-score-normalization","title":"Z-Score Normalization","text":"<p>Z-Score normalization, also known as standardization, scales the data based on the mean and standard deviation of an attribute.</p> Definition: Z-Score Normalization \\[ X' = \\frac{X - \\mu}{\\sigma} \\] <p>where \\(\\mu\\) is the mean of the feature and \\(\\sigma\\) is the standard deviation.</p> <p>This technique centers the data around zero with a standard deviation of one, which is useful for algorithms assuming normally distributed data.</p> Apply Z-Score normalization <p>Use the <code>StandardScaler</code> from <code>scikit-learn</code> to apply Z-Score normalization to the attribute campaign (number of times a customer was contacted).</p> <ol> <li>Fit the <code>StandardScaler</code> on the data.</li> <li>Transform the data.</li> <li>Calculate and print the mean and standard deviation of the transformed data.</li> </ol>"},{"location":"data-science/data/preprocessing/#one-hot-encoding","title":"One-Hot Encoding","text":"<p>So far we have focused on numerical attributes. But what about categorical variables? Since, many machine learning algorithms can't handle categorical attributes directly, they need to be encoded. One common technique is to one-hot encode these attributes.</p> <p>Imagine the toy example below to illustrate the concept of one-hot encoding on the feature job.</p>        Your browser does not support the video, consider updating your browser.      Definition: One-Hot Encoding <p>One-hot encoding is a technique to convert categorical attributes into numerical attributes. Each category is represented as a binary vector where only one bit is set to 1 (hot) and the rest are set to 0 (cold).</p> <p>The class <code>OneHotEncoder</code> from <code>scikit-learn</code> can be used to encode categorical attributes to a one-hot encoded representation.</p> Apply One-Hot Encoding <p>Use the <code>OneHotEncoder</code> to encode the the attribute job from the following toy <code>DataFrame</code> (same as in the video).</p> <pre><code>toy_data = pd.DataFrame(\n    {\"id\": [1, 2, 3, 4], \"job\": [\"engineer\", \"student\", \"teacher\", \"student\"]}\n)\n</code></pre> <ol> <li>Apply an instance of <code>OneHotEncoder(sparse_output=False)</code> to job.</li> <li>Check if the resulting matrix matches with the one in the video.</li> </ol>"},{"location":"data-science/data/preprocessing/#label-encoding","title":"Label Encoding","text":"<p>Lastly, we introduce label encoding. Label encoding is another technique to encode categorical attributes. Instead of creating a binary vector for each category, label encoding assigns a unique integer to each category.</p> <p><code>scikit-learn</code>'s <code>LabelEncoder</code> is specifically designed to encode the target variable (i.e., the attribute we want to predict). In our case, we apply the <code>LabelEncoder</code> to the column named <code>\"y\"</code>.</p> <p><code>\"y\"</code>  represents if the client subscribed to a term deposit or not.</p> <pre><code>from sklearn.preprocessing import LabelEncoder\n\nprint(f\"Unique values: {data['y'].unique()}\")\n\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(data[\"y\"])\nprint(y_encoded)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Unique values: ['no' 'yes']\n[0 0 0 ... 0 0 0]\n</code></pre> <p>As <code>\"y\"</code> contains the values <code>\"yes\"</code> and <code>\"no\"</code>, we retrieve a binary representation with <code>0</code> and <code>1</code>.</p>"},{"location":"data-science/data/preprocessing/#recap","title":"Recap","text":"<p>In this chapter, we have extensively covered missing values. The challenges to detect them in the first place and how to properly encode them. We explored different strategies to deal with missing values, from dropping columns/rows to imputation techniques.</p> <p>Using <code>scikit-learn</code> we were able to easily apply transformation to the Portuguese retail bank data set. We discretized (<code>KBinsDiscretizer</code>) numerical attributes, normalized them (<code>MinMaxScaler</code>, <code>StandardScaler</code>), and encoded categorical features with one-hot encoding (<code>OneHotEncoder</code>). Lastly, we briefly covered the encoding of target variables with the <code>LabelEncoder</code>.</p> <p>With all these preprocessing steps, we are now well-equipped to dive into the machine learning part and are closer to training our first model.</p> <ol> <li> <p>Decision Support Systems, Volume 62, June 2014, Pages 22-31: https://doi.org/10.1016/j.dss.2014.03.001 \u21a9</p> </li> </ol>"},{"location":"data-science/practice/","title":"Data Science in Practice","text":""},{"location":"data-science/practice/#introduction","title":"Introduction","text":"<p>In this course block and its subsequent chapters we will demonstrate how to build a machine learning model in practice. In the end, you will have a \"ready-to-go\" model that can predict whether a bank customer will subscribe to a term deposit.</p> <p>Along the way we will explore useful functionalities of <code>scikit-learn</code>, common pitfalls in data science projects, how to properly save a model, and conclude with a bonus section on pipelines to automate the entire modelling process.</p> <p>Let's get started! </p> <p>Remember the bank marketing data set that we used to explore in the Data Preparation &amp; Preprocessing portion and then completely abandoned in the last couple of chapters? Well, it's time to bring it back!</p> Info <p>The bank marketing data was adapted from:</p> <p>S. Moro, P. Cortez and P. Rita (2014). A Data-Driven Approach to Predict the Success of Bank Telemarketing<sup>1</sup></p> <p>The publicly available dataset is from a Portuguese retail bank and houses information on direct marketing campaigns (phone calls). Bank customers were contacted and asked to subscribe to a term deposit.</p>"},{"location":"data-science/practice/#prerequisites","title":"Prerequisites","text":""},{"location":"data-science/practice/#0-whats-our-goal","title":"0.  What's our goal?","text":"<p>First, let's define the end goal:</p> <p> Build a machine learning model that can predict whether a bank          customer will subscribe to a term deposit. </p> Tip <p>Put simply, a term deposit is a type of bank account where you agree to lock away your money for a fixed period of time (the \"term\") in exchange for a guaranteed interest rate that's typically higher than a regular savings account.</p> <p>Using information such as clients' demographic details, economic indicators, and marketing campaign data, we aim to solve this binary classification task.</p> <p>Before we dive in, you have to set up the project which will be used throughout the remainder of this course.</p>"},{"location":"data-science/practice/#1-project-structure","title":"1.  Project structure","text":"<p>Start with creating the following project structure:</p> <pre><code>\ud83d\udcc1 bank_model/\n\u251c\u2500\u2500 \ud83d\udcc1 data/\n</code></pre>"},{"location":"data-science/practice/#2-download-data","title":"2.  Download data","text":"Danger <p>Since we want to make sure that everyone uses the same initial data set, we urge you to re-download it and place it within your <code>data/</code> folder.</p> <p> Bank marketing  </p> <pre><code>\ud83d\udcc1 bank_model/\n\u251c\u2500\u2500 \ud83d\udcc1 data/\n\u251c\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank-merged.csv\n</code></pre>"},{"location":"data-science/practice/#3-project-initialization","title":"3.  Project initialization","text":"<p>Initialize the project with <code>uv</code> (<code>uv</code> wrap-up) Now, you should have the following structure:</p> <pre><code>\ud83d\udcc1 bank_model/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 data/\n\u251c\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank-merged.csv\n\u251c\u2500\u2500 \ud83d\udcc4 .python-version\n\u251c\u2500\u2500 \ud83d\udcc4 pyproject.toml\n\u251c\u2500\u2500 \ud83d\udcc4 README.md\n\u251c\u2500\u2500 \ud83d\udcc4 uv.lock\n</code></pre>"},{"location":"data-science/practice/#4-install-packages","title":"4.  Install packages","text":"<p>Install the necessary packages - <code>pandas</code> and <code>scikit-learn</code>.</p> <ol> <li> <p>Decision Support Systems, Volume 62, June 2014, Pages 22-31: https://doi.org/10.1016/j.dss.2014.03.001 \u21a9</p> </li> </ol>"},{"location":"data-science/practice/bonus/","title":"Bonus","text":""},{"location":"data-science/practice/bonus/#introduction","title":"Introduction","text":"<p>This bonus chapter demonstrates the usage of a pipeline in conjunction with a grid search to automate the modelling process. Again, we are utilizing the bank marketing data. However, this time around we streamline the following:</p> <ul> <li>Data preprocessing</li> <li>Model evaluation</li> <li>Hyperparameter tuning</li> <li>Model selection</li> <li>Re-training the model on the entire dataset</li> </ul> <p>... basically every step we had taken in \"Data Science in Practice\" block. Moreover, with a pipeline and grid search, we can easily evaluate additional model types and apply a more sophisticated way to evaluate their performance.</p> Tip <p>This chapter serves as an additional outlook for further topics you could explore, targeting your curiosity. Some concepts and techniques used in this chapter were not covered in this course. We won't explain them in much detail here, as they are beyond the scope of this course. Nonetheless, they could prove valuable for your future machine learning journey.</p> <p>If you're still around, great! Let's get started with some code. </p>"},{"location":"data-science/practice/bonus/#quickstart","title":"Quickstart","text":"<p>If you just need a blueprint for your next project, here's the whole thing.</p> Code <pre><code>import pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import (\n    RandomForestClassifier,\n)\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import (\n    KBinsDiscretizer,\n    OneHotEncoder,\n    StandardScaler,\n)\nfrom sklearn.svm import SVC\n\n\nclass DataFrameImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy=\"most_frequent\"):\n        self.strategy = strategy\n        self.imputer = SimpleImputer(\n            strategy=strategy, missing_values=\"unknown\"\n        )\n\n    def fit(self, X, y=None):\n        self.imputer.fit(X)\n        return self\n\n    def transform(self, X):\n        return pd.DataFrame(\n            self.imputer.transform(X), columns=X.columns, index=X.index\n        )\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\n            \"nominal\",\n            OneHotEncoder(handle_unknown=\"ignore\"),\n            [\n                \"default\",\n                \"housing\",\n                \"loan\",\n                \"contact\",\n                \"poutcome\",\n                \"job\",\n                \"marital\",\n            ],\n        ),\n        (\n            \"ordinal\",\n            OneHotEncoder(handle_unknown=\"ignore\"),\n            [\"month\", \"day_of_week\", \"education\"],\n        ),\n        (\n            \"binning\",\n            KBinsDiscretizer(n_bins=5, strategy=\"uniform\", encode=\"onehot\"),\n            [\"age\", \"campaign\", \"pdays\", \"previous\"],\n        ),\n        (\n            \"zscore\",\n            StandardScaler(),\n            [\n                \"emp.var.rate\",\n                \"cons.price.idx\",\n                \"cons.conf.idx\",\n                \"euribor3m\",\n                \"nr.employed\",\n            ],\n        ),\n    ]\n)\n\npipe = Pipeline(\n    [\n        (\"imputer\", DataFrameImputer()),\n        (\"preprocessor\", preprocessor),\n        (\"variance\", VarianceThreshold(threshold=0.0)),\n        (\"classifier\", None),\n    ]\n)\n\n\ngrid = [\n    {\n        \"classifier\": [\n            RandomForestClassifier(random_state=42, class_weight=\"balanced\")\n        ],\n        \"classifier__n_estimators\": [100, 200],\n        \"classifier__max_depth\": [5, 10],\n        \"classifier__min_samples_leaf\": [1, 2],\n    },\n    {\n        \"classifier\": [SVC(random_state=42, class_weight=\"balanced\")],\n        \"classifier__C\": [0.1, 1, 10],\n    },\n    {\"classifier\": [LogisticRegression(class_weight=\"balanced\")]},\n    {\n        \"classifier\": [MLPClassifier(random_state=42, max_iter=1_000)],\n    },\n]\n\nsearch = GridSearchCV(\n    pipe,\n    grid,\n    n_jobs=-1,\n    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=654),\n    scoring=[\"balanced_accuracy\", \"roc_auc\"],\n    refit=\"balanced_accuracy\",\n    verbose=2,\n)\n\n\n# load data\ndata = pd.read_csv(\"data/bank-merged.csv\")\nX, y = data.drop(columns=\"y\"), data[\"y\"]\n\n# fit the grid search\nsearch.fit(X, y)\n\nprint(\n    f\"Best score: {search.best_score_}\\n\"\n    f\"Best estimator: {search.best_params_}\"\n)\n</code></pre> <ol> <li>Open the <code>bank_model</code> project (from the Data Science in Practice block).</li> <li>Copy and execute the code.</li> <li>Done!</li> </ol> <p>If you want to know more about the individual parts, keep reading.</p>"},{"location":"data-science/practice/bonus/#plan-of-attack","title":"Plan of attack","text":"<p>We start by defining a bunch of things:</p> <ol> <li>Custom transformer, for data imputation and returning a <code>DataFrame</code></li> <li><code>ColumnTransformer</code> for preprocessing the data</li> <li>Pipeline to combine all steps</li> <li>Grid defining all models and parameters to be evaluated</li> <li>Grid search to find the best model</li> </ol> <p>Then we simply need to apply the pipeline and grid search to the data. Finally, we save the best model.</p>"},{"location":"data-science/practice/bonus/#implementation","title":"Implementation","text":""},{"location":"data-science/practice/bonus/#1-custom-transformer","title":"1. Custom transformer","text":"<p>We start by defining a custom transformer that imputes missing values in a <code>DataFrame</code>.</p> <pre><code>import pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\n\n\nclass DataFrameImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy=\"most_frequent\"):\n        self.strategy = strategy\n        self.imputer = SimpleImputer(\n            strategy=strategy, missing_values=\"unknown\"\n        )\n\n    def fit(self, X, y=None):\n        self.imputer.fit(X)\n        return self\n\n    def transform(self, X):\n        return pd.DataFrame(\n            self.imputer.transform(X), columns=X.columns, index=X.index\n        )\n</code></pre> <p>The custom transformer has implement the <code>fit()</code> and <code>transform()</code> methods. Since we are not passing the target variable <code>y</code> to the <code>fit</code> method, we \"ignore\" it by defining it as <code>y=None</code>.</p> <p>If you want to know more about custom transformers or even custom estimators (models), check out these resources:</p> <ul> <li>Custom transformer from a function     </li> <li><code>TransformerMixin</code> </li> <li>Custom estimator     </li> </ul> Tip <p><code>DataFrameImputer</code> returns a <code>pandas</code> <code>DataFrame</code> which allows us to easily chain the imputation step together with our trusted <code>ColumnTransformer</code> within a pipeline. In this case, that's the whole purpose of the custom transformer.</p>"},{"location":"data-science/practice/bonus/#2-columntransformer","title":"2. <code>ColumnTransformer</code>","text":"<p>Speaking of the <code>ColumnTransformer</code>, it simply stays the same as before.</p> <pre><code>from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import (\n    KBinsDiscretizer,\n    OneHotEncoder,\n    StandardScaler,\n)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\n            \"nominal\",\n            OneHotEncoder(handle_unknown=\"ignore\"),\n            [\n                \"default\",\n                \"housing\",\n                \"loan\",\n                \"contact\",\n                \"poutcome\",\n                \"job\",\n                \"marital\",\n            ],\n        ),\n        (\n            \"ordinal\",\n            OneHotEncoder(handle_unknown=\"ignore\"),\n            [\"month\", \"day_of_week\", \"education\"],\n        ),\n        (\n            \"binning\",\n            KBinsDiscretizer(n_bins=5, strategy=\"uniform\", encode=\"onehot\"),\n            [\"age\", \"campaign\", \"pdays\", \"previous\"],\n        ),\n        (\n            \"zscore\",\n            StandardScaler(),\n            [\n                \"emp.var.rate\",\n                \"cons.price.idx\",\n                \"cons.conf.idx\",\n                \"euribor3m\",\n                \"nr.employed\",\n            ],\n        ),\n    ]\n)\n</code></pre>"},{"location":"data-science/practice/bonus/#3-pipeline","title":"3. Pipeline","text":"<p>A pipeline is a sequence of steps where each step is a tuple containing a name and a transformer/estimator.</p> <pre><code>from sklearn.feature_selection import VarianceThreshold\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline(\n    [\n        (\"imputer\", DataFrameImputer()),\n        (\"preprocessor\", preprocessor),\n        (\"variance\", VarianceThreshold(threshold=0.0)),\n        (\"classifier\", None),\n    ]\n)\n</code></pre> <p>Our pipeline consists of the following sequential steps:</p> <ol> <li><code>\"imputer\"</code> - Impute missing values</li> <li><code>\"preprocessor\"</code> - Apply all further preprocessing steps</li> <li><code>\"variance\"</code> - Remove features with zero variance (removes all     constant features)</li> <li><code>\"classifier\"</code> - Apply a classifier (to be defined later)</li> </ol> Tip <p>You can modify pipelines to your liking. For example you could add another feature selection step. Or what about applying a PCA and then a classifier? The possibilities are endless!</p>"},{"location":"data-science/practice/bonus/#4-grid","title":"4. Grid","text":"<p>Next, we define a grid with all models and hyperparameters to be evaluated.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\n\ngrid = [\n    {\n        \"classifier\": [\n            RandomForestClassifier(random_state=42, class_weight=\"balanced\")\n        ],\n        \"classifier__n_estimators\": [100, 200],\n        \"classifier__max_depth\": [5, 10],\n        \"classifier__min_samples_leaf\": [1, 2],\n    },\n    {\n        \"classifier\": [SVC(random_state=42, class_weight=\"balanced\")],\n        \"classifier__C\": [0.1, 1, 10],\n    },\n    {\"classifier\": [LogisticRegression(class_weight=\"balanced\")]},\n    {\n        \"classifier\": [MLPClassifier(random_state=42, max_iter=1_000)],\n    },\n]\n</code></pre> <p>The grid contains four different models:</p> <ol> <li>Random Forest</li> <li>Support Vector Machine (not discussed in this course)</li> <li>Logistic Regression</li> <li>Multi-layer Perceptron (Neural Network - not discussed in this course)</li> </ol> <p>We will evaluate all these models and each hyperparameter combination.</p> Info <p>Names in the grid dictionary must match the names in the pipeline (<code>\"classifier\"</code>). The double underscore <code>\"__\"</code> is used to indicate that the parameter belongs to the classifier in the pipeline.</p>"},{"location":"data-science/practice/bonus/#5-grid-search","title":"5. Grid search","text":"<p>Finally, we define the grid search, that's where we put everything together.</p> <pre><code>from sklearn.model_selection import GridSearchCV, StratifiedKFold\n\nsearch = GridSearchCV(\n    pipe,\n    grid,\n    n_jobs=-1,  # (1)!\n    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=654),  # (2)!\n    scoring=[\"balanced_accuracy\", \"roc_auc\"],\n    refit=\"balanced_accuracy\",\n    verbose=2,\n)\n</code></pre> <ol> <li>Use all available CPU cores (<code>n_jobs=-1</code>). This speeds up the     process significantly.</li> <li>We use a stratified k-fold cross-validation with 5 splits. Each fold     preserves the percentage of samples for each class.</li> </ol> <p>Basically, we are evaluating all models and hyperparameters using a (stratified) k-fold cross-validation (read more about cross-validation here). The <code>StratifiedKFold</code> thus replaces our simple <code>train_test_split()</code>.</p> <p>To evaluate the models, we are calculating two performance metrics: balanced accuracy and ROC AUC (<code>scoring=[\"balanced_accuracy\", \"roc_auc\"]</code>). The best model is selected based on the balanced accuracy (<code>refit=\"balanced_accuracy\"</code>) and then retrained on the entire dataset!</p> Info <p>The grid search eliminates the need to compare models manually, it performs hyperparameter tuning, and it selects the best model for us. Lastly, we won't even have to re-train it on the entire dataset, as the grid search already does that for us! </p>"},{"location":"data-science/practice/bonus/#application","title":"Application     If you can surpass the performance of this pipeline's estimator,      please let us know. We are curious to hear from you! \ud83d\ude0a","text":"<p>With all things defined, we simply need to apply the grid search to the data.</p> <pre><code># load data\ndata = pd.read_csv(\"data/bank-merged.csv\")\nX, y = data.drop(columns=\"y\"), data[\"y\"]\n\n# fit the grid search\nsearch.fit(X, y)\n\nprint(\n    f\"Best score: {search.best_score_}\\nBest estimator: {search.best_params_}\"\n)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Best score: 0.7407486855434444\nBest estimator: {\n    'classifier': RandomForestClassifier(class_weight='balanced', random_state=42),\n    'classifier__max_depth': 5, 'classifier__min_samples_leaf': 1, 'classifier__n_estimators': 200\n}\n</code></pre> <p>Again, a random forest is the best model.</p> <p>To predict new data, use following method:</p> <pre><code>search.best_estimator_.predict()\n</code></pre> <p>That's it! You've automated the whole modelling process. </p>"},{"location":"data-science/practice/data-preparation/","title":"1. Data Preparation","text":""},{"location":"data-science/practice/data-preparation/#introduction","title":"Introduction","text":"<p>To achieve our end goal we have to carefully analyze and preprocess the data. We will start by exploring the data set, handling missing values, identifying attribute types, and then proceed to preprocessing techniques.</p> Info <p>To build a proper machine learning model for the bank marketing data set, we need to channel all our knowledge obtained so far!</p> <p>Create a new notebook or script.</p> <pre><code>\ud83d\udcc1 bank_model/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 data/\n\u251c\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank-merged.csv\n\u251c\u2500\u2500 \ud83d\udc0d preparation.py\n\u251c\u2500\u2500 ...\n</code></pre>"},{"location":"data-science/practice/data-preparation/#data","title":"Data","text":"<p>We start by loading the data.</p> <pre><code>import pandas as pd\n\ndata = pd.read_csv(\"data/bank-merged.csv\")\n</code></pre> A look at the data <p>It's always a good idea to take a look at the data before proceeding.</p> <ol> <li>Check the shape of the data.</li> <li>Display the first few rows.</li> </ol>"},{"location":"data-science/practice/data-preparation/#missing-values","title":"Missing values","text":"<p>In the data preprocessing chapter we discussed missing values. Recall that in this specific data set, the missing values are a bit more hidden. They are encoded as <code>\"unknown\"</code>. So let's replace these values with <code>None</code>.</p> <pre><code>data = data.replace(\"unknown\", None)\n</code></pre> <p>With a cleaned data set, we can now proceed to the next step - data exploration.</p>"},{"location":"data-science/practice/data-preparation/#attribute-types","title":"Attribute types","text":"<p>Start by checking the attribute types.</p> Attribute types <p>Again, look at the data set. The task is to identify which attribute types are generally present in the dataset. Answer, the following quiz question.</p> <p>If you need a refresher on attribute types, check out the appropriate chapter.</p> # <p>Which attribute types are present in the data set?</p> numericalnominal, ordinalnominal, ordinal, numericalordinalordinal, numericalnominalnominal, numerical <p>We are dealing with quite a mixed data set. All three different types (nominal, ordinal and numerical) are present. For example:</p> <ul> <li>Job - nominal</li> <li>Education - ordinal</li> <li>Age - numerical</li> </ul>"},{"location":"data-science/practice/data-preparation/#feature-description","title":"Feature description","text":"<p>With a broad overview, let's explore the different features/attributes more in-depth. Since we are dealing with a couple of features, categories were built.</p> <ul> <li> <p> Client Demographics</p> <p>Demographic information about each client such as the education level (high school, university, etc.).</p> Variable Description id Client identifier (we will ignore the identifier) age Age job Type of occupation marital Marital status education Education level </li> <li> <p> Financial Status</p> <p>Does the client have a housing or personal loan, etc.</p> Variable Description default Credit default status housing Housing loan status loan Personal loan status </li> <li> <p> Campaign Information</p> <p>Remember, bank clients were contacted by phone. Some were contacted multiple times over the span of multiple campaigns.</p> Variable Description contact Contact type month Last contact month day_of_week Last contact day campaign Number of contacts in current campaign pdays Days since last contact from previous campaign previous Number of contacts before this campaign poutcome Outcome of previous campaign </li> <li> <p> Economic Indicators</p> <p>Some economic indicators at the time of the contact like the current interest rate (Euribor rate).</p> Variable Description emp.var.rate Employment variation rate cons.price.idx Consumer price index cons.conf.idx Consumer confidence index euribor3m Euribor 3-month rate nr.employed Number of employees </li> </ul> Info <p>Lastly, one column remains - <code>\"y\"</code>. This column is the target, whether a customer subscribed to a term deposit (<code>1</code>) or not (<code>0</code>).</p> <p>With a better understanding of the features at hand, we can proceed to the next step, assigning attribute types to the columns. Doing so, will help us later to pick the appropriate preprocessing steps.</p>"},{"location":"data-science/practice/data-preparation/#assigning-attribute-types","title":"Assigning attribute types","text":"Assigning attributes <p>Assign an attribute type to each column. Look at the data and go over each column/attribute and add the column name to one of the three empty lists.</p> <p>Disregard the unique identifier <code>\"id\"</code> and the target <code>\"y\"</code>.</p> <pre><code>nominal = []\nordinal = []\nnumerical = []\n</code></pre> <p>For example (part of the solution):</p> <ul> <li> <p><code>\"age\"</code> is a \"measurable\" quantity and expressed as a number, thus     is a numerical attribute.</p> </li> <li> <p>The next attribute <code>\"default\"</code> is clearly categorical with its     unique values <code>[\"no\", None, \"yes]</code>. But since the attribute has no     meaningful order, it is nominal.</p> </li> </ul> <p>Resulting so far in:</p> <pre><code>nominal = [\"default\"]\nordinal = []\nnumerical = [\"age\"]\n</code></pre> <p>Now, go ahead and assign all of the remaining attributes.</p> Danger <p>Since the attribute assignment is crucial, we strongly urge you to solve the task. It will help your understanding of the data set and the next steps.</p> <p>Check your solution with the answer below and correct any mistakes you've made.</p> Info <p>The solution is as follows (column names are ordered according to <code>data</code>):</p> <pre><code>nominal = [\n    \"default\",\n    \"housing\",\n    \"loan\",\n    \"contact\",\n    \"poutcome\",\n    \"job\",\n    \"marital\",\n]\n\nordinal = [\n    \"month\",\n    \"day_of_week\",\n    \"education\",\n]\n\nnumerical = [\n    \"age\",\n    \"campaign\",\n    \"pdays\",\n    \"previous\",\n    \"emp.var.rate\",\n    \"cons.price.idx\",\n    \"cons.conf.idx\",\n    \"euribor3m\",\n    \"nr.employed\",\n]\n</code></pre>"},{"location":"data-science/practice/data-preparation/#visualizing-the-data","title":"Visualizing the data","text":"<p>To get an even better understanding of the data, we can visualize it. For convenience, we will use <code>pandas</code> built-in plotting capabilities.</p> Tip <p>If you want to know more on visualizing different attribute types, visit the Frequency Distribution chapter of the Statistics course.</p> <p>For example, we can plot numerical attributes like <code>\"campaign\"</code> as a box plot.</p> <pre><code>import matplotlib.pyplot as plt\n\ndata.plot(\n    kind=\"box\", y=\"campaign\", title=\"Number of contacts in current campaign\"\n)\nplt.show()\n</code></pre> Info <p>As you might have noticed, you need to install <code>matplotlib</code>.</p> <p>Or how about a pie chart for nominal attributes like <code>\"marital\"</code>?</p> <pre><code># first, count the occurrence of each category\nmarital_count = data[\"marital\"].value_counts()\nmarital_count.plot(\n    kind=\"pie\", autopct=\"%1.0f%%\", title=\"Marital status\"\n)  # (1)!\nplt.show()\n</code></pre> <ol> <li>The <code>autopct</code> parameter is used to display the percentage on the pie chart.</li> </ol> Visualize <p>Pick two more attributes of your choice and plot them.</p> <ol> <li>Choose a numerical attribute and plot it as a histogram.</li> <li>Select a nominal or ordinal attribute and plot it as a bar chart.</li> </ol> <p>Use these <code>pandas</code> resources, if you're having trouble:</p> <ul> <li><code>DataFrame.plot()</code> docs     </li> <li>Chart visualization     </li> </ul> Info <p>It's crucial to visualize your data before diving into further analysis. Visualizations can help you understand the distribution, identify patterns, and detect anomalies or outliers in your data. This step ensures that you have a clear understanding of your data, which is essential for making informed decisions in your analysis process.</p>"},{"location":"data-science/practice/data-preparation/#preprocessing","title":"Preprocessing","text":"<p>Now that we have a better understanding of the data, we can proceed to the preprocessing steps. Depending on the attribute type, we will apply different techniques.</p> <p>Since we are dealing with a mixed data set, we will keep things relatively simple and plan our approach accordingly:</p> <ul> <li>For <code>nominal</code> attributes, we apply one-hot encoding.</li> <li>For <code>ordinal</code> attributes, we use one-hot encoding as well.</li> <li>For <code>numerical</code> attributes, we follow two strategies:<ul> <li>Create bins for <code>age</code>, <code>campaign</code>, <code>pdays</code>, and <code>previous</code>.</li> <li>Z-Score normalization for the remaining features: <code>emp.var.rate</code>,     <code>cons.price.idx</code>, <code>cons.conf.idx</code>, <code>euribor3m</code>, and <code>nr.employed</code>.</li> </ul> </li> </ul> Info <p><code>nominal</code> and <code>ordinal</code> attributes are categorical and require one-hot encoding to be suitable for machine learning algorithms.</p> <p>We are creating bins for <code>age</code>, <code>campaign</code>, <code>pdays</code>, and <code>previous</code>, since these features have a large number of outliers. By binning these features, we can try to reduce the impact of outliers and noise in the data.</p> <p>Z-Score normalization is applied to the remaining numerical features to ensures that features don't have a larger impact on the model just because of their larger magnitude.</p> <p>To apply these preprocessing steps, we have to look for the corresponding <code>scikit-learn</code> classes.</p> <ul> <li> <p> Preprocessing technique</p> <ul> <li>One hot encoding</li> <li>Binning</li> <li>Z-Score normalization (standardization)</li> </ul> </li> <li> <p> Corresponding <code>scikit-learn</code> class</p> <ul> <li><code>OneHotEncoder</code></li> <li><code>KBinsDiscretizer</code></li> <li><code>StandardScaler</code></li> </ul> </li> </ul> Tip <p>All these techniques and classes were previously introduced in the Data preprocessing chapter.</p> <p>Just like in the Data preprocessing chapter we could apply each technique one at a time, e.g.:</p> <pre><code>from sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder\n\nnominal_encoder = OneHotEncoder()\nnominal_encoder.fit_transform(data[nominal])\n\nordinal_encoder = OneHotEncoder()\nordinal_encoder.fit_transform(data[ordinal])\n\nbinning = KBinsDiscretizer(n_bins=5, strategy=\"uniform\")\nbinning.fit_transform(data[[\"age\", \"campaign\", \"pdays\", \"previous\"]])\n\n# and so on...\n</code></pre> <p>... the above approach itself is perfectly fine, but we can do better! But first, we need to get the term information leakage out of the way, a common pitfall in machine learning/data science projects.</p>"},{"location":"data-science/practice/data-preparation/#information-leakage","title":"Information leakage","text":"<p>To explain the term information leakage, let's look at an example.</p> Information leakage <p>Assume, we want to predict the target <code>\"y\"</code> based on the features <code>\"emp.var.rate\"</code> and <code>\"euribor3m\"</code>. First, we apply Z-Score normalization to these features.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nfeatures = scaler.fit_transform(data[[\"emp.var.rate\", \"euribor3m\"]])\n</code></pre> <p>As always, we are splitting the data into training and test set to later evaluate the model.</p> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    features, data[\"y\"], test_size=0.2, random_state=42\n)\n</code></pre> <p>Now, we are already dealing with information leakage. Put simply - the train set <code>X_train</code> already \"knows\" something about the test set <code>X_test</code>.</p> <p>Why?</p> <p>Remember the definition of Z-Score normalization - it calculates the mean and standard deviation of the data set. If we calculate these values on the whole data set; in our case <code>data</code> - just like we did above, <code>X_train</code> contains information about <code>X_test</code>. Thus, the test set is no longer a good representation of unseen data, hence any scores calculated with the test set are no longer a good indicator of the model's performance.</p> <p>This is a common pitfall in machine learning! To prevent information leakage, we have to split the data before applying any preprocessing steps.</p> <p>With information leakage in mind, we introduce a more elegant way to apply multiple preprocessing steps.</p>"},{"location":"data-science/practice/data-preparation/#columntransformer","title":"<code>ColumnTransformer</code>","text":"Not the kind of transformer you are expecting, but cool nonetheless!         \ud83e\udd16      <p>Since we do not want to apply each preprocessing step one at a time, we simply bundle them.</p> <p>The <code>ColumnTransformer</code> is a class in <code>scikit-learn</code> that allows us to bundle our preprocessing steps together. This way, we can apply all transformations in one go.</p> <p>First, we import all necessary classes:</p> <pre><code>from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import (\n    KBinsDiscretizer,\n    OneHotEncoder,\n    StandardScaler,\n)\n</code></pre> <p>Next, we can already initiate our transformer. We define the exact same steps as we did in written form at the beginning of this section. Note that the <code>ColumnTransformer</code> takes a <code>list</code> of <code>tuple</code>.</p> <pre><code>preprocessor = ColumnTransformer(\n    transformers=[\n        (\n            \"nominal\",\n            OneHotEncoder(),\n            [\n                \"default\",\n                \"housing\",\n                \"loan\",\n                \"contact\",\n                \"poutcome\",\n                \"job\",\n                \"marital\",\n            ],\n        ),\n        (\"ordinal\", OneHotEncoder(), [\"month\", \"day_of_week\", \"education\"]),\n        (\n            \"binning\",\n            KBinsDiscretizer(\n                n_bins=5, strategy=\"uniform\", encode=\"onehot\"\n            ),  # (1)!\n            [\"age\", \"campaign\", \"pdays\", \"previous\"],\n        ),\n        (\n            \"zscore\",\n            StandardScaler(),\n            [\n                \"emp.var.rate\",\n                \"cons.price.idx\",\n                \"cons.conf.idx\",\n                \"euribor3m\",\n                \"nr.employed\",\n            ],\n        ),\n    ]\n)\n</code></pre> <ol> <li>Conveniently, we can create categories (bins) with the <code>KBinsDiscretizer</code>     and directly apply one-hot encoding with <code>encode=\"oneheot\"</code>.</li> </ol> <p>Let's break it down:</p> <ul> <li>Our instance <code>preprocessor</code> has 4 steps, named <code>nominal</code>, <code>ordinal</code>,     <code>binning</code>, and <code>zscore</code>.</li> <li>Each step is defined as a <code>tuple</code>, with the first element being the     name of the step, the second element the preprocessing technique, and the     third element being a list of columns to apply the technique to.</li> <li>By default, all columns which are not specified in the <code>ColumnTransformer</code>     will be dropped! See the     <code>remainder</code>     parameter in the docs.</li> </ul> <p>So far we only defined the necessary preprocessing steps, but didn't apply them just yet (that's part of the next chapter).</p>"},{"location":"data-science/practice/data-preparation/#detour-didnt-we-forget-something","title":"Detour: Didn't we forget something?","text":"<p>We completely neglected the missing values in the data set. Thus, we still need to handle them with an imputation technique.</p> Tip <p>During the development process of a data science project, you will often find yourself jumping back and forth between different steps. This is perfectly normal and part of the process. Seldom will you follow a linear path from start to finish.</p> <pre><code>print(data.isna().sum())\n</code></pre> <p>If you execute the above line, you will see that we still have many missing values in a couple of columns. No worries, we can easily handle them with:</p> <pre><code>from sklearn.impute import SimpleImputer\n\nimpute = SimpleImputer(strategy=\"most_frequent\", missing_values=None)\n</code></pre> <p>The <code>SimpleImputer</code> lets us fill in missing values with the most frequent value in the respective column. But why did we choose this specific strategy?</p> # <p>Why do we plan to fill missing values with the most frequent value (the mode) and not the mean or median?</p> The mode is the most common imputation strategy.The columns with any missing values are either nominal or ordinal. Thus, the most frequent value (mode) is a valid choice for imputation. Mean and median are not suitable for nominal and ordinal data.It is just an initial choice, we could have used any other strategy.The mode is the most robust imputation strategy. <p>Correct! \ud83d\udc4d\ud83c\udffd</p> Info <p>You might wonder why we didn't include the imputation step in the <code>ColumnTransformer</code>. The reason is that passing the same column to more than one step leads to issues. As the <code>ColumnTransformer</code> runs in parallel and does not apply the steps sequentially.</p>"},{"location":"data-science/practice/data-preparation/#recap","title":"Recap","text":"<p>In this chapter, we started our practical data science project by exploring the bank marketing data set further. We handled missing values and identified attribute types. We then visualized the data to get a better understanding of the features. During our discussion of appropriate preprocessing methods, we discovered the term information leakage and how to prevent it. Finally, we introduced the <code>ColumnTransformer</code> to bundle preprocessing steps together.</p>"},{"location":"data-science/practice/data-preparation/#code-recap","title":"Code recap","text":"<p>This time around, we also do a code recap. The essential findings in this chapter can be distilled to:</p> <pre><code>import pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import (\n    KBinsDiscretizer,\n    OneHotEncoder,\n    StandardScaler,\n)\n\ndata = pd.read_csv(\"data/bank-merged.csv\")\ndata = data.replace(\"unknown\", None)\n\nimpute = SimpleImputer(strategy=\"most_frequent\", missing_values=None)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\n            \"nominal\",\n            OneHotEncoder(),\n            [\n                \"default\",\n                \"housing\",\n                \"loan\",\n                \"contact\",\n                \"poutcome\",\n                \"job\",\n                \"marital\",\n            ],\n        ),\n        (\"ordinal\", OneHotEncoder(), [\"month\", \"day_of_week\", \"education\"]),\n        (\n            \"binning\",\n            KBinsDiscretizer(n_bins=5, strategy=\"uniform\", encode=\"onehot\"),\n            [\"age\", \"campaign\", \"pdays\", \"previous\"],\n        ),\n        (\n            \"zscore\",\n            StandardScaler(),\n            [\n                \"emp.var.rate\",\n                \"cons.price.idx\",\n                \"cons.conf.idx\",\n                \"euribor3m\",\n                \"nr.employed\",\n            ],\n        ),\n    ]\n)\n</code></pre> <p>In the next chapter we will apply the preprocessing steps to a train and test split. Subsequently, we fit the first model.</p>"},{"location":"data-science/practice/end-to-end/","title":"3. End-to-End","text":""},{"location":"data-science/practice/end-to-end/#introduction","title":"Introduction","text":"<p>We distill all relevant code blocks from the previous two chapters into one cohesive script/notebook. This file will be an end-to-end example to fit a machine learning model on the bank marketing data set. Lastly, we will save the model to disk.</p> Tip <p>The script/notebook we will create, can serve as a reference point for your further data science projects.</p> <p>So start by creating yet another script/notebook.</p> <pre><code>\ud83d\udcc1 bank_model/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 data/\n\u251c\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank-merged.csv\n\u251c\u2500\u2500 \ud83d\udc0d preparation.py\n\u251c\u2500\u2500 \ud83d\udc0d modelling.py\n\u251c\u2500\u2500 \ud83d\udc0d end-to-end.py\n</code></pre>"},{"location":"data-science/practice/end-to-end/#previously","title":"Previously...","text":"<p>In the previous chapters, we:</p> <ol> <li>Loaded the data</li> <li>Defined techniques to impute (<code>SimpleImputer</code>) and preprocess the data     (<code>ColumnTransformer</code>)</li> <li>Split the data into train and test sets</li> <li>Applied imputation and preprocessing techniques to the data</li> <li>Evaluated different model types and concluded that a     <code>RandomForestClassifier</code> is the best model (we found) for this task</li> <li>Fit and evaluated the random forest</li> </ol> <p>Here are the bullet points distilled in one code block:</p> <pre><code>import pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import (\n    KBinsDiscretizer,\n    LabelEncoder,\n    OneHotEncoder,\n    StandardScaler,\n)\n\ndata = pd.read_csv(\"data/bank-merged.csv\")\ndata = data.replace(\"unknown\", None)\n\nimpute = SimpleImputer(strategy=\"most_frequent\", missing_values=None)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\n            \"nominal\",\n            OneHotEncoder(),\n            [\n                \"default\",\n                \"housing\",\n                \"loan\",\n                \"contact\",\n                \"poutcome\",\n                \"job\",\n                \"marital\",\n            ],\n        ),\n        (\"ordinal\", OneHotEncoder(), [\"month\", \"day_of_week\", \"education\"]),\n        (\n            \"binning\",\n            KBinsDiscretizer(n_bins=5, strategy=\"uniform\", encode=\"onehot\"),\n            [\"age\", \"campaign\", \"pdays\", \"previous\"],\n        ),\n        (\n            \"zscore\",\n            StandardScaler(),\n            [\n                \"emp.var.rate\",\n                \"cons.price.idx\",\n                \"cons.conf.idx\",\n                \"euribor3m\",\n                \"nr.employed\",\n            ],\n        ),\n    ]\n)\n\n# remove the target from data and assign it to y\ny = data.pop(\"y\")\nX = data\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# impute missing values\nimpute.fit(X_train)\nX_train = impute.transform(X_train)\nX_test = impute.transform(X_test)\n\n# convert back to DataFrame\nX_train = pd.DataFrame(X_train, columns=X.columns)\nX_test = pd.DataFrame(X_test, columns=X.columns)\n\n# apply preprocessing (OneHotEncoder, KBinsDiscretizer &amp; StandardScaler)\npreprocessor.fit(X_train)\nX_train = preprocessor.transform(X_train)\nX_test = preprocessor.transform(X_test)\n\n# encode target\nencoder = LabelEncoder()\ny_train = encoder.fit_transform(y_train)\ny_test = encoder.transform(y_test)\n\n# fit on train set\nforest = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_leaf=10,\n    random_state=42,\n    class_weight=\"balanced\",\n)\nforest.fit(X_train, y_train)\n\n# evaluate on test set\ny_pred = forest.predict(X_test)\nscore = balanced_accuracy_score(y_test, y_pred)\nprint(f\"Balanced accuracy: {round(score, 4)}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Balanced accuracy: 0.7445\n</code></pre> Copy and execute the block <p>Since the code block is nothing new, simply copy and execute it. If everything went smoothly, you should see the balanced accuracy score printed.</p>"},{"location":"data-science/practice/end-to-end/#re-fit-on-whole-data-set","title":"Re-fit on whole data set","text":"<p>Previously, we split our data into train and test sets. Using the test set we were able to estimate the performance of our model. That's the whole purpose of the test set.</p> <p>Now, our goal is to save the trained model for future use. Therefore, in practice, we want to leverage the power of the whole data set. Thus, we re-fit the model on the whole data set to make use of all available data.</p> <pre><code># preprocess the whole data set\nX = impute.transform(X)\nX = pd.DataFrame(X, columns=data.columns)\nX = preprocessor.transform(X)\n\n# encode target\ny = encoder.transform(y)\n</code></pre> <p>To preprocess the whole data set, we can reuse the <code>impute</code> and <code>preprocessor</code> objects. We only need to transform the data and encode the target. Lastly, we fit the model on the whole data set. It's as simple as:</p> <pre><code>forest.fit(X, y)\n</code></pre> Info <p>Note, we can simply call <code>fit()</code> again, this will \"overwrite\" the previous model and uses the whole data set to fit the model once-again.</p> <p>The <code>forest</code> is now fitted on the whole data set. That's it! We have our final model which we will save to disk. </p>"},{"location":"data-science/practice/end-to-end/#model-persistence","title":"Model persistence","text":"<p>To save the model to disk, we can use <code>pickle</code>. It's a part of base  Python. With <code>pickle</code>, you can save any Python object and load it back later.</p> <code>pickle</code> comes from the concept of          \"pickling\" in food preservation. Similarly, <code>pickle</code> is          used to \"preserve\" Python objects."},{"location":"data-science/practice/end-to-end/#simple-example","title":"Simple example","text":"<p>For example, we can save any object such as a simple <code>list</code>:</p> <pre><code>import pickle\n\nsimple_list = [1, 2, 3, 4, 5]\n\nwith open(\"list.pkl\", \"wb\") as file:\n    pickle.dump(simple_list, file)\n</code></pre> <p>Let's break down the code block:</p> <ol> <li>We open a new file named <code>list.pkl</code>; <code>.pkl</code> is just a common extension for     <code>pickle</code> files.</li> <li>The file is opened in write-binary mode (<code>\"wb\"</code>) - as pickle files are     binary files.</li> <li>We use <code>pickle.dump()</code> to save the object <code>simple_list</code> to the file.</li> </ol> Info <p>You can delete <code>list.pkl</code>, it was just an example.</p>"},{"location":"data-science/practice/end-to-end/#save-the-model","title":"Save the model","text":"<p>Let's extend this knowledge to save our model. Unfortunately, it's not just a matter of saving the <code>forest</code> object. First, we look at the steps we need to take to make a prediction for a new client:</p> The prediction process <pre><code>graph\n  A[New client data] --&gt; B[Impute potential missing values: &lt;code&gt;impute.transform&lt;/code&gt;];\n  B --&gt; C[Preprocess data: &lt;code&gt;preprocessor.transform&lt;/code&gt;];\n  C --&gt; D[Make predictions: &lt;code&gt;forest.predict&lt;/code&gt;];\n  D --&gt; E[Transform prediction to yes or no: &lt;code&gt;encoder.inverse_transform&lt;/code&gt;&lt;br&gt;];</code></pre> <p>To get our prediction process working, we need to save all objects involved:</p> <ul> <li><code>impute</code></li> <li><code>preprocessor</code></li> <li><code>encoder</code></li> <li><code>forest</code></li> </ul> Critical: Save ALL preprocessing objects! <p>You must save every single object used in the prediction pipeline, not just the model!</p> <p>Missing even one object will break your predictions:</p> <ul> <li>Missing <code>impute</code> \u2192 Cannot handle new missing values</li> <li>Missing <code>preprocessor</code> \u2192 Cannot transform features correctly</li> <li>Missing <code>encoder</code> \u2192 Cannot convert predictions back to original labels</li> <li>Missing <code>forest</code> \u2192 Cannot make predictions</li> </ul> <p>The model is useless without its preprocessing pipeline! </p> <p>We can save all these objects in one file using a simple <code>dict</code>:</p> <pre><code>model = {\n    \"imputer\": impute,\n    \"preprocessor\": preprocessor,\n    \"forest\": forest,\n    \"target-encoder\": encoder,\n}\n\nwith open(\"bank-model.pkl\", \"wb\") as file:\n    pickle.dump(model, file)\n</code></pre> <p>Bundling all objects in a dictionary ensures you never accidentally forget a component. When you load <code>bank-model.pkl</code>, you have everything needed for predictions in one place.</p> Load the model <p>Create a new script or notebook which we will use to test the saved model.</p> <p>Use the following code block to load the <code>model</code> <code>dict</code>.</p> <pre><code>import pickle\n\nwith open(\"bank-model.pkl\", \"rb\") as file:  # (1)!\n    model = pickle.load(file)\n</code></pre> <ol> <li><code>\"rb\"</code> stands for read-binary mode.</li> </ol> Danger <p>Do not download and load <code>pickle</code> files from the internet, unless you trust the source. Since, <code>pickle</code> can execute arbitrary code, it can be a security risk.</p>"},{"location":"data-science/practice/end-to-end/#predictions","title":"Predictions","text":"<p>Let's run the prediction process. Assume the bank contacted another client with following attributes:</p> <pre><code>import pandas as pd\n\nclient = pd.DataFrame(\n    {\n        \"id\": 155611,\n        \"age\": 54,\n        \"default\": None,\n        \"housing\": \"no\",\n        \"loan\": \"no\",\n        \"contact\": \"cellular\",\n        \"month\": \"aug\",\n        \"day_of_week\": \"tue\",\n        \"campaign\": 3,\n        \"pdays\": 999,\n        \"previous\": 0,\n        \"poutcome\": \"nonexistent\",\n        \"emp.var.rate\": -2.9,\n        \"cons.price.idx\": 92.201,\n        \"cons.conf.idx\": -31.4,\n        \"euribor3m\": 0.878,\n        \"nr.employed\": 5087.2,\n        \"job\": \"retired\",\n        \"marital\": \"divorced\",\n        \"education\": \"professional.course\",\n    },\n    index=[0],\n)\n</code></pre> <p>Does the client subscribe to a term deposit? </p> Make a new prediction <p>Predict if the client will subscribe to a term deposit.</p> <ol> <li>Use the above code snippet to create a new observation <code>client</code>.</li> <li>Use all objects in the dictionary <code>model</code> to make a prediction.</li> </ol> <p>Hint: To make a prediction, simply implement the above prediction process illustrated as a graph.</p> <p>Try to solve the task on your own. For completeness, we provide one possible solution.</p> Info <pre><code>def predict(model, client):\n    # preprocess the client data\n    X = model[\"imputer\"].transform(client)\n    X = pd.DataFrame(X, columns=client.columns)\n    X = model[\"preprocessor\"].transform(X)\n\n    # make a prediction\n    prediction = model[\"forest\"].predict(X)\n    # inverse transform (0, 1) to (\"no\", \"yes\")\n    prediction = model[\"target-encoder\"].inverse_transform(prediction)\n\n    return prediction\n</code></pre>"},{"location":"data-science/practice/end-to-end/#conclusion","title":"Conclusion","text":"<p>Across three chapters, we successfully reached our end goal: To build a machine learning model on the bank marketing data set. We ended up with a random forest model with a balanced accuracy of 74.45%.</p> <p>The saved model can be deployed in a production environment. The prediction process is straightforward and can be easily applied on new clients.</p>          Congratulations, you have reached the end of this practical guide! \ud83c\udf89         You are now well-equipped to tackle your own data science projects."},{"location":"data-science/practice/end-to-end/#outlook","title":"Outlook","text":"<p>There are many more avenues to explore in the data science/machine learning landscape:</p>"},{"location":"data-science/practice/end-to-end/#model-deployment","title":"Model deployment","text":"<p>Learn how to deploy a model in a production environment. This can be done with a REST API, a web application or a mobile application (among others).</p> <p>Start with:</p> <ul> <li><code>fastapi</code> for building APIs     </li> </ul> <p>which is a great way to serve your model.</p>"},{"location":"data-science/practice/end-to-end/#model-persistence-with-onnx","title":"Model persistence with <code>onnx</code>","text":"<p>The Open Neural Network Exchange (ONNX) format provides an interesting alternative to <code>pickle</code>. ONNX allows you to convert your trained models into a standardized format that can be run efficiently across different platforms and programming languages.</p> <p>For example, <code>onnx</code> allows you to build the model in  Python and deploy it with  JavaScript.</p> <p>Start with:</p> <ul> <li><code>onnx</code> documentation for Python     </li> <li><code>onnx</code> with <code>scikit-learn</code> </li> </ul>"},{"location":"data-science/practice/end-to-end/#expand-your-model-toolkit","title":"Expand your model toolkit","text":"<p>We covered a selection of different model types, yet there are many more to explore. <code>scikit-learn</code> offers many more models for classification, regression, clustering or dimensionality reduction.</p> <p>Since you're already familiar with <code>scikit-learn</code>, applying these models is straightforward.</p> <p>Start with:</p> <ul> <li><code>scikit-learn</code> documentation.     </li> </ul>"},{"location":"data-science/practice/end-to-end/#advanced-pipeline-techniques","title":"Advanced pipeline techniques","text":"<p><code>scikit-learn</code> offers more sophisticated ways for modelling through pipelines. In a bonus chapter we explore advanced techniques for hyperparameter tuning, custom transformers and more.</p>"},{"location":"data-science/practice/modelling/","title":"2. Modelling","text":""},{"location":"data-science/practice/modelling/#introduction","title":"Introduction","text":"<p>In this chapter, we start our modelling efforts. Let's dive right in.</p> <p>Create a new notebook or script. Your project should now look like this:</p> <pre><code>\ud83d\udcc1 bank_model/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 data/\n\u251c\u2500\u2500\u2500\u2500\u2500 \ud83d\udcc4 bank-merged.csv\n\u251c\u2500\u2500 \ud83d\udc0d preparation.py\n\u251c\u2500\u2500 \ud83d\udc0d modelling.py\n\u251c\u2500\u2500 ...\n</code></pre> <p>Copy the code block from the previous recap section to get started. </p>"},{"location":"data-science/practice/modelling/#apply-preprocessing","title":"Apply preprocessing","text":"<p>Now it is time to actually apply all preprocessing steps. To prevent information leakage, we will split the data into training and test sets first.</p> <pre><code>from sklearn.model_selection import train_test_split\n\n# remove the target from data and assign it to y\ny = data.pop(\"y\")\nX = data\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n</code></pre> Tip <p>By default <code>train_test_split()</code> shuffles the data before splitting which is a good practice. It helps to avoid any bias that might be present in the order of the data.</p> <p>However, if you are ever working with data that is dependent on the order (time series data), you should not shuffle the data. In that case, set <code>shuffle=False</code>.</p> <p>Now apply the preprocessing steps (from the previous chapter) to the training and test sets:</p> <pre><code># impute missing values\nimpute.fit(X_train)\nX_train = impute.transform(X_train)\nX_test = impute.transform(X_test)\n\n# convert back to DataFrame\nX_train = pd.DataFrame(X_train, columns=X.columns)\nX_test = pd.DataFrame(X_test, columns=X.columns)\n\n# apply preprocessing (OneHotEncoder, KBinsDiscretizer &amp; StandardScaler)\npreprocessor.fit(X_train)\nX_train = preprocessor.transform(X_train)\nX_test = preprocessor.transform(X_test)\n</code></pre> Info <p>The <code>impute.transform()</code> method returns an array. Since the <code>preprocessor</code> requires the column names of our data, we need to convert the array back to a <code>DataFrame</code>. Else the <code>preprocessor</code> will not work!</p> <p>The general rule is to never call <code>fit</code> on the test data.</p> <p><code>scikit-learn</code>: Common pitfalls and recommended practices</p> Info <p>For example, when <code>impute.fit(X_train)</code> is called, the <code>SimpleImputer</code> calculates the mode solely from the training data - <code>X_train</code>. When <code>impute.transform(X_test)</code> is called, the <code>SimpleImputer</code> uses the mode calculated from <code>X_train</code> to fill in missing values in <code>X_test</code>. So we never use the test set to calculate the mode.</p> <p>The same principles apply to the <code>preprocessor</code> and thus we can prevent information leakage.</p>"},{"location":"data-science/practice/modelling/#train-a-model","title":"Train a model","text":"<p>For starters, we will train a simple decision tree. First, we need to encode our target <code>\"y\"</code> as we are still dealing with <code>str</code> labels (<code>\"yes\"</code> or <code>\"no\"</code>). For this purpose, we can use the <code>LabelEncoder</code>.</p> <pre><code>from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\nencoder.fit(y_train)\ny_train = encoder.transform(y_train)\ny_test = encoder.transform(y_test)\n</code></pre> <p>Now, we fit the first model. We set the parameters <code>max_depth</code> and <code>min_samples_leaf</code> to prune the tree.</p> <pre><code>from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(\n    random_state=42, max_depth=15, min_samples_leaf=10\n)\ntree.fit(X_train, y_train)\nscore = tree.score(X_test, y_test)\nprint(f\"Accuracy: {round(score, 2)}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Accuracy: 0.89\n</code></pre> <p>89 % accuracy seems like a perfect start! But don't get too excited yet, we overlooked a small yet crucial detail.</p>"},{"location":"data-science/practice/modelling/#detour-class-imbalance","title":"Detour: Class imbalance","text":"<p>In classification, the accuracy is a good metric to evaluate the performance of a model. However, it is not always the most appropriate one. Here's why:</p> <pre><code>print(y.value_counts(normalize=True))\n</code></pre> &gt;&gt;&gt; Output<pre><code>y\nno     0.89002\nyes    0.10998\n</code></pre> <p>The target variable <code>\"y\"</code> is imbalanced. The class <code>\"no\"</code> occurs in 89% of the cases, while <code>\"yes\"</code> accounts for roughly 11%. This means that a model that constantly predicts <code>\"no\"</code> for every observation would achieve an accuracy of 89%.</p> <p>Thus, our decision tree is not as good as it seems, and we need to consider other metrics to evaluate its performance. For our task, we pick the balanced accuracy score.</p>"},{"location":"data-science/practice/modelling/#confusion-matrix","title":"Confusion matrix","text":"<p>To understand the balanced accuracy score, we first need to understand the confusion matrix. Let's calculate it and explain it step by step.</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ny_pred = tree.predict(X_test)\n# calculate the matrix\nconf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n# plot it\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=conf_matrix, display_labels=[\"no\", \"yes\"]\n)\ndisp.plot()\nplt.show()\n</code></pre> Confusion matrix calculated using the test set. Info <p>The labels in red were added by the author and their creation is not part of the code block. They are simply used to facilitate the following explanation.</p> <p>Put simply, the confusion matrix compares the actual values with the predicted values (of our test set). The matrix is divided into four quadrants:</p> <ul> <li>True Positives (TP): The model correctly predicted the positive class     (yes - client subscribed to a term deposit). We have <code>20</code> true     positive cases.</li> <li>True Negatives (TN): The model correctly predicted the negative class (no     - client did not subscribe to a term deposit). In our instance,     <code>683</code> true negative cases.</li> </ul> <p>With the first diagonal covered, we move on to the instances that were incorrectly predicted:</p> <ul> <li>False Positives (FP): The model predicted the positive class, but the     actual class was negative. <code>16</code> to be specific.</li> <li>False Negatives (FN): The model predicted the negative class, but the     actual class was positive. <code>67</code> to be exact.</li> </ul> Tip <p>A perfect model would have 0 false positives and 0 false negatives. Hence, we want to minimize the false positives and false negatives.</p> <p>Generally, the confusion matrix is a great tool to understand the performance of a model. It is especially useful when dealing with imbalanced classes.</p> <p>Regarding our first model, we can simply conclude that there is still a lot of room for improvement. With an understanding of the confusion matrix, we can extend our knowledge to the balanced accuracy score.</p>"},{"location":"data-science/practice/modelling/#balanced-accuracy","title":"Balanced accuracy","text":"<p>The balanced accuracy score is defined as:</p> Balanced accuracy score \\[ \\text{Balanced accuracy} = \\frac{1}{2} \\left( \\frac{TP}{TP + FN} + \\frac{TN}{TN + FP} \\right) \\] <p>In the binary case, balanced accuracy is equal to the arithmetic mean of sensitivity (true positive rate) and specificity (true negative rate)</p> <p><code>scikit-learn</code>: Balanced accuracy score</p> <p>The balanced accuracy score ranges from 0 to 1. A score of 1 indicates a perfect model.</p> Manual calculation <p>Calculate the balanced accuracy score for the decision tree model.</p> <p>Use the results from the above confusion matrix and the formula for the balanced accuracy score. You can perform the calculation on a piece of paper or use simple arithmetic in Python.</p> <p>Let's compare your result with the one calculated by <code>scikit-learn</code>.</p> <pre><code>from sklearn.metrics import balanced_accuracy_score\n\nbalanced_accuracy = balanced_accuracy_score(y_test, y_pred)\nprint(f\"Balanced accuracy: {round(balanced_accuracy, 4)}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Balanced accuracy: 0.6035\n</code></pre> <p>Hopefully, your result matches the one calculated by <code>scikit-learn</code>.</p> <p>Compared to the accuracy of 89%, the balanced accuracy score of 60% gives a more realistic view of the model's performance. In turn, this means we have to improve our model.</p> Tip <p>If you want to know more about different metrics and scoring, check out this excellent guide. It not only covers classification metrics, but also multiple ways to score regression models (apart from the \\(R^2\\)).</p> <p><code>scikit-learn</code>: Metrics and scoring: quantifying the quality of predictions</p>"},{"location":"data-science/practice/modelling/#detour-reproducibility","title":"Detour: Reproducibility","text":"<p>Since, we are already on detours, let's take another one. Up until now, we have always set the <code>random_state</code> parameter (if available). As we have covered multiple times, this ensures the reproducibility of our results. We set it when splitting the data, when initializing a model, etc.</p> <p>But what happens if you forget to set the <code>random_state</code> parameter? To demonstrate the outcome, we generate a data set. In a loop we split the data, train a tree and calculate the balanced accuracy score. We repeat this process 10 times:</p> <pre><code>from sklearn.datasets import make_classification\n\nX_synthetic, y_synthetic = make_classification(\n    n_samples=1000,\n    n_features=20,\n    n_classes=2,\n    random_state=None,\n)\n\nfor i in range(10):\n    (\n        X_train_synthetic,\n        X_test_synthetic,\n        y_train_synthetic,\n        y_test_synthetic,\n    ) = train_test_split(\n        X_synthetic, y_synthetic, test_size=0.2, random_state=None\n    )\n    tree = DecisionTreeClassifier(\n        random_state=None, max_depth=15, min_samples_leaf=10\n    )\n    tree.fit(X_train_synthetic, y_train_synthetic)\n    y_pred = tree.predict(X_test_synthetic)\n    score = balanced_accuracy_score(y_test_synthetic, y_pred)\n    print(f\"Iteration {i + 1}: {round(score, 4)}\")\n</code></pre> <p>Here are my results, yours look drastically different:</p> &gt;&gt;&gt; Output<pre><code>Iteration 1: 0.8853\nIteration 2: 0.9457\nIteration 3: 0.8997\nIteration 4: 0.8947\nIteration 5: 0.9407\nIteration 6: 0.9097\nIteration 7: 0.9004\nIteration 8: 0.9451\nIteration 9: 0.919\nIteration 10: 0.8886\n</code></pre> <p>As you can see the model's performance varies greatly!</p> Danger <p>The code block illustrates the importance of the <code>random_state</code> parameter. Without it, you won't be able to reproduce your own results and others won't be able to reproduce your results either.</p> <p>Specifically, in a science setting and real-world applications, reproducibility is crucial to validate findings and conclusions. So ensure reproducibility!</p>"},{"location":"data-science/practice/modelling/#more-modelling","title":"More modelling","text":"<p>Let's get back on track and try out more models. We will compare their performance with the balanced accuracy score.</p>"},{"location":"data-science/practice/modelling/#random-forest","title":"Random forest","text":"<p>Naturally, since we started with a CART (decision tree), we try a random forest.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=15,\n    min_samples_leaf=10,\n    random_state=42,  # (1)!\n)\nforest.fit(X_train, y_train)\n\ny_pred = forest.predict(X_test)\nscore_forest = balanced_accuracy_score(y_test, y_pred)\n\nprint(f\"Forest balanced accuracy: {round(score_forest, 4)}\")\n</code></pre> <ol> <li>We adopt the values for <code>max_depth</code> and <code>min_samples_leaf</code> from the decision     tree.</li> </ol> &gt;&gt;&gt; Output<pre><code>Forest balanced accuracy: 0.5927\n</code></pre> <p>Compared to the decision tree (balanced accuracy of 60.35%), the random forest has a balanced accuracy of 59.27%. Somehow, the performance got even worse!</p>"},{"location":"data-science/practice/modelling/#class_weight-parameter","title":"<code>class_weight</code> parameter","text":"<p>We can try to improve the performance by setting the <code>class_weight</code> parameter to <code>balanced</code>. This takes the class imbalance into consideration.</p> <pre><code>forest = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=15,\n    min_samples_leaf=10,\n    random_state=42,\n    class_weight=\"balanced\",\n)\nforest.fit(X_train, y_train)\n\ny_pred = forest.predict(X_test)\nbalanced_forest = balanced_accuracy_score(y_test, y_pred)\n\nprint(f\"Forest balanced accuracy: {round(balanced_forest, 4)}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Forest balanced accuracy: 0.7337\n</code></pre> <p>Now we were able to improve the performance significantly, namely to 73.37%.</p>"},{"location":"data-science/practice/modelling/#logistic-regression","title":"Logistic regression","text":"Logistic regression <p>What about a logistic regression model? How does it perform?</p> <ol> <li>Initialize and fit a <code>LogisticRegression</code> model with     <code>class_weight=\"balanced\"</code>. Don't forget to set the     <code>random_state</code>.</li> <li>Calculate the balanced accuracy score for the test set.</li> <li>Compare the results to the decision tree and random forest.</li> </ol> Info <p>Depending on the parameter settings, the logistic regression model achieves similar performance to the random forest.</p> <p>As you can see, with a preprocessed data set, we can now easily compare different models.</p> <p>These are our results so far:</p> <ul> <li>Decision tree: 60.35%</li> <li>Random forest: 73.37%</li> <li>Logistic regression: ? (your result)</li> </ul>"},{"location":"data-science/practice/modelling/#hyperparameter-tuning","title":"Hyperparameter tuning","text":"<p>So far, we've used arbitrary values for our model parameters (<code>max_depth=15, min_samples_leaf=10</code>, etc.). However, these might not be optimal for our specific problem. Therefore, we apply hyperparameter tuning. Hyperparameter tuning is the process of finding the best combination of model parameters to maximize performance.</p> <p>For starters, we will perform a manual hyperparameter tuning for the maximum depth (<code>max_depth</code>) parameter. We will test the values <code>[5, 10, 15, 20, 25]</code>.</p> <pre><code>max_depth = [5, 10, 15, 20, 25]\n\nfor n in max_depth:\n    forest = RandomForestClassifier(\n        n_estimators=100,\n        max_depth=n,\n        min_samples_leaf=10,\n        random_state=42,\n        class_weight=\"balanced\",\n    )\n    forest.fit(X_train, y_train)\n    y_pred = forest.predict(X_test)\n    score = balanced_accuracy_score(y_test, y_pred)\n    print(f\"max_depth={n}: {round(score, 4)}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>max_depth=5: 0.7352\nmax_depth=10: 0.7445\nmax_depth=15: 0.7337\nmax_depth=20: 0.733\nmax_depth=25: 0.733\n</code></pre> <p>The best performance is achieved with a <code>max_depth</code> of <code>10</code>. So the initial value of <code>15</code> was not optimal. Next, we could try to optimize the number of trees (<code>n_estimators</code>), the minimum number of samples required to be at a leaf node (<code>min_samples_leaf</code>), etc. You get the point ...</p> Tip <p>However, this manual tuning is time-consuming and not always feasible. In the last (advanced) chapter of this course, we will introduce you to automated hyperparameter tuning.</p> Info <p>You can spend hours tuning hyperparameters. So, don't get lost in the hyperparameter tuning process.</p> <p> Spoiler alert : With this specific data set and hyperparameter tuning, you won't significantly surpass the results we have achieved so far.</p>"},{"location":"data-science/practice/modelling/#the-result","title":"The result","text":"<p>We conclude that a</p> <pre><code>RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_leaf=10,\n    random_state=42,\n    class_weight=\"balanced\",\n)\n</code></pre> <p>is the best model we have found for our task. It achieves a balanced accuracy of 74.45%.</p> Here is the main takeaway: Tip <p>Unfortunately, with real world data sets you won't always achieve astounding results. But that's okay! </p> <p>If the performance does not meet your expectations, you can try following things:</p> <ul> <li>Feature engineering: Create new features or modify existing ones.</li> <li>Preprocessing: Try different preprocessing steps.</li> <li>Model selection: Try different models.</li> </ul> <p>But sometimes, it is also a possibility that the features can't describe the target variable well enough or you simply need more data.</p>"},{"location":"data-science/practice/modelling/#recap","title":"Recap","text":"<p>We tried different models and evaluated their performance using the balanced accuracy score. Ultimately, we concluded that a random forest model performed best.</p> <p>Along the way, we introduced class imbalance, confusion matrix, balanced accuracy and hyperparameter tuning. Another example illustrated the importance of reproducibility.</p> <p>Next, we distill our findings in an end-to-end example and save the final model to disk.</p>"},{"location":"databasics/Data/","title":"Data vs. Big Data","text":"<p>In its simplest form, data represents information stored digitally , serving as a foundation for analysis and decision-making. Data acts as building blocks for information and knowledge when it is organized and processed. Data is the fundamental element upon which insights, decisions, and actions are based in fields like science, business, and technology.</p>"},{"location":"databasics/Data/#classical-data","title":"Classical Data","text":"<p>Classical data refers to traditional datasets that have been analyzed for decades. These datasets are typically structured, meaning they are organized in a predictable way, such as rows and columns in a relational database. Classical data is generally smaller in volume, ranging from megabytes (MB) to gigabytes (GB), and can be processed using standard tools like SQL databases or spreadsheets . Key characteristics of classical data include:</p> <ul> <li>Structured Format: Data is arranged in a predefined schema, such as tables with rows and columns.</li> <li>Low to Moderate Volume: Datasets are relatively small and manageable.</li> <li>Predictability: The data follows a consistent structure, making it easier to analyze using traditional methods.</li> <li>Centralized Storage: Classical data is typically stored in centralized databases, managed by database management systems (DBMS).</li> </ul> <p>Classical data is ideal for tasks where data volumes are manageable, the structure is well-defined, and computational requirements are not excessive. Examples include customer transaction records, survey data, or small-scale experimental data.</p>"},{"location":"databasics/Data/#big-data","title":"Big Data","text":"<p>Big Data refers to datasets that are large, complex, or fast-changing, often surpassing the capabilities of traditional data tools. Big Data is characterized by the \"three Vs\": Volume, Velocity, and Variety. These datasets are often unstructured or semi-structured and do not fit neatly into traditional database formats. The size of Big Data can range from terabytes (TB) to petabytes (PB) or more. Key characteristics of Big Data include:</p> <ul> <li>Volume: The data size is massive, requiring distributed systems like Hadoop or cloud-based solutions for storage and processing.</li> <li>Velocity: Data is generated and processed at high speeds, often in real-time.</li> <li>Variety: Big Data comes in various formats:<ul> <li>Structured: Data that can be mapped in databases like addresses, product lists, HR management</li> <li>Unstructured: Files like PDF, scanned mails, presentations, images, and videos.</li> <li>Semi-structured: Partly structured, partly unstructured like mails (recipient, sender and subject show a structure, but content is unstructured)</li> </ul> </li> <li>Complexity: The diversity and interconnectedness of data sources add to the complexity of managing and analyzing Big Data.</li> </ul> Visualisation of Big Data according to IBM (Source: BlogDozouza) <p>Big Data requires specialized tools and technologies to handle its characteristics. Distributed computing frameworks like Apache Hadoop and Spark, NoSQL databases, and advanced machine learning algorithms are often employed to extract insights from these vast and complex datasets. Examples of Big Data include social media interactions, sensor data from the Internet of Things (IoT), and genomic sequences.</p> 60 Seconds in the Internet (Source: eDiscovery Today &amp; LTMG)"},{"location":"databasics/Data/#understanding-the-difference","title":"Understanding the Difference","text":"<p>Understanding the differences between classical data and Big Data is essential for professionals in data analysis, data engineering, or data science. Classical data involves structured and manageable datasets processed with conventional tools, while Big Data deals with vast, fast-moving, and complex datasets requiring innovative storage solutions and advanced processing methods.</p> Classical Data Big Data Scale Small Massive Structure Structured Structured, Semi-structured, Unstructured Processing Requirements Traditional tools Advanced algorithms Storage Centralized database Distributed storage Analytical Approaches Statistics Machine learning, Data mining, Real-time processing"},{"location":"databasics/DataBasics/","title":"Data Basics","text":"<p>Before starting with Data Science, it's essential to get to know the data you'll be working with. This means thoroughly examining the attributes and values to understand their characteristics. Real-world data is often messy, large, and diverse, which can make it difficult to handle. For example, data from sensors might include missing or corrupted values, while social media data might be unstructured and include text, images, or even videos. In financial data, outliers or extreme values may skew analysis results.</p> <p>Having a deep understanding of the data is crucial for successful data preprocessing, which is the first major step after acquiring the data. For instance, in sensor data, you may need to filter noise, while in social media data, you might need to convert unstructured text into a structured format for analysis. The main objective is to gather useful insights about the data, which will aid in the preprocessing stage, such as identifying patterns, missing values, or outliers. Grasping these aspects early on provides a solid foundation for the rest of your data analysis process.</p>"},{"location":"databasics/DataBasics/#dataset-objects-attributes","title":"Dataset, Objects, Attributes","text":"<p>In order to do that, we need to distinguish between the terms data set, object and attribute:</p> <ul> <li>Data Set: A data set is a collection of related data organized in a structured format. It consists of multiple objects, each described by a set of attributes. A data set can be represented as a table, where each row corresponds to a data object and each column corresponds to an attribute. Data sets are commonly used in data analysis, machine learning, and other data-driven tasks, serving as the primary source of input for these processes.</li> <li>Object: An object (or sometimes records, instances, or entries) is an individual unit of data within a data set. It represents a single entity or instance, such as a person, a product, or an event, depending on the context of the data. Each data object is characterized by a set of attributes, which define its specific properties or features. In a tabular data set, a data object corresponds to a row, with each attribute value for that object stored in the respective columns.</li> <li>Attributes: Attributes (or sometimes variable, field, dimension, feature or observations) are the characteristics or properties that describe data objects in a data set. Each attribute represents a specific feature of the data object and is associated with a particular value. For example, in a data set of customer information, attributes might include \"Name,\" \"Age,\" \"Gender,\" and \"Purchase History.\" In a tabular representation, attributes are typically the column headers, with each column containing the attribute values for the corresponding data objects (rows). Attributes can be of different types as described later.</li> </ul> Example: Data Set <p>  The displayed data shows a dataset consisting of three objects and seven attributes.</p>"},{"location":"databasics/DataBasics/#qualitative-vs-quantitative","title":"Qualitative vs Quantitative","text":"<p>A variable is called qualitative (categorical) when each observation distinctly falls into a specific category. Qualitative variables express different qualitative properties, but do not convey any magnitude. Conversely, a variable is termed quantitative (numerical) when it measures the magnitude of a property. Quantitative variables can be either Discrete (The variable can only take on a finite number of values) or Continuous (The variable can take on any value within a given interval).</p> Example: Qualitative vs Quantitative Attributes <ul> <li>Qualitative: Race, religious affiliation, gender, children in the household (yes/no)</li> <li>Quantitative: Age, test scores, number of children in a household<ul> <li>Discrete: Number of children in a household  </li> <li>Continuous: Height, weight, length measurements</li> </ul> </li> </ul>"},{"location":"databasics/DataBasics/#attribute-types","title":"Attribute Types","text":"<p>We now know, that attributes define the properties of data objects and are crucial in determining the methods and algorithms that can be applied during analysis. Different types of attributes - such as nominal, ordinal, interval, and ratio - each have unique characteristics that influence how they should be handled and interpreted. Recognizing and appropriately categorizing these attribute types is a key step in ensuring accurate data analysis and meaningful results.</p>"},{"location":"databasics/DataBasics/#nominal","title":"Nominal","text":"<p>Nominal attributes refer to those that are associated with names or labels. </p> <pre><code>cars = ['BMW', 'Audi', 'VW', 'Skoda', 'Tesla', 'Audi']\n</code></pre> <p>The values of nominal attributes are typically symbols or names representing different categories, codes, or states. These values are used to classify data into distinct groups, often referred to as categorical attributes. Importantly, the values of nominal attributes do not have any inherent or meaningful order; they simply indicate membership within a particular category without implying any rank or sequence. </p> Example: Nominal Attributes <ul> <li>Occupation: teacher, dentist, programmer, farmer,...</li> <li>Hair color: black, brown, blond, red, gray, white,...</li> </ul> <p>You can only determine whether two people have the same hair color or not. It is not possible to establish a greater-than or less-than relationship, and the differences between hair colors cannot be meaningfully interpreted.</p> <p>The symbols or names associated with nominal attributes can also be represented by numbers.</p> <pre><code>cars_num = [1, #'BMW'\n            2, #'Audi'\n            3, #'VW'\n            4, #'Skoda'\n            5, #'Tesla'\n            2] #'Audi'  \n</code></pre> <p>However, in such cases, these numbers are not meant to be used quantitatively, meaning that mathematical operations on them are not meaningful. For instance, calculating the mean or median of these numbers would not make sense. </p> <pre><code>import statistics \nstatistics.mean(cars_num)\n</code></pre> &gt;&gt;&gt; Output<pre><code>2.8333333333333335\n</code></pre> <p>But what does that mean? Is it half <code>'Audi'</code>, half <code>'VW'</code>? And we also could have used the numer <code>32</code> for <code>'Audi'</code> and <code>0</code> for <code>'VW'</code>. So its meaningless! </p> <p>Instead, the mode, which identifies the most frequently occurring value, can be used and is particularly useful in this context.</p> <pre><code>statistics.mode(cars)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Audi\n</code></pre> Example: Nominal Attributes Represented by Numbers <ul> <li>Customer ID: 0001, 0002, 0003, 0004, 0005,...</li> <li>Hair color: black=001, brown=010, blond=011,...</li> </ul> <p>Binary attributes are a specific type of nominal attribute that consist of only two categories. These categories are often represented by the numbers 0 and 1, where 0 indicates the absence of the attribute and 1 indicates its presence. This binary classification is commonly used in data analysis to represent simple variables</p> Example: Binary Attributes <ul> <li>Smoker: Yes=1, No=0</li> <li>Medical Test: Positive=1, Negative=0</li> </ul>"},{"location":"databasics/DataBasics/#ordinal","title":"Ordinal","text":"<p>For certain types of attributes, the possible values have a meaningful order or ranking among them, indicating that one value can be considered greater or less than another. However, while this order is significant, the exact magnitude or distance between successive values is not known. This means that while the sequence of values is meaningful, we cannot quantify the difference between them with precise measurements. Therefore it is possible and meaningful to calculate the median and mode. Mean on the other hand is not meaningful.</p> <pre><code>drinks = ['small', 'small', 'small', 'medium', 'medium', 'medium', 'large']\n\nprint('Median: ' + statistics.median(drinks))\nprint('Mode: ' + statistics.mode(drinks))\n</code></pre> &gt;&gt;&gt; Output<pre><code>Median: medium\nMode: small\n</code></pre> Example: Ordinal Attributes <ul> <li>Professional Rank: private, specialist, corporal, sergeant</li> <li>Drink size: small, medium, large</li> </ul> <p>As with the nominal scale, you can determine whether two drinks are the same size or not. Additionally, you can say whether one drink is larger than another. However, it is still not possible to meaningfully interpret the differences between sizes. You cannot specify by how much one drink is larger than another.</p>"},{"location":"databasics/DataBasics/#numerical","title":"Numerical","text":"<p>Numeric attributes are quantitative in nature, meaning they represent measurable quantities. These attributes can be expressed as either integer or real values. One of the key characteristics of numeric attributes is their ability to quantify the difference between values, allowing for meaningful comparisons. Statistical measures such as the mean, median, and mode can be calculated from numeric attributes, and these measures are both possible and useful for analyzing the data.</p> <pre><code>income = [1005, 2500, 2500, 5100, 6011, 10500]\n\nprint('Mean: ' + str(statistics.mean(income)))\nprint('Median: ' + str(statistics.median(income)))\nprint('Mode: ' + str(statistics.mode(income)))\n</code></pre> &gt;&gt;&gt; Output<pre><code>Mean: 4602.67\nMedian: 3800.0\nMode: 2500\n</code></pre>"},{"location":"databasics/DataBasics/#interval-scaled","title":"Interval-scaled","text":"<p>Interval-scaled attributes can be measured on a scale with equal-sized units, allowing for consistent and comparable intervals between values. These attributes have an inherent order and they can take on positive, zero, or negative values. This means that the ranking of values is possible and meaningful, providing a clear sense of progression or regression along the scale.</p> Example: Interval-scaled Attributes <ul> <li>Calendar dates: For instance, the years 2002 and 2010 are eight years apart</li> <li>Celsius temperature: \\(20^\\circ C\\) is five degrees higher than a temperature of \\(15^\\circ C\\)</li> </ul> <p>As with the ordinal scale, you can determine whether two temperatures are the same and whether one temperature is higher than another. Additionally, the difference between temperatures can be meaningfully interpreted. However, because the zero point is arbitrary, ratios cannot be meaningfully interpreted.</p>"},{"location":"databasics/DataBasics/#ratio-scaled","title":"Ratio-scaled","text":"<p>Ratio-scaled attributes possess an inherent zero-point, which indicates the complete absence of the attribute. This characteristic allows us to meaningfully discuss one value as being a multiple of another. Because of this, ratio-scaled data supports a wide range of mathematical operations, including meaningful comparisons of both differences and ratios between values.</p> Example: Ratio-scaled Attributes <ul> <li>Kelvin temperature: has true zero-point</li> <li>Count attributes: years of experience, number of words, Income in Euros</li> </ul> <p>As with the interval scale, you can determine whether two people have the same income and whether one person earns more than another. Additionally, the differences between incomes can be meaningfully interpreted. Furthermore, the ratio between two incomes can now also be interpreted, such as determining how many times one income is compared to another.</p> Task: Attribute Types <p>Name the types of attributes in the following data set and justify  </p>"},{"location":"databasics/Terms/","title":"Definition of Key Terms","text":"<p>Johannes Kepler (1571-1630) is often considered one of the first Data Scientists due to his groundbreaking work in analyzing astronomical data to uncover the laws governing planetary motion. In the early 1600s, Kepler meticulously studied large amounts of observational data gathered by the astronomer Tycho Brahe. By applying mathematical models and repeated analysis, Kepler discovered that planets move in elliptical orbits around the sun, challenging the long-standing belief in circular orbits. His findings were published in the Rudolphine Tables in 1627, which were the most precise planetary position tables of the time. These tables, based on Kepler's laws of planetary motion, were crucial for navigation and scientific research for over a century, illustrating his innovative use of data to revolutionize our understanding of the solar system. This work laid the foundation for modern data analysis practices.</p> <ul> <li> <p> Johannes Keppler (Source: Wikipedia) </p> </li> <li> <p> Rudolphine Tables (Source: Wikipedia) </p> </li> </ul> <p>In the world of data analysis and processing, terms like Data Science, Machine Learning, Artificial Intelligence (AI), Business Intelligence (BI), Predictive Analytics, and Data Engineering are often used. Each of these terms refers to distinct but interconnected fields and methods. To gain a better understanding of these areas, it is important to clearly define these terms and explore how they are related.</p> Definitions of Key Terms <p>To familiarize yourself with fundamental concepts, research the following terms:</p> <ul> <li> <p>Data Science: Investigate what Data Science encompasses and how it differs from other disciplines. What are the central tasks and methods involved in Data Science?</p> </li> <li> <p>Machine Learning: Research how Machine Learning is defined and what role it plays within Data Science. Explore the different types of Machine Learning and their applications.</p> </li> <li> <p>Artificial Intelligence (AI): What is Artificial Intelligence, and how does it differ from Machine Learning? What are the application areas of AI that extend beyond Machine Learning?</p> </li> <li> <p>Business Intelligence (BI): BI is a commonly used term in businesses. What does BI mean, and how does it differ from Data Science? What tools and methods are typical for BI?</p> </li> <li> <p>Predictive Analytics: Investigate what Predictive Analytics is and how it is used in the context of Data Science and BI. How is Predictive Analytics related to Machine Learning?</p> </li> <li> <p>Data Engineering: Find out what tasks and responsibilities are included in Data Engineering and how it differs from Data Science. Why is Data Engineering an essential part of successful data projects?</p> </li> </ul>"},{"location":"micropython/","title":"Home","text":"<p>Welcome to the <code>MicroPython</code> Course! </p> <p>This course is designed to provide you with a comprehensive introduction to building an IoT project with MicroPython on the ESP32. You will learn how to read sensor data, control a water pump, communicate via MQTT, and create a dashboard for remote monitoring - all through hands-on code examples!</p>"},{"location":"micropython/#course-overview","title":"Course overview","text":"<ol> <li>Setup &amp; Basics:  Introduction to MicroPython, flashing the ESP32 with the firmware, setting up your development environment and learn the basics of MicroPython by controlling the onboard LED.</li> <li>Sensor Integration:  Interface with a soil moisture sensor to monitor your plant\u2019s water needs.</li> <li>Actuator Control:  Learn to drive a pump (via a relay module) based on sensor readings.</li> <li>MQTT Communication:  Transmit sensor data and receive remote commands using MQTT.</li> <li>Dashboard Development:  Create an interactive dashboard to visualize sensor data and control your watering system.</li> </ol> <pre><code>graph LR\n    subgraph TOP[ ]\n        direction BT\n            Cloud[Cloud Dashboard]\n    end\n\n    subgraph MID[ ]\n        direction LR\n            Sensors\n            ESP32[ESP32 Basics]\n            Actuators\n    end\n\n    Cloud &lt;--MQTT--&gt; ESP32\n    Sensors --&gt; ESP32\n    ESP32 --&gt; Actuators\n\n    click ESP32 \"./setup\" _self\n    click Sensors \"./sensors\" _self\n    click Actuators \"./actuator\" _self\n    click Cloud \"./mqtt\" _self\n\n\n    %% Styling\n    classDef active fill:#950f42,stroke:#333,stroke-width:1px;\n    class MID subgraphBox;\n    class TOP subgraphBox2;\n\n    %% Subgraph styling workaround (pseudo-class)\n    classDef subgraphBox fill:#ff000000,stroke:#950f42,stroke-width:2px,color:#fff;\n    classDef subgraphBox2 fill:#ff000000,stroke:#950f42,stroke-width:0px,color:#fff;</code></pre>"},{"location":"micropython/#tools","title":"Tools","text":"<p>In this course, we use MicroPython running on the ESP32 along with these great tools:</p>"},{"location":"micropython/#sneak-peek","title":"Sneak peek","text":"<p>Here is a sneak peek of selected topics we cover in this course:</p> <ul> <li> <p> MicroPython Basics &amp; LED Control </p> <p>Learn how to flash your ESP32 with MicroPython and start by blinking the onboard LED.</p> </li> <li> <p> </p> </li> <li> <p> </p> </li> <li> <p> Sensor Integration </p> <p>Read data from a soil moisture sensor to detect your plant\u2019s water needs.</p> </li> <li> <p> Pump Control </p> <p>Control a water pump (via a relay) to automate the irrigation process.</p> </li> <li> <p> </p> </li> <li> <p> </p> </li> <li> <p> MQTT Communication </p> <p>Transmit sensor data and receive remote commands using MQTT for real-time control.</p> </li> <li> <p> Dashboard Development </p> <p>Build a user-friendly dashboard to monitor sensor readings and control the pump.</p> </li> <li> <p> </p> </li> </ul>"},{"location":"micropython/#needed-hardware","title":"Needed Hardware","text":"<p>The following hardware is needed for this course:</p> <ul> <li>1x ESP32-Wroom-32 Breakout Board</li> <li>1x Adafruit Submersible 3V DC Water Pump</li> <li>1x MOSFET Modul</li> <li>1x HW-390 Moisture Sensor</li> <li>1x Micro USB Cable</li> <li>1x Jumper Wires (F-M &amp; F-F)</li> <li>1x 1k Ohm Resistor</li> </ul>"},{"location":"micropython/#expected-outcome","title":"Expected outcome","text":"<p>By the end of this course, you will have built a complete smart plant watering system. You will know how to:</p> <ul> <li>Monitor soil moisture using a sensor.</li> <li>Automatically control a water pump based on sensor data.</li> <li>Communicate with your device via MQTT.</li> <li>Visualize data and interact with your system through a dashboard.</li> </ul> Let's get started! \ud83d\ude80"},{"location":"micropython/actuator/","title":"Controlling a Water Pump","text":"<p>Now that we can successfully read the soil moisture, it\u2019s time to take action: if the soil is too dry, our system should water the plant instead of turning on the LED. To do this, we\u2019ll use a small water pump and control it with our ESP32.</p>"},{"location":"micropython/actuator/#the-hardware","title":"The Hardware","text":""},{"location":"micropython/actuator/#submersible-pump","title":"Submersible Pump","text":"<p>A submersible pump is a compact electric pump designed to operate underwater, typically used to move liquids from one place to another. In our project, it draws water from a reservoir to irrigate a plant.</p> <p>Submersible pumps work by pushing water to the surface using an impeller, which creates fluid pressure. They are quiet, efficient, and ideal for applications where space and simplicity are important - like automatic plant watering systems.</p> <p>The wiring of the pump is quite simple. It has 2 wires: Red for ~3V and Black for GND.</p>"},{"location":"micropython/actuator/#pump-power-supply","title":"Pump Power Supply","text":"<p>Microcontrollers like the ESP32 cannot directly power components such as motors or pumps - they simply don't provide enough current. For example, our water pump operates at 3\u202fV and requires 100\u2013200\u202fmA, while a single GPIO pin of the ESP32 can only supply around 15\u202fmA. Therefore, we need an interface that allows us to switch higher currents using the ESP32\u2019s low-power control signals. The most common solutions are relay modules and MOSFET drivers.</p> <p>In this project, we use a MOSFET driver module like this one:</p> <p>It acts like an electronically controlled switch: the small signal from a GPIO pin controls whether a larger current flows through the pump. A MOSFET has three terminals:</p> <ul> <li>Gate: the control input (connected to the ESP32 GPIO)</li> <li>Drain: where current flows out (connected to the pump's negative side)</li> <li>Source: the return path to ground</li> </ul> <p>When the gate is set <code>HIGH</code>, the MOSFET allows current to flow from source to drain, completing the circuit and powering the pump. When the gate is set <code>LOW</code>, the switch is off and no current flows.</p> <p>Although pumps are often powered by external sources (e.g. AAA batteries), in this case the ESP32's onboard 3.3\u202fV output is sufficient. While the pump is rated for 3\u202fV, running it at 3.3\u202fV is acceptable - small voltage differences are typically tolerated, especially with minor losses through wires and switching components.</p> Power Supply Limitations <p>However, this simplified setup works only under certain conditions. If you plan to use larger motors or additional components, or if your code requires heavy processing, the ESP32 may no longer be able to provide enough power.</p>"},{"location":"micropython/actuator/#wiring-the-pump","title":"Wiring the Pump","text":"<p>For this chapter, we want to go further in our project and combine all the components we have learned so far with a simple connection of a pump. Therefore, we need to connect all the components as shown in the following diagram:</p>"},{"location":"micropython/actuator/#coding","title":"Coding","text":"<p>To control the pump, we simply need to control one single GPIO pin connected to the MOSFET driver module. By switching the GPIO pin from <code>HIGH</code> to <code>LOW</code>, we directly control the pump.</p> Task: Watering System Automation <p>Now it's time to build a first version of our watering system. Perform the following steps:</p> <ul> <li>Use the code from the previous chapters to read the sensor and turn on the pump instead of the LED.</li> <li>Adjust the code to:<ul> <li>Read the sensor every 10 seconds.</li> <li>If moisture is below 30%, turn on the pump for 5 seconds.</li> <li>Print appropriate messages in the terminal.</li> <li>Test the behavior with wet and dry sensor conditions.</li> </ul> </li> </ul> <p> </p>"},{"location":"micropython/actuator/#conclusion","title":"Conclusion","text":"<p>You now know how to switch a water pump using a MOSFET driver. In the next chapter, we\u2019ll learn how to send the sensor values to the cloud and receive commands using MQTT!</p> <pre><code>graph LR\n    subgraph TOP[ ]\n        direction BT\n            Cloud[Cloud Dashboard]\n    end\n\n    subgraph MID[ ]\n        direction LR\n            Sensors:::active\n            ESP32[ESP32 Basics]:::active\n            Actuators:::active\n    end\n\n    Cloud &lt;--MQTT--&gt; ESP32\n    Sensors --&gt; ESP32\n    ESP32 --&gt; Actuators\n\n    click ESP32 \"../setup\" _self\n    click Sensors \"../sensors\" _self\n    click Actuators \"../actuator\" _self\n    click Cloud \"../mqtt\" _self\n\n\n    %% Styling\n    classDef active fill:#950f42,stroke:#333,stroke-width:1px;\n    class MID subgraphBox;\n    class TOP subgraphBox2;\n\n    %% Subgraph styling workaround (pseudo-class)\n    classDef subgraphBox fill:#ff000000,stroke:#950f42,stroke-width:2px,color:#fff;\n    classDef subgraphBox2 fill:#ff000000,stroke:#950f42,stroke-width:0px,color:#fff;</code></pre>"},{"location":"micropython/mqtt/","title":"Cloud Dashboard","text":"<p>Let's take a deep breath and think about what we've done so far. We've learned how to work with an ESP32 microcontroller, how to read a sensor and how to control an actuator.  But so far, we are only able to interact with our system locally over the USB connection. So, to update the threshold or to manually turn the pump on and off, we need to be physically present at the system. Now, we want to change this! </p> <p>The goal of this chapter is to make our system more interactive. We want to be able to see the sensor values and control the actuator from everywhere (at least where you have an internet connection). We will build a browser-based dashboard using MQTT Tiles from flespi.io. The dashboard will allow you to monitor sensor values (like soil moisture) and send control commands (like humidity thresholds) directly to your ESP32 device via MQTT.</p>"},{"location":"micropython/mqtt/#what-is-mqtt","title":"What is MQTT?","text":"<p>MQTT (Message Queuing Telemetry Transport) is a lightweight, publish-subscribe network protocol designed for reliable communication between devices over low-bandwidth or high-latency networks.</p> <p>The key concepts of MQTT are:</p> <ul> <li>Broker: The central server that manages communication between clients.</li> <li>Topic: The channel to which devices can publish or subscribe.</li> <li>Payload: The message content.</li> </ul> <p>MQTT follows a publisher/subscriber model, which makes it scalable and well-suited for IoT systems.</p> Publish-Subscribe Model <p>Think of it like Instagram: users post messages to a channel (topic), and followers (subscribers) get updates in real time.</p>"},{"location":"micropython/mqtt/#setting-up-a-mqtt-broker","title":"Setting up a MQTT Broker","text":"<p>Before we can start programming our ESP32, we need to create a MQTT broker. In this example we will use HiveMQ as a free cloud-based broker. Follow these steps to create and configure your own MQTT broker:</p> <ol> <li>Visit HiveMQ</li> <li>Sign in using Google, GitHub, or LinkedIn \u2014 or create a free account</li> <li> <p>Select the Free Plan and create a serverless cluster</p> <p></p> </li> <li> <p>Click on 'Manage Cluster'</p> <p></p> </li> <li> <p>Switch to the 'Getting Started' tab</p> <p></p> <ul> <li>Left side: you'll find all the details needed to connect (Cluster/Broker URL, TLS Port (8883), WebSocket URL)</li> <li>Right side:In order to connect to the broker, you need to create a connection credential. Therefore, enter a username and password and click on 'Add'. After creation, credentials can be reviewed and managedunder the 'Access Management' tab</li> </ul> </li> <li> <p>To test your connection. Go to the 'Web Client' tab, enter your MQTT credentials and click 'Connect'. You should see a green 'connected' message. +</p> <p></p> </li> </ol>"},{"location":"micropython/mqtt/#esp32-setup","title":"ESP32 Setup","text":""},{"location":"micropython/mqtt/#hardware","title":"Hardware","text":"<p>To send and receive data from the ESP32, no additional hardware is required. However, since we will continue building on our irrigation system later, we will keep using the same circuit setup as before. So, no changes to the wiring are needed at this point.</p>"},{"location":"micropython/mqtt/#wi-fi-connection","title":"Wi-Fi Connection \ud83c\udf10","text":"<p>The standard MicroPython installation on the ESP32 includes a built-in Wi-Fi library, making it easy to connect to a wireless network with just a few lines of code. For detailed documentation, see the MicroPython Wi-Fi guide.</p> <p>To establish an internet connection, the following code can be placed in the <code>boot.py</code> file, as the connection only needs to be initialized once at startup:</p> Security Tip <p>Never store credentials directly in your main code. Instead, create a separate <code>config.py</code> file:</p> config.py<pre><code>ssid = \"your_ssid\"\npassword = \"your_password\"\n</code></pre> boot.py<pre><code># WIFI CONFIGURATION\nimport machine, network\nimport config # own config.py file\n\ndef do_connect():\n\n    wlan = network.WLAN()\n    wlan.active(True)\n    if not wlan.isconnected():\n        print('connecting to network...')\n        wlan.connect(config.ssid, config.password)\n        while not wlan.isconnected():\n            machine.idle()\n    print('connection successful!')\n    print('network config:', wlan.ipconfig('addr4'))\n    return wlan\n\nwlan = do_connect()\n</code></pre> WPA2 Enterprise (Eduroam) <p>Unfortunately, the ESP32 does not support WPA2 Enterprise networks like Eduroam. To work around this, use a different Wi-Fi router or create a personal mobile hotspot.</p> Task: Connect and Disconnect from Wi-Fi <ul> <li>Connect to your Wi-Fi network using the <code>do_connect()</code> function.</li> <li>After a successful connection, disconnect from the Wi-Fi network. Take a look in the documentation for the <code>wlan</code> object to find the function.</li> </ul> <p>Your console log should look something like this: </p> <pre><code>connecting to network...\nconnection successful!\nnetwork config: ('172.20.10.2', '255.255.255.240')\ndisconnecting from network...\ndisconnected!\n</code></pre>"},{"location":"micropython/mqtt/#mqtt-communication","title":"MQTT Communication \ud83d\udce1","text":"<p>Now that we don't just want to connect our ESP32 to the outside world, but also interact with it (send and receive data), we need to choose an appropriate communication protocol. As discussed earlier, MQTT is widely used in IoT applications and supported by many providers. In this section, we will program the ESP32 to connect to Wi-Fi and the MQTT broker. After that we will send and receive messages over defined MQTT topics.</p>"},{"location":"micropython/mqtt/#initial-setup","title":"Initial Setup","text":"<p>We will now expand our existing Wi-Fi connection code by adding MQTT functionality. Before sending or receiving any data, we must connect to the MQTT broker using credentials. Add the following parameters to your existing <code>config.py</code> file and replace them with your HiveMQ details:</p> <p>config.py<pre><code>mqtt_server = 'your_mqtt_server'    # MQTT URL (e.g. 'xy.s1.eu.hivemq.cloud')\nmqtt_user = 'your_mqtt_user'        # MQTT-Broker Username \nmqtt_pass = 'your_mqtt_password'    # MQTT-Broker Password\nmqtt_port = 8883                    # MQTT-Broker Port\n</code></pre> Since we already import <code>config.py</code> in <code>boot.py</code>, we can now begin setting up the MQTT connection.</p>"},{"location":"micropython/mqtt/#mqtt-library","title":"MQTT Library","text":"<p>To avoid writing the full MQTT protocol implementation ourselves, we'll use a prebuilt MicroPython-compatible MQTT library. This file is not included in the MicroPython standard library and must be downloaded manually:</p> <p>upymqtt.py</p> <p>Save <code>upymqtt.py</code> in the same folder as your <code>boot.py</code> (your project root).</p> <p>Then extend your <code>boot.py</code> with the following:</p> boot.py<pre><code>...\n\n# MQTT CONFIGURATION\nfrom upymqtt import MQTTClient\nimport ubinascii  #(1)!\n\n# Generate Unique Client ID\nclient_id = ubinascii.hexlify(machine.unique_id())\n\n# Topics to subscribe and publish\ntopic_sub = b'settings'\ntopic_pub = b'kpi'\n\n# MQTT Server Credentials\nmqtt_server = config.mqtt_server\nmqtt_user = config.mqtt_user\nmqtt_pass = config.mqtt_pass\nmqtt_port = config.mqtt_port\n</code></pre> <ol> <li><code>ubinascii</code> is part of the MicroPython standard library and helps generate a unique device ID.</li> </ol> <p>Let's take a closer look at the code. The first lines are for importing the required libraries. Then a unique device ID is created, which we need for the connection to the MQTT broker. Then we can define the topics we want to receive (e.g. <code>topic_sub</code>) and the topics we want to send data to (e.g. <code>topic_pub</code>). Finally, the MQTT credentials from the <code>config.py</code> file are loaded. </p>"},{"location":"micropython/mqtt/#connection","title":"Connection","text":"<p>Now we can finally establish a connection to the MQTT broker. We use the <code>MQTTClient</code> class from the <code>upymqtt</code> library. We create an instance of the class and pass the client ID, the MQTT server credentials and the port number.</p> <pre><code># Create a client instance; enable SSL for encrypted transport (TLS).\nclient = MQTTClient(\n    client_id,\n    mqtt_server,\n    user=mqtt_user,\n    password=mqtt_pass,\n    port=mqtt_port,\n    ssl=True\n)\n</code></pre> <p>Before connecting, we define a callback function that will be called every time a message is received on a subscribed topic:</p> <pre><code># Register the callback so incoming packets trigger `sub_cb`.\ndef sub_cb(topic, msg):\n    print(f\"New Message: {topic}, {msg}\")\n\nclient.set_callback(sub_cb)\n</code></pre> <p>This function will print each received message to the console.</p> <p>Now we can connect to the broker and subscribe to our chosen topic:</p> <pre><code># Open a network connection to the MQTT broker.\nclient.connect()\nprint(f\"Connected to {mqtt_server} MQTT broker\")\n\n\n# Tell the broker which topic we want to listen to.\nclient.subscribe(topic_sub)\nprint(f\"Subscribed to {topic_sub} topic\")\n</code></pre> <p>At this point, your ESP32 is ready to send and receive MQTT messages.</p>"},{"location":"micropython/mqtt/#send-and-receive-data","title":"Send and Receive Data","text":"<p>In your <code>main.py</code>, you can now use the following commands to communicate via MQTT:</p> <p><pre><code># Sending the word 'hello'\u201d' on topic_pub.\nclient.publish(topic_pub, b'hello')\n\n# Receiving messages on topic_sub.\nclient.check_msg()\n</code></pre> The <code>check_msg()</code> function checks the broker for new messages. It processes one message per call, so if you expect continuous updates, it's best to place it inside a loop:</p> <pre><code>while True:\n    # Receive messages from the broker and process them.\n    # the callback (`sub_cb`) will be invoked automatically.\n    client.check_msg()\n</code></pre> Task: Receive and send messages <p>Now it's your turn to send and receive messages. Try the code above and see if you can receive and send messages. Therefore:</p> <ul> <li>Follow the steps above to connect to the broker (<code>boot.py</code> and <code>config.py</code>)</li> <li>Check for new messages in the subscribed topic (<code>settings</code>) in a loop and publish a message to the <code>kpi</code> topic once before the loop starts. </li> <li>Got to HiveMQ and click on 'Manage Cluster'. Switch to the 'Web Client' tab and connect with your credentials. On the right side click on 'Subscribe to all messages'. Now you should see the message you published with your ESP32. (Hint: since the publish function is called once before the loop starts, it is a good idea to soft-reset you ESP32 in order to send the message again) </li> <li>On the left side in the 'Web Client' tab click on 'Send Message'. Enter the <code>kpi</code> topic and publish a message with a payload of your choice. You should see the message in the console of your ESP32.  <pre><code>Connected to xyxyxy.s1.eu.hivemq.cloud MQTT broker\nSubscribed to b'settings' topic\nNew Message: b'settings', b'test'\n</code></pre></li> </ul> <p>If everything works, there is only one thing left to do: We want to visualize the data in a dashboard. </p>"},{"location":"micropython/mqtt/#dashboard","title":"Dashboard","text":"<p>The basic features of our system are now complete. We can read out sensors, control actuators and send and receive data via MQTT.  The final step is to visualize the data in a dashboard. For this we will use MQTT Tiles. MQTT Tiles is a free tool that allows you to create a dashboard to visualize your data without signing up.</p> <p>Once you visit the website you will find</p> <p></p>"},{"location":"micropython/mqtt/#connection-to-the-broker","title":"Connection to the Broker","text":"<p>On the right upper corner you can click on 'Connect' followed by . A new pop-up will appear where you need to enter you connection credentials: </p> Construct the Host URL <p>As host enter <code>wss://</code> followed by the URL of your broker, followed by <code>:8884/mqtt</code>.</p> <p>If everything is correct you should see a green 'online' status in the upper right corner of MQTT Tiles.</p>"},{"location":"micropython/mqtt/#create-a-dashboard","title":"Create a Dashboard","text":"<p>Now it's time to create a dashboard. Click on 'New Board' and enter a name for your dashboard. Leave everything else as it is and click on 'Save &amp; Open'.</p> <p>Now you can click on 'Add Widget' or the  on the right bottom corner. We want to create two widgets now: </p> <ul> <li> <p>Slider (topic: <code>settings</code>) </p> <p>A slider that publishes a value to the topic <code>settings</code>. The ESP32 is subscribed to this topic and will receive the value of the slider. If the ESP32 is running, you should see the value of the slider in the console.</p> <p></p> </li> <li> <p>Text (topic: <code>kpi</code>)</p> <p>A text widget that is subscribed to the topic <code>kpi</code>. Every time the ESP32 publishes a message to this topic, the text widget will display the message.</p> <p></p> </li> </ul> <p>The result should look like this:</p> Task: Create a Dashboard <p>It's time to create a dashboard yourself. Follow the steps above to create a dashboard and:</p> <ul> <li>Add a slider (topic: <code>settings</code>) and a text widget (topic: <code>kpi</code>).</li> <li>Try different payloads and see how the text widget displays the message.</li> <li>Try different widgets and see how they work.</li> </ul>"},{"location":"micropython/mqtt/#conclusion","title":"Conclusion","text":"<p>Now you know everything you need to know to build a smart plant watering system. We can measure the soil moisture, control the pump, send and receive data via MQTT and visualize the data in a dashboard. It's time to build the final system using the knowledge you have gained so far. </p> <pre><code>graph LR\n    subgraph TOP[ ]\n        direction BT\n        Cloud[Cloud Dashboard]:::active\n    end\n\n    subgraph MID[ ]\n        direction LR\n        Sensors:::active\n        ESP32[ESP32 Basics]:::active\n        Actuators:::active\n    end\n\n    Cloud &lt;--MQTT--&gt; ESP32\n    Sensors --&gt; ESP32\n    ESP32 --&gt; Actuators\n\n    click ESP32 \"../setup\" _self\n    click Sensors \"../sensors\" _self\n    click Actuators \"../actuator\" _self\n    click Cloud \"../mqtt\" _self\n\n    classDef active fill:#950f42,stroke:#333,stroke-width:1px;\n    class MID subgraphBox;\n    class TOP subgraphBox2;\n\n    classDef subgraphBox fill:#ff000000,stroke:#950f42,stroke-width:2px,color:#fff;\n    classDef subgraphBox2 fill:#ff000000,stroke:#950f42,stroke-width:0px,color:#fff;</code></pre>"},{"location":"micropython/sensors/","title":"Reading a Sensor","text":"<p>Now that we\u2019ve successfully written and run our first blink program, it\u2019s time to dive into the real heart of our project: reading data from a sensor. In this section, you\u2019ll learn how to wire a sensor to your ESP32, read its output with MicroPython, and process the results in code.</p>"},{"location":"micropython/sensors/#practical-guideline","title":"Practical Guideline","text":""},{"location":"micropython/sensors/#organizing-your-code","title":"Organizing Your Code","text":"<p>Whenever you sync your PyMakr project, all files in that folder are pushed to the ESP32. As we go along in this course,  you\u2019ll accumulate many scripts - blink tests, sensor readers, utilities, etc. - but you only want to deploy what\u2019s actually in use. To keep things clean:</p> <ol> <li> <p>Maintain a \"script library\" on your PC:</p> <pre><code>\ud83d\udcc1 script_collection/\n    \u251c\u2500\u2500 \ud83d\udcc4 blinking_led_main.py\n    \u251c\u2500\u2500 \ud83d\udcc4 sos_blinking_main.py\n    \u251c\u2500\u2500 \ud83d\udcc4 read_sensor_main.py\n    \u2514\u2500\u2500 \ud83d\udcc4 ...\n</code></pre> </li> <li> <p>Keep your active project separate:</p> <pre><code>\ud83d\udcc1 esp_project/\n    \u251c\u2500\u2500 \ud83d\udcc4 main.py\n    \u251c\u2500\u2500 \ud83d\udcc4 boot.py\n    \u2514\u2500\u2500 \ud83d\udcc4 pymakr.conf\n</code></pre> </li> <li> <p>Deploy only what you need: Once you have finished a task and want to start a new one, copy the scripts you\u2019re working on (in the <code>esp_project</code> folder) into the <code>script_collection</code> folder.</p> </li> </ol> <p>By giving each file a clear, descriptive name and only syncing the files in <code>esp_project/</code>, you\u2019ll prevent accidental pin overlaps, minimize memory usage and make it trivial to swap in new routines (just overwrite <code>main.py</code>)</p>"},{"location":"micropython/sensors/#pin-types","title":"Pin Types","text":"<p>When working with microcontrollers, knowing which pin to use and how is one of the most important things you need to know. Pins are the connections between the microcontroller and the outside world. Usually the first thing you do is to look at the pinout of the microcontroller and see which pins are available. A typical microcontroller has between 6 and 60 pins. Each microcontroller (sometimes different models of the 'same' controller) have different configurations. Pins often have multiple functions. This is called pin multiplexing.</p> <p></p> Pinout <p>Each microcontroller has pin names specific to its hardware or architecture. This means that controllers from different manufacturers may use different designations.  Common manufacturers of microcontrollers for IoT applications are Espressif, Arduino and Raspberry Pi. The advantage over noname controllers is that there are always data sheets (see 'ESP32_datasheet') with exact pin assignment.  So it's better to spend 10\u20ac on a good \u00b5C instead of buying one from an unknown manufacturer for 2\u20ac and then the pins don't match!</p> <p>There are different types of pins which can use to interface with the outside world. The most common ones are:</p> Pin Type Common Labels Purpose Common Use Cases Power VCC, VIN, 3.3V, 5V Provides power to the board/sensors Power for sensors/modules Ground GND (\u23da) Returns current to complete the circuit Essential for all connections GPIO GPIO, D0, G0, etc. General-purpose input/output Flexible pin for input/output Digital I/O D0, D1, D2... Reads/writes digital states Buttons, LEDs, simple sensors PWM ~ or PWM Provides variable control (analog output) Dim LEDs, control motors Analog Input A0, A1, ADC1 Reads varying voltages Temperature, light, humidity sensors Serial (UART) TX/RX Basic data communication GPS, Bluetooth modules, PC-\u00b5C connection I2C SDA, SCL Multi-device communication with 2 wires OLED displays, accelerometers SPI MOSI, MISO, SCK, SS High-speed communication SD cards, TFT displays <p>By following these practical organization guidelines so far and using the pinout diagram above, you\u2019ll wire up sensors and actuators correctly, write clear code, and keep your ESP32 project\u2019s filesystem neat and efficient. Happy tinkering!</p>"},{"location":"micropython/sensors/#reading-a-sensor_1","title":"Reading a Sensor","text":"<p>Now it's time to move forward with our project to keep our plant alive. The first step is to measure the soil moisture: is it too dry or too wet? </p>"},{"location":"micropython/sensors/#how-can-we-measure-soil-moisture","title":"How can we measure soil moisture?","text":""},{"location":"micropython/sensors/#theory","title":"Theory","text":"<p>There are two common types of sensors used to measure soil moisture: resistive and capacitive sensors. Both are generally referred to as hygrometers.</p> <ul> <li> <p>Resistive moisture sensors work by placing a hygroscopic (water-attracting) material between two conductive electrodes. This material is typically a non-conductive polymer that becomes increasingly conductive as it absorbs water. The change in conductivity alters the voltage between the electrodes, which can then be measured. These sensors offer a large surface area, making them effective for detecting small moisture variations\u2014even in already damp environments. However, their performance drops at very low moisture levels. A major drawback for our use case is durability. Since resistive sensors require direct contact with moisture in the soil, they are prone to corrosion. In particular, cheap resistive sensors can suffer from electrolysis, where the sensor material begins to degrade and potentially release toxic substances into the soil \u2014 harmful for plants and unsuitable for long-term use.</p> </li> <li> <p>Capacitive moisture sensors, on the other hand, rely on a capacitor-like structure whose electrical field is affected by the moisture content of the surrounding soil. As soil moisture increases, so does the soil's dielectric constant, which alters the sensor\u2019s capacitance. These sensors do not require direct water contact \u2014 hence the black protective coating \u2014 and are much more durable and maintenance-free, as corrosion is not an issue. Capacitive sensors are typically more accurate, reliable, and resistant to wear and temperature fluctuations. For these reasons, they are often used in professional applications like agriculture, despite being slightly more expensive.</p> </li> </ul>"},{"location":"micropython/sensors/#hardware","title":"Hardware","text":"<p>For our project we\u2019ll be using the HW-390 capacitive moisture sensor, a widely available and cost-effective sensor with a simple 3-wire interface. While different manufacturers may offer this model, its functionality and wiring remain consistent across versions.</p>"},{"location":"micropython/sensors/#wiring","title":"Wiring","text":"<p>The HW-390 sensor has three pins:</p> <ul> <li>VCC \u2013 Connect to 5V on the ESP32</li> <li>GND \u2013 Connect to GND</li> <li>AOUT \u2013 Connect to any analog-capable GPIO</li> </ul> <p>To find an analog-capable GPIO, we can take a closer look at the pinout diagram shown above. The ESP32 has many GPIO pins, but only some of them are capable of analog input. Look for pins labeled ADC - these are connected to an Analog-to-Digital Converter (ADC). Since the sensor outputs an analog signal and microcontrollers operate digitally, we need an ADC to convert the analog voltage into a digital value the ESP32 can process. In short, the ADC acts as a bridge between the analog world of sensors and the digital world of microcontrollers.</p> <p>For our project we will use GPIO32 as input pin for the sensor value. The wiring diagram is shown below. Connect all components as shown. </p> <p>Now we are all set up and we can start coding!</p>"},{"location":"micropython/sensors/#coding","title":"Coding","text":"<p>As with our first program, we only need to edit the <code>main.py</code> file. We begin by importing the required libraries:</p> <pre><code>from machine import ADC, Pin\nfrom time import sleep\n</code></pre> <p>This time, instead of importing only the <code>Pin</code> class, we're also using the <code>ADC</code> class from the <code>machine</code> module to enable analog-to-digital conversion. You can find the official MicroPython documentation for the ESP32 here.</p> <p>Next, we initialize the <code>ADC</code> class:</p> <p><pre><code># Set GPIO 32 as ADC pin\nadc_pin = Pin(32)\nadc = ADC(adc_pin, atten=ADC.ATTN_11DB)\n</code></pre> First, we choose pin 32 as the ADC pin and then initialize it with the ADC class. </p> <p>We can now read values from the sensor using the <code>read_u16()</code> method. The term <code>u16</code> stands for unsigned 16-bit integer, which means the returned value ranges from <code>0</code> to <code>65,535</code> (2<sup>16</sup> distinct levels).</p> <p>In the context of an ADC (Analog-to-Digital Converter), this value represents the voltage level read on the pin, scaled across the ADC's resolution. A 16-bit ADC can distinguish between <code>65,536</code> different voltage steps - the higher the resolution (number of bits), the more precisely it can measure small changes in voltage.</p> <pre><code>val16 = adc.read_u16()\nprint('ADC Value: ', val16)\n</code></pre> <p>If you run this code while the sensor is on your desk, you\u2019ll get a value roughly like this:</p> <pre><code>ADC Value:  48139\n</code></pre> Task: Calibrate the Sensor <p>Now it's your turn to code!</p> <p>Create a program that reads the sensor value and prints it to the console.</p> <ul> <li>Continuous Reading: Use a simple inifinity loop to read the sensor value and print it to the console. Include a delay of 1 second between readings.</li> <li> <p>Min/Max Values: Read the minimum and maximum value of the sensor. Therefore measure and note the value when the sensor is dry (leave it on the desk) and when it is wet (put it in a glass of water).</p> Sensor Damage <p>The sensor consists of a capacitive plate and some electronics. Do not immerse the electronics in the water! There is a line indicating the maximum water level on the sensor.  </p> </li> <li> <p>Mapping to Percentage: Map the sensor value to a percentage of soil moisture (0% = dry, 100% = wet). A Min-Max-Normalization can be used therefore. If you need a refresher on the calculation, take a look here. Careful: As you may have noticed, with a capacitive sensor, a low value means more moisture! Furthermore, ensure that the percentage is always between 0 and 100.</p> </li> <li>Function: Write a function that returns the moisture percentage and the raw sensor value. This function should be called in the loop.</li> <li>Moisture Warning System: Now combine your knowledge from before and implement a warning system that turns on a LED if the moisture is below 30%. Test your system with a glass of water. </li> </ul>          The system is working! Once the moisture is below 30%, the LED turns on."},{"location":"micropython/sensors/#conclusion","title":"Conclusion","text":"<p>Now we have a basic understanding of how to use the ADC Pins, read a sensor value and how to process it. In the next section we will use this knowledge to control a water pump.</p> <pre><code>graph LR\n    subgraph TOP[ ]\n        direction BT\n            Cloud[Cloud Dashboard]\n    end\n\n    subgraph MID[ ]\n        direction LR\n            Sensors:::active\n            ESP32[ESP32 Basics]:::active\n            Actuators\n    end\n\n    Cloud &lt;--MQTT--&gt; ESP32\n    Sensors --&gt; ESP32\n    ESP32 --&gt; Actuators\n\n    click ESP32 \"../setup\" _self\n    click Sensors \"../sensors\" _self\n    click Actuators \"../actuator\" _self\n    click Cloud \"../mqtt\" _self\n\n\n    %% Styling\n    classDef active fill:#950f42,stroke:#333,stroke-width:1px;\n    class MID subgraphBox;\n    class TOP subgraphBox2;\n\n    %% Subgraph styling workaround (pseudo-class)\n    classDef subgraphBox fill:#ff000000,stroke:#950f42,stroke-width:2px,color:#fff;\n    classDef subgraphBox2 fill:#ff000000,stroke:#950f42,stroke-width:0px,color:#fff;</code></pre>"},{"location":"micropython/setup/","title":"Getting Started","text":""},{"location":"micropython/setup/#what-do-we-want-to-do","title":"What Do We Want to Do?","text":"<p>In this course, we will explore key principles of the Internet of Things (IoT) by developing an automated plant monitoring and watering system. Our goal is to create a smart irrigation solution using a microcontroller as the project\u2019s core. In our system, a sensor will continuously measure soil humidity (and temperature) and send these readings via <code>MQTT</code> to a central server. There, the data can be processed and visualized using a browser-based dashboard, allowing you to monitor the plant\u2019s status. Furthermore, you will be able to remotely set parameters - such as the optimal humidity level - and these settings will be transmitted back to the microcontroller to trigger a water pump when necessary.</p> <p>And let\u2019s face it: not everyone is blessed with a natural green thumb. With this system, even if you have the gardening skills of a cactus \ud83c\udf35, your plants will still thrive!</p>"},{"location":"micropython/setup/#and-therefore-we-use","title":"And Therefore We Use...","text":"<p>While <code>Python</code>  is typically associated with data analysis and software development on PCs, it can also be used to program microcontrollers. In this course, we leverage Python\u2019s flexibility to develop firmware for embedded systems.</p> <p>We are now in the realm of embedded systems. Embedded systems are essentially 'computers'  integrated into technical systems that combine electronic and often mechanical components. Unlike a typical PC, a microcontroller usually runs either without an operating system or with a highly specialized one, and always relies on firmware. The firmware is generally structured into three main components:</p> <ul> <li>Bootloader: Loads the operating system and the application software.</li> <li>Operating System (if present): Manages multitasking, memory, and file systems.</li> <li>Application Software: This is the code you write - in our case, using MicroPython.</li> </ul>"},{"location":"micropython/setup/#software-micropython","title":"Software: MicroPython","text":"<p>Although the traditional approach for programming embedded systems is to use <code>C</code>, this method requires deep hardware knowledge and specialized expertise. Since our focus is on learning IoT concepts without getting lost in low-level programming, we will use MicroPython. MicroPython is a lean implementation of <code>Python 3</code>, written in <code>C</code> and optimized for microcontrollers. It compiles Python code to bytecode, which is then interpreted at runtime.</p> CircuitPython <p>For beginners, there\u2019s also CircuitPython - a variant designed with an even friendlier interface - but for this course, we\u2019ll stick with MicroPython.</p>"},{"location":"micropython/setup/#hardware-esp32","title":"Hardware: ESP32","text":"<p>The ESP32 microcontroller is an ideal choice for IoT projects. It comes in various versions and supports wireless communication via Wi-Fi and Bluetooth, as well as wired interfaces like SPI, SDIO, I2C, and UART. Thanks to its power efficiency, robustness, and affordability (basic models are available for around \u20ac10), the ESP32 is well suited for a wide range of applications - from simple prototypes to complex systems. While other popular microcontrollers include Arduino, STM32, and Raspberry Pi, the ESP32 strikes a great balance between performance and cost for our smart plant watering project.</p> Get Familiar with ESP32 <p>Open the corresponding ESP32 datasheet and answer the following questions:</p> <ul> <li>How many cores does the ESP32 have?</li> <li>How much flash memory does the ESP32 have?</li> <li>What kind of Bluetooth does the ESP32 support?</li> <li>What are the electrical characteristics of the ESP32 (voltage, current, etc.)?</li> </ul> <p>We will explore the functionalities of the ESP32 step by step in the following sections. </p>"},{"location":"micropython/setup/#setting-up-our-project","title":"Setting up our Project","text":"<p>Before we can start programming our ESP32, we need to prepare some tools and the hardware. </p>"},{"location":"micropython/setup/#firmware-upload","title":"Firmware Upload","text":"<p>To be able to write code to the microcontroller, the MicroPython firmware must be loaded onto the ESP32. For this, we will use Thonny IDE - a friendly and intuitive Integrated Development Environment that simplifies the process of programming MicroPython on the ESP32.</p> <p>Below are the step-by-step instructions:</p> <ol> <li> <p>Download Thonny IDE: </p> <ul> <li>Head over to thonny.org and download the IDE software appropriate for your operating system.</li> <li>Install thonny by following the instructions. </li> </ul> </li> <li> <p>Connect the ESP32: </p> <ul> <li>Plug your ESP32 into your computer using a USB cable.</li> <li>Note that the ESP32 will be detected as a COM port on Windows. To check, if the ESP32 is detected correctly, we look in the Device Manager under Ports. You should see something like this:  </li> </ul> CP210X Driver <p>If you cannot find the correct port, it might be because your PC or laptop does not have the necessary USB-to-UART driver installed. The chip responsible for the USB-to-UART conversion is usually a large, black, square component located next to the connector. If you shine a light on it, you should be able to read CP2102 on the second line, indicating that the chip is manufactured by Silicon Labs. Since the driver is specific to the chip\u2019s architecture, you can download the appropriate driver from Silicon Labs USB-to-UART Bridge VCP Drivers. </p> <p>For Windows users, select the CP210x VCP Windows file, unzip the downloaded .zip file, and run the installer for your system architecture (x64 or x86). Once the driver is installed, the correct port should appear in your device manager.</p> </li> <li> <p>Install or Update the Firmware in Thonny: </p> <ul> <li>Open Thonny IDE and go to Tools &gt; Options &gt; Interpreter. </li> <li>Select MicroPython (ESP32) from the interpreter options </li> <li>Select the correct port (see step before).</li> <li>Click the Install or update MicroPython (esptool) button. </li> </ul> <p> </p> <p>A new window will open. Make the following selections:</p> <ul> <li>Target port: USB to UART  </li> <li>MicroPython family: ESP32  </li> <li>Variant: Espressif ESP32 / WROOM  </li> <li>Version: 1.24.1 (this is the latest version at the time of writing)</li> </ul> <p> </p> <p>After clicking on Install the firmware will be flashed onto the ESP32. Sometimes the flashing process can lead to an error. Try holding the boot button on the ESP32 while clicking on Install. </p> Verify the Installation <ul> <li>In Thonny, you can select the Micropython (ESP32) from the bottom right corner.</li> <li>You should see a connection established in the shell at the bottom.  </li> <li>Type <code>help()</code> and press Enter to check that the controller is responding correctly.  </li> </ul> </li> </ol> <p>Happy coding and welcome to the world of MicroPython ! </p>"},{"location":"micropython/setup/#prepare-visual-studio-code","title":"Prepare Visual Studio Code","text":"<p>Although you can program directly in Thonny, we've already explored Visual Studio Code - which offers advantages such as enhanced code completion and seamless GitHub integration. These features make VS Code a powerful option, especially for larger projects or collaborative work. If you need a refresher on how to set up VS Code, you can check out the IDE Setup section.</p> Install Node.js <p>Before continuing, ensure you have <code>Node.js</code> installed on your computer. <code>Node.js</code> is a cross-platform, open-source JavaScript runtime that lets you execute JavaScript code outside of a web browser.</p> <p>If you don't have it installed yet, please download and install <code>Node.js</code>. During installation, it's highly recommended to tick the option to install any required additional programs. Once you proceed, a terminal window may open where you might need to confirm a few prompts. The installation process might take a few minutes; the terminal will close automatically when the installation is complete. Afterward, you should see <code>Node.js</code> listed among your installed programs.</p> <p>Once <code>Node.js</code> is successfully installed, you can move on to the next steps.  </p> <p>To run our code on the ESP32, we'll use the <code>PyMakr</code> extension in Visual Studio Code. This extension allows you to easily upload and execute your MicroPython scripts directly on the ESP32. We already covered how to install extensions.</p> <p>After adding the extension, you should see <code>PyMakr</code> as a new button on the left-hand side of the VS Code window.</p> <p>Now we are all set up and can start programming! </p>"},{"location":"micropython/setup/#blink-the-hello-world-of-embedded-systems","title":"Blink  | The Hello World of Embedded Systems","text":"<p>Now it's time to start our first project  and get familiar with PyMakr, hardware setup and the basics of MicroPython.</p> <p>In this first mini-project, we'll make an external LED blink using the ESP32. Blinking an LED is a classic \"Hello World\" exercise in microcontroller programming - it demonstrates how to set up an output pin and control it with code. </p>"},{"location":"micropython/setup/#hardware-setup","title":"Hardware Setup","text":"<p>Before we start coding, we need to setup the hardware. The core element of our project is - as already mentioned - the ESP32 microcontroller. The ESP32 it self is the large silver component on the breakout board shown below. The breakout board helps us to connect the ESP32 to the real world and also deals with other topics like power supply and programming the microcontroller. From now on, whenever we refer to the ESP32, we specifically mean the ESP32 Breakout Board.</p> <p>For the Blink project, we need to connect the ESP32 to an LED via a resistor. The resistor is necessary to limit the current flowing through the LED, which can damage the LED if too much current flows through it. In the below image you can see the wiring scheme. Connect all components as shown. </p> LED Pinout <p>The LED has two legs: the cathode (shorter leg) and the anode (longer leg). The cathode is connected to <code>GND</code>, and the anode to the supply voltage (here to a <code>1 k\u03a9</code> resistor, and from the resistor to <code>G2</code> on the ESP32 which will be controlled by the code.)</p> ESP32 Pinout <p>Until now, we have not talked about the different Pins on the ESP32. The ESP32 has many Pins, which can be used to control the microcontroller. What they do and what different kinds of Pins exist, will be covered in the next sections. </p> <p>For now, just remember that we can use the Pin <code>G2</code> as an output pin to control the LED.</p>"},{"location":"micropython/setup/#pymakr-project-setup","title":"PyMakr Project Setup","text":"<p>Below is a quick guide on how to create a new PyMakr project, connect your ESP32, and get started with MicroPython in Visual Studio Code.</p>"},{"location":"micropython/setup/#1-create-a-new-project","title":"1. Create a New Project","text":"<ul> <li>Open Visual Studio Code (VS Code).</li> <li> <p>Select \"PyMakr\" from the left sidebar.</p> <p> </p> <p>If you don\u2019t see 'PyMakr' in the sidebar, make sure the PyMakr extension is installed.</p> </li> <li> <p>Click on \"Create Project\"</p> </li> <li>Select the folder where you want to store your project (e.g., <code>Blink_Project</code>).</li> <li> <p>A popup will appear asking you to name your project. </p> <p> </p> <p>The default name is often the folder name. For example, <code>Blink_Project</code>. Press Enter to confirm.</p> </li> <li> <p>A second popup appears: \"Please select a template for your project\". Leave this blank and press Enter.</p> </li> <li> <p>If VS Code asks, \"Do you trust the authors of the files in this folder?\", confirm with yes.</p> <p> </p> </li> <li> <p>A new project is now created! </p> <p> </p> <p>     Your project will be shown in the PyMakr sidebar (left) and the files in the VS Code Explorer (right). </p> </li> </ul>"},{"location":"micropython/setup/#2-connect-the-esp32","title":"2. Connect the ESP32","text":"<ul> <li>Plug the ESP32 into your computer via USB.  </li> <li> <p>In the PyMakr sidebar, you should see a \"Devices\" section showing your ESP32.</p> <p> </p> </li> </ul>"},{"location":"micropython/setup/#3-add-the-device","title":"3. Add the Device","text":"<ul> <li>Under \"PyMakr Projects\", select your project (e.g., \"empty project\") and click \"ADD DEVICE\" </li> <li> <p>Choose the device (your ESP32) from the list.</p> <p> </p> <p>Confirm by clicking \"OK.\"</p> </li> <li> <p>The device should now appear under your project.  </p> </li> </ul>"},{"location":"micropython/setup/#4-connect-to-device","title":"4. Connect to Device","text":"<ul> <li> <p>Hover your mouse over your device in the \"PyMakr Projects\" section to see various icons:</p> <p> </p> <ul> <li>Create terminal  : Opens a new terminal window for executing commands and interacting with the system or device.  </li> <li>Sync project to device  : Uploads the current project files from the local system to the connected device.  </li> <li>Download project from device  : Retrieves project files from the device and saves them to the local system.  </li> <li>Open device in file explorer  : Opens the file explorer to browse and manage files stored on the connected device.  </li> <li>Connect device  / Disconnect device: Establishes or terminates the connection between the computer and the external device.</li> </ul> </li> <li> <p>Click on \"Connect\" </p> </li> <li>Click on \"Create Terminal\" </li> <li> <p>In the newly opened terminal, you should see the output of the ESP32:</p> <pre><code>MicroPython v1.24.1 on 2024-11-29; Generic ESP32 module with ESP32\nType \"help()\" for more information.\n&gt;&gt;&gt;\n</code></pre> </li> </ul> <p>This indicates that your ESP32 is successfully connected and ready to receive MicroPython commands. </p> <p>You can now upload scripts and run code directly on the device.</p>"},{"location":"micropython/setup/#5-default-file-structure","title":"5. Default File Structure","text":"<p>When you create a new project, VS Code will automatically create a folder with a default file structure. In the file explorer, you should see the following files:</p> <ul> <li><code>main.py</code> - This is the main file that will contain our code.</li> <li><code>boot.py</code> - This file is automatically executed when the ESP32 starts (executed once before the main.py file is executed).</li> <li><code>pymakr.conf</code> - This file contains the configuration for the PyMakr extension.</li> </ul> <p>All programmes and libraries that we need are edited and loaded in this folder. If you click on Sync project to device in the PyMakr tab, the entire project folder will be loaded onto the microcontroller. </p> Important Files <p>Please do not rename <code>boot.py</code> and <code>main.py</code>. If you do not need <code>boot.py</code>, you can delete it or leave it empty until you use it again, but <code>main.py</code> should always be there!</p> Configuration File <p>The <code>pymakr.conf</code> file typically contains only one entry, such as <code>\"name\": \"Empty Project\"</code>. If you haven\u2019t created an empty project, it may have additional entries, but these are not crucial. This file is a configuration file, historically used for setting up Pymakr. While it was more significant in older versions, modern VSCode automatically generates JSON-based configuration files in the background, where these settings are stored.</p> <p>However, knowing where to find these settings can still be useful. You can access them in VSCode by navigating to: File \u2192 Preferences \u2192 Settings (or press Ctrl+,) \u2192 Extensions \u2192 Pymakr.  </p> <p>Under Devices, you'll find the option Edit in settings.json, where various connection settings are defined. At the top, you\u2019ll see the path to your Python interpreter, followed by default connection settings and their properties.  </p> <p>If you want your laptop to automatically connect to the microcontroller, you could add an entry specifying the port and set options like <code>\"autoConnect\": \"always\"</code>. Further down, there are notification settings that inform you if something goes wrong. The file may also list device manufacturers, such as Silicon Labs, but modifying these entries is usually unnecessary.  </p> <p>Understanding these settings can be helpful if you encounter issues and seek troubleshooting advice from ChatGPT or other sources. While the <code>pymakr.conf</code> file is rarely used today, settings.json and the built-in VSCode configurations have taken over its role.</p>"},{"location":"micropython/setup/#start-coding","title":"Start Coding","text":"<p>Now that we have our project setup, we can start coding. For our first project, we will only need to work in the <code>main.py</code> file. We will start by importing the necessary modules (we do not need to install them with pip or something else. There are already preinstalled).</p> <pre><code>from machine import Pin\nfrom time import sleep\n</code></pre> MicroPython Libraries <p>Unlike standard Python, where you can use virtually any library available on your computer, MicroPython relies on a limited set of libraries specifically designed for microcontrollers.</p> <p>When working in VSCode, you may notice that some libraries used in your program are underlined or marked as missing. This is because they don\u2019t exist on your computer\u2014they are only available on the microcontroller. Do not attempt to install these libraries manually! They are built into the MicroPython firmware and are intended to run only on the device.</p> <p>This limitation can make debugging more challenging, as errors often don\u2019t appear until the code is actually running on the microcontroller. Make sure to check the console output, which can provide helpful hints. Some common issues are already discussed in this guide.</p> <p>This is one of the key differences between MicroPython and languages like C: In C, code must be compiled before being uploaded to the microcontroller, and most errors are caught during compilation. In contrast, Python and MicroPython upload the raw script, which is only interpreted and executed after it's on the device\u2014so runtime errors may only appear during execution.</p> <p>Keep this in mind when troubleshooting your code.</p> <p>The <code>machine</code> library in MicroPython provides low-level access to hardware components, such as GPIO pins (General-Purpose Input/Output), ADCs, I2C, SPI, and other peripherals, allowing direct interaction with microcontrollers. The <code>Pin</code> class from the <code>machine</code> module is used to control the GPIO pins on the microcontroller.</p> GPIO Insights <p>A GPIO (General Purpose Input/Output) is a digital pin on a microcontroller or processor that can be freely configured as an input or output through programming. By default, GPIOs are unassigned and can be controlled in a binary manner (<code>HIGH</code> or <code>LOW</code>).  </p> <p>GPIOs typically operate at 3.3V and can supply 2-16 mA of current, making them suitable for driving components like LEDs. For example, if a GPIO pin is set to HIGH, the LED turns on; when set to LOW, the LED turns off. GPIOs are fundamental for interfacing with sensors, actuators, and other peripherals in embedded systems. </p> <p>We will cover the different types of Pins in the sensors chapter.</p> <p>The second line of code imports the <code>sleep</code> function from the MicroPython (or standard Python) <code>time</code> module, which is used to pause the execution of a program for a specified number of seconds.</p> <p>Next, we need to define the interface to the real world. In our case, we want to control the LED. Therefore, we need an output pin on the ESP32 which can be set to high (led on) or low (led off). Each pin on the ESP32 breakout board is assigned a number.  As we have connected it before, for this project, we will use the <code>G2</code> pin as an output pin (<code>Pin.OUT</code>) and assign it to the variable <code>led</code>.</p> <pre><code># Set G2 as output\nled = Pin(2, Pin.OUT)\n</code></pre> <p>Now we can start coding the logic to blink the LED. We can set the <code>led</code> to high or low by calling the <code>value()</code> method of the <code>led</code> object.</p> <pre><code>led.value(1) # set the led to high\n</code></pre> <p>Afterwards, we need to pause the program for a second to see the LED light up</p> <pre><code>sleep(1)\n</code></pre> <p>and then set the <code>led</code> to low again.</p> <pre><code>led.value(0) # set the led to low\nsleep(1)\n</code></pre> <p>In order to make the LED blink, we need to repeat this process in an infinite loop (<code>while True:</code>).</p> <p>The complete code can be distilled to: </p> main.py<pre><code>from machine import Pin\nfrom time import sleep\n\n# Set G2 as Output Pin\nled = Pin(2, Pin.OUT)\n\n# Blink the LED\nwhile True:\n    led.value(1)     # Turn on the LED\n    print('LED on')  # Print to Terminal\n    sleep(1)         # Delay for 1 second\n    led.value(0)     # Turn off the LED\n    print('LED off') # Print to Terminal\n    sleep(1)         # Delay for 1 second\n</code></pre> <p>We added some print statements to the terminal to make it easier to see what is happening. In the next section, we will use the terminal to debug our code.</p>"},{"location":"micropython/setup/#upload-and-run-the-code","title":"Upload and Run the Code","text":"<p>Now we are ready to upload and run the code on our microcontroller. Once the <code>main.py</code> file is saved, we can upload the code to the microcontroller by clicking on the Sync project to device  button by hovering over the device in our PyMakr project. The files will be uploaded to the ESP32. </p> <p>After the upload, the program will not start automatically. We will need to reset the microcontroller by pressing the RST button on the ESP32 breakout or performing a soft reset.</p> Control Commands <p>In the terminal, you can use the following control commands to interact with the microcontroller:</p> <ul> <li>Ctrl+C: interrupt the running program. This can be helpfull, if something is not working as expected. Additionally, it is also necessary to stop the program before you can upload new code.</li> <li>Ctrl+D: soft reset. It wipes the program in the memory and reruns the <code>boot.py</code> and <code>main.py</code> file.</li> </ul> Auto-Start <p>If you unplug the microcontroller and plug it back in, it will automatically restart and run the <code>boot.py</code> and <code>main.py</code> file. So, once the coding is done, you can unplug it from the computer and use a power source to power it. In this case you are completely independend from your computer and the ESP is just using your code. </p> <p>Now you should see, the LED blink on and off at one-second intervals.  Open the terminal (click Create Terminal at the device in your PyMakr project) and check for the correct output. You should see the print statements <code>LED on</code> and <code>LED off</code> to verify that everything is working.</p> Troubleshooting <p>If you have issues with the LED not blinking while the terminal output is correct, make sure you have the correct GPIO pin referenced, and verify that your wiring and resistor are connected properly.</p> <p>Congratulations - you've completed your first hardware test with MicroPython!</p> Task: SOS blinking <p>Now it's your turn to code!</p> <p>Create a program that makes the LED blink in the <code>SOS</code> morse code sequence. Therefore use the official Morse Code Timing. As base unit use 0.2 seconds.</p> <ul> <li>Basic: Use a simple loop to blink the LED in the SOS sequence.</li> <li>Pro (optional): Use loops and functions to make the code more readable and modular.</li> </ul> <p> (Source: Imgflip Meme Generator)  </p> <pre><code>graph LR\n    subgraph TOP[ ]\n        direction BT\n            Cloud[Cloud Dashboard]\n    end\n\n    subgraph MID[ ]\n        direction LR\n            Sensors\n            ESP32[ESP32 Basics]:::active\n            Actuators\n    end\n\n    Cloud &lt;--MQTT--&gt; ESP32\n    Sensors --&gt; ESP32\n    ESP32 --&gt; Actuators\n\n    click ESP32 \"../setup\" _self\n    click Sensors \"../sensors\" _self\n    click Actuators \"../actuator\" _self\n    click Cloud \"../mqtt\" _self\n\n\n    %% Styling\n    classDef active fill:#950f42,stroke:#333,stroke-width:1px;\n    class MID subgraphBox;\n    class TOP subgraphBox2;\n\n    %% Subgraph styling workaround (pseudo-class)\n    classDef subgraphBox fill:#ff000000,stroke:#950f42,stroke-width:2px,color:#fff;\n    classDef subgraphBox2 fill:#ff000000,stroke:#950f42,stroke-width:0px,color:#fff;</code></pre>"},{"location":"micropython/system/","title":"Plant Watering System \ud83c\udf31","text":"<p>Now that all components are working - the moisture sensor, the pump control, and the MQTT communication and a basic dashboard - it's time to bring everything together into a fully automated plant watering system.</p>"},{"location":"micropython/system/#final-system","title":"Final System","text":"Task: Plant Watering System <p>It's time to all your knowledge together and build a plant watering system with the following features:</p> <ul> <li>Measure the soil moisture and show it in the dashboard with a gauge.</li> <li>The threshold for the pump is controlled by a slider in the dashboard.</li> <li>Automatic mode: Once the moisture is below the threshold, the pump is turned on for 5 seconds. after that wait for 10 seconds to measure again.</li> <li>Manual mode: There is a possibility to manually trigger the pump for 5 seconds.</li> </ul> <p>So far we have build everything we asked for. But you can do more if you want to. The following section includes some optional topics to think about and to extend your system to become a more sophisticated plant watering system.</p>"},{"location":"micropython/system/#optional-watering-strategy","title":"Optional: Watering Strategy","text":"<p>Now that your system is able to water plants, the key question becomes: how much water is needed, and what are good threshold values for different types of plants and environments? Here are some ideas to optimize your system.</p>"},{"location":"micropython/system/#recommended-moisture-thresholds-by-plant-type","title":"Recommended Moisture Thresholds by Plant Type","text":"Plant Type Moisture Threshold Notes Succulents &amp; Cacti 10\u201330% Let soil dry completely between waterings Medium-water Plants 30\u201350% e.g. pothos, spider plants; let top soil dry Tropical Plants 50\u201370% e.g. ferns, peace lilies; maintain consistent moisture, avoid sogginess Herbs &amp; Edibles 40\u201360% Prefer moist, well-drained soil Flowering Plants 40\u201370% e.g. violets, begonias; avoid extreme wet/dry cycles"},{"location":"micropython/system/#adjusting-thresholds-based-on-environment","title":"Adjusting Thresholds Based on Environment","text":"<ul> <li>Light: In bright, sunny spots, increase moisture thresholds by 5\u201310% to compensate for faster drying.</li> <li>Humidity: In high-humidity rooms, decrease thresholds by 5\u201310% to avoid overwatering.</li> <li> <p>Soil Type:</p> <ul> <li>Sandy soil: drains quickly \u2192 use lower thresholds</li> <li>Clay soil: retains water \u2192 use higher thresholds</li> </ul> </li> </ul>"},{"location":"micropython/system/#how-much-water-to-dispense","title":"How Much Water to Dispense?","text":"<p>Here are some recommentations for the amount of water to dispense based on AI:</p> Plant Size Recommended Water Volume Small pots (&lt;15\u202fcm) 100\u2013200\u202fml Medium pots (15\u201330\u202fcm) 250\u2013500\u202fml Large pots (&gt;30\u202fcm) 500\u20131000\u202fml Tip <p>Note: Our pump delivers approximately 30\u202fml per second. To determine how long to run the pump, adjust the <code>pump_on_time</code> based on your desired volume.</p>"},{"location":"python/","title":"Home","text":"<p>This course will teach you the basics of the   Python programming language.</p>      Creation of a 3D surface plot of the Lattenspitze. \ud83c\udfd4\ufe0f     That's the power of Python - ease of use paired with a wide range of      functionalities stemming from a large developer community! \ud83e\uddbe"},{"location":"python/#why-python","title":"Why  Python?","text":"<ul> <li> <p> Ease of use</p> <p><code>Python</code> with its syntax is easy to learn and yet very powerful.</p> </li> <li> <p> Flexible</p> <p><code>Python</code> is a versatile language that can be used for data analysis,  automation, artificial intelligence, and many more applications.</p> </li> </ul> <p>The below section should give you an impression of what you can do with  <code>Python</code>. This is not an extensive list by all means. It might sound  trashy, but if you can imagine something, you can most likely build it in  <code>Python</code>.</p> Info <p>Don't worry about the code snippets too much, after finishing the  course you'll have a better understanding and will be able to run  and modify code yourself. For now, the following snippets illustrate the capabilities of the language and what complex things you can achieve with little code. There is no need to execute it - Just take a look!</p>"},{"location":"python/#examples","title":"Examples","text":"Just the beginning... <p>All of the following examples are from one of the courses featured on our website. If you stick around and explore subsequent <code>Python</code> courses you  will be able to easily implement all examples yourself! </p>"},{"location":"python/#machine-learning","title":"Machine Learning","text":"<p>With <code>Python</code> you can easily train your own machine learning models. In this  example, with just a few lines of code, one such model (a decision tree) is  fit and visualized<sup>1</sup>.</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\n\n# load data\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\n\n# fit a decision tree\ntree = DecisionTreeRegressor(\n    random_state=784, max_depth=2, min_samples_leaf=15\n)\ntree.fit(X, y)\n\n# visualize the tree\nplot_tree(tree, filled=True, feature_names=X.columns, proportion=True)\nplt.show()\n</code></pre> A decision tree visualized."},{"location":"python/#computer-visionai","title":"Computer Vision/AI","text":"<p>Or how about state-of-the-art computer vision with YOLO<sup>2</sup>?</p> <pre><code>from ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run inference on the source\nresults = model(\"https://ultralytics.com/images/bus.jpg\")\nresults[0].show()\n</code></pre> Object detection with YOLO."},{"location":"python/#automation","title":"Automation","text":"<p>But it's not just machine learning and AI, you can also automate mundane tasks. This code snippet fetches some data (from an online service) and writes an  Excel file<sup>3</sup>.</p> <pre><code>import pandas as pd\nimport requests\n\nurl = \"https://api.coincap.io/v2/assets/pepe-cash/history?interval=d1\"\nresponse = requests.get(url)\n\ndata = pd.DataFrame(response.json()[\"data\"])\ndata.to_excel(\"price-history.xlsx\", index=False)\n</code></pre>"},{"location":"python/#visualizations","title":"Visualizations","text":"<p>You can create stunning and interactive visualizations<sup>4</sup>. Let's visualize  the above written Excel file.</p> <pre><code>import pandas as pd\nimport plotly.express as px\n\ndata = pd.read_excel(\"price-history.xlsx\")\nfig = px.area(\n    data_frame=data,\n    x=\"date\",\n    y=\"priceUsd\",\n    title=\"Price History in USD\",\n    color_discrete_sequence=[\"#009485\"],\n)\nfig.show()\n</code></pre>"},{"location":"python/#web-development","title":"Web Development","text":"<p>You can create websites, just like this one. In fact, all the  heavy lifting of this site is done by <code>Python</code> and tools developed by its  community.</p> <p>The most important package used to build this site was  Material for MkDocs, a widely used and customizable static site generator. </p>"},{"location":"python/#getting-started","title":"Getting Started...","text":"<p>In the next sections, we will install <code>Python</code> including the code editor  <code>Visual Studio Code</code>.</p> Tip <p>Both Python and Visual Studio Code are already pre-installed on PCs in the MCI computer rooms. If you are working with your own computer,  please proceed to the next page.</p> <ol> <li> <p>Scikit-learn is a Python package  for machine learning.\u00a0\u21a9</p> </li> <li> <p>YOLO is an advanced real-time object detection model known for its speed and accuracy, enabling efficient identification and localization of objects within images and videos.\u00a0\u21a9</p> </li> <li> <p>requests is a Python package to interact with APIs.\u00a0\u21a9</p> </li> <li> <p>Plotly is a Python graphing package that  lets you create interactive, publication-quality graphs.\u00a0\u21a9</p> </li> </ol>"},{"location":"python/comparisons_and_logic/","title":"Comparisons &amp; Logical Operators","text":""},{"location":"python/comparisons_and_logic/#comparisons","title":"Comparisons","text":"<p>Now, that we have covered all basic <code>Python</code> types, we can start comparing them. As the name suggests, comparisons are used to compare two values. The result  of a comparison is a boolean value.</p>"},{"location":"python/comparisons_and_logic/#equality","title":"Equality","text":"<pre><code>print(\"Abc\" == \"abc\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>False\n</code></pre> <p>We can compare any type with each other. In the above case, the comparison  checks if both strings are equal, using the <code>==</code> operator. The result is  <code>False</code>, because the case of the strings do not match.</p> <p>Let's check if two integers are equal:</p> <pre><code>print(1 == 1)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre>"},{"location":"python/comparisons_and_logic/#inequality","title":"Inequality","text":"<p>We can also check if two values are not equal with the <code>!=</code> operator:</p> <pre><code>user_name = \"Eric\"\nprint(user_name != \"admin\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre> <pre><code>print(2 != 2.1)\nprint(2 != 2)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\nFalse\n</code></pre>"},{"location":"python/comparisons_and_logic/#numerical-comparisons","title":"Numerical comparisons","text":"<p>... are done with:</p> Operator Description <code>&lt;</code> less than <code>&gt;</code> greater than <code>&lt;=</code> less than or equal <code>&gt;=</code> greater than or equal <pre><code>print(1 &lt; 2)\nprint(1 &gt; 2)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\nFalse\n</code></pre> <pre><code>print(10.2 &lt;= 10.2)\nprint(9.99 &gt;= 10)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\nFalse\n</code></pre>"},{"location":"python/comparisons_and_logic/#logical-operators","title":"Logical Operators","text":"<p>You may want to check multiple conditions at the same time. For example, sometimes you might need two conditions to evaluate to <code>True</code> in  order to perform an action. Hence, logical operators are introduced.  There are three logical operators:</p> Operator Meaning Example Result <code>and</code> Returns <code>True</code> if both statements are <code>True</code> <code>True and True</code> <code>True</code> <code>or</code> Returns <code>True</code> if one of the statements is <code>True</code> <code>True or False</code> <code>True</code> <code>not</code> Reverses a result <code>not True</code> <code>False</code>"},{"location":"python/comparisons_and_logic/#and","title":"<code>and</code>","text":"<pre><code>age = 20\nprint(age &gt;= 18 and age &lt;= 25) # True and True -&gt; True\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre>"},{"location":"python/comparisons_and_logic/#or","title":"<code>or</code>","text":"<pre><code>age = 20\nprint(age &gt;= 50 or age &lt;= 25) # False or True -&gt; True\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre>"},{"location":"python/comparisons_and_logic/#not","title":"<code>not</code>","text":"<pre><code>age = 20\nprint(not(age &gt;= 18)) # not(True) -&gt; False\n</code></pre> &gt;&gt;&gt; Output<pre><code>False\n</code></pre> Evaluate password security requirements: Part 1 <p>You are given two variables that describe properties of a password:</p> <ul> <li><code>password_length</code> - represents how many characters are in the password     (<code>int</code>)</li> <li><code>has_special_characters</code> - represents whether the password contains      special characters (<code>True</code>/<code>False</code>)</li> </ul> <p>Variables to use:</p> <pre><code>password_length = 18\nhas_special_characters = False\n</code></pre> <p>Task:</p> <p>Write code that checks if this password is secure based on these requirements:</p> <ol> <li>The password must be longer than 10 characters</li> <li>The password must contain special characters</li> </ol> <p>Use comparisons and logical operators to create a single expression  that evaluates whether BOTH requirements are met.</p> Evaluate password security requirements: Part 2 <p>To increase security, a third variable is introduced alongside the  previous password properties:</p> <p><code>already_used</code> - represents whether this password has been used before     (<code>True</code>/<code>False</code>)</p> <p>Variables to use:</p> <pre><code>password_length = 18\nhas_special_characters = True\nalready_used = False\n</code></pre> <p>Task:</p> <p>Write code that checks if this password is secure based on these three requirements:</p> <ol> <li>The password must be longer than 10 characters</li> <li>The password must contain special characters</li> <li>The password must not have been used before</li> </ol> <p>Build on your previous solution and evaluate whether all THREE  requirements are met.</p>"},{"location":"python/comparisons_and_logic/#recap","title":"Recap","text":"<p>We have covered the basic comparison and logical operators in <code>Python</code> .</p> <ul> <li> <p>Comparisons</p> <ul> <li><code>==</code> for equality</li> <li><code>!=</code> for inequality</li> <li><code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code> for numerical comparisons</li> </ul> </li> <li> <p>Logical operators</p> <ul> <li><code>and</code></li> <li><code>or</code></li> <li><code>not</code></li> </ul> </li> </ul>"},{"location":"python/functions/","title":"Functions","text":"<p>In this section, you\u2019ll learn to write functions, which are named blocks of code that are designed to do one specific task. If you need to perform that task multiple times throughout your program, you don\u2019t need to type all the code for the same task again and again; you just call the function dedicated to handling that task. By defining functions, your programs will get easier to write, read, test, and fix.</p>"},{"location":"python/functions/#defining-a-function","title":"Defining a function","text":"<p>Here\u2019s a simple function named <code>greet_user()</code> that prints a greeting:</p> <pre><code>def greet_user():\n    print(\"Hello!\")\n</code></pre> <p>This example shows the simplest structure of a function. With the keyword <code>def</code> we define a function, followed by the name of our function.  Within the parentheses we can (optionally) specify any information the function needs to do its job. This information is defined in the form of parameters (more on that later).</p> <p>Any indented lines that follow <code>def greet_user():</code> make up the body of the function. The line <code>print(\"Hello!\")</code> is the only line of actual code in the body of this function, so <code>greet_user()</code> has just one job: <code>print(\"Hello!\")</code>.</p> <p>When you want to use a function, you have to call it. To do so, simply write the function name, followed by any required information in parentheses.</p> <pre><code># call the function\ngreet_user()\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello!\n</code></pre>"},{"location":"python/functions/#detour-docstrings","title":"Detour: docstrings","text":"<p>As previously discussed, it is always good practice to add comments to your code in order to improve readability. That applies to functions as well. In  case of functions, one can add a docstring, which is essentially a longer  comment that describes the function. A docstring is written in triple quotes <code>\"\"\"...\"\"\"</code> and is placed directly after the function definition.</p> <pre><code>def greet_user():\n    \"\"\"Display a simple greeting.\"\"\"\n    print(\"Hello!\")\n</code></pre> <p>Now, you can display the docstring by calling the built-in <code>help()</code>  function with the function name as an argument:</p> <pre><code>help(greet_user)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Help on function greet_user in module __main__:\ngreet_user()\n    Display a simple greeting.\n</code></pre> <p>Docstrings facilitate the proper documentation of your code. Most of all, they will help you in the long run to remember what your code does.</p>"},{"location":"python/functions/#parameters","title":"Parameters","text":"<p>After some modification, the function <code>greet_user()</code> should not only tell the user \"Hello!\" but also greet them by name. Therefore, we have to  define a parameter called <code>user_name</code>. Now, each time you call the function,  you need to pass a <code>user_name</code> such as <code>\"admin\"</code> to the function.</p> <pre><code>def greet_user(user_name):\n    \"\"\"\n    Display a simple greeting.\n    Pass a string with the user's name.\n    \"\"\"\n    print(f\"Hello, {user_name}!\")\n\ngreet_user(\"admin\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello, admin!\n</code></pre> Info <p>As you can see in the example above, a docstring can span multiple lines!</p> <p>Up until now, the functions had no parameters at all or just a single  parameter. However, you can define as many parameters as you like, seperated  by a comma (<code>,</code>):</p> <pre><code>def greet(first_name, last_name):\n    print(f\"Hello, {first_name} {last_name}!\")\n</code></pre> Break-even point <p>Remember the task to calculate the break-even point? Now, you'll wrap  following formula within a function:</p> \\[ \\text{BEP (units)} = \\frac{\\text{Fixed Costs}}{\\text{Price per Unit} - \\text{Variable Cost per Unit}} \\] <p>Write a function called <code>calculate_bep()</code> that takes the following parameters:</p> <ul> <li><code>fixed_costs</code></li> <li><code>price_per_unit</code></li> <li><code>variable_cost_per_unit</code></li> </ul> <p>Print the calculation result (break-even point) within the function.  Call the function with following arguments:</p> <pre><code>fixed_costs = 30000\nprice_per_unit = 75\nvariable_cost_per_unit = 45\n</code></pre>"},{"location":"python/functions/#terminology","title":"Terminology","text":"<p>parameter vs. argument</p> <p>A parameter is the variable inside the parentheses of the function definition - <code>def greet_user(user_name):</code>. Here <code>user_name</code> is the parameter. When you call the function with, for example <code>greet_user(\"admin\")</code>, the  value <code>\"admin\"</code> is referred to as an argument. You can think of the parameter  as a placeholder and the argument as the actual value.</p>"},{"location":"python/functions/#positional-arguments","title":"Positional arguments","text":"<p>When you call a function, <code>Python</code> must match each argument in the function  call with a parameter in the function definition. The simplest way to do this  is based on the order of the arguments provided which is referred to as positional arguments.</p> <pre><code>def add_numbers(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    print(a + b)\n\nadd_numbers(3, 5)\n</code></pre> &gt;&gt;&gt; Output<pre><code>8\n</code></pre>"},{"location":"python/functions/#order-matters","title":"Order matters!","text":"<p>You can get unexpected results, if you mix up the order of the arguments in a function call when using positional arguments:</p> <pre><code>def perform_calculation(a, b):\n    \"\"\"Calculate something.\"\"\"\n    print(a + b**b)\n\na = 12\nb = 5\n\n# correct order\nperform_calculation(a, b)\n# incorrect order (produces a different result)\nperform_calculation(b, a)\n</code></pre> &gt;&gt;&gt; Output<pre><code>3137\n8916100448261\n</code></pre> <p>Next up, we'll introduce keyword arguments to avoid such mistakes.</p>"},{"location":"python/functions/#keyword-arguments","title":"Keyword arguments","text":"<p>A keyword argument is a name-value pair that you pass to a function. You directly associate the name and the value within the argument, so when you pass the argument to the function, there\u2019s no confusion.</p> <pre><code>perform_calculation(a=12, b=5)\n\n#  you can switch the order of the named keyword arguments\nperform_calculation(b=5, a=12)\n</code></pre> <p>No matter the order, the result is the same!</p> &gt;&gt;&gt; Output<pre><code>3137\n3137\n</code></pre>"},{"location":"python/functions/#default-values","title":"Default values","text":"<p>When writing a function, you can optionally define a default value for each parameter. If an argument for a parameter is provided in the function call, <code>Python</code> uses the argument value. If not, the parameter\u2019s default value is used. Using default values can simplify your function calls and clarify the ways in which your functions are typically used. Let's look at an example.</p> <pre><code>def describe_lab(lab_name, lab_supervisor=\"Miriam\"):\n    print(f\"{lab_name} is supervised by {lab_supervisor}\")\n\n# use the default value for lab_supervisor\ndescribe_lab(lab_name=\"Chemistry\")\n\n# provide a value for lab_supervisor\ndescribe_lab(lab_name=\"IT\", lab_supervisor=\"Alex\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Chemistry is supervised by Miriam\nIT is supervised by Alex\n</code></pre>"},{"location":"python/functions/#return-values","title":"Return values","text":"<p>A function doesn\u2019t have to display its output directly with <code>print()</code>.  Instead, usually a function executes a task and returns the result. The result, the return value can be of any type. To return a value, use the <code>return</code> statement.</p> <pre><code>def square_number(number):\n    square = number ** 2\n    return square\n</code></pre> <p>When you call the function, you can assign the result to a variable.</p> <pre><code>result = square_number(5)\n</code></pre> Password check <p>Write a function to check if a user is able to log-in.</p> <p>You have to determine if the given username and password (in  plain text) are within the database. The database is a <code>dict</code>  with IDs as keys and another <code>dict</code> as value, containing  username and hashed password.</p> <p>Given 'database':</p> <pre><code>very_secure_db = {\n    0: {\n        \"username\": \"SwiftShark22\",\n        \"password\": \"ef92b778bafe771e89245b89ecbc08a44a4e166c06659911881f383d4473e94f\",\n    },\n    1: {\n        \"username\": \"FierceFalcon66\",\n        \"password\": \"07ac6e7d83aa285293fc325fecd04a51e933ab94d43dbc6434ddca652718fb95\",\n    },\n    2: {\n        \"username\": \"admin\",\n        \"password\": \"6feb4c700de1982f91ee7a1b40ca4ded05d155af3987597cb179f430dd60da0b\",\n    },\n    3: {\n        \"username\": \"BraveWolf11\",\n        \"password\": \"c430e4368aff7c1bc75c3865343730500d7c1a5f65758ade56026b08e94686cc\",\n    },\n}\n</code></pre> <p> </p> <p>Use following function to convert a password to its hash:</p> <pre><code>import hashlib\n\ndef hash_password(password):\n    return hashlib.sha256(password.encode()).hexdigest()\n</code></pre> <p>Now, write a function that takes at least two arguments, <code>username</code> and  <code>password</code> (in plain text!). Check if the given username and hashed  password are within the <code>very_secure_db</code>. Return <code>True</code> if the  user is able to log-in, otherwise <code>False</code>.</p> <p>Call your function for following users:</p> <pre><code>user1 = (\"SwiftShark22\", \"password123\")\nuser2 = (\"FierceFalcon\", \"sdkjf34\u00a7\")\nuser3 = (\"admin\", \"1b40ca4ded0\")\n</code></pre> Info <p>As you have seen in the above example, functions help you to structure your code. For instance, the function <code>hash_password()</code> was reused multiple  times (to generate the <code>very_secure_db</code> and within your own function).</p> <p>Functions also help you to break down complex problems. You can write a  function for each subtask and then combine them to solve the problem as a  whole.</p>"},{"location":"python/functions/#recap","title":"Recap","text":"<p>This section introduced the concept of functions to better structure your  code, make it more readable and reusable. We have covered:</p> <ul> <li>How to define a function</li> <li>Docstrings as a tool to document your functions</li> <li>Parameters vs arguments</li> <li>Positional and keyword arguments</li> <li>Defining default values for parameters</li> <li>The <code>return</code> statement</li> <li>How to use functions to solve smaller subtasks and structure your code</li> </ul>"},{"location":"python/ide/","title":"IDE","text":"<p>After the successful installation of Python, we use an IDE (=Integrated Development Environment) which is simply put, a place to write and  execute <code>Python</code> code. There are many IDEs available, but we recommend using  Visual Studio Code (VS Code/VSC).</p>"},{"location":"python/ide/#visual-studio-code","title":"Visual Studio Code","text":""},{"location":"python/ide/#general-information","title":"General Information","text":"<p>VSCode is a free, open-source code editor developed by Microsoft . It has gained immense popularity among developers for its versatility and extensive extension ecosystem, making it a powerful tool for various  programming tasks, including Python and Jupyter Notebook programming.  Some key features of VSCode include:</p> <ul> <li>Cross-Platform: VSCode is available for Windows , macOS , and Linux ,  making it accessible to developers on different operating systems.</li> <li>Lightweight: It\u2019s known for its speed and efficiency. VSCode launches quickly and consumes minimal system resources.</li> <li>Extensible: VSCode supports a wide range of programming languages and  technologies through extensions. You can customize the editor with extensions  to add new features, integrations, and tools. VSCode offers intelligent code  completion and suggestions, which can significantly boost your productivity  while writing code. Additionally, there is an extension for GitHub Copilot which gives you real-time AI-based suggestions (free for students; sign-up here)</li> <li>Version Control: It has built-in <code>Git</code> support, making it easy to manage  version control and collaborate with others using Git repositories.</li> <li>Large Community: VSCode has a large and active community, which means you can find plenty of resources, extensions, and tutorials to enhance your coding experience.</li> </ul>"},{"location":"python/ide/#setup","title":"Setup","text":"<p>Download the installer from the official website. The installation is straightforward, so we won't cover it in detail. </p>"},{"location":"python/ide/#extensions","title":"Extensions","text":"<p>As already mentioned, VSCode can be used for a wide range of programming languages. To do this, we need to install the corresponding extensions.  Therefore, start VSCode and click on the sidebar on <code>Extensions</code>. Then search  and install <code>Jupyter</code> and <code>Python</code> (both from Microsoft). </p> Install the Python and Jupyter extension <p>Now, restart VSCode.</p>"},{"location":"python/ide/#jupyter-notebook","title":"Jupyter notebook","text":"<p>Next, we create a new file to execute our first Python code.  To do so, we use Jupyter notebooks. Jupyter notebooks are basically composed of cells. A cell can either contain  code or text. However, first, we have to create our first notebook.</p> <p>Hence, we first select a folder in which we want to save our work. We go to  <code>File</code> <code>Open Folder</code> and choose a folder. Then click on explorer in the sidebar where your folder should be opened. Right  click somewhere in the explorer and select <code>New File</code>. Type a name for  your file with the extension <code>*.ipynb</code>. If not automatically, open the new file. Click on <code>Select Kernel</code> in the  upper right corner of VSCode and select <code>Python Environment</code>  your <code>Python</code> installation.</p> Select your Python kernel. <p>If your firewall asks, allow access.</p> Allow access. <p>Now, add your first code cell with the <code>+ Code</code> button in the upper left  corner. Add following line.</p> <pre><code>print(\"Hello World!\")\n</code></pre> Run your first code snippet. <p>After clicking on <code>Run All</code>, a popup will appear to install the <code>ipykernel</code>. Click on <code>Install</code>. </p> Last missing piece - the ipykernel. <p>After the installation, you should be greeted with following output</p> &gt;&gt;&gt; Output<pre><code>Hello World!\n</code></pre> <p>Congratulations \ud83c\udf89, you've successfully executed your first <code>Python</code>  code!</p>"},{"location":"python/ide/#more-on-jupyter-notebooks","title":"... more on Jupyter notebooks","text":""},{"location":"python/ide/#why","title":"Why?","text":"<p>One of the key features of Jupyter Notebook is the combination of code cells  with rich text elements, allowing you to create comprehensive documents that  blend code, visualizations, and explanatory text. This makes it a powerful tool for creating data analysis reports, sharing research findings, or documenting  code workflows.</p> <p>In addition to code execution and documentation capabilities, Jupyter Notebook  offers a wide range of extensions and integrations with popular data science  libraries, plotting libraries, and other tools. It provides a flexible and  interactive environment for data manipulation, visualization, and analysis.</p>"},{"location":"python/ide/#cells","title":"Cells","text":"<p>As previously discussed, Jupyter notebooks are composed of cells. A cell can  contain Python code or text. To add a text cell, click on <code>+ Markdown</code>.  Markdown  is a lightweight markup language with  plain text formatting syntax. You can simply write text, add images and links within a markdown cell. This guide offers a nice comprehensive overview of Markdown.</p> Info <p>Don't worry about Markdown too much, it is simple to use and 'supports'  plain text. So just start writing.</p>"},{"location":"python/ide/#execution","title":"Execution","text":"<p>You can execute cells one by one. Either by clicking on the <code>Exceute Cell</code>  button on the left side of your current cell. Or by using the shortcut  Ctrl+Enter.</p> <p>Run all cells with the corresponding <code>Run All</code> button on top.</p>"},{"location":"python/ide/#coming-up","title":"Coming up ...","text":"<p>Next, we will cover some basic Python concepts, and you will get more familiar  with code cells.</p>"},{"location":"python/installation/","title":"Installation","text":"<p>In this section, we will guide you through the installation process of  Python.</p> Danger <p>Before you skip the content and proceed with the installation, we encourage you to read our instructions. Following them, will save you some time and potential headaches with the setup process!</p>"},{"location":"python/installation/#step-1-download","title":"Step 1:  Download","text":"<p>We urge you to install  Python <code>3.12.9</code>.  Visit the official website python.org ,  scroll to the bottom of the page and download the installer for your operating  system.</p> <p>Now, do not run the installer just yet - watch  the below video first! It will save you time! </p>  Are you on Apple silicon? <p>If you are using an Apple silicon Mac (M1, ... M4), you can also pick  the <code>macOS 64-bit universal2 installer</code>.</p>"},{"location":"python/installation/#step-2-run-installer","title":"Step 2:  Run installer","text":"<p>No matter which operating system you're on, When installing Python, make sure  that you check the box <code>Add python to PATH</code>!</p> <p>Now run the Python installer.</p>  Windows macOS <p> </p> <p>After the successful installation, we recommend to open a command prompt (use the Windows search with the keyword <code>cmd</code>) and verify the installation by typing </p> <pre><code>python --version\n</code></pre> <p>which should result in</p> CMD Output<pre><code>Python 3.12.9\n</code></pre> Info <p>Unfortunately, we do not have a installation video for macOS (yet).  If you're having any trouble, please reach out to us!</p> <p>Nevertheless, the installation process is straightforward. Double click the downloaded <code>python-3.12.9-macos11.pkg</code> file and follow the installation instructions.</p> <p>Make sure to <code>Add python to PATH</code> during the installation process!</p> <p>After the successful installation, open a terminal (use the spotlight search with the keyword <code>terminal</code>) and verify the installation by typing </p> <pre><code>python3 --version\n</code></pre> <p>which should result in</p> <pre><code>Python 3.12.9\n</code></pre>"},{"location":"python/installation/#step-3-done","title":"Step 3:  Done!","text":"<p>If everything went smoothly, you have successfully installed   Python! You can now skip the  troubleshooting part and proceed with the next chapter. </p>"},{"location":"python/installation/#optional-troubleshooting","title":"Optional: Troubleshooting","text":"Info <p>The troubleshooting section is specific to  Windows. If you're on  macOS and encounter  issues, please reach out to us!</p>"},{"location":"python/installation/#path-issues","title":"PATH issues","text":"<p>If you didn't check the box <code>Add python.exe to PATH</code> during  installation, or you encounter an error message along the lines of </p> <pre><code>'python' is not recognized as an internal or external command\n</code></pre> <p>you need to add Python to your PATH (the error means that  Python is simply not found).</p> <p>We cover two options to fix the PATH issue, either use the command prompt  or the GUI.</p> Option 1: GUIOption 2: Command prompt <p>Step 1:</p> <p>First, we need to find the path to the executable. </p> <p>Open the  Windows search and type <code>python</code>. Select <code>Dateispeicherort \u00f6ffnen</code> (open file location). Open the context menu of <code>Python</code> (that's just a shortcut) and select <code>Eigenschaften</code> (properties)  <code>Dateipfad \u00f6ffnen</code> (open file path).  Lastly, copy the path of the newly opened explorer window.</p> <p>      Determine the path to the Python executable. </p> <p>Step 2:</p> <p>Now, we need to add the path to the environment variables. Again use  the  Windows search and type  <code>Umgebungsvariablen</code> (Environment variables). Select the Path value in the  <code>Benutzervariablen f\u00fcr &lt;user-name&gt;</code> (User variables) section. Click on  <code>Neu</code> (New) and paste the copied path.</p> <p>          Add the path to the user variables.      </p> <p>Step 1:</p> <p>Determine the path to the  Python executable  using the Python launcher <code>py</code> (which is part of the Python installation and  is on PATH by default).</p> <pre><code>py -3.12 -c \"import sys; print(sys.executable)\"\n</code></pre> <p>In my case, the output is:</p> CMD Output<pre><code>C:\\Users\\ztklotz\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n</code></pre> <p>Copy your path without the <code>python.exe</code> part.</p> <p>Step 2:</p> <p>Set the PATH variable using the command prompt.</p> <pre><code>setx PATH \"%PATH%;&lt;copied-path&gt;\"\n</code></pre> <p>For instance (using my path):</p> <pre><code>setx PATH \"%PATH%;C:\\Users\\ztklotz\\AppData\\Local\\Programs\\Python\\Python312\"\n</code></pre> <p>Step 3:</p> <p>Again, verify the installation by typing <code>python --version</code> within a command  prompt.</p> <p>With  Python installed, the next step is to set up a code editor. In the following section, we will install Visual Studio Code (VS Code).</p>"},{"location":"python/packages/","title":"Package Management","text":""},{"location":"python/packages/#introduction","title":"Introduction","text":"<p>One reason, why <code>Python</code> is widespread, is its vibrant community. This community develops code to solve a variety of problems in the widest range of scientific fields. This code is bundled and shared for free (as open-source) in the form of packages. You can download and use these packages. The usage of packages will facilitate your coding process as they offer implementations to solve common problems. Therefore, you won't have to reinvent the wheel.</p> <p>For example the package <code>pandas</code> is the go-to tool for data manipulation and analysis. With <code>pandas</code> you can read text and Excel files   among a lot of other formats and it offers a lot of functionality to manipulate and even plot your data. Hence, you will rarely see <code>Python</code> projects that are not dependent on <code>pandas</code>. Apart from <code>pandas</code> there are a wide variety of popular packages:</p> <ul> <li><code>scipy</code> - statistics (which will be covered in the next   course)</li> <li><code>tqdm</code> - build progress bars</li> <li><code>scikit-learn</code> - for machine learning</li> <li><code>numpy</code> - scientific computing</li> <li>... and many many more</li> </ul> <p>This section serves as a guide on how to install and manage packages. Additionally, the concept of virtual environments is explained.</p>"},{"location":"python/packages/#standard-library","title":"Standard library","text":"<p><code>Python</code> comes with a couple of modules which do not need to be installed and can be used 'out of the box'. For simplicity, we will call these modules packages as well. If you're interested in the difference between packages and modules, Real Python has a nice article on the topic. Here is an extensive list of all the packages that <code>Python</code> ships with.</p> <p>Let's use the <code>random</code> package to generate some random numbers.  First, we have to import the package with the following command:</p> <pre><code>import random\n\n# with the package imported we can use its functions\n# e.g., random integer (between 1 and 100)\nprint(random.randint(1, 100))\n</code></pre> &gt;&gt;&gt; Output<pre><code>42\n</code></pre> <p>Note, the output will be different when you run the code, since it is random.</p> <p>The corresponding documentation is available here. Generally speaking, almost all packages offer an online documentation page. It is good practice, to consult these documentation sites as they offer a lot of information on how to use their package and which methods/functions are available. Usually, functionalities are illustrated with examples that can be a good starting point for your project.</p> Info <p>If you remember, <code>random</code> was used in one of the previous  sections on control structures to generate  passwords of variable length.</p> Calculate the median <p>Use the built-in <code>statistics</code> package to calculate the median of  the below given values. Use Google to search for the <code>statistics</code>  documentation page and try to find the appropriate function.</p> <pre><code>values = [13, 58, 90, 34, 49, 41]\n</code></pre> <p>We will continue with another exercise.</p> Variance of random values <p>Generate a list of random values (can be integers and/or floats) and  calculate the variance. Hint: Use both the <code>random</code> and  <code>statistics</code> package.</p>"},{"location":"python/packages/#installing-packages","title":"Installing packages","text":"<p>To get access to all the packages available online, we need to install them using a package manager. One such manager is <code>pip</code> which is  automatically installed alongside <code>Python</code>. To check if <code>pip</code> is available  on your system open a new terminal within VSC by navigating in the menu bar  <code>Terminal</code> <code>New Terminal</code> </p> VSC Terminal <p>and execute the following command:</p> <pre><code>pip\n</code></pre> <p>... you should see a list of commands and their description:</p> <pre><code>Usage:   \n  pip &lt;command&gt; [options]\n\nCommands:\n  install                     Install packages.\n  download                    Download packages.\n  uninstall                   Uninstall packages.\n  freeze                      Output installed packages in requirements format.\n  inspect                     Inspect the python environment.\n  list                        List installed packages.\n  show                        Show information about installed packages.\n...\n</code></pre> Info <p>You can run shell commands directly from your notebook by using an  exclamation mark (<code>!</code>) as a prefix (e.g., <code>!pip</code>). However, in some cases, such as when uninstalling a package, this approach may cause issues.  Therefore, it's often recommended to use the terminal instead.</p> <p>Now, we'll install our first package, called <code>seaborn</code>. To install a package use pip's <code>install</code> command followed by the package name  (<code>pip install &lt;package-name&gt;</code>). Don't worry, it might take a couple of seconds.</p> <pre><code>pip install seaborn\n</code></pre> <p><code>seaborn</code> is a quite common package to visualize data. Now, run the following code to create your first plot. The code snippet was copied from the <code>seaborn</code> documentation here.</p> <pre><code># taken from https://seaborn.pydata.org/examples/grouped_boxplot.html\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\n\n# Load the example tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Draw a nested boxplot to show bills by day and time\nsns.boxplot(x=\"day\", y=\"total_bill\",\n            hue=\"smoker\", palette=[\"m\", \"g\"],\n            data=tips)\nsns.despine(offset=10, trim=True)\n</code></pre> <p>You don't have to fully understand the code snippet. It's more about the successful usage of a package. You might have noticed, that you didn't solely install <code>seaborn</code>. Among <code>seaborn</code>, <code>pip</code> also installed <code>pandas</code> (for data handling). We can 'verify' that by checking the type of <code>tips</code> (from the code snippet above).</p> <pre><code>print(type(tips))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> <p>Most of the time, a package does not 'stand on its own'. It uses the functionalities of other packages as well. In our case, <code>seaborn</code> also needs <code>pandas</code> to properly function. Hence, a lot of packages are dependent on each other.</p> Remove a package <p>Remove the <code>seaborn</code> package. Like above, use <code>pip</code> within a terminal to  list all commands and find the appropriate one. Execute the command  to remove the package.</p>"},{"location":"python/packages/#pypi","title":"PyPI","text":"<p>You might wonder where <code>pip</code> downloads the packages?! In short, all packages are downloaded from the Python Package Index (PyPI). That's where the open-source community (usually) publishes their packages. Simply put, if you type <code>pip install seaborn</code>, <code>pip</code> looks for a package called <code>seaborn</code> on PyPI and downloads it. <code>PyPI</code> is a valuable resource if you're searching for packages, certain versions, etc.</p>"},{"location":"python/packages/#virtual-environments","title":"Virtual environments","text":"<p>Previously, we have installed the package <code>seaborn</code>. The package itself was available system-wide as we did not create a virtual environment beforehand. This means, if you open a new folder/project and you select the same python kernel (typically the global python installation), the package will be available and you do not need to install it again. That might not sound too bad, but it's actually considered bad practice. But what is good practice and what the heck is a virtual environment?</p>"},{"location":"python/packages/#why","title":"Why?","text":"<p>To understand virtual environments, let's use an analogy from everyday life: cooking in a kitchen. Imagine you are baking two different cakes in the same kitchen. One is a regular chocolate cake, and the other must be gluten-free because someone has an allergy.</p> Allergic Reaction (Source: Tenor) <p>Even though both cakes are made in the same kitchen, you would not casually reuse the same bowls, spoons, and surfaces without cleaning them carefully. If flour from the regular cake gets into the gluten-free one, the result is ruined - and potentially harmful. </p> <p>So what do you do? You create separate, clean work areas with the exact ingredients and tools needed for each cake.</p> <p>A virtual environment in Python works the same way.</p> <ul> <li>Your computer is the kitchen.</li> <li>Each project is a different recipe.</li> <li>The packages (like seaborn, numpy, or pandas) are the ingredients.</li> </ul> <p>If all projects share the same global Python installation, it's like throwing all ingredients into one giant bowl. Sooner or later, versions clash, dependencies break, and one project can accidentally ruin another.</p> <p>A virtual environment gives each project its own clean workspace, with its own set of packages and versions, completely separated from other projects - even though everything still runs on the same computer.</p> Virtual Environment (Source: kelvininc) <p>That is why using virtual environments is considered best practice.</p> <p>To summarize, the <code>pip</code>/virtual environment combination facilitates:</p> <ul> <li>Dependency management: You can keep track of the packages that your   project needs to function. Packages are typically built on top of other packages.    For example, <code>seaborn</code> is built on top of <code>pandas</code> and <code>matplotlib</code>. If you want to use <code>seaborn</code>, you need to install <code>pandas</code> and <code>matplotlib</code> first and sometimes in a specific version.</li> <li>Version management: You can specify the exact versions of a package that   your project needs. This is important, because different versions of a    package may have different functionalities or bugs.</li> <li>Environment management: It's easier to work on multiple projects on a   single machine as you can install multiple versions of a package on a   per-project basis.</li> <li>Shareable: Your projects will be shareable with other developers as they   can easily install all dependencies with a single command. No more \"it worked   on my machine\" excuses!</li> </ul>"},{"location":"python/packages/#how","title":"How?","text":"<p>To work with virtual environments, you need to follow three steps: </p> The three steps to work with virtual environments <ol> <li>Create a virtual environment</li> <li>Activate the virtual environment</li> <li>Select the virtual environment as your Jupyter or Python kernel</li> </ol>"},{"location":"python/packages/#create-a-virtual-environment","title":"Create a virtual environment","text":"<p>To create a virtual environment, open a new command prompt within VSCode (you can use the shortcut Ctrl + Shift + <code>\u00f6</code>). Check if the terminal is opened in the correct folder. This should be your current project folder. If not, you can change the folder by typing <code>cd &lt;path/folder-name&gt;</code> in the terminal or by right clicking in the file explorer and selecting \"Open in Integrated Terminal\".</p> <p>Then execute the following command:</p> <pre><code>python -m venv .venv\n</code></pre> <p>This command creates a new folder called <code>.venv</code> within your project folder. Instead of <code>.venv</code> you can choose any name you want. However, this section assumes that you named it <code>.venv</code>.</p> Warning <p>The virtual environment folder should never be touched by the user. Initially a clean copy of your global Python installation will be created.  This includes absolute paths to the Python installation and the Python executable.  Therefore the virtual environment folder cannot be moved or sent to another machine.</p> <p>Furthermore, your Jupyter or Python files should always be in the project folder. NEVER in the virtual environment folder. A typical structure of a project folder might look like this:</p> <pre><code>project_folder/\n\u251c\u2500\u2500 .venv/\n\u251c\u2500\u2500 data/\n\u2514\u2500\u2500 my_script.ipynb\n</code></pre>"},{"location":"python/packages/#activate-an-environment","title":"Activate an environment","text":"<p>So far we have created the virtual environment. But that is not enough. We need to activate it in order to use it. Depending on your operating system, the command to activate the environment is slightly different.</p> Windows macOS/Linux / <p>As a Windows user type</p> <pre><code>.venv\\Scripts\\activate\n</code></pre> If an error occurs <p>If an error occurs (\"the execution of scripts is deactivated on this  system\") run</p> <pre><code>Set-ExecutionPolicy Unrestricted -Scope Process\n</code></pre> <p>... and use the previous command again.</p> <p>Type</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>to activate your environment.</p> <p>Once the environment is activated, you can see the name of the environment (here <code>.venv</code>) in the terminal.</p> <p></p> <p>From now on, every package you install from this activated terminal (for example with <code>pip install</code>) will be installed into the virtual environment. In VS Code or Jupyter you also need to select this virtual environment as the Python/Jupyter kernel for your notebook or script (see next section); this kernel selection is separate from activating the environment in the terminal. Use activation whenever you run terminal commands that should use the virtual environment. Note that once you close the terminal or VS Code, the environment in that terminal will be deactivated, but files that use the virtual-environment kernel will still run with the packages from that environment.</p> <p>Deactivating the environment is the same on all operating systems. To deactivate it, simply use</p> <pre><code>deactivate\n</code></pre> <p>in your command prompt/terminal.</p>"},{"location":"python/packages/#select-the-virtual-environment","title":"Select the virtual environment","text":"<p>So now we have created the virtual environment and activated it in order to install packages. Now the third and last step is to select the virtual environment for your file as Jupyter or Python kernel.</p> <p></p> Fit a machine learning model <p>Assuming your virtual environment is activated, try to get the following code cell running. </p> <pre><code>from matplotlib import pyplot  # (1)!\n\nfrom sklearn.datasets import fetch_california_housing  #(2)!\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\n\n# load data\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\n\n# fit a decision tree\ntree = DecisionTreeRegressor(\n    random_state=784, max_depth=2, min_samples_leaf=15\n)\ntree.fit(X, y)\n\n# visualize the tree\nplot_tree(tree, filled=True, feature_names=X.columns, proportion=True)\npyplot.show()\n</code></pre> <ol> <li><code>pyplot</code> is a submodule of <code>matplotlib</code> and can be directly imported      with the <code>from</code> statement.</li> <li>Or you can import functions (like <code>fetch_california_housing()</code>) directly     from its submodule <code>datasets</code>.</li> </ol> <p>Install the packages <code>matplotlib</code> and <code>scikit-learn</code> with <code>pip</code>. Then try to execute the code cell.</p> <p>Congratulations \ud83c\udf89, you've just fitted a machine learning model (simple decision tree) on a data set and visualized the model. That's the power of <code>Python</code> - easily accessible packages with a lot of functionality ready to use. \ud83e\uddbe</p> <p>Don't worry too much about the actual code lines above. Again, the important thing is to get the code running. With the above exercise, you've  reproduced the result from the motivational section.</p>"},{"location":"python/packages/#requirementstxt","title":"<code>requirements.txt</code>","text":"<p>As we have mentioned before, virtual environments are a great way to isolate project dependencies. However, sharing the whole virtual environment folder (e.g. <code>.venv</code>) is impractical. It often contains thousands of files, OS-specific binaries and absolute paths, so copying it to another machine or location usaually breaks. A better approach is to export the environment's installed packages to a simple text file that others can use to recreate the environment no matter if they are working on MacOS, Linux or Windows.</p> Export dependencies <p>Assume you want to share the code snippet from the previous task with  someone. First, your colleague might not know which packages you used to  get the code running. With no more information, one has to read the code  and manually determine which packages are necessary. To circumvent such situations, you export all your packages to a file.  Open a command prompt/terminal and execute</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>A <code>requirements.txt</code> is written which contains all your used  packages.</p> <p>Your colleague can now take the file and install all packages needed, at once.</p> <p><pre><code>pip install -r requirements.txt\n</code></pre> ... is the corresponding command.</p> Info <p>A <code>requirements.txt</code> file is a common way to share project dependencies. However, it will also help you, to restore your environment, in case  something goes wrong. Hence, keep your requirements file up-to-date.</p>"},{"location":"python/packages/#other-choicesoutlook","title":"Other choices?/Outlook","text":"<p>Apart from <code>pip</code> there are a couple of other package managers available. For example, there are</p> <ul> <li><code>uv</code></li> <li><code>pipenv</code></li> <li><code>poetry</code></li> <li><code>miniconda</code></li> </ul> <p>... and this is by no means an extensive list. All of these tools let you install and manage packages. Nevertheless, they have their differences. In the end, it is up to you, the developer which tool fits best. <code>pip</code> is always a solid choice (and the go-to choice to get the hang of package/virtual environment management). However, if you're working on larger scale projects with a couple of other developers, one of these package managers might offer some functionalities which facilitates the development workflow.</p>"},{"location":"python/packages/#recap","title":"Recap","text":"<p>In this section, you have learned how to install packages and manage them  within virtual environments. The topics covered:</p> <ul> <li><code>pip</code></li> <li>How to install/uninstall packages</li> <li>PyPI - the package hub</li> <li>Concept and benefits of virtual environments</li> <li>Creation and basic usage of a virtual environment</li> </ul>"},{"location":"python/pandas/","title":"Pandas","text":""},{"location":"python/pandas/#pandas","title":"<code>pandas</code>","text":"<p>As the last topic in our <code>Python</code>  Crash Course, we provide a brief introduction to <code>pandas</code> in order to handle data  sets. The package will be heavily used in the upcoming chapters (e.g., Statistics). Therefore, knowledge of the package is needed to properly follow the  chapters from now on.</p> Info <p>This section is heavily based on the excellent 10 minutes to pandas  guide.</p>"},{"location":"python/pandas/#the-data-set","title":"The data set","text":"<p>We will use a custom Spotify data set, containing the current<sup>1</sup> top 50  songs in Austria. You can find the corresponding playlist here.</p> Info <p>If you're interested in the creation of the data set, you can find the code  below. Note, <code>pandas</code> was the only package needed, and we will cover some of  the used functionalities in this section.</p> Create Spotify data set <pre><code># Create a Spotify data set, containing the top 50 tracks in Austria\n\n# Original data (Top Spotify Songs in 73 Countries (Daily Updated)) from:\n# https://www.kaggle.com/datasets/asaniczka/top-spotify-songs-in-73-countries-daily-updated/data\nfrom pathlib import Path\n\nimport pandas as pd\n\n# read initial data set\ndata = pd.read_csv(Path(\"data/universal_top_spotify_songs.csv\"))\n\n# only Austrian chart topping songs\ndata = data[data[\"country\"] == \"AT\"]\n\n# subset by latest snapshot date\nlatest_snapshot = data[\"snapshot_date\"].max()\ndata = data[data[\"snapshot_date\"] == latest_snapshot]\n# sort by daily_rank\ndata = data.sort_values(by=\"daily_rank\").reset_index(drop=True)\n\n# write data to csv\ndata.to_csv(Path(\"data/spotify-top50.csv\"), index=False)\n\n# excerpt of the data set for inclusion in markdown\nwith Path(\"data/spotify-top50.md\").open(\"w\", encoding=\"UTF-8\") as f:\n    prefix = (\n        f\"Excerpt of the data set (snapshot date: **{latest_snapshot}**):\\n\\n\"\n    )\n    _data = data[\n        [\n            \"daily_rank\",\n            \"name\",\n            \"artists\",\n            \"popularity\",\n            \"is_explicit\",\n            \"energy\",\n        ]\n    ]\n    # only top 5 songs\n    _data = _data.head(5)\n    markdown = _data.to_markdown(index=False)\n    f.write(prefix + markdown)\n</code></pre> <p>Excerpt of the data set (snapshot date: 2024-09-25):</p> daily_rank name artists popularity is_explicit energy 1 The Emptiness Machine Linkin Park 93 True 0.872 2 Rote Flaggen Berq 76 True 0.336 3 Bauch Beine Po Shirin David 80 True 0.746 4 Die With A Smile Lady Gaga, Bruno Mars 100 False 0.592 5 BIRDS OF A FEATHER Billie Eilish 99 False 0.507 <p>Download the whole data set to follow this section:</p> <p>Spotify Top 50 Austria </p>"},{"location":"python/pandas/#prerequisites","title":"Prerequisites","text":"<p>For this section, we recommend, to make a new project folder with a  Jupyter notebook. Additionally, create a new virtual environment and  activate it. Please refer to the previous section on packages and virtual  environments if you're having trouble. Lastly, install <code>pandas</code>.</p> <p>You should end up with a project structure similar to the following:</p> <pre><code>\ud83d\udcc1 pandas-course/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc4 spotify-top50.csv\n\u2514\u2500\u2500 \ud83d\udcc4 pandas-course.ipynb\n</code></pre>"},{"location":"python/pandas/#tabular-data","title":"Tabular data","text":"<p>Before we dive into <code>pandas</code>, let's briefly discuss tabular data. At its simplest, tabular data consists of rows and columns. Looking at the Spotify  table above; each row contains information about a specific track  (e.g., <code>name</code>, <code>artists</code>), while each column represents a specific attribute  (e.g., <code>popularity</code>, <code>energy</code>).</p> <p>Tabular data has a clear structure which makes it easy to work with. On the other hand sources for tabular data can be manifold. However, one of the most  common format is the XLSX ( - Excel) or CSV  ( - Comma Separated Values) format which is the one we are working with in this chapter. Nevertheless, tabular data is  also present in various other text based formats like TXT, TSV or even in  databases (e.g. MySQL, PostgreSQL).</p> <p>No matter the source, <code>pandas</code> is the go-to tool to work with tabular data.</p>"},{"location":"python/pandas/#getting-started","title":"Getting started","text":"<p>Let's explore some of <code>pandas</code> functionalities on the example of the Spotify data set. First, we need to import the package.</p> <pre><code>import pandas as pd\n</code></pre> <p>The <code>as</code> statement is used to create an alias for the package in  order to quickly reference it within our next code snippets. An alias  simply reduces the amount of characters you have to type. Moreover, the  alias <code>pd</code> is commonly used for <code>pandas</code>. Therefore, you can more  easily  employ code snippets you find online.</p>"},{"location":"python/pandas/#reading-files","title":"Reading files","text":"<p>With the package imported, we can already read the data set (given as <code>.csv</code>).</p> <pre><code>data = pd.read_csv(\"spotify-top50.csv\")\n</code></pre> <p>The above code snippet assumes, that both data set and notebook are located at the same directory level. Else, you have to adjust the path  <code>\"spotify-top50.csv\"</code> accordingly.</p> <p>Besides <code>.csv</code> files, <code>pandas</code> supports reading from various other file types like Excel, text files or a SQL database. The <code>pandas</code> documentation  provides a comprehensive overview of different file types and their corresponding function. Have a look, to get an idea which file formats are supported not  only for reading but also for writing. </p>"},{"location":"python/pandas/#displaying-data","title":"Displaying data","text":"<p>With a data set at hand, we will most likely want to view it. To view the  first rows of our data frame use the <code>head()</code> method.</p> <pre><code>print(data.head())\n</code></pre> &gt;&gt;&gt; Output<pre><code>               spotify_id                   name  ...    tempo  time_signature\n0  2PnlsTsOTLE5jnBnNe2K0A  The Emptiness Machine  ...  184.115               4\n1  7bkUa9kDFGxgCC7d36dzFI           Rote Flaggen  ...  109.940               3\n2  64f3yNXsi2Vk76odcHCOnw         Bauch Beine Po  ...  123.969               4\n3  2plbrEY59IikOBgBGLjaoe       Die With A Smile  ...  157.969               3\n4  6dOtVTDdiauQNBQEDOtlAB     BIRDS OF A FEATHER  ...  104.978               4\n</code></pre> <p>To display the last rows of the data frame, use the <code>tail()</code> method.</p> <pre><code>print(data.tail())\n</code></pre> &gt;&gt;&gt; Output<pre><code>                spotify_id  ... time_signature\n45  6leQi7NakJQS1vHRtZsroe  ...              4\n46  5E4jBLx4P0UBji68bBThSw  ...              4\n47  6qzetQfgRVyAGEg8QhqzYD  ...              4\n48  3WOhcATHxK2SLNeP5W3v1v  ...              4\n49  7xLbQTeLpeqlxxTPLSiM20  ...              4\n</code></pre> <p>Columns can be viewed with:</p> <pre><code>print(data.columns)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Index(['spotify_id', 'name', 'artists', 'daily_rank', 'daily_movement',\n       'weekly_movement', 'country', 'snapshot_date', 'popularity',\n       'is_explicit', 'duration_ms', 'album_name', 'album_release_date',\n       'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'],\n      dtype='str')\n</code></pre> <p>Similarly, we can print the row indices.</p> <pre><code>print(data.index)\n</code></pre> <p>&gt;&gt;&gt; Output<pre><code>RangeIndex(start=0, stop=50, step=1)\n</code></pre> This data set is consecutively indexed from <code>0</code> to <code>49</code>. If  you recall, a range does not include its <code>stop</code> value (<code>50</code>).</p> <p>By default, (if not otherwise specified) <code>pandas</code> will assign a range index to a data set in order to label the rows.</p> <p>The data set dimensions are accessed with the <code>shape</code> attribute.</p> <pre><code>print(data.shape)\n</code></pre> &gt;&gt;&gt; Output<pre><code>(50, 25)\n</code></pre> <p>The data set has <code>50</code> rows and <code>25</code> columns.</p>"},{"location":"python/pandas/#data-structures","title":"Data structures","text":"<p><code>pandas</code> has two main data structures: <code>Series</code> and <code>DataFrame</code>. As you would expect, a <code>DataFrame</code> is a two-dimensional data structure such as  our whole Spotify data set assigned to the variable <code>data</code>:</p> <pre><code>print(type(data))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> <p>Whereas, a single column of a <code>DataFrame</code> is referred to as a <code>Series</code>.  Generally, selections of the <code>DataFrame</code> can be accessed with square  brackets (<code>[]</code>). To get a column, you can simply use its name.</p> <pre><code>print(data[\"artists\"])\n\nprint(type(data[\"artists\"]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>0                                       Linkin Park\n1                                              Berq\n2                                      Shirin David\n3                             Lady Gaga, Bruno Mars\n...                                             ...\n\n&lt;class 'pandas.core.series.Series'&gt;\n</code></pre> <p>A <code>DataFrame</code> is composed of at least one <code>Series</code>.</p>"},{"location":"python/pandas/#detour-series-and-dataframe-from-scratch","title":"Detour: <code>Series</code> and <code>DataFrame</code> from scratch","text":"<p>It's not always the case that you have a data set (in form of a file) at hand. Sometimes you have to create a <code>Series</code> or <code>DataFrame</code> yourself.</p> <p>A <code>Series</code> can be easily created from a list.</p> <pre><code>austrian_artists = [\"Bibiza\", \"Wanda\", \"Bilderbuch\"]\naustrian_artists = pd.Series(austrian_artists)\n</code></pre> <p>To initiate a <code>DataFrame</code>, you can use a dictionary (among others).</p> <pre><code>austrian_artists = {\n    \"name\": [\"Bibiza\", \"Wanda\", \"Bilderbuch\"],\n    \"album\": [\"bis einer weint\", \"Amore\", \"mea culpa\"],\n    \"release_year\": [2024, 2014, 2019]\n}\naustrian_artists = pd.DataFrame(austrian_artists)\n</code></pre> <p>Dictionary keys are used as column names and the corresponding values as the column values.</p> Info <p>Apart from a <code>dict</code>, a <code>DataFrame</code> can be created from multiple other data structures like a <code>list</code> or <code>tuple</code>. For an  extensive guide, visit the <code>pandas</code> documentation on Intro to data  structures (specifically the  section DataFrame). </p>"},{"location":"python/pandas/#selecting-data","title":"Selecting data","text":"<p>Let's dive deeper into selecting data. To access specific rows, you can use  a slice (just like with lists).</p> <pre><code># rows 5 and 6\nprint(data[5:7])\n</code></pre> &gt;&gt;&gt; Output<pre><code>               spotify_id        name  ...    tempo  time_signature\n5  0io16MKpbeDIdYzmGpQaES  Embrace It  ...  114.933               4\n6  3aJT51ya8amzpT3TKDVipL         FTW  ...   91.937               4\n</code></pre> <p>Select multiple columns by passing a list of column names.</p> <pre><code>print(data[[\"name\", \"artists\"]])\n</code></pre> &gt;&gt;&gt; Output<pre><code>                                  name    artists\n0                The Emptiness Machine    Linkin Park\n1                         Rote Flaggen    Berq\n2                       Bauch Beine Po    Shirin David\n...\n</code></pre>"},{"location":"python/pandas/#boolean-indexing","title":"Boolean indexing","text":"<p>Most of the time, we want to filter the data based on criterias.  For example, we can select the tracks with a tempo higher than <code>120</code> beats per minute (BPM).</p> <pre><code>high_tempo = data[data[\"tempo\"] &gt; 120]\n</code></pre> <p>Let's break the example down:</p> <ul> <li>First, we select the column <code>tempo</code> from the data set with <code>data[\"tempo\"]</code>.</li> <li>Next, we expand our expression to <code>data[\"tempo\"] &gt; 120</code>.   This will return a <code>Series</code> of boolean values.</li> <li>Lastly, we wrap the expression in another set of square brackets to    filter the whole data set based on our boolean values.</li> </ul> <p>We end up with 27 tracks that meet the criteria. <code>high_tempo</code> is a new  <code>DataFrame</code> containing entries that exceed <code>120</code> BPM.</p> Danceable tracks <p> Saturday Night Fever </p> <p>We assume that tracks with a danceability score higher than <code>0.8</code> are danceable. </p> <p>How many of the tracks are danceable?</p> <p>Danceability describes how suitable a track is for dancing based on a  combination of musical elements including tempo, rhythm stability, beat  strength, and overall regularity. A value of 0.0 is least danceable and  1.0 is most danceable.</p> <p>-- Spotify for Developers</p>"},{"location":"python/pandas/#mathematical-operations","title":"Mathematical operations","text":"<p><code>pandas</code> supports mathematical operations on both <code>Series</code> and <code>DataFrame</code>. For instance, we weigh the popularity of a track by its energy level.</p> <p>Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. </p> <p>-- Spotify for Developers</p> <pre><code>weighted_popularity = data[\"popularity\"].mul(data[\"energy\"])\nprint(type(weighted_popularity))\n\n# assign the Series to the DataFrame\ndata[\"weighted_popularity\"] = weighted_popularity\n</code></pre> <p>The <code>mul()</code> method is used to multiply the <code>popularity</code> and <code>energy</code> columns. The resulting <code>Series</code> is assigned to <code>data</code> as a new column.</p> &gt;&gt;&gt; Output<pre><code>&lt;class 'pandas.core.series.Series'&gt;\n</code></pre> Track length <p> </p> <p>Since songs are getting shorter and shorter, we want to know how long the tracks in our data set are.  To do so, calculate the length in minutes.</p> <ul> <li>Explore the given data set to find the appropriate column (which    holds information on the song length). </li> <li>Calculate the length in minutes (hint: use the <code>pandas</code> documentation    or Google.)</li> <li>Assign the result to the data frame.</li> <li>Use boolean indexing, to check if there are any tracks longer than    <code>4</code> minutes.</li> <li>Lastly, calculate the average track length in minutes.</li> </ul>"},{"location":"python/pandas/#basic-statistics","title":"Basic statistics","text":"<p><code>pandas</code> provides a variety of methods to calculate basic statistics. For  instance, <code>min()</code>, <code>max()</code>, <code>mean()</code> can be easily retrieved for a numeric  <code>Series</code> in the data set.</p> <pre><code>print(data[\"tempo\"].min())\n</code></pre> &gt;&gt;&gt; Output<pre><code>80.969\n</code></pre> <p>Conveniently, statistics can be calculated for each column at once using  the <code>DataFrame</code>. In this example, we calculate the standard deviation.</p> <pre><code>print(data.std())\n</code></pre> <p>If you execute the above snippet, a <code>TypeError</code> is raised.</p> Fix the error <p>Try to determine, why the error was raised in the first place. Now, circumvent/fix the error.</p> <p>Hint: Look at the documentation of the <code>std()</code> method and its  parameters.</p> <p>If you want to calculate multiple statistics, you can call the <code>describe()</code>  method.</p> <pre><code>stats = data.describe()\nprint(stats)\n</code></pre> &gt;&gt;&gt; Output<pre><code>       daily_rank  daily_movement  ...       tempo  time_signature\ncount    50.00000        50.00000  ...   50.000000       50.000000\nmean     25.50000         1.04000  ...  125.087260        3.920000\nstd      14.57738         8.14902  ...   26.751323        0.340468\nmin       1.00000       -22.00000  ...   80.969000        3.000000\n25%      13.25000        -3.00000  ...  104.990750        4.000000\n50%      25.50000         1.00000  ...  123.981500        4.000000\n75%      37.75000         3.00000  ...  137.487250        4.000000\nmax      50.00000        29.00000  ...  184.115000        5.000000\n</code></pre> <p><code>describe()</code> provides descriptive statistics for each column. The result of  <code>describe()</code> is a <code>DataFrame</code> itself.</p>"},{"location":"python/pandas/#other-functionalities","title":"Other functionalities","text":"<p><code>pandas</code> offers a plethora of functionalities. There's simply too much to  cover in a brief introductory section. Still, there are some common  <code>DataFrame</code> methods/properties that are worth mentioning:</p> <ul> <li><code>sort_values()</code>: Sort the data frame by a specific column.</li> <li><code>groupby()</code>: Group the data frame by a column.</li> <li><code>merge()</code>: Merge two data frames.</li> <li><code>T</code>: Transpose of the data frame.</li> <li><code>drop_duplicates()</code>: Remove duplicates.</li> <li><code>dropna()</code>: Remove missing values.</li> </ul> <p>All methods are linked to its corresponding documentation with examples  that help you get started.</p>"},{"location":"python/pandas/#recap","title":"Recap","text":"<p>We covered <code>pandas</code> and some selected functionalities which should provide  you with a solid foundation to work with tabular data sets. Moreover, you  should be able to follow the code portions in the upcoming courses more easily.</p> \ud83c\udf89 <p>Congratulations, you've reached the end of the Python  Crash Course!</p> <p>parserTongue byu/Ange1ofD4rkness inProgrammerHumor</p> <p>With the knowledge gained, you're now well-equipped to write your own  scripts. Additionally, you're able to automate tasks, write functions to  tackle complex problems and work with data sets. You can leverage the functionalities of Python  packages and using virtual environments, you can efficiently manage the packages required for your projects.</p> <p>Lastly, let's have a brief look at the upcoming chapter.</p>"},{"location":"python/pandas/#whats-next","title":"What's next?","text":"<p>The upcoming statistics chapter introduces foundational tools to further  analyse data. Among the topics are descriptive and inferential statistics which are previewed in this section. After completion, you  will be equipped with methods to explore, summarize and effectively visualize your data sets. </p> Maybe a not so effective chart, if the actual sales numbers are missing..."},{"location":"python/pandas/#example-descriptive-statistics","title":"Example: Descriptive statistics","text":"<p>One common tool within the descriptive realm is the box plot. For instance, using our Spotify data set, we can visualize the loudness (in dB) based on  whether a track contains explicit lyrics.</p> <p>The above plot suggests that songs with explicit lyrics  tend to be slightly louder than those without (looking at the median).  Apart from the more in-depth explanation of box plot, concepts are  introduced to further validate such claims.</p>"},{"location":"python/pandas/#example-inferential-statistics","title":"Example: Inferential statistics","text":"<p>To illustrate another example, the inferential statistics part contains a  section on linear regression which equips you with the necessary knowledge to  fit your first model. The following code snippet models the popularity of a track with the given features (such as danceability, loudness, tempo, liveness,  etc.).</p> <pre><code>import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndata = pd.read_csv(\"spotify-top50.csv\")\n\nX = data[\n    [\n        \"danceability\",\n        \"loudness\",\n        \"speechiness\",\n        \"acousticness\",\n        \"instrumentalness\",\n        \"liveness\",\n        \"valence\",\n        \"tempo\",\n    ]\n]\ny = data[\"popularity\"]\n\n# fit the model\nreg = LinearRegression().fit(X, y)\n</code></pre> <p>With the model at hand, we can predict the popularity of a track and compare it to the actual value.</p> <p>You will learn how to interpret these results and measure the goodness of  fit (i.e., if the popularity is accurately modelled) and apply it to your  own data sets.</p> <p>See you in the Statistics chapter! </p> <ol> <li> <p>The full data set is available on Kaggle and contains the most streamed songs for multiple different countries. For our purpose, the data was subset.\u00a0\u21a9</p> </li> </ol>"},{"location":"python/variables/","title":"Variables","text":"Info <p>The structure of following sections is based on the official Python tutorial.</p> <p>Many thanks to @jhumci for providing the initial resource materials!</p>"},{"location":"python/variables/#getting-started","title":"Getting started","text":"<p>    Important    </p> <p>We encourage you to execute all upcoming code snippets on your machine. You can easily copy each code snippet to your clipboard, by clicking the icon in the top right corner. By doing so, you will be prepared for all upcoming tasks within the sections. Tasks are indicated by a -icon.</p> <p>We recommend to create a new notebook for each chapter, e.g. create <code>variables.ipynb</code> for this chapter. Doing so, your notebooks will follow the structure of this crash course.</p> <p>You will encounter multiple info boxes   throughout the course. They provide additional information, tips, and tricks.  Be sure to read them thoroughly.</p> <p>Let's start with the first task.</p> Notebook &amp; first code cell <p>Create a new Jupyter notebook and name it <code>variables.ipynb</code>. Paste the following code snippet into a new cell and execute it.</p> <pre><code>print(\"Hello World!\")\n</code></pre> <p>The output should be:</p> &gt;&gt;&gt; Output<pre><code>Hello World!\n</code></pre> Info <p>The upcoming content contains a lot of code snippets. They are easily  recognizable due to their colourful syntax highlighting, such as:</p> <pre><code>print(1+1)\n</code></pre> <p>Code snippets are an integral part, to illustrate concepts, which are  introduced and explained along the way. Commonly, these code snippets are accompanied by an output block to display the result, for instance:</p> &gt;&gt;&gt; Output<pre><code>2\n</code></pre> <p>Nevertheless note, output blocks can be missing as there is not always  an explicit result.</p> <p>Again, execute and experiment with all code snippets on your machine to  verify the results and get familiar with <code>Python</code> !</p>"},{"location":"python/variables/#variable","title":"Variable","text":"<p>Computers can store lots of information. To do so, in <code>Python</code>  we use variables. A variable is a name that refers to a value. The following code snippet, assigns the value <code>4</code> to the variable <code>number</code>. In  general, you pick the variable name on the left hand side, assign a value  with <code>=</code> and the value itself is on the right hand side.</p> <pre><code>number = 4\n</code></pre> <p>You can change the value of a variable in your program at any time, and Python will always keep track of its current value.</p> <pre><code>number = 4\nnumber = 4000\n</code></pre> <p>You will notice that none of the cells had any output. To display the value of a variable we use the <code>print()</code> function.</p>"},{"location":"python/variables/#print","title":"<code>print()</code>","text":"<pre><code>number = 4\nprint(number)\n</code></pre> &gt;&gt;&gt; Output<pre><code>4\n</code></pre> <p>Now, we can also verify that in the above snippet the value of <code>number</code> was actually changed.</p> <pre><code>number = 4\nprint(number)\nnumber = 4000\nprint(number)\n</code></pre> &gt;&gt;&gt; Output<pre><code>4\n4000\n</code></pre> Info <p>Within a notebook, the variables are stored in the background and can be  overwritten at any time. Therefore, it is good practice to execute all  cells from top to bottom of the notebook in the right order so that  nothing unexpected is stored in a variable.</p>"},{"location":"python/variables/#comments","title":"Comments","text":"<p>Comments exist within your code but are not executed. They are used to describe your code and are ignored by the <code>Python</code> interpreter. Comments are prefaced by a <code>#</code>.</p> <pre><code># this is a comment\nprint(\"Hello World!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello World!\n</code></pre> Info <p>Comments help you and others understand what your code is doing. It is good practice to use comments as a tool for documentation.</p>"},{"location":"python/variables/#variable-naming","title":"Variable naming","text":"<p>When you\u2019re using variables in <code>Python</code>, you need to adhere to a few rules and guidelines. Breaking some of these rules will cause errors; other guidelines just help you write code that\u2019s easier to read and understand. Be sure to keep the following rules in mind:</p> <ul> <li>Variable names are lower case and can contain only letters, numbers, and   underscores.   They can start with a letter or an underscore, but not with a number.   For instance, you can call a variable <code>message_1</code> but not <code>1_message</code>.</li> <li>Whitespace is not allowed in variable names, but an underscores <code>_</code> can be   used to separate words in variable names. For example, <code>greeting_message</code>   works, but <code>greeting message</code> won't.</li> <li>Avoid using <code>Python</code> keywords and function names as variable names;   that is, do not use words that <code>Python</code> has reserved for a particular   programmatic purpose, such as the word <code>print</code>.</li> <li>Variable names should be short but descriptive. For example, <code>name</code> is better   than <code>n</code>, <code>student_name</code> is better than <code>s_n</code>, and <code>name_length</code> is better   than <code>length_of_persons_name</code>.</li> </ul>"},{"location":"python/variables/#errors-nameerror","title":"Errors (<code>NameError</code>)","text":"<p>Every programmer makes mistakes and even after years of experience, mistakes are part of the process. With time, you get more efficient in debugging  (=process of finding and fixing errors).</p> Debugging tactics byu/0ajs0jas inProgrammerHumor <p>Let\u2019s look at an error you\u2019re likely to make early on and learn how to fix it. We\u2019ll write some code that throws an error message on purpose. Copy the code and run your cell.</p> <pre><code>message = \"Hello Python Crash Course reader!\"\nprint(mesage)\n</code></pre> <p>Which should result in:</p> <pre><code>NameError: name 'mesage' is not defined\n</code></pre> <p>When an error occurs in your program, the <code>Python</code> interpreter does its best to help you figure out where the problem is. The interpreter provides a traceback which is a record of where the interpreter ran into trouble when trying to execute your code. Here\u2019s an example of the traceback that <code>Python</code> provides, after you\u2019ve accidentally misspelled a variable\u2019s name:</p> <pre><code>Traceback (most recent call last):\n  File \"C:\\\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-35-c8f2adeaed02&gt;\", line 2, in &lt;module&gt;\n    print(mesage)\n          ^^^^^^\nNameError: name 'mesage' is not defined. Did you mean: 'message'?\n</code></pre> <p>The output reports that an error occurs in line 2. The interpreter shows this line to help us spot the error quickly and tells us what kind of error it found. In this case, it found a <code>NameError</code> and reports that the variable <code>mesage</code> has not been defined. A name error usually means we made a spelling mistake when entering the variable\u2019s name or that the variable simply does not exist.</p> Your first fix <p>Fix the <code>NameError</code> in your code cell.</p>"},{"location":"python/variables/#recap","title":"Recap","text":"<p>In this section, we have covered variables in <code>Python</code> .</p> <p>You have learned (about):</p> <ul> <li>To create and assign a value to a variable</li> <li><code>print()</code> to display the value of a variable</li> <li>Comments</li> <li>Naming conventions for variables</li> <li>How to fix a <code>NameError</code></li> </ul>"},{"location":"python/containers/dict/","title":"Dictionaries","text":"<p>In this section, you\u2019ll learn how to use dictionaries which allow you to connect pieces of related information. Dictionaries let you model a variety of real-world objects more accurately. We will create, modify and  access elements of a dictionary.</p>"},{"location":"python/containers/dict/#creating-a-dictionary","title":"Creating a dictionary","text":"<pre><code>experiment = {\"description\": \"resource optimization\"}\nprint(type(experiment))\n</code></pre> <p>Above code snippet creates a simple dictionary and prints its type:</p> &gt;&gt;&gt; Output<pre><code>&lt;class 'dict'&gt;\n</code></pre> <p>In <code>Python</code>, a dictionary is wrapped in curly braces (<code>{}</code>), with a series  of key-value pairs inside the braces. Each key is connected to a value, and  you can use a key to access the value associated with that key.  A key\u2019s value can be any type, like a string, integer, list, or even  another dictionary. In the above example, the key is <code>\"description\"</code>  and its value <code>\"resource optimization\"</code></p> <p>Every key is connected to its value by a colon. Individual key-value pairs are separated by commas. You can store as many key-value pairs as you want in a dictionary.</p> <pre><code>experiment = {\n    \"description\": \"resource optimization\",\n    \"sample_weight_in_grams\": 5,\n}\n</code></pre>"},{"location":"python/containers/dict/#accessing-values","title":"Accessing values","text":"<p>To get the value associated with a key, give the name of the dictionary and then place the key inside a set of square brackets.</p> <pre><code>experiment = {\"sample_weight_in_grams\": 5}\nprint(experiment[\"sample_weight_in_grams\"])\n</code></pre> &gt;&gt;&gt; Output<pre><code>5\n</code></pre> Create a dictionary <p>Manage the cost of raw materials in a dictionary. The dictionary should  contain the following key-value pairs:</p> <ul> <li><code>\"steel\"</code>: <code>100</code></li> <li><code>\"aluminium\"</code>: <code>150</code></li> <li><code>\"copper\"</code>: <code>200</code></li> <li><code>\"plastic\"</code>: <code>50</code></li> </ul> <p>Create the dictionary and print the price of copper.</p>"},{"location":"python/containers/dict/#adding-key-value-pairs","title":"Adding key-value pairs","text":"<p>You can add new key-value pairs to a dictionary at any time. For example,  to add a new key-value pair, you would give the name of the dictionary followed by the new key in square brackets along with the new value.</p> <pre><code>experiment = {}\nexperiment[\"description\"] = \"resource optimization\"\nprint(experiment)\n</code></pre> <p>In the above example, we start with an empty dictionary and add a key-value pair to it.</p> &gt;&gt;&gt; Output<pre><code>{'description': 'resource optimization'}\n</code></pre> <p>However, we can't add the same key a second time to the dictionary. Every key is unique within the dictionary.</p>"},{"location":"python/containers/dict/#modifying-values","title":"Modifying values","text":"<p>Values can be overwritten:</p> <pre><code>experiment = {\"sample_weight_in_grams\": 10}\nexperiment[\"sample_weight_in_grams\"] = 10.2\n</code></pre>"},{"location":"python/containers/dict/#removing-key-value-pairs","title":"Removing key-value pairs","text":"<p>We can remove key-value-pairs using the key and the <code>del</code> statement:</p> <pre><code>experiment = {\n    \"supervisor\": \"Alex\",\n    \"sample_weight_in_grams\": 10,\n}\n\nprint(experiment)\n\ndel experiment[\"supervisor\"]\n\nprint(experiment)\n</code></pre> &gt;&gt;&gt; Output<pre><code>{'supervisor': 'Alex', 'sample_weight_in_grams': 10}\n{'sample_weight_in_grams': 10}\n</code></pre> Modify a dictionary <p>Remember that a value can hold any data type? You are given a dictionary with production data.</p> <pre><code>production = {\n    \"singapore\": {\"steel\": 100, \"aluminium\": 150},\n    \"taipeh\": {\"steel\": 200, \"aluminium\": 250},\n    \"linz\": {\"steel\": 300, \"aluminium\": 350, \"copper\": 100},\n}\n</code></pre> <p>Each key represents a location and has another dictionary as value. This dictionary contains the production quantity of different materials.</p> <ul> <li>Remove <code>linz</code> from the dictionary.</li> <li>Add a new location <code>vienna</code> with the production of 200 steel  and 250 aluminium.</li> <li>Print the <code>aluminium</code> value of <code>taipeh</code> (try accessing it step by step and use variables for each step).</li> </ul> Info <p>At first, the above example might seem a bit too overcomplicated. However,  nesting (in this example: storing a dictionary within a dictionary) is  common practice and as already discussed, lets you represent more  complex data structures. Even databases like Redis and MongoDB are at it's core key value stores, just like our dictionary above.</p>"},{"location":"python/containers/dict/#recap","title":"Recap","text":"<p>We have covered following topics in this section:</p> <ul> <li>Dictionaries store key-value pairs</li> <li>How to get and modify values</li> <li>Adding key-value pairs</li> <li>Removing key-value pairs with <code>del</code></li> </ul> <p>Lastly, as part of the <code>Containers</code> topic, we will have a look at tuples.</p>"},{"location":"python/containers/list/","title":"Lists","text":""},{"location":"python/containers/list/#introduction","title":"Introduction","text":"<p>In <code>Python</code>, a container is a type of data structure that holds and organizes multiple values or objects. Containers are used to store collections of elements, allowing you to group related data together for easier management and manipulation. <code>Python</code> provides several built-in container types, each with its own characteristics and use cases. In this first section, we cover  <code>list</code> objects, followed by dictionaries and tuples.</p>"},{"location":"python/containers/list/#what-is-a-list","title":"What is a <code>list</code>?","text":"<p>A <code>list</code> is a collection of items. You can make a <code>list</code>  including the letters of the alphabet or the digits from <code>0</code> to  <code>9</code>. You can put anything you want into a <code>list</code> and the items in your <code>list</code> don\u2019t have to be related in any particular way.</p> <p>Because a <code>list</code> usually contains more than one element, it\u2019s a good  idea to use the plural for a variable of type <code>list</code>, such as  <code>letters</code>, <code>digits</code>, or <code>names</code>.</p> <p>A <code>list</code> is created with an opening and closing square bracket  <code>[]</code>. Individual elements in the <code>list</code> are separated by commas. Here\u2019s a  simple example of a <code>list</code>:</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['John', 'Paul', 'George', 'Ringo']\n</code></pre> <pre><code>print(type([]))  # print the type of an empty list\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'list'&gt;\n</code></pre>"},{"location":"python/containers/list/#accessing-elements","title":"Accessing elements","text":"<p>You can access any element in a <code>list</code> by using the <code>index</code> of the desired item. To access an element in a <code>list</code>, write the name of the <code>list</code> followed by the index of the item enclosed in square brackets. For example, let\u2019s pull out  the first Beatle in <code>beatles</code>:</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles[0])\n</code></pre> &gt;&gt;&gt; Output<pre><code>John\n</code></pre>"},{"location":"python/containers/list/#indexerror","title":"<code>IndexError</code>","text":"Info <p>In Python, index positions start at 0, not 1. This is true for most  programming languages. If you\u2019re receiving unexpected results, determine  whether you are making a simple off-by-one error.</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles[4])\n</code></pre> <p>... results in</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;ipython-input-7-68dd8df4c868&gt;\", line 2, in &lt;module&gt;\n    print(beatles[4])\n          ~~~~~~~^^^\nIndexError: list index out of range\n</code></pre> <p>... since there is no official 5<sup>th</sup> Beatle. </p> <p>There is a special syntax for accessing the last element in a <code>list</code>. Use the index <code>-1</code> to access the last element, <code>-2</code> to access the second-to-last element, and so on.</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles[-1])  # Ringo\nprint(beatles[-2])  # George\n</code></pre> Indexing <p>Define a list that stores following programming languages:</p> <ul> <li>R</li> <li>Python</li> <li>Julia</li> <li>Java</li> <li>C++</li> </ul> <p>and use <code>print()</code> to output: <code>\"My favourite language is Python!\"</code></p>"},{"location":"python/containers/list/#list-manipulation","title":"List manipulation","text":"<p><code>Python</code> provides several ways to add or remove data to existing lists.</p>"},{"location":"python/containers/list/#adding-elements","title":"Adding elements","text":"<p>The simplest way to add a new element to a <code>list</code>, is to append it.  When you append an item to a <code>list</code>, the new element is added at  the end.</p> <pre><code>numbers = [1, 2, 3]\nprint(numbers)\nnumbers.append(4)\nprint(numbers)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1, 2, 3]\n[1, 2, 3, 4]\n</code></pre> <p>The <code>append()</code> method makes it easy to build lists dynamically. For example, you can start with an empty <code>list</code> and then add items by  repeatedly calling <code>append()</code>.</p> <pre><code>numbers = [1.0, 2.0, 0.5]\nnumbers.append(4.0)\nnumbers.append(3.0)\nnumbers.append(\"one hundred\")\n\nprint(numbers)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1.0, 2.0, 0.5, 4.0, 3.0, 'one hundred']\n</code></pre> <p>Up until now, our lists contained only one type of elements -  strings or integers. However, as in the example above, you can store  multiple different types of data in a <code>list</code>. Moreover, you can do  nesting (for example, you can store a <code>list</code> within a <code>list</code> - more on that later). Hence, lists can represent complex data structures. Nevertheless, don't mix and match every imaginable data type within a single  <code>list</code> (just because you can) as it makes the handling of your  <code>list</code> quite difficult.</p> Info <p>Later, we will learn how to perform the same task without repeatedly  calling the same <code>append()</code> method over and over.</p>"},{"location":"python/containers/list/#inserting-elements","title":"Inserting elements","text":"<p>You can add a new element at any position in your <code>list</code> by using the <code>insert()</code> method. You do this by specifying the index of the new element and the value of the new item.</p> <pre><code>pokemon = [\"Charmander\", \"Charizard\"]\npokemon.insert(1, \"Charmeleon\")\nprint(pokemon)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['Charmander', 'Charmeleon', 'Charizard']\n</code></pre>"},{"location":"python/containers/list/#removing-elements","title":"Removing elements","text":"<p>To remove an item from a <code>list</code>, you can use the <code>remove()</code>  method. You need to specify the value which you want to remove. However,  this will only remove the first occurrence of the item.</p> <pre><code>pokemon = [\"Charmander\", \"Squirtle\", \"Charmeleon\", \"Charizard\", \"Squirtle\"]\npokemon.remove(\"Squirtle\")\nprint(pokemon)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['Charmander', 'Charmeleon', 'Charizard', 'Squirtle']\n</code></pre>"},{"location":"python/containers/list/#popping-elements","title":"Popping elements","text":"<p>Sometimes you\u2019ll want to use the value of an item after you remove it from a <code>list</code>. The <code>pop()</code> method removes a specified element of a  <code>list</code>. Additionally, the item is returned so you can work with that  item after removing it. </p> <p>The term pop comes from thinking of a <code>list</code> as a stack of items and popping one item off the top of the stack. In this analogy, the top of a stack  corresponds to the end of a <code>list</code>.</p> <pre><code>pokemon = [\"Charmander\", \"Charmeleon\", \"Bulbasaur\", \"Charizard\"]\nbulbasaur = pokemon.pop(2)\nprint(pokemon)\nprint(bulbasaur)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['Charmander', 'Charmeleon', 'Charizard']\nBulbasaur\n</code></pre> List manipulation <p>Define a <code>list</code> with a couple of elements (of your choice). Play around with the methods <code>append()</code>, <code>insert()</code>,  <code>remove()</code> and <code>pop()</code>. Print the <code>list</code> after  each operation to see the changes.</p>"},{"location":"python/containers/list/#organizing-a-list","title":"Organizing a <code>list</code>","text":"<p>For various reasons, often, your lists will be unordered. If you want to  present your <code>list</code> in a particular order, you can use the method  <code>sort()</code>, or the function <code>sorted()</code>.</p>"},{"location":"python/containers/list/#sort","title":"<code>sort()</code>","text":"<p>The <code>sort()</code> method operates on the <code>list</code> itself and  changes its order.</p> <pre><code>numbers = [5, 4, 1, 3, 2]\nnumbers.sort()  # sort in ascending order\nprint(numbers)\n\nnumbers.sort(reverse=True)  # sort in descending order\nprint(numbers)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1, 2, 3, 4, 5]\n[5, 4, 3, 2, 1]\n</code></pre>"},{"location":"python/containers/list/#sorted","title":"<code>sorted()</code>","text":"<p>The <code>sorted()</code> function maintains the original order of a  <code>list</code> and returns a sorted <code>list</code> as well.</p> <pre><code>numbers = [5, 4, 1, 3, 2]\nsorted_numbers = sorted(numbers)\n\nprint(f\"Original list: {numbers}; Sorted list: {sorted_numbers}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Original list: [5, 4, 1, 3, 2]; Sorted list: [1, 2, 3, 4, 5]\n</code></pre>"},{"location":"python/containers/list/#length","title":"Length","text":"<p>You can easily find the length of a <code>list</code> with <code>len()</code>.</p> <pre><code>print(len([3.0, 1.23, 0.5]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>3\n</code></pre>"},{"location":"python/containers/list/#slicing","title":"Slicing","text":"<p>To make a slice (part of a <code>list</code>), you specify the index of the  first and last elements you want to work with. Elements up until the second index are  included. To output the first three elements in a <code>list</code>, you would request indices 0 through 3, which would return elements 0, 1, and 2.</p> <pre><code>players = [\"charles\", \"martina\", \"michael\", \"florence\", \"eli\"]\nprint(players[0:3])\n</code></pre> &gt;&gt;&gt; Output<pre><code>['charles', 'martina', 'michael']\n</code></pre> Slicing <p>Define a <code>list</code> of your choice with at least <code>5</code>  elements. </p> <ul> <li>Now, perform a slice from the second up to and including the fourth  element.</li> <li>Next, omit the first index in the slice (only omit the number!). What  happens?</li> <li>Lastly, re-add the first index and omit the second index of your  slice. Print the result.</li> </ul>"},{"location":"python/containers/list/#copy","title":"Copy","text":"<p>To copy a <code>list</code>, you can use the <code>copy()</code> method.</p> <pre><code>original_list = [1, 2, 3]\ncopied_list = original_list.copy()\n\n# perform some changes to both lists\noriginal_list.append(4)\ncopied_list.insert(0, \"zero\")\n\nprint(f\"Original list: {original_list}, Copied list: {copied_list}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Original list: [1, 2, 3, 4], Copied list: ['zero', 1, 2, 3]\n</code></pre>"},{"location":"python/containers/list/#be-careful","title":"Be careful!","text":"<p>You might wonder why we can't simply do something along the lines of  <code>copied_list = original_list</code>. With lists, we have to be careful,  as this syntax simply creates a reference to the original <code>list</code>. Let's look at an example:</p> <pre><code>original_list = [1, 2, 3]\ncopied_list = original_list\n\n# perform some changes to the original list\noriginal_list.append(4)\n\nprint(f\"Original list: {original_list}, Copied list: {copied_list}\")\n</code></pre> <p>which leaves us with: &gt;&gt;&gt; Output<pre><code>Original list: [1, 2, 3, 4], Copied list: [1, 2, 3, 4]\n</code></pre></p> <p>As you can see, the changes to the original <code>list</code> are reflected in  the copied one. You can read about this in more detail  here.</p> Note <p>We can actually check whether both lists point to the same object in memory by using <code>id()</code> which returns the memory address of an object.  Just remember, to be careful when copying lists and check if your program  behaves as intended!</p> <pre><code>original_list = [1, 2, 3]\ncopied_list = original_list\n\nprint(id(original_list) == id(copied_list))  # True\n</code></pre> Unknown method <p>Use the given <code>list</code> (don't worry about the syntax, it's just a  short expression to create a huge <code>list</code>):</p> <pre><code>long_list = [True] * 1000\n</code></pre> <ul> <li>Check the length of the <code>list</code>.</li> <li>Apply a method that deletes all elements in the <code>list</code> and  returns an empty list <code>[]</code>. You might need to use Google,  since it is a method not previously discussed.</li> <li>Check the length of the <code>list</code> again.</li> </ul>"},{"location":"python/containers/list/#recap","title":"Recap","text":"<p>We extensively covered lists and their manipulation.</p> <ul> <li>Accessing elements with indices (including slicing)</li> <li>The <code>IndexError</code></li> <li>Adding elements with <code>append()</code> and <code>insert()</code></li> <li>Removing elements with <code>remove()</code> and <code>pop()</code></li> <li>Sorting with <code>sort()</code> and <code>sorted()</code></li> <li>Length of a <code>list</code> with <code>len()</code></li> <li>Make a copy with <code>copy()</code></li> </ul>"},{"location":"python/containers/tuple/","title":"Tuples","text":"<p>Lists and dictionaries work well for storing and manipulating data during the execution of a program. Both lists and dictionaries are mutable. However, sometimes you\u2019ll want to create a collection of elements that are immutable (can't change). Tuples allow you to do just that.</p> <pre><code># coordinates of MCI IV\ncoordinates = (47.262996862335854, 11.393082185178823)\nprint(type(coordinates))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'tuple'&gt;\n</code></pre> <p>A <code>tuple</code> is created with round brackets (<code>()</code>). As with lists and dictionaries, the elements are separated by commas. Tuples can hold any type of data.</p>"},{"location":"python/containers/tuple/#accessing-elements","title":"Accessing elements","text":"<p>With indexing, the individual elements of a <code>tuple</code> can be retrieved.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\nprint(coordinates[0])\nprint(coordinates[1])\n</code></pre> &gt;&gt;&gt; Output<pre><code>47.262996862335854\n11.39308218517882\n</code></pre>"},{"location":"python/containers/tuple/#immutability","title":"Immutability","text":"<p>Let's try to change the value of an element in a <code>tuple</code>.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\ncoordinates[0] = 50.102\n</code></pre> <p>we will encounter following error:</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;ipython-input-29-d74dc80ea879&gt;\", line 2, in &lt;module&gt;\n    coordinates[0] = 50.102\n    ~~~~~~~~~~~^^^\nTypeError: 'tuple' object does not support item assignment\n</code></pre> <p>As a <code>tuple</code> is immutable, you can only redefine the entire  <code>tuple</code>.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\n# redefine the entire tuple\ncoordinates = (5.513615392318705, 95.2060492604128)\n</code></pre>"},{"location":"python/containers/tuple/#tuple-unpacking","title":"<code>tuple</code> unpacking","text":"<p>Tuples can be unpacked, to use them separately.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\nlatitude, longitude = coordinates\n</code></pre> Info <p>Tuples are often used for constants. In the above examples, we used  coordinates. As these coordinates are not going to change, a  <code>tuple</code> is a fitting data type.</p> Tuple unpacking <p>Use the following <code>tuple</code> with cities. <pre><code>cities = (\"New York\", \"Los Angeles\", \"Chicago\")\n</code></pre></p> <ul> <li>Print the first city.</li> <li>Use <code>tuple</code> unpacking and print the resulting variables.</li> </ul>"},{"location":"python/containers/tuple/#recap","title":"Recap","text":"<p>In this rather short section, we introduced tuples and covered:</p> <ul> <li>Mutability vs. immutability</li> <li>How to define a <code>tuple</code></li> <li>Access elements with indexing</li> <li>... and <code>tuple</code> unpacking</li> </ul>"},{"location":"python/control-structures/for/","title":"Loops - <code>for</code>","text":""},{"location":"python/control-structures/for/#introduction","title":"Introduction","text":"<p>In this section, you\u2019ll learn how to loop through elements using just a few lines of code. Looping allows you to take the same action, or set of actions with every item in an iterable. Among iterables are for example, lists or dictionaries. As a result, you'll be able to streamline tedious tasks. First, we'll loop over lists.</p>"},{"location":"python/control-structures/for/#looping-over-lists","title":"Looping over lists","text":"<p>You\u2019ll often want to run through all entries in a <code>list</code>, performing the same task with each item. In the below example, we loop through a <code>list</code> of passwords and print the length of each one.</p> <pre><code>passwords = [\"1234\", \"password\", \"admin\", \"123456\"]\nfor password in passwords:\n    print(f\"Password: {password} is {len(password)} characters long.\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Password: 1234 is 4 characters long.\nPassword: password is 8 characters long.\nPassword: admin is 5 characters long.\nPassword: 123456 is 6 characters long.\n</code></pre> <p>A loop is written with the <code>for</code> statement. The <code>password</code> is a temporary variable that holds the current item in the <code>list</code>. You can  choose any name you want for the temporary variable that holds each value.  However, it\u2019s helpful to choose a meaningful name that represents a single item from the <code>list</code>. For example:</p> <pre><code>for experiment in experiments:\n    ...\nfor user in users:\n    ...\n</code></pre> Info <p>When you\u2019re using loops for the first time, keep in mind that the set  of steps is repeated once for each item in the <code>list</code>, no matter  how many items are in the <code>list</code>. If you have a million items  in your <code>list</code>, <code>Python</code> repeats these steps a million times.</p>"},{"location":"python/control-structures/for/#scope","title":"Scope","text":"<p>Python uses indentation (whitespace) to indicate, what is part of the loop.  With an indentation being four characters of whitespace. For a faster way to  intend, use the tab key Tab.</p> <p>Let's extend the example from above:</p> <pre><code>passwords = [\"1234\", \"password\", \"admin\", \"123456\"]\nfor password in passwords:\n    print(f\"Password: {password} is {len(password)} characters long.\")\n\nprint(\"All passwords have been checked.\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Password: 1234 is 4 characters long.\nPassword: password is 8 characters long.\nPassword: admin is 5 characters long.\nPassword: 123456 is 6 characters long.\nAll passwords have been checked.\n</code></pre> <p>You can easily see that only the first <code>print</code> statement is part of  the loop, simply because it is indented. The second <code>print</code> statement is executed after the loop has finished as it is outside the loop.</p>"},{"location":"python/control-structures/for/#indentationerror","title":"<code>IndentationError</code>","text":"<p>In longer programs, you\u2019ll notice blocks of code indented at a few different levels. These indentation levels help you gain a general sense of the overall program\u2019s organization.</p> <p>As you begin to write code that relies on proper indentation, you\u2019ll need to watch for a few common indentation errors.</p>"},{"location":"python/control-structures/for/#expected-indentation","title":"Expected indentation","text":"<pre><code>for number in [1, 2, 3]:\nprint(number)\n</code></pre> <pre><code>  Cell In[4], line 2\n    print(number)\n    ^\nIndentationError: expected an indented block after 'for' statement on line 1\n</code></pre> <p>As the <code>IndentationError</code> states, <code>Python</code> expects an indented  block of code after the <code>for</code> statement.</p>"},{"location":"python/control-structures/for/#unexpected-indentation","title":"Unexpected indentation","text":"<pre><code>message = \"Hello\"\n    print(message)\n</code></pre> <pre><code>  Cell In[9], line 2\n    print(message)\n    ^\nIndentationError: unexpected indent\n</code></pre> <p>In this case, the code snippet contains an unnecessary indentation.</p> Square numbers <p>Square each number in a given list and print the result. First, initialize a list of numbers from 1 to 10. Square each number and <code>print</code> it. Use a <code>for</code> loop.</p>"},{"location":"python/control-structures/for/#range","title":"<code>range()</code>","text":"<p>The <code>range()</code> function makes it easy to generate a series of numbers.  For example, you can use <code>range()</code> to print a series of numbers like this:</p> <pre><code>for value in range(3):\n  print(value)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0\n1\n2\n</code></pre> <p>Remember, that <code>Python</code> 'starts counting at <code>0</code>'. <code>3</code> is not  included in the output, as <code>range()</code> generates a sequence up to, but not including, the number you provide. You can also pass two arguments to <code>range()</code>, the first and the last number of the sequence. In this case, the sequence will start at the first number and end at the last number minus one.</p> <pre><code>for value in range(3, 6):\n    print(value)\n</code></pre> &gt;&gt;&gt; Output<pre><code>3\n4\n5\n</code></pre> <code>range()</code> <p>Use <code>range()</code> to build a <code>list</code> which holds the numbers from 15 to 20 - including 20.</p> Savings account growth <p>Write a <code>for</code> loop to calculate the growth of savings over a  period of time. Use following formula to calculate the future value of  savings in year \\(t\\):</p> \\[ \\text{A} = \\text{P} \\times \\left(1 + \\frac{\\text{r}}{100} \\right)^{\\text{t}} \\] <p>where:</p> <ul> <li>\\(\\text{A}\\) is the future value of the savings account or investment.</li> <li>\\(\\text{P}\\) is the present value of the savings account or investment.</li> <li>\\(\\text{r}\\) is the annual interest rate.</li> <li>\\(\\text{t}\\) is the number of years the money is invested for.</li> </ul> <p>Given values:</p> <ul> <li>\\(\\text{P} = 1000\\)</li> <li>\\(\\text{r} = 5\\)</li> </ul> <p>Print the future value of the savings account over a period of 10 years.  Skip each second year. Use  Python's documentation on range() as a starting point.</p>"},{"location":"python/control-structures/for/#detour-simple-statistics-on-lists-with-numbers","title":"Detour: Simple statistics on lists with numbers","text":"<p>A few functions are specific to lists of numbers. For example, you can easily find the minimum, maximum, and sum of a list of numbers:</p> <pre><code>numbers = [1.0, 8.38, 3.14, 7.0, 2.71]\nprint(\n    f\"Minimum: {min(numbers)}\",\n    f\"Maximum: {max(numbers)}\",\n    f\"Sum: {sum(numbers)}\", sep=\"\\n\"\n)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Minimum: 1.0\nMaximum: 8.38\nSum: 22.23\n</code></pre> Calculate the average <p>Calculate the average of the following list: <pre><code>numbers = [4.52, 3.14, 2.71, 1.0, 8.38]\n</code></pre></p>"},{"location":"python/control-structures/for/#list-comprehensions","title":"List comprehensions","text":"<p>... are a concise way to create lists.</p> <p>A list comprehension combines a <code>for</code> loop to create a new list in a single line.</p> Rewrite a list comprehension <p>Rewrite the following list comprehension in a regular for-loop to  achieve the same result:</p> <pre><code>squares = [value**2 for value in range(1,11)]\nprint(squares)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n</code></pre>"},{"location":"python/control-structures/for/#looping-over-dictionaries","title":"Looping over dictionaries","text":"<p>As previously discussed, you can not only loop over a <code>list</code>, but  also iterate over a variety of different data types, such as dictionaries. You can loop over a dictionary\u2019s key-value pairs, solely over the keys  or just the values.</p>"},{"location":"python/control-structures/for/#items","title":"<code>items()</code>","text":"<p>Using the <code>.items()</code> method, we can loop over the key-value pairs. Take note, that the method returns two values, which we store in two separate variables (<code>key</code> and <code>value</code>).</p> <p>We can freely choose the variable names in the <code>for</code>-loop. It does  not have to be <code>key</code> and <code>value</code> respectively.</p> <pre><code>parts = {\n    \"P100\": \"Bolt\",\n    \"P200\": \"Screw\",\n    \"P300\": \"Hinge\",\n}\n\nfor key, value in parts.items():\n    print(key, value)\n</code></pre> &gt;&gt;&gt; Output<pre><code>P100 Bolt\nP200 Screw\nP300 Hinge\n</code></pre>"},{"location":"python/control-structures/for/#values-keys","title":"<code>values()</code>, <code>keys()</code>","text":"Dictionary methods <p>Define a (non-empty) dictionary of your choice and use both methods <code>.values()</code> and <code>.keys()</code> to access solely values and keys respectively.</p>"},{"location":"python/control-structures/for/#recap","title":"Recap","text":"<p>With the introduction of the <code>for</code> loop, you can now start to  automate re-occurring tasks. We have covered:</p> <ul> <li>Looping over lists</li> <li>Indentation and possible resulting <code>IndentationError</code></li> <li><code>range()</code> to generate a series of numbers</li> <li>Simple statistics on lists of numbers</li> <li>List comprehensions</li> <li>Specific methods to loop over dictionaries</li> </ul>"},{"location":"python/control-structures/if/","title":"More Control Structures","text":""},{"location":"python/control-structures/if/#introduction","title":"Introduction","text":"<p>In this section, we will cover additional control structures. First, we  discuss the <code>if</code> statement, which allows us to execute code based on a  condition. Followed by the <code>elif</code>, <code>else</code> and  <code>while</code> statements.</p>"},{"location":"python/control-structures/if/#if","title":"<code>if</code>","text":"<p>The <code>if</code> statement lets you evaluate conditions. The simplest kind of  <code>if</code> statement has one condition and one action. Here is some  pseudocode:</p> <pre><code>if condition is True:\n    do something\n</code></pre> <p>You can put any condition in the first line and just about any action in the  indented block following the test. If the condition evaluates to <code>True</code>, <code>Python</code> executes the indented code following the <code>if</code> statement.  If the test evaluates to <code>False</code>, the indented code block (following the  <code>if</code>) is ignored.</p> <pre><code>user = \"admin\"\n\nif user == \"admin\":\n  print(f\"Welcome {user}!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Welcome admin!\n</code></pre> <p>First, the condition <code>user == \"admin\"</code> is evaluated. If it  evaluates to <code>True</code>, the indented print is executed. If the condition evaluates to <code>False</code>, the indented code block is ignored.</p> <p>Indentation plays the same role in <code>if</code> statements as it did in  <code>for</code> loops (see the previous section).</p> Password strength: Part 1 <p> </p> <p>In the section on comparisons and logical operators, you had to check whether a password meets certain criterias. The following example expands on this task as you are  given a list of passwords. You have to check if each password exceeds a length of 12 characters.</p> <p>Execute the first code cell to generate some random passwords  (note every time you rerun the code snippet, different passwords will be  generated).</p> <pre><code># generate passwords - simply execute the code to generate some random\n# passwords\nimport random\nimport string\n\npasswords = []\nfor i in range(10):\n    length = random.randint(3, 25)\n    password = \"\".join(random.choices(string.ascii_letters, k=length))\n    passwords.append(password)\n</code></pre> <p>The <code>list</code> <code>passwords</code> should look something like this:</p> <pre><code>['PWgOYxQgnxgXm',\n 'gpOMVTmCSjAcndowkUd',\n 'ADKIEthzsGBr',\n 'VRLzOIZtEz',\n 'uOckmTJjeonUyMlnG',\n 'gjOpWuHrIbG',\n 'doxIylbRkNLdvdLNgVgYsDGzd',\n 'KvUdsgZhPIrS',\n 'LrdpffEqlBVQYr',\n 'ncyqXNLnVstVxlx']\n</code></pre> <p>Now, loop over the passwords and check if each password exceeds the  character limit of 12. If so, print the password.</p>"},{"location":"python/control-structures/if/#else","title":"<code>else</code>","text":"<p>Previously, every time the condition in the <code>if</code> statement  evaluated to <code>False</code>,  no action was taken. Hence, the <code>else</code> clause is introduced which  allows you to define a set of actions that are executed when the conditional test fails.</p> <pre><code>user = \"random_user\"\n\nif user == \"admin\":\n  print(f\"Welcome {user}!\")\nelse:\n  print(\"Only admins can enter this area!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Only admins can enter this area!\n</code></pre> Password strength: Part 2 <p>Let's expand on our previous example. Re-use your code to check the length of the generated passwords. Now, we would like to store all passwords that did not meet our criteria in the empty list <code>invalid_passwords</code>.</p> <p>Hint: Introduce an <code>else</code> statement to save the invalid passwords.</p>"},{"location":"python/control-structures/if/#elif","title":"<code>elif</code>","text":"<p>Often, you\u2019ll need to test more than two possible situations, and to evaluate these, you can use an <code>if-elif-else</code> syntax. <code>Python</code> executes only one block in an <code>if-elif-else</code> chain. It runs each conditional test in order until one passes. When a test passes, the code following that test is executed and the rest is skipped.</p> <pre><code>user = \"xX_user_Xx\"\nregistered_users = [\n    \"admin\",\n    \"guest\",\n    \"SwiftShark22\",\n    \"FierceFalcon66\",\n    \"BraveWolf11\"\n]\n\nif user == \"admin\":\n  print(f\"Welcome {user}!\")\nelif user not in registered_users:\n  print(\"Please create an account first!\")\nelse:\n  print(\"Only admins can enter this area!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Please create an account first!\n</code></pre> Info <p>As you might have noticed, you can use a single <code>if</code> statement or <code>if</code> in combination with <code>else</code>. For multiple conditions  you can add as many <code>elif</code> parts as you wish.</p>"},{"location":"python/control-structures/if/#while","title":"<code>while</code>","text":"<p>The <code>for</code> loop takes an iterable and executes a block of  code once for each element. In contrast, the <code>while</code> loop runs as  long as a certain condition is <code>True</code>.</p> <p>For instance, you can use a <code>while</code> loop to count up through a  series of numbers. Here is an example:</p> <pre><code># set a counter\ncurrent_number = 1\n\nwhile current_number &lt;= 5:\n  print(current_number)\n  # increment the counter value by one\n  current_number += 1\n</code></pre> &gt;&gt;&gt; Output<pre><code>1\n2\n3\n4\n5\n</code></pre> <p>Note, that the variable, that is checked in the <code>while</code>-condition  must be defined prior to the loop, otherwise we will encounter a  <code>NameError</code>.</p> Infinite loops <p>Moreover, the variable must be updated within the loop to avoid an infinite loop. For example, if <code>current_number</code> is not incremented by one, the condition <code>current_number &lt;= 5</code> will always evaluate to <code>True</code>, leaving us stuck in an infinite loop. In such cases, simply click the <code>Stop</code> button (on the left-hand side of the  respective code cell) to interrupt the execution.</p> Addition assignment <p>In the above example, we used the <code>+=</code> operator, referred to as  addition assignment. It is a shorthand for incrementing a variable by a  certain value.</p> <pre><code>a = 10\na += 5\nprint(a)\n</code></pre> &gt;&gt;&gt; Output<pre><code>15\n</code></pre> <p>The above code is equivalent to <code>a = a + 5</code>. This shorthand assignment can be used with all arithmetic operators, such as subtraction <code>-=</code> or division <code>/=</code>.</p> While loop <p>Write some code, to print all even numbers up to 42 using a while  loop.</p>"},{"location":"python/control-structures/if/#detour-user-input","title":"Detour: User input","text":"<p>Most programs are written to solve an end user\u2019s problem. To do so, usually  we need to get some information from the user. For a simple example, let\u2019s  say someone wants to enter a username.</p> <p>You can store the user input in the variable <code>user_name</code> like in the example  below.</p> <pre><code>user_name = input(\"Please enter your username:\")\n</code></pre> <p>However, the <code>input()</code> function always returns a string.</p> <pre><code>age = input(\"Please enter your age:\")\nprint(type(age))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'str'&gt;\n</code></pre> <p>... use casting to  convert the input to the desired type.</p>"},{"location":"python/control-structures/if/#break","title":"<code>break</code>","text":"<p>To exit any loop immediately without running any remaining 'loop code', use  the <code>break</code> statement. The <code>break</code> statement directs the flow of your program; you can use it to control which lines of code are executed and  which aren\u2019t, so the program only executes code that you want it to, when you want it to.</p> <pre><code>for i in range(5):\n    if i == 3:\n        break\n    print(i)\n\nprint(\"Continue running the program...\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>0\n1\n2\nContinue running the program...\n</code></pre>"},{"location":"python/control-structures/if/#continue","title":"<code>continue</code>","text":"<p>Rather than breaking out of a loop entirely, you can use the <code>continue</code>  statement to return to the beginning of the loop based on the result of a  condition.</p> <pre><code>for i in range(5):\n    if i == 3:\n        continue\n    print(i)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0\n1\n2\n4\n</code></pre>"},{"location":"python/control-structures/if/#recap","title":"Recap","text":"<p>In this section we have expanded on control structures. We discussed:</p> <ul> <li><code>if</code> statements and how to use them</li> <li><code>else</code> clauses</li> <li><code>elif</code> statements for multiple conditions</li> <li><code>while</code> loops</li> <li><code>break</code> and <code>continue</code> statements for more 'fine-grained'   control</li> </ul>"},{"location":"python/types/bool_and_none/","title":"Boolean &amp; None","text":"<p>In this section, we introduce two more data types, namely boolean (<code>bool</code>) and None (<code>NoneType</code>). Let's start with the latter one,  <code>NoneType</code>.</p>"},{"location":"python/types/bool_and_none/#none-nonetype","title":"None (<code>NoneType</code>)","text":"<p><code>NoneType</code> is a special data type in Python that represents the absence of a value.</p> <pre><code>nothing = None\nprint(type(nothing))\n</code></pre> <p>... which outputs:</p> &gt;&gt;&gt; Output<pre><code>&lt;class 'NoneType'&gt;\n</code></pre> <p><code>None</code> can be used as a placeholder for a variable which will be assigned a value later on. Furthermore, if a program was not able to return  a value, <code>None</code> can be used as a return value. </p> <p>Later, <code>None</code> will play a bigger role. For now, we simply keep in mind that <code>None</code> is a thing.</p>"},{"location":"python/types/bool_and_none/#detour-typeerror","title":"Detour: <code>TypeError</code>","text":"... yet another error. <p>Often, you\u2019ll want to use a variable\u2019s value within a message. For example, say you want to wish someone a happy birthday. You might write code like this:</p> <pre><code>age = 23\nmessage = \"Happy \" + age + \"rd Birthday!\"\nprint(message)\n</code></pre> <p>... results in.</p> <pre><code>Traceback (most recent call last):\n  File \"C:\\\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-34-80a141e301d6&gt;\", line 2, in &lt;module&gt;\n    message = \"Happy \" + age + \"rd Birthday!\"\n              ~~~~~~~~~^~~~~\nTypeError: can only concatenate str (not \"int\") to str\n</code></pre> <p>You might expect this code to print the simple birthday greeting, <code>Happy 23rd birthday!</code>. But if you run this code, you\u2019ll see that it generates an  error.</p> <p>This is a <code>TypeError</code>. It means Python encounters an unexpected  type in <code>age</code>, as strings were mixed with integers in the expression. We will  easily fix the <code>TypeError</code> in the next section.</p>"},{"location":"python/types/bool_and_none/#casting","title":"Casting","text":"<p>When you use integers within strings like this, you need to specify explicitly  that you want Python to use the integer as a string of characters.  You can do this by wrapping the variable in the <code>str()</code> function, which tells <code>Python</code> to represent non-string values as strings:</p> <pre><code>age = 23\nmessage = \"Happy \" + str(age) + \"rd Birthday!\"\nprint(message)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Happy 23rd Birthday!\n</code></pre> <p>Changing the type of <code>age</code> to string is called casting.</p> Info <p>A <code>TypeError</code> can stem from various 'sources'. This is just one  common example.</p> <p>In the following example the integers <code>3</code> and <code>2</code> were  implicitly cast to floating point numbers, to calculate the result,  which is a floating point number.</p> <pre><code>print(type(3 / 2))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'float'&gt;\n</code></pre> <p>With the function <code>int()</code> any value can be explicitly cast into an  integer, if possible. The value to be cast is passed as the input parameter.</p> <pre><code>number = 3.0\nprint(type(number))\n\n# casting\nnumber = int(number)\nprint(type(number))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'float'&gt;\n&lt;class 'int'&gt;\n</code></pre> <p>To explicitly cast a value into a float, use the function <code>float()</code>.</p> Casting <p>For each of the given variables, check whether you can cast them to another type. First, print the type of each variable. Then, use <code>int()</code>, and <code>float()</code> to cast the variables.</p> <pre><code># variables\nfirst = \"12\"\nsecond = \"1.2\"\nthird = 12\n</code></pre> Note <p>Remember the f-string (<code>f\"...\"</code>) from the previous section? Try a slightly modified example from above.</p> <pre><code>age = 23\nmessage = f\"Happy {age}rd Birthday!\"\nprint(message)\n</code></pre> <p>You'll notice, that there's no need for any explicit casting of <code>age</code>.</p> <p>Whenver, you want to include a variable in a string, remember that  f-strings might be more convenient. \ud83d\ude09</p>"},{"location":"python/types/bool_and_none/#booleans","title":"Booleans","text":"<p>Computers work with binary (e.g. <code>True</code> or <code>False</code>). Such information can be stored in a single bit. A boolean is either <code>True</code> or <code>False</code>. Booleans are often used to keep  track of certain conditions, such as whether a game is running or whether a  user can edit certain content on a website:</p> <pre><code>game_active = True\ncan_edit = False\n\nprint(type(True))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'bool'&gt;\n</code></pre>"},{"location":"python/types/bool_and_none/#recap","title":"Recap","text":"<p>In this section, we have introduced two more data types in Python:</p> <ul> <li>None (<code>NoneType</code>)</li> <li>and Booleans (<code>bool</code>)</li> </ul> <p>Now, we have covered all basic types! \ud83c\udf89 With that knowledge, we can already  start to do comparisons and logical operations.</p>"},{"location":"python/types/numbers/","title":"Integers &amp; Floats","text":""},{"location":"python/types/numbers/#integers","title":"Integers","text":"<p><code>Python</code>  treats numbers in several different ways, depending on how they are used. Let\u2019s first look at how <code>Python</code> manages whole  numbers called integers (<code>int</code>).</p> <p>Any number without decimal places is automatically interpreted as an integer.</p> <pre><code>number = 10176\n</code></pre> <p>We can verify the type of the variable <code>number</code> with <code>type()</code>.</p> <pre><code>number = 10176\nprint(type(number))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'int'&gt;\n</code></pre>"},{"location":"python/types/numbers/#arithmetic-operators","title":"Arithmetic operators","text":"<p>Of course, with integers, you can perform basic arithmetic operations. These are:</p> Operator Description <code>+</code> Addition <code>-</code> Subtraction <code>*</code> Multiplication <code>/</code> Division <code>**</code> Exponentiation <code>%</code> Modulo (Used to calculate the remainder of a division) <code>//</code> Floor division <pre><code># Modulo\n20 % 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>2\n</code></pre> <pre><code># Floor division\n20 // 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>6\n</code></pre> Info <p>The floor division <code>//</code> is often referred to as integer division. It rounds down to the nearest whole number.</p> Integer division<pre><code>20 // 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>6\n</code></pre> <p>Contrary, the divison operator <code>/</code> does not round the result.</p> Float division<pre><code>20 / 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>6.666666666666667\n</code></pre> <p>Hence, <code>/</code> is referred to as float division as it always returns  a  <code>float</code> (a number with decimal places). More on floats in a  second.</p> <p>Moreover, you can use multiple operations in one expression. You can also use parentheses to modify the order of operations so Python can evaluate your  expression in the order you specify. For example:</p> <pre><code>2 + 3 * 4\n</code></pre> &gt;&gt;&gt; Output<pre><code>14\n</code></pre> <pre><code>(2 + 3) * 4\n</code></pre> &gt;&gt;&gt; Output<pre><code>20\n</code></pre>"},{"location":"python/types/numbers/#floats","title":"Floats","text":"<p>Any number with decimal places is automatically interpreted as a <code>float</code>.</p> <pre><code>number = 10176.0\nprint(type(number))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'float'&gt;\n</code></pre> <p>All previously introduced arithmetic operations can be used for floats as well.</p> <pre><code>3.0 + 4.5\n# operations with floats and integers\n3.0 * (4 / 2)\n</code></pre>"},{"location":"python/types/numbers/#limitations","title":"Limitations","text":"Info <p>Floats are not 100% precise. \"[...] In general, the decimal floating-point  numbers you enter are only approximated by the binary floating-point  numbers actually stored in the machine.\" (The Python Tutorial, 2024)<sup>1</sup></p> <p>So be aware that these small numerical errors could add up in complex  calculations.</p> <pre><code>34.3 + 56.4\n</code></pre> <p>... results in</p> &gt;&gt;&gt; Output<pre><code>90.69999999999999\n</code></pre> Calculate the BEP <p>Use variables and arithmetic operations to calculate the break-even point (the number of units that need to be sold to cover the costs) for a product. The break-even point is given as:</p> \\[ \\text{BEP (units)} = \\frac{\\text{Fixed Costs}}{\\text{Price per Unit} - \\text{Variable Cost per Unit}} \\] <p>Calculate the \\(\\text{BEP}\\) for the following values:</p> <ul> <li>Fixed Costs: 30000</li> <li>Variable Cost per Unit: 45</li> <li>Price per Unit: 75</li> </ul> <p>Assign each given value to a variable. Print the result in a sentence, e.g.  <code>\"The break-even point is X units.\"</code></p>"},{"location":"python/types/numbers/#recap","title":"Recap","text":"<p>This section was all about numbers in Python. We have covered:</p> <ul> <li>Whole numbers  <code>int</code></li> <li>Decimal numbers  <code>float</code></li> <li>Floating-point arithmetic issues and limitations</li> </ul> <p>Next up, we will introduce the <code>bool</code> and <code>NoneType</code> type in Python.</p> <ol> <li> <p>The Python Tutorial \u21a9</p> </li> </ol>"},{"location":"python/types/strings/","title":"Strings","text":"<p>So far, we have already stored some text in a variable. For example  <code>\"Hello World!\"</code> which is called a string. A string is a primitive data type. Integer, float, boolean and None are also primitive data types which we  will cover later.</p> <p>A string is simply a series of characters. Anything inside quotes is considered a string in <code>Python</code>, and you can use single (<code>'</code>) or double  quotes (<code>\"</code>) around your strings like this:</p> <pre><code>text = \"This is a string.\"\nanother_text = 'This is also a string.'\n</code></pre> <p>This flexibility allows you to use quotes and apostrophes within your strings:</p> <pre><code>text = \"One of Python's strengths is its diverse and supportive community.\"\nprint(text)\n</code></pre> &gt;&gt;&gt; Output<pre><code>One of Python's strengths is its diverse and supportive community.\n</code></pre> <pre><code>text = 'I told my friend, \"Python is my favorite language!\"'\nprint(text)\n</code></pre> &gt;&gt;&gt; Output<pre><code>I told my friend, \"Python is my favorite language!\"\n</code></pre>"},{"location":"python/types/strings/#type","title":"<code>type()</code>","text":"<p>Let's check the type of the variable <code>text</code>.</p> <pre><code>text = \"The language 'Python' is named after Monty Python, not the snake.\"\nprint(type(text))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'str'&gt;\n</code></pre> <p><code>type()</code> comes in handy to check the type of variables. In this  case, we can verify that <code>text</code> is indeed a string. Just like  <code>print()</code>, <code>type()</code>  is an important tool in your programming arsenal.</p> Info <p>It is advisable to consistently enclose your strings with either single  <code>'...'</code> or double quotes <code>\"...\"</code>. This will make your  code easier to read and maintain.</p>"},{"location":"python/types/strings/#string-methods","title":"String methods","text":"<p>One of the simplest string manipulation, is to change the case of  the words in a string.</p> <pre><code>name = \"paul atreides\"\nprint(name.title())\n</code></pre> &gt;&gt;&gt; Output<pre><code>Paul Atreides\n</code></pre> <p>A method is an action performed on an object (in our case the  variable). The dot (.) in <code>name.title()</code> tells <code>Python</code> to  make the <code>title()</code> method act on the variable <code>name</code> which holds  the value <code>\"paul atreides\"</code>.</p> <p>Every method is followed by a set of parentheses, because methods often need additional information to do their work. That information is provided inside the parentheses. The <code>title()</code> method doesn\u2019t need any additional information, so its parentheses are empty.</p>"},{"location":"python/types/strings/#methods-vs-functions","title":"Methods vs. functions","text":"<p>We have already encountered functions like <code>print()</code> and <code>type()</code>. Functions are standalone entities that perform a specific task.</p> <p>On the other hand, methods are associated with objects. In this case, the  <code>title()</code> method is associated with the string object <code>name</code>.</p> String methods <p>You start with the variable <code>input_string</code> that holds the value  <code>\"fEyD rAuThA\"</code>. </p> <pre><code>input_string = \"fEyD rAuThA\"\n</code></pre> <p>Experiment and apply a combination of the following methods:</p> <ul> <li><code>capitalize()</code></li> <li><code>title()</code></li> <li><code>istitle()</code></li> <li><code>isupper()</code></li> <li><code>upper()</code></li> <li><code>lower()</code></li> </ul> <p>Eventually you should end up with the string <code>\"Feyd Rautha\"</code>,  print it.</p>"},{"location":"python/types/strings/#concatenation","title":"Concatenation","text":"<p>It\u2019s often useful to combine strings. For example, you might want to store first and last name in separate variables, and then combine them when you want to display someone\u2019s full name.</p> <p>Python uses the plus symbol (<code>+</code>) to combine strings. In this  example, we use <code>+</code> to create a full name by combining a  <code>first_name</code>, a space, and a <code>last_name</code>:</p> <pre><code>first_name = \"paul\"\nlast_name = \"atreides\"\nfull_name = first_name + \" \" + last_name\nprint(full_name)\n</code></pre> &gt;&gt;&gt; Output<pre><code>paul atreides\n</code></pre> <p>Here, the full name is used in a sentence that greets the user, and the <code>title()</code> method is used to format the name appropriately. This code returns a simple but nicely formatted greeting:</p> <pre><code>full_name = \"paul atreides\"\nprint(\"Hello, \" + full_name.title() + \"!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello, Paul Atreides!\n</code></pre> <p>Another way to nicely format strings is by using f-strings. To achieve the same result as above, simply put an <code>f\"...\"</code> in front of the string and use  curly braces <code>{}</code> to insert the variables. </p> <pre><code>full_name = \"Alia Atreides\"\n\nprint(f\"Hello, {full_name}!\")\n\n# you can even apply methods directly to the variables \n# (within the curly braces)\nprint(f\"Hello, {full_name.lower()}!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello, Alia Atreides!\nHello, alia atreides!\n</code></pre> A quote <p>Find a quote from a famous person you admire. Store the  quote and name in variables and print both with an f-string.</p> <p>Your output should look something like the following,  including the quotation marks: </p> <p><code>Frank Herbert (Dune opening quote): \"I must not fear. Fear is the  mind-killer.\"</code></p>"},{"location":"python/types/strings/#recap","title":"Recap","text":"<p>This section was all about strings, we have covered:</p> <ul> <li>How to create strings</li> <li>Use <code>type()</code> to check a variable's type</li> <li>String methods, such as <code>title()</code> and <code>lower()</code></li> <li>Distinction between methods and functions</li> <li>String concatenation with <code>+</code> and f-strings (<code>f\"...\"</code>)</li> </ul> <p>Next, up we will introduce numbers in Python, namely integers and floats. </p>"},{"location":"python-extensive/","title":"Home","text":"<p>Welcome to the Python Extensive Course!  This course will teach you the  Python  programming language.</p>      Creation of a 3D surface plot of the Lattenspitze. \ud83c\udfd4\ufe0f     That's the power of Python - ease of use paired with a wide range of      functionalities stemming from a large developer community! \ud83e\uddbe"},{"location":"python-extensive/#why-python","title":"Why  Python?","text":"<ul> <li> <p> Ease of use</p> <p><code>Python</code> with its syntax is easy to learn and yet very powerful.</p> </li> <li> <p> Flexible</p> <p><code>Python</code> is a versatile language that can be used for data analysis,  automation, artificial intelligence, and many more applications.</p> </li> </ul> <p>The below section should give you an impression of what you can do with  <code>Python</code>. This is not an extensive list by all means. It might sound  trashy, but if you can imagine something, you can most likely build it in  <code>Python</code>.</p> Info <p>Don't worry about the code snippets too much, after finishing the  course you'll have a better understanding and will be able to run  and modify code yourself. For now, the following snippets illustrate the capabilities of the language and what complex things you can achieve with little code. There is no need to execute it - Just take a look!</p>"},{"location":"python-extensive/#examples","title":"Examples","text":"Just the beginning... <p>All of the following examples are from one of the courses featured on our website. If you stick around and explore subsequent <code>Python</code> courses you  will be able to easily implement all examples yourself! </p>"},{"location":"python-extensive/#machine-learning","title":"Machine Learning","text":"<p>With <code>Python</code> you can easily train your own machine learning models. In this  example, with just a few lines of code, one such model (a decision tree) is  fit and visualized<sup>1</sup>.</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\n\n# load data\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\n\n# fit a decision tree\ntree = DecisionTreeRegressor(\n    random_state=784, max_depth=2, min_samples_leaf=15\n)\ntree.fit(X, y)\n\n# visualize the tree\nplot_tree(tree, filled=True, feature_names=X.columns, proportion=True)\nplt.show()\n</code></pre> A decision tree visualized."},{"location":"python-extensive/#computer-visionai","title":"Computer Vision/AI","text":"<p>Or how about state-of-the-art computer vision with YOLO<sup>2</sup>?</p> <pre><code>from ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run inference on the source\nresults = model(\"https://ultralytics.com/images/bus.jpg\")\nresults[0].show()\n</code></pre> Object detection with YOLO."},{"location":"python-extensive/#automation","title":"Automation","text":"<p>But it's not just machine learning and AI, you can also automate mundane tasks. This code snippet fetches some data (from an online service) and writes an  Excel file<sup>3</sup>.</p> <pre><code>import pandas as pd\nimport requests\n\nurl = \"https://api.coincap.io/v2/assets/pepe-cash/history?interval=d1\"\nresponse = requests.get(url)\n\ndata = pd.DataFrame(response.json()[\"data\"])\ndata.to_excel(\"price-history.xlsx\", index=False)\n</code></pre>"},{"location":"python-extensive/#visualizations","title":"Visualizations","text":"<p>You can create stunning and interactive visualizations<sup>4</sup>. Let's visualize  the above written Excel file.</p> <pre><code>import pandas as pd\nimport plotly.express as px\n\ndata = pd.read_excel(\"price-history.xlsx\")\nfig = px.area(\n    data_frame=data,\n    x=\"date\",\n    y=\"priceUsd\",\n    title=\"Price History in USD\",\n    color_discrete_sequence=[\"#009485\"],\n)\nfig.show()\n</code></pre>"},{"location":"python-extensive/#web-development","title":"Web Development","text":"<p>You can create websites, just like this one. In fact, all the  heavy lifting of this site is done by <code>Python</code> and tools developed by its  community.</p> <p>The most important package used to build this site was  Material for MkDocs, a widely used and customizable static site generator. </p>"},{"location":"python-extensive/#getting-started","title":"Getting Started...","text":"<p>In the next sections, we will install <code>Python</code> including the code editor  <code>Visual Studio Code</code>.</p> Tip <p>Both Python and Visual Studio Code are already pre-installed on PCs in the MCI computer rooms. If you are working with your own computer,  please proceed to the next page.</p> <ol> <li> <p>Scikit-learn is a Python package  for machine learning.\u00a0\u21a9</p> </li> <li> <p>YOLO is an advanced real-time object detection model known for its speed and accuracy, enabling efficient identification and localization of objects within images and videos.\u00a0\u21a9</p> </li> <li> <p>requests is a Python package to interact with APIs.\u00a0\u21a9</p> </li> <li> <p>Plotly is a Python graphing package that  lets you create interactive, publication-quality graphs.\u00a0\u21a9</p> </li> </ol>"},{"location":"python-extensive/DataBasics/","title":"Data Basics","text":""},{"location":"python-extensive/DataBasics/#data-basics","title":"Data Basics","text":"<p>Before starting with Data Science, it's essential to get to know the data you'll be working with. This means thoroughly examining the attributes and values to understand their characteristics. Real-world data is often messy, large, and diverse, which can make it difficult to handle. For example, data from sensors might include missing or corrupted values, while social media data might be unstructured and include text, images, or even videos. In financial data, outliers or extreme values may skew analysis results.</p> <p>Having a deep understanding of the data is crucial for successful data preprocessing, which is the first major step after acquiring the data. For instance, in sensor data, you may need to filter noise, while in social media data, you might need to convert unstructured text into a structured format for analysis. The main objective is to gather useful insights about the data, which will aid in the preprocessing stage, such as identifying patterns, missing values, or outliers. Grasping these aspects early on provides a solid foundation for the rest of your data analysis process.</p>"},{"location":"python-extensive/DataBasics/#dataset-objects-attributes","title":"Dataset, Objects, Attributes","text":"<p>In order to do that, we need to distinguish between the terms data set, object and attribute:</p> <ul> <li>Data Set: A data set is a collection of related data organized in a structured format. It consists of multiple objects, each described by a set of attributes. A data set can be represented as a table, where each row corresponds to a data object and each column corresponds to an attribute. Data sets are commonly used in data analysis, machine learning, and other data-driven tasks, serving as the primary source of input for these processes.</li> <li>Object: An object (or sometimes records, instances, or entries) is an individual unit of data within a data set. It represents a single entity or instance, such as a person, a product, or an event, depending on the context of the data. Each data object is characterized by a set of attributes, which define its specific properties or features. In a tabular data set, a data object corresponds to a row, with each attribute value for that object stored in the respective columns.</li> <li>Attributes: Attributes (or sometimes variable, field, dimension, feature or observations) are the characteristics or properties that describe data objects in a data set. Each attribute represents a specific feature of the data object and is associated with a particular value. For example, in a data set of customer information, attributes might include \"Name,\" \"Age,\" \"Gender,\" and \"Purchase History.\" In a tabular representation, attributes are typically the column headers, with each column containing the attribute values for the corresponding data objects (rows). Attributes can be of different types as described later.</li> </ul> Example: Data Set <p>  The displayed data shows a dataset consisting of three objects and seven attributes.</p>"},{"location":"python-extensive/DataBasics/#qualitative-vs-quantitative","title":"Qualitative vs Quantitative","text":"<p>A variable is called qualitative (categorical) when each observation distinctly falls into a specific category. Qualitative variables express different qualitative properties, but do not convey any magnitude. Conversely, a variable is termed quantitative (numerical) when it measures the magnitude of a property. Quantitative variables can be either Discrete (The variable can only take on a finite number of values) or Continuous (The variable can take on any value within a given interval).</p> Example: Qualitative vs Quantitative Attributes <ul> <li>Qualitative: Race, religious affiliation, gender, children in the household (yes/no)</li> <li>Quantitative: Age, test scores, number of children in a household<ul> <li>Discrete: Number of children in a household  </li> <li>Continuous: Height, weight, length measurements</li> </ul> </li> </ul>"},{"location":"python-extensive/DataBasics/#attribute-types","title":"Attribute Types","text":"<p>We now know, that attributes define the properties of data objects and are crucial in determining the methods and algorithms that can be applied during analysis. Different types of attributes - such as nominal, ordinal, interval, and ratio - each have unique characteristics that influence how they should be handled and interpreted. Recognizing and appropriately categorizing these attribute types is a key step in ensuring accurate data analysis and meaningful results.</p>"},{"location":"python-extensive/DataBasics/#nominal","title":"Nominal","text":"<p>Nominal attributes refer to those that are associated with names or labels. </p> <pre><code>cars = ['BMW', 'Audi', 'VW', 'Skoda', 'Tesla', 'Audi']\n</code></pre> <p>The values of nominal attributes are typically symbols or names representing different categories, codes, or states. These values are used to classify data into distinct groups, often referred to as categorical attributes. Importantly, the values of nominal attributes do not have any inherent or meaningful order; they simply indicate membership within a particular category without implying any rank or sequence. </p> Example: Nominal Attributes <ul> <li>Occupation: teacher, dentist, programmer, farmer,...</li> <li>Hair color: black, brown, blond, red, gray, white,...</li> </ul> <p>You can only determine whether two people have the same hair color or not. It is not possible to establish a greater-than or less-than relationship, and the differences between hair colors cannot be meaningfully interpreted.</p> <p>The symbols or names associated with nominal attributes can also be represented by numbers.</p> <pre><code>cars_num = [1, #'BMW'\n            2, #'Audi'\n            3, #'VW'\n            4, #'Skoda'\n            5, #'Tesla'\n            2] #'Audi'  \n</code></pre> <p>However, in such cases, these numbers are not meant to be used quantitatively, meaning that mathematical operations on them are not meaningful. For instance, calculating the mean or median of these numbers would not make sense. </p> <pre><code>import statistics \nstatistics.mean(cars_num)\n</code></pre> &gt;&gt;&gt; Output<pre><code>2.8333333333333335\n</code></pre> <p>But what does that mean? Is it half <code>'Audi'</code>, half <code>'VW'</code>? And we also could have used the numer <code>32</code> for <code>'Audi'</code> and <code>0</code> for <code>'VW'</code>. So its meaningless! </p> <p>Instead, the mode, which identifies the most frequently occurring value, can be used and is particularly useful in this context.</p> <pre><code>statistics.mode(cars)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Audi\n</code></pre> Example: Nominal Attributes Represented by Numbers <ul> <li>Customer ID: 0001, 0002, 0003, 0004, 0005,...</li> <li>Hair color: black=001, brown=010, blond=011,...</li> </ul> <p>Binary attributes are a specific type of nominal attribute that consist of only two categories. These categories are often represented by the numbers 0 and 1, where 0 indicates the absence of the attribute and 1 indicates its presence. This binary classification is commonly used in data analysis to represent simple variables</p> Example: Binary Attributes <ul> <li>Smoker: Yes=1, No=0</li> <li>Medical Test: Positive=1, Negative=0</li> </ul>"},{"location":"python-extensive/DataBasics/#ordinal","title":"Ordinal","text":"<p>For certain types of attributes, the possible values have a meaningful order or ranking among them, indicating that one value can be considered greater or less than another. However, while this order is significant, the exact magnitude or distance between successive values is not known. This means that while the sequence of values is meaningful, we cannot quantify the difference between them with precise measurements. Therefore it is possible and meaningful to calculate the median and mode. Mean on the other hand is not meaningful.</p> <pre><code>drinks = ['small', 'small', 'small', 'medium', 'medium', 'medium', 'large']\n\nprint('Median: ' + statistics.median(drinks))\nprint('Mode: ' + statistics.mode(drinks))\n</code></pre> &gt;&gt;&gt; Output<pre><code>Median: medium\nMode: small\n</code></pre> Example: Ordinal Attributes <ul> <li>Professional Rank: private, specialist, corporal, sergeant</li> <li>Drink size: small, medium, large</li> </ul> <p>As with the nominal scale, you can determine whether two drinks are the same size or not. Additionally, you can say whether one drink is larger than another. However, it is still not possible to meaningfully interpret the differences between sizes. You cannot specify by how much one drink is larger than another.</p>"},{"location":"python-extensive/DataBasics/#numerical","title":"Numerical","text":"<p>Numeric attributes are quantitative in nature, meaning they represent measurable quantities. These attributes can be expressed as either integer or real values. One of the key characteristics of numeric attributes is their ability to quantify the difference between values, allowing for meaningful comparisons. Statistical measures such as the mean, median, and mode can be calculated from numeric attributes, and these measures are both possible and useful for analyzing the data.</p> <pre><code>income = [1005, 2500, 2500, 5100, 6011, 10500]\n\nprint('Mean: ' + str(statistics.mean(income)))\nprint('Median: ' + str(statistics.median(income)))\nprint('Mode: ' + str(statistics.mode(income)))\n</code></pre> &gt;&gt;&gt; Output<pre><code>Mean: 4602.67\nMedian: 3800.0\nMode: 2500\n</code></pre>"},{"location":"python-extensive/DataBasics/#interval-scaled","title":"Interval-scaled","text":"<p>Interval-scaled attributes can be measured on a scale with equal-sized units, allowing for consistent and comparable intervals between values. These attributes have an inherent order and they can take on positive, zero, or negative values. This means that the ranking of values is possible and meaningful, providing a clear sense of progression or regression along the scale.</p> Example: Interval-scaled Attributes <ul> <li>Calendar dates: For instance, the years 2002 and 2010 are eight years apart</li> <li>Celsius temperature: \\(20^\\circ C\\) is five degrees higher than a temperature of \\(15^\\circ C\\)</li> </ul> <p>As with the ordinal scale, you can determine whether two temperatures are the same and whether one temperature is higher than another. Additionally, the difference between temperatures can be meaningfully interpreted. However, because the zero point is arbitrary, ratios cannot be meaningfully interpreted.</p>"},{"location":"python-extensive/DataBasics/#ratio-scaled","title":"Ratio-scaled","text":"<p>Ratio-scaled attributes possess an inherent zero-point, which indicates the complete absence of the attribute. This characteristic allows us to meaningfully discuss one value as being a multiple of another. Because of this, ratio-scaled data supports a wide range of mathematical operations, including meaningful comparisons of both differences and ratios between values.</p> Example: Ratio-scaled Attributes <ul> <li>Kelvin temperature: has true zero-point</li> <li>Count attributes: years of experience, number of words, Income in Euros</li> </ul> <p>As with the interval scale, you can determine whether two people have the same income and whether one person earns more than another. Additionally, the differences between incomes can be meaningfully interpreted. Furthermore, the ratio between two incomes can now also be interpreted, such as determining how many times one income is compared to another.</p> Task: Attribute Types <p>Name the types of attributes in the following data set and justify  </p>"},{"location":"python-extensive/comparisons_and_logic/","title":"Comparisons & Logical Operators","text":""},{"location":"python-extensive/comparisons_and_logic/#comparisons-logical-operators","title":"Comparisons &amp; Logical Operators","text":""},{"location":"python-extensive/comparisons_and_logic/#comparisons","title":"Comparisons","text":"<p>Now, that we have covered all basic <code>Python</code> types, we can start comparing them. As the name suggests, comparisons are used to compare two values. The result  of a comparison is a boolean value.</p>"},{"location":"python-extensive/comparisons_and_logic/#equality","title":"Equality","text":"<pre><code>print(\"Abc\" == \"abc\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>False\n</code></pre> <p>We can compare any type with each other. In the above case, the comparison  checks if both strings are equal, using the <code>==</code> operator. The result is  <code>False</code>, because the case of the strings do not match.</p> <p>Let's check if two integers are equal:</p> <pre><code>print(1 == 1)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre>"},{"location":"python-extensive/comparisons_and_logic/#inequality","title":"Inequality","text":"<p>We can also check if two values are not equal with the <code>!=</code> operator:</p> <pre><code>user_name = \"Eric\"\nprint(user_name != \"admin\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre> <pre><code>print(2 != 2.1)\nprint(2 != 2)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\nFalse\n</code></pre>"},{"location":"python-extensive/comparisons_and_logic/#numerical-comparisons","title":"Numerical comparisons","text":"<p>... are done with:</p> Operator Description <code>&lt;</code> less than <code>&gt;</code> greater than <code>&lt;=</code> less than or equal <code>&gt;=</code> greater than or equal <pre><code>print(1 &lt; 2)\nprint(1 &gt; 2)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\nFalse\n</code></pre> <pre><code>print(10.2 &lt;= 10.2)\nprint(9.99 &gt;= 10)\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\nFalse\n</code></pre>"},{"location":"python-extensive/comparisons_and_logic/#logical-operators","title":"Logical Operators","text":"<p>You may want to check multiple conditions at the same time. For example, sometimes you might need two conditions to evaluate to <code>True</code> in  order to perform an action. Hence, logical operators are introduced.  There are three logical operators:</p> Operator Meaning Example Result <code>and</code> Returns <code>True</code> if both statements are <code>True</code> <code>True and True</code> <code>True</code> <code>or</code> Returns <code>True</code> if one of the statements is <code>True</code> <code>True or False</code> <code>True</code> <code>not</code> Reverses a result <code>not True</code> <code>False</code>"},{"location":"python-extensive/comparisons_and_logic/#and","title":"<code>and</code>","text":"<pre><code>age = 20\nprint(age &gt;= 18 and age &lt;= 25) # True and True -&gt; True\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre>"},{"location":"python-extensive/comparisons_and_logic/#or","title":"<code>or</code>","text":"<pre><code>age = 20\nprint(age &gt;= 50 or age &lt;= 25) # False or True -&gt; True\n</code></pre> &gt;&gt;&gt; Output<pre><code>True\n</code></pre>"},{"location":"python-extensive/comparisons_and_logic/#not","title":"<code>not</code>","text":"<pre><code>age = 20\nprint(not(age &gt;= 18)) # not(True) -&gt; False\n</code></pre> &gt;&gt;&gt; Output<pre><code>False\n</code></pre> Evaluate password security requirements: Part 1 <p>You are given two variables that describe properties of a password:</p> <ul> <li><code>password_length</code> - represents how many characters are in the password     (<code>int</code>)</li> <li><code>has_special_characters</code> - represents whether the password contains      special characters (<code>True</code>/<code>False</code>)</li> </ul> <p>Variables to use:</p> <pre><code>password_length = 18\nhas_special_characters = False\n</code></pre> <p>Task:</p> <p>Write code that checks if this password is secure based on these requirements:</p> <ol> <li>The password must be longer than 10 characters</li> <li>The password must contain special characters</li> </ol> <p>Use comparisons and logical operators to create a single expression  that evaluates whether BOTH requirements are met.</p> Evaluate password security requirements: Part 2 <p>To increase security, a third variable is introduced alongside the  previous password properties:</p> <p><code>already_used</code> - represents whether this password has been used before     (<code>True</code>/<code>False</code>)</p> <p>Variables to use:</p> <pre><code>password_length = 18\nhas_special_characters = True\nalready_used = False\n</code></pre> <p>Task:</p> <p>Write code that checks if this password is secure based on these three requirements:</p> <ol> <li>The password must be longer than 10 characters</li> <li>The password must contain special characters</li> <li>The password must not have been used before</li> </ol> <p>Build on your previous solution and evaluate whether all THREE  requirements are met.</p>"},{"location":"python-extensive/comparisons_and_logic/#recap","title":"Recap","text":"<p>We have covered the basic comparison and logical operators in <code>Python</code> .</p> <ul> <li> <p>Comparisons</p> <ul> <li><code>==</code> for equality</li> <li><code>!=</code> for inequality</li> <li><code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code> for numerical comparisons</li> </ul> </li> <li> <p>Logical operators</p> <ul> <li><code>and</code></li> <li><code>or</code></li> <li><code>not</code></li> </ul> </li> </ul>"},{"location":"python-extensive/functions/","title":"Functions","text":""},{"location":"python-extensive/functions/#functions","title":"Functions","text":"<p>In this section, you\u2019ll learn to write functions, which are named blocks of code that are designed to do one specific task. If you need to perform that task multiple times throughout your program, you don\u2019t need to type all the code for the same task again and again; you just call the function dedicated to handling that task. By defining functions, your programs will get easier to write, read, test, and fix.</p>"},{"location":"python-extensive/functions/#defining-a-function","title":"Defining a function","text":"<p>Here\u2019s a simple function named <code>greet_user()</code> that prints a greeting:</p> <pre><code>def greet_user():\n    print(\"Hello!\")\n</code></pre> <p>This example shows the simplest structure of a function. With the keyword <code>def</code> we define a function, followed by the name of our function.  Within the parentheses we can (optionally) specify any information the function needs to do its job. This information is defined in the form of parameters (more on that later).</p> <p>Any indented lines that follow <code>def greet_user():</code> make up the body of the function. The line <code>print(\"Hello!\")</code> is the only line of actual code in the body of this function, so <code>greet_user()</code> has just one job: <code>print(\"Hello!\")</code>.</p> <p>When you want to use a function, you have to call it. To do so, simply write the function name, followed by any required information in parentheses.</p> <pre><code># call the function\ngreet_user()\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello!\n</code></pre>"},{"location":"python-extensive/functions/#detour-docstrings","title":"Detour: docstrings","text":"<p>As previously discussed, it is always good practice to add comments to your code in order to improve readability. That applies to functions as well. In  case of functions, one can add a docstring, which is essentially a longer  comment that describes the function. A docstring is written in triple quotes <code>\"\"\"...\"\"\"</code> and is placed directly after the function definition.</p> <pre><code>def greet_user():\n    \"\"\"Display a simple greeting.\"\"\"\n    print(\"Hello!\")\n</code></pre> <p>Now, you can display the docstring by calling the built-in <code>help()</code>  function with the function name as an argument:</p> <pre><code>help(greet_user)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Help on function greet_user in module __main__:\ngreet_user()\n    Display a simple greeting.\n</code></pre> <p>Docstrings facilitate the proper documentation of your code. Most of all, they will help you in the long run to remember what your code does.</p>"},{"location":"python-extensive/functions/#parameters","title":"Parameters","text":"<p>After some modification, the function <code>greet_user()</code> should not only tell the user \"Hello!\" but also greet them by name. Therefore, we have to  define a parameter called <code>user_name</code>. Now, each time you call the function,  you need to pass a <code>user_name</code> such as <code>\"admin\"</code> to the function.</p> <pre><code>def greet_user(user_name):\n    \"\"\"\n    Display a simple greeting.\n    Pass a string with the user's name.\n    \"\"\"\n    print(f\"Hello, {user_name}!\")\n\ngreet_user(\"admin\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello, admin!\n</code></pre> Info <p>As you can see in the example above, a docstring can span multiple lines!</p> <p>Up until now, the functions had no parameters at all or just a single  parameter. However, you can define as many parameters as you like, seperated  by a comma (<code>,</code>):</p> <pre><code>def greet(first_name, last_name):\n    print(f\"Hello, {first_name} {last_name}!\")\n</code></pre> Break-even point <p>Remember the task to calculate the break-even point? Now, you'll wrap  following formula within a function:</p> \\[ \\text{BEP (units)} = \\frac{\\text{Fixed Costs}}{\\text{Price per Unit} - \\text{Variable Cost per Unit}} \\] <p>Write a function called <code>calculate_bep()</code> that takes the following parameters:</p> <ul> <li><code>fixed_costs</code></li> <li><code>price_per_unit</code></li> <li><code>variable_cost_per_unit</code></li> </ul> <p>Print the calculation result (break-even point) within the function.  Call the function with following arguments:</p> <pre><code>fixed_costs = 30000\nprice_per_unit = 75\nvariable_cost_per_unit = 45\n</code></pre>"},{"location":"python-extensive/functions/#terminology","title":"Terminology","text":"<p>parameter vs. argument</p> <p>A parameter is the variable inside the parentheses of the function definition - <code>def greet_user(user_name):</code>. Here <code>user_name</code> is the parameter. When you call the function with, for example <code>greet_user(\"admin\")</code>, the  value <code>\"admin\"</code> is referred to as an argument. You can think of the parameter  as a placeholder and the argument as the actual value.</p>"},{"location":"python-extensive/functions/#positional-arguments","title":"Positional arguments","text":"<p>When you call a function, <code>Python</code> must match each argument in the function  call with a parameter in the function definition. The simplest way to do this  is based on the order of the arguments provided which is referred to as positional arguments.</p> <pre><code>def add_numbers(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    print(a + b)\n\nadd_numbers(3, 5)\n</code></pre> &gt;&gt;&gt; Output<pre><code>8\n</code></pre>"},{"location":"python-extensive/functions/#order-matters","title":"Order matters!","text":"<p>You can get unexpected results, if you mix up the order of the arguments in a function call when using positional arguments:</p> <pre><code>def perform_calculation(a, b):\n    \"\"\"Calculate something.\"\"\"\n    print(a + b**b)\n\na = 12\nb = 5\n\n# correct order\nperform_calculation(a, b)\n# incorrect order (produces a different result)\nperform_calculation(b, a)\n</code></pre> &gt;&gt;&gt; Output<pre><code>3137\n8916100448261\n</code></pre> <p>Next up, we'll introduce keyword arguments to avoid such mistakes.</p>"},{"location":"python-extensive/functions/#keyword-arguments","title":"Keyword arguments","text":"<p>A keyword argument is a name-value pair that you pass to a function. You directly associate the name and the value within the argument, so when you pass the argument to the function, there\u2019s no confusion.</p> <pre><code>perform_calculation(a=12, b=5)\n\n#  you can switch the order of the named keyword arguments\nperform_calculation(b=5, a=12)\n</code></pre> <p>No matter the order, the result is the same!</p> &gt;&gt;&gt; Output<pre><code>3137\n3137\n</code></pre>"},{"location":"python-extensive/functions/#default-values","title":"Default values","text":"<p>When writing a function, you can optionally define a default value for each parameter. If an argument for a parameter is provided in the function call, <code>Python</code> uses the argument value. If not, the parameter\u2019s default value is used. Using default values can simplify your function calls and clarify the ways in which your functions are typically used. Let's look at an example.</p> <pre><code>def describe_lab(lab_name, lab_supervisor=\"Miriam\"):\n    print(f\"{lab_name} is supervised by {lab_supervisor}\")\n\n# use the default value for lab_supervisor\ndescribe_lab(lab_name=\"Chemistry\")\n\n# provide a value for lab_supervisor\ndescribe_lab(lab_name=\"IT\", lab_supervisor=\"Alex\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Chemistry is supervised by Miriam\nIT is supervised by Alex\n</code></pre>"},{"location":"python-extensive/functions/#return-values","title":"Return values","text":"<p>A function doesn\u2019t have to display its output directly with <code>print()</code>.  Instead, usually a function executes a task and returns the result. The result, the return value can be of any type. To return a value, use the <code>return</code> statement.</p> <pre><code>def square_number(number):\n    square = number ** 2\n    return square\n</code></pre> <p>When you call the function, you can assign the result to a variable.</p> <pre><code>result = square_number(5)\n</code></pre> Password check <p>Write a function to check if a user is able to log-in.</p> <p>You have to determine if the given username and password (in  plain text) are within the database. The database is a <code>dict</code>  with IDs as keys and another <code>dict</code> as value, containing  username and hashed password.</p> <p>Given 'database': <pre><code>very_secure_db = {\n    0: {\n        \"username\": \"SwiftShark22\",\n        \"password\": \"ef92b778bafe771e89245b89ecbc08a44a4e166c06659911881f383d4473e94f\",\n    },\n    1: {\n        \"username\": \"FierceFalcon66\",\n        \"password\": \"07ac6e7d83aa285293fc325fecd04a51e933ab94d43dbc6434ddca652718fb95\",\n    },\n    2: {\n        \"username\": \"admin\",\n        \"password\": \"6feb4c700de1982f91ee7a1b40ca4ded05d155af3987597cb179f430dd60da0b\",\n    },\n    3: {\n        \"username\": \"BraveWolf11\",\n        \"password\": \"c430e4368aff7c1bc75c3865343730500d7c1a5f65758ade56026b08e94686cc\",\n    },\n}\n</code></pre></p> <p> </p> <p>Use following function to convert a password to its hash: <pre><code>import hashlib\n\ndef hash_password(password):\n    return hashlib.sha256(password.encode()).hexdigest()\n</code></pre></p> <p>Now, write a function that takes at least two arguments, <code>username</code> and  <code>password</code> (in plain text!). Check if the given username and hashed  password are within the <code>very_secure_db</code>. Return <code>True</code> if the  user is able to log-in, otherwise <code>False</code>.</p> <p>Call your function for following users:</p> <pre><code>user1 = (\"SwiftShark22\", \"password123\")\nuser2 = (\"FierceFalcon\", \"sdkjf34\u00a7\")\nuser3 = (\"admin\", \"1b40ca4ded0\")\n</code></pre> Info <p>As you have seen in the above example, functions help you to structure your code. For instance, the function <code>hash_password()</code> was reused multiple  times (to generate the <code>very_secure_db</code> and within your own function).</p> <p>Functions also help you to break down complex problems. You can write a  function for each subtask and then combine them to solve the problem as a  whole.</p>"},{"location":"python-extensive/functions/#recap","title":"Recap","text":"<p>This section introduced the concept of functions to better structure your  code, make it more readable and reusable. We have covered:</p> <ul> <li>How to define a function</li> <li>Docstrings as a tool to document your functions</li> <li>Parameters vs arguments</li> <li>Positional and keyword arguments</li> <li>Defining default values for parameters</li> <li>The <code>return</code> statement</li> <li>How to use functions to solve smaller subtasks and structure your code</li> </ul>"},{"location":"python-extensive/ide/","title":"IDE","text":""},{"location":"python-extensive/ide/#ide","title":"IDE","text":"<p>After the successful installation of Python, we use an IDE (=Integrated Development Environment) which is simply put, a place to write and  execute <code>Python</code> code. There are many IDEs available, but we recommend using  Visual Studio Code (VS Code/VSC).</p>"},{"location":"python-extensive/ide/#visual-studio-code","title":"Visual Studio Code","text":""},{"location":"python-extensive/ide/#general-information","title":"General Information","text":"<p>VSCode is a free, open-source code editor developed by Microsoft . It has gained immense popularity among developers for its versatility and extensive extension ecosystem, making it a powerful tool for various  programming tasks, including Python and Jupyter Notebook programming.  Some key features of VSCode include:</p> <ul> <li>Cross-Platform: VSCode is available for Windows , macOS , and Linux ,  making it accessible to developers on different operating systems.</li> <li>Lightweight: It\u2019s known for its speed and efficiency. VSCode launches quickly and consumes minimal system resources.</li> <li>Extensible: VSCode supports a wide range of programming languages and  technologies through extensions. You can customize the editor with extensions  to add new features, integrations, and tools. VSCode offers intelligent code  completion and suggestions, which can significantly boost your productivity  while writing code. Additionally, there is an extension for GitHub Copilot which gives you real-time AI-based suggestions (free for students; sign-up here)</li> <li>Version Control: It has built-in <code>Git</code> support, making it easy to manage  version control and collaborate with others using Git repositories.</li> <li>Large Community: VSCode has a large and active community, which means you can find plenty of resources, extensions, and tutorials to enhance your coding experience.</li> </ul>"},{"location":"python-extensive/ide/#setup","title":"Setup","text":"<p>Download the installer from the official website. The installation is straightforward, so we won't cover it in detail. </p>"},{"location":"python-extensive/ide/#extensions","title":"Extensions","text":"<p>As already mentioned, VSCode can be used for a wide range of programming languages. To do this, we need to install the corresponding extensions.  Therefore, start VSCode and click on the sidebar on <code>Extensions</code>. Then search  and install <code>Jupyter</code> and <code>Python</code> (both from Microsoft). </p> Install the Python and Jupyter extension <p>Now, restart VSCode.</p>"},{"location":"python-extensive/ide/#jupyter-notebook","title":"Jupyter notebook","text":"<p>Next, we create a new file to execute our first Python code.  To do so, we use Jupyter notebooks. Jupyter notebooks are basically composed of cells. A cell can either contain  code or text. However, first, we have to create our first notebook.</p> <p>Hence, we first select a folder in which we want to save our work. We go to  <code>File</code> <code>Open Folder</code> and choose a folder. Then click on explorer in the sidebar where your folder should be opened. Right  click somewhere in the explorer and select <code>New File</code>. Type a name for  your file with the extension <code>*.ipynb</code>. If not automatically, open the new file. Click on <code>Select Kernel</code> in the  upper right corner of VSCode and select <code>Python Environment</code>  your <code>Python</code> installation.</p> Select your Python kernel. <p>If your firewall asks, allow access.</p> Allow access. <p>Now, add your first code cell with the <code>+ Code</code> button in the upper left  corner. Add following line.</p> <pre><code>print(\"Hello World!\")\n</code></pre> Run your first code snippet. <p>After clicking on <code>Run All</code>, a popup will appear to install the <code>ipykernel</code>. Click on <code>Install</code>. </p> Last missing piece - the ipykernel. <p>After the installation, you should be greeted with following output</p> &gt;&gt;&gt; Output<pre><code>Hello World!\n</code></pre> <p>Congratulations \ud83c\udf89, you've successfully executed your first <code>Python</code>  code!</p>"},{"location":"python-extensive/ide/#more-on-jupyter-notebooks","title":"... more on Jupyter notebooks","text":""},{"location":"python-extensive/ide/#why","title":"Why?","text":"<p>One of the key features of Jupyter Notebook is the combination of code cells  with rich text elements, allowing you to create comprehensive documents that  blend code, visualizations, and explanatory text. This makes it a powerful tool for creating data analysis reports, sharing research findings, or documenting  code workflows.</p> <p>In addition to code execution and documentation capabilities, Jupyter Notebook  offers a wide range of extensions and integrations with popular data science  libraries, plotting libraries, and other tools. It provides a flexible and  interactive environment for data manipulation, visualization, and analysis.</p>"},{"location":"python-extensive/ide/#cells","title":"Cells","text":"<p>As previously discussed, Jupyter notebooks are composed of cells. A cell can  contain Python code or text. To add a text cell, click on <code>+ Markdown</code>.  Markdown  is a lightweight markup language with  plain text formatting syntax. You can simply write text, add images and links within a markdown cell. This guide offers a nice comprehensive overview of Markdown.</p> Info <p>Don't worry about Markdown too much, it is simple to use and 'supports'  plain text. So just start writing.</p>"},{"location":"python-extensive/ide/#execution","title":"Execution","text":"<p>You can execute cells one by one. Either by clicking on the <code>Exceute Cell</code>  button on the left side of your current cell. Or by using the shortcut  Ctrl+Enter.</p> <p>Run all cells with the corresponding <code>Run All</code> button on top.</p>"},{"location":"python-extensive/ide/#coming-up","title":"Coming up ...","text":"<p>Next, we will cover some basic Python concepts, and you will get more familiar  with code cells.</p>"},{"location":"python-extensive/installation/","title":"Installation","text":""},{"location":"python-extensive/installation/#installation","title":"Installation","text":"<p>In this section, we will guide you through the installation process of  Python.</p> Danger <p>Before you skip the content and proceed with the installation, we encourage you to read our instructions. Following them, will save you some time and potential headaches with the setup process!</p>"},{"location":"python-extensive/installation/#step-1-download","title":"Step 1:  Download","text":"<p>We urge you to install  Python <code>3.12.9</code>.  Visit the official website python.org ,  scroll to the bottom of the page and download the installer for your operating  system.</p> <p>Now, do not run the installer just yet - watch  the below video first! It will save you time! </p>  Are you on Apple silicon? <p>If you are using an Apple silicon Mac (M1, ... M4), you can also pick  the <code>macOS 64-bit universal2 installer</code>.</p>"},{"location":"python-extensive/installation/#step-2-run-installer","title":"Step 2:  Run installer","text":"<p>No matter which operating system you're on, When installing Python, make sure  that you check the box <code>Add python to PATH</code>!</p> <p>Now run the Python installer.</p>  Windows macOS <p> </p> <p>After the successful installation, we recommend to open a command prompt (use the Windows search with the keyword <code>cmd</code>) and verify the installation by typing </p> <pre><code>python --version\n</code></pre> <p>which should result in CMD Output<pre><code>Python 3.12.9\n</code></pre></p> Info <p>Unfortunately, we do not have a installation video for macOS (yet).  If you're having any trouble, please reach out to us!</p> <p>Nevertheless, the installation process is straightforward. Double click the downloaded <code>python-3.12.9-macos11.pkg</code> file and follow the installation instructions.</p> <p>Make sure to <code>Add python to PATH</code> during the installation process!</p> <p>After the successful installation, open a terminal (use the spotlight search with the keyword <code>terminal</code>) and verify the installation by typing </p> <pre><code>python3 --version\n</code></pre> <p>which should result in <pre><code>Python 3.12.9\n</code></pre></p>"},{"location":"python-extensive/installation/#step-3-done","title":"Step 3:  Done!","text":"<p>If everything went smoothly, you have successfully installed   Python! You can now skip the  troubleshooting part and proceed with the next chapter. </p>"},{"location":"python-extensive/installation/#optional-troubleshooting","title":"Optional: Troubleshooting","text":"Info <p>The troubleshooting section is specific to  Windows. If you're on  macOS and encounter  issues, please reach out to us!</p>"},{"location":"python-extensive/installation/#path-issues","title":"PATH issues","text":"<p>If you didn't check the box <code>Add python.exe to PATH</code> during  installation, or you encounter an error message along the lines of </p> <pre><code>'python' is not recognized as an internal or external command\n</code></pre> <p>you need to add Python to your PATH (the error means that  Python is simply not found).</p> <p>We cover two options to fix the PATH issue, either use the command prompt  or the GUI.</p> Option 1: GUIOption 2: Command prompt <p>Step 1:</p> <p>First, we need to find the path to the executable. </p> <p>Open the  Windows search and type <code>python</code>. Select <code>Dateispeicherort \u00f6ffnen</code> (open file location). Open the context menu of <code>Python</code> (that's just a shortcut) and select <code>Eigenschaften</code> (properties)  <code>Dateipfad \u00f6ffnen</code> (open file path).  Lastly, copy the path of the newly opened explorer window.</p> <p>      Determine the path to the Python executable. </p> <p>Step 2:</p> <p>Now, we need to add the path to the environment variables. Again use  the  Windows search and type  <code>Umgebungsvariablen</code> (Environment variables). Select the Path value in the  <code>Benutzervariablen f\u00fcr &lt;user-name&gt;</code> (User variables) section. Click on  <code>Neu</code> (New) and paste the copied path.</p> <p>          Add the path to the user variables.      </p> <p>Step 1:</p> <p>Determine the path to the  Python executable  using the Python launcher <code>py</code> (which is part of the Python installation and  is on PATH by default).</p> <pre><code>py -3.12 -c \"import sys; print(sys.executable)\"\n</code></pre> <p>In my case, the output is:</p> CMD Output<pre><code>C:\\Users\\ztklotz\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n</code></pre> <p>Copy your path without the <code>python.exe</code> part.</p> <p>Step 2:</p> <p>Set the PATH variable using the command prompt.</p> <pre><code>setx PATH \"%PATH%;&lt;copied-path&gt;\"\n</code></pre> <p>For instance (using my path):</p> <pre><code>setx PATH \"%PATH%;C:\\Users\\ztklotz\\AppData\\Local\\Programs\\Python\\Python312\"\n</code></pre> <p>Step 3:</p> <p>Again, verify the installation by typing <code>python --version</code> within a command  prompt.</p> <p>With  Python installed, the next step is to set up a code editor. In the following section, we will install Visual Studio Code (VS Code).</p>"},{"location":"python-extensive/oop/","title":"Object-Oriented Programming","text":""},{"location":"python-extensive/oop/#introduction","title":"Introduction","text":"<p>As code complexity grows, managing and understanding functionality becomes increasingly challenging. Object-Oriented Programming (OOP) addresses this by breaking down large tasks into smaller, modular components.</p>"},{"location":"python-extensive/oop/#example-self-driving-car","title":"Example: Self-Driving Car","text":"<p>Imagine developing software for a self-driving car. Instead of coding everything in a single block, you can organize the project into modules like camera systems, lane detection, navigation, and battery management. Each module is easier to develop, test, and maintain. Moreover, these modules can be reused in other applications, such as drone control, if similar functionalities are required.</p>"},{"location":"python-extensive/oop/#how-oop-helps","title":"How OOP Helps","text":"<p>OOP enhances modularity by allowing each module (or class) to operate independently. Different team members can work on separate classes, improving scalability and collaboration. Think of it like running a restaurant: instead of handling every task yourself, roles like chef, waiter, and cleaner are delegated to specialized staff, making operations efficient and scalable.</p> <p></p>"},{"location":"python-extensive/oop/#core-concepts-of-oop","title":"Core Concepts of OOP","text":"<p>Object-Oriented Programming (OOP) is based on the concept of objects, which are instances of classes (modules).</p> <p>Classes and Objects:  </p> <ul> <li>Class: A blueprint defining attributes (properties) and methods (actions) for objects.</li> <li>Object: An instance of a class, created using the class blueprint.</li> </ul> <p>Objects combine attributes (what they \"have\") and methods (what they \"do\"). </p> <p>Attributes and Methods:  </p> <ul> <li>Attributes: Properties specific to the object (e.g., resolution, lens type).</li> <li>Methods: Functions defining object behavior (e.g., capturing images, detecting objects).</li> </ul> <p>For instance, a <code>Camera</code> class in a self-driving car may define attributes like resolution and lens type, along with methods for capturing images or detecting objects. Multiple objects (e.g., front and rear cameras) can be created from this class, each with unique attribute values.</p> <p>OOP makes it easy to reuse and extend code, simplifying the development of complex systems like self-driving cars.</p> <p></p>"},{"location":"python-extensive/oop/#class-definition","title":"Class Definition","text":"<p>Classes define the structure of objects, specifying their attributes and methods. Use the syntax: <code>class ClassName:</code>  Just like with functions, the code inside the class is indented. Classes can be defined in the same script or in a separate script file, which can then be included using <code>import</code>.  Changes to the class definition only apply to new objects, meaning that all objects from the old definition must be removed from the workspace.     </p> <p>A class for a <code>Camera</code> module in a self-driving car could be defined as follows:</p> <pre><code>class Camera:\n    pass\n</code></pre> Info <p><code>pass</code> is a placeholder that indicates that no action is executed. It is used to define an empty code block.</p> <p>You can create Camera objects \u2014 instances of the class \u2014 by assigning them using  <code>Camera()</code>.</p> <p><pre><code>class Camera:\n    pass\n\n# Creating instances of the Camera class\nfront_camera = Camera()\nrear_camera = Camera()\n\nprint(type(front_camera)) \nprint(type(rear_camera))  \n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class '__main__.Camera'&gt;\n&lt;class '__main__.Camera'&gt;\n</code></pre> This demonstrates that <code>front_camera</code> and <code>rear_camera</code> are instances of the <code>Camera</code> class, even though the class currently has no defined attributes or methods.    </p> <p>Variables or attributes can be defined within a class. As a result, all objects of the class will have this variable with the specified value. The attribute can be accessed using a dot operator.</p> <p><pre><code>class Camera:\n    # Defining a class attribute\n    lens_type = \"wide-angle\" \n\n# Creating an instance of the Camera class\nfront_camera = Camera()\n\n# Accessing the attribute using the dot operator\nprint(front_camera.lens_type)\n</code></pre> The <code>lens_type</code> attribute is a class attribute, meaning it is shared by all instances of the class.   Changing the value of the class attribute (<code>Camera.lens_type</code>) affects all objects created from the class, as they share the same attribute. </p> <pre><code>class Camera:\n    # Defining a class attribute\n    lens_type = \"wide-angle\"  \n\n# Creating two instances of the Camera class\nfront_camera = Camera()\nrear_camera = Camera()\n\n# Accessing the shared attribute\nprint(f\"Front camera lens type: {front_camera.lens_type}\")  \nprint(f\"Rear camera lens type: {rear_camera.lens_type}\")    \n\n# Changing the class attribute\nCamera.lens_type = \"telephoto\"\n\n# Both instances reflect the updated value\nprint(f\"Front camera lens type: {front_camera.lens_type}\")\nprint(f\"Rear camera lens type: {rear_camera.lens_type}\") \n</code></pre>"},{"location":"python-extensive/oop/#initialization-method","title":"Initialization Method","text":"<p>The <code>__init__(self, property)</code> method is called each time a new object is instantiated.   </p> <p>Attributes are characteristics that describe an object (e.g., camera_type, lens_type). Within the <code>__init__</code> method, the term <code>self</code> refers to the object being created, and additional attributes can be added to it. This initialization method ensures that the Camera object is set up with specific values (e.g., camera_type and lens_type) right when it is created. An error message occurs if these specific values are missing.</p> <pre><code>class Camera:\n    # Setting the attributes camera_type and lens_tpye\n    def __init__(self, camera_type, lens_type):\n        self.camera_type = camera_type\n        self.lens_type = lens_type\n\n# Creating an instance of the Camera class       \nfront_camera = Camera(\"front\",\"wide-angle\")\nprint(f\"{front_camera.camera_type} and {front_camera.lens_type}\")\n\n# Creating another instance of the Camera class\nrear_camera = Camera()\n</code></pre> Initialization Method <p>Why does this code generate an error message? Identify the cause and modify the code to ensure it runs without errors.</p> <p>Initialization parameters allow optional customization when creating an object. If no values are provided, default values will be used.</p> <p><pre><code>class Camera:\n    # Setting the attributes camera_type, lens_tpye and resolution\n    def __init__(self, camera_type, lens_type, resolution=1080):\n        self.camera_type = camera_type\n        self.lens_type = lens_type\n        self.resolution = resolution\n\n# Creating an instance of the Camera class w/o defining the resolution      \nfront_camera = Camera(\"front\",\"wide-angle\")\nprint(\n    f\"{front_camera.camera_type}, {front_camera.lens_type} \"\n    f\"and {front_camera.resolution}\"\n)\n</code></pre> <pre><code># Creating an instance of the Camera class with defining the resolution       \nfront_camera = Camera(\"front\",\"wide-angle\",720)\nprint(\n    f\"{front_camera.camera_type}, {front_camera.lens_type} \"\n    f\"and {front_camera.resolution}\"\n)\n</code></pre></p>"},{"location":"python-extensive/oop/#encapsulation","title":"Encapsulation","text":"<p>Encapsulation separates what a class shows (public properties and methods) from its hidden internal details (private implementation). If data is public, it can be directly accessed and changed using the dot operator.</p> <p><pre><code>class Camera:\n    # Setting the attributes camera_type and lens_tpye\n    def __init__(self, camera_type, lens_type):\n        self.camera_type = camera_type\n        self.lens_type = lens_type\n\n# Creating an instance of the Camera class          \nfront_camera = Camera(\"front\",\"wide-angle\")\nprint(front_camera.camera_type)\n</code></pre> <pre><code># Changing the attribute camera_type\nfront_camera.camera_type = \"rear\"\nprint(front_camera.camera_type)\n</code></pre> If data is private, it allows access only through specific methods, protecting the object's internal workings. Using double underscores before the attribute name (<code>__name</code>), restricts access to the private attribute. </p> <p><pre><code>class Camera:\n    # Setting the private attributes camera_type and lens_tpye\n    def __init__(self, camera_type, lens_type):\n        self.__camera_type = camera_type\n        self.__lens_type = lens_type\n\n# Creating an instance of the Camera class \nfront_camera = Camera(\"front\",\"wide-angle\")\n</code></pre> <pre><code># Incorrect usage: Accessing the attribute camera_type\nprint(front_camera.camera_type)\n</code></pre> &gt;&gt;&gt; Output<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[8], line 2\n      1 # Incorrect usage: Accessing the attribute camera_type\n----&gt; 2 print(front_camera.camera_type)\n\nAttributeError: 'Camera' object has no attribute 'camera_type'\n</code></pre></p> <p>By defining appropriate methods, interface functions can be provided to allow the user to modify and read private attributes (e.g., change_camera_type, display_data). The dot operator is used when calling the function.</p> <pre><code>class Camera:\n    # Setting the private attributes camera_type and lens_tpye\n    def __init__(self, camera_type, lens_type):\n        self.__camera_type = camera_type\n        self.__lens_type = lens_type\n\n    # Creating the method change_camera_type\n    def change_camera_type(self, camera_type_new):\n        self.__camera_type = camera_type_new\n\n    # Creating the method display\n    def disp(self):\n        print(f\"Object of Camera Class:\\n\")\n        print(f\"Camera Type: {self.__camera_type}\")\n        print(f\"Lens Type: {self.__lens_type}\")\n</code></pre> Encapsulation <p>Create a new instance of the <code>Camera</code> class (e.g., <code>front_camera</code>), then update its <code>camera_type</code> attribute.</p>"},{"location":"python-extensive/oop/#definition-of-the-data-structure","title":"Definition of the Data Structure","text":"<p>The structure of data \u2014 such as data types and dimensions \u2014 can still be freely chosen by the user, which may lead to undesired behavior. For example, the attribute camera_type might be assigned a list (list) instead of a string (str), or resolution might be given a string (str) instead of an integer (int) without any warning about the incorrect input. Also a method could receive an attribute with the correct type but an invalid value (e.g., a negative number where only positives make sense).  </p> Causing errors for others <p>Up until now, you have encountered various different errors. For example, we encountered a <code>NameError</code> when misspelling a  variable name, a <code>TypeError</code> when using an incorrect data type,  or a <code>IndentationError</code> when the code was not properly indented.</p> <p>Now it's your time to raise an error (or often called exception) yourself,  which can be a helpful and informative way to guide the user in case of  incorrect use.  Here is a comprehensive list of all built-in exceptions in Python. </p> <p>To prevent this, data structures can be validated within the class definition. If the input is incorrect, a general error message can be raised using: <code>raise ValueError(\"Error message\")</code>  To specifically check if an attribute is of an incorrect type (e.g., passing a string when a number is expected), you can raise a <code>TypeError</code> with a descriptive message:   <code>raise TypeError(\"Error message\")</code> </p> <p></p> <p>You can check the data type with the command: <code>isinstance(variable, data_type)</code> </p> <p>Possible attributes of a category can also be defined in a list, for example: <code>__camera_types = [\"front\", \"rear\", \"left\", \"right\", \"top\", \"not stated\"]</code> <code>orientation = [\"horizontal\", \"vertical\", \"not stated\"]</code> </p> <p>By validating the data types during object creation, you can ensure the object behaves as expected and avoid unexpected errors later in the program.   </p> <pre><code>class Camera:\n    __camera_types = [\"front\",\"rear\",\"left\",\"right\",\"top\",\"not stated\"]\n\n    # Setting the attributes with restrictions\n    def __init__(\n        self, camera_type, lens_type, resolution, orientation=\"not stated\"\n    ):\n        if camera_type not in self.__camera_types:\n            # Check if camera_type is one attribute from list.\n            raise ValueError(\n                f\"Camera type must be one of {self.__camera_types}.\"\n            )  \n        if not isinstance(lens_type, str): \n            # Check if lens_type is a string.                     \n            raise TypeError(\"Lens type must be a string.\")  \n        if not isinstance(resolution, int):   \n            # Check if resolution is an integer.                                                      \n            raise TypeError(\"Resolution must be an integer.\")  \n        if orientation not in [\"horizontal\",\"vertical\",\"not stated\"]:\n            # Check if orientation is one attribute from list.\n            raise ValueError(\n                \"Orientation must be either 'horizontal', \"\n                \"'vertical' or 'not stated'.\"\n            )  \n        self.__camera_type = camera_type\n        self.__lens_type = lens_type\n        self.resolution = resolution\n        self.orientation = orientation\n\n    # Creating the method change_camera_type with data type restrictions\n    def change_camera_type(self, camera_type_new):\n        if camera_type_new not in self.__camera_types:\n            raise ValueError(\n                f\"Camera type must be one of {self.__camera_types}.\"\n            )\n        self.__camera_type = camera_type_new\n\n    # Creating the method set_orientation with data type restrictions\n    def set_orientation(self, orientation):\n        if orientation not in [\"horizontal\",\"vertical\",\"not stated\"]:\n            raise ValueError(\n                \"Orientation must be either 'horizontal', \"\n                \"'vertical' or 'not stated'.\"\n            )\n        self.orientation = orientation\n\n    # Creating the method display\n    def disp(self):\n        print(f\"Object of Camera Class\\n:\")\n        print(f\"Camera Type: {self.__camera_type}\")\n        print(f\"Lens Type: {self.__lens_type}\")\n        print(f\"Resolution: {self.resolution}p\")\n        print(f\"Orientation: {self.orientation}\")\n</code></pre> Data Structure <p>Create two instances of the <code>Camera</code> class. Ensure one is created correctly, and intentionally cause an error with the other.</p>"},{"location":"python-extensive/packages/","title":"Package Management","text":""},{"location":"python-extensive/packages/#package-management","title":"Package Management","text":""},{"location":"python-extensive/packages/#introduction","title":"Introduction","text":"<p>One reason, why <code>Python</code> is widespread, is its vibrant community. This community develops code to solve a variety of problems in the widest range of scientific fields. This code is bundled and shared for free (as open-source) in the form of packages. You can download and use these packages. The usage of packages will facilitate your coding process as they offer implementations to solve common problems. Therefore, you won't have to reinvent the wheel.</p> <p>For example the package <code>pandas</code> is the go-to tool for data manipulation and analysis. With <code>pandas</code> you can read text and Excel files   among a lot of other formats and it offers a lot of functionality to manipulate and even plot your data. Hence, you will rarely see <code>Python</code> projects that are not dependent on <code>pandas</code>. Apart from <code>pandas</code> there are a wide variety of popular packages:</p> <ul> <li><code>scipy</code> - statistics (which will be covered in the next   course)</li> <li><code>tqdm</code> - build progress bars</li> <li><code>scikit-learn</code> - for machine learning</li> <li><code>numpy</code> - scientific computing</li> <li>... and many many more</li> </ul> <p>This section serves as a guide on how to install and manage packages. Additionally, the concept of virtual environments is explained.</p>"},{"location":"python-extensive/packages/#standard-library","title":"Standard library","text":"<p><code>Python</code> comes with a couple of modules which do not need to be installed and can be used 'out of the box'. For simplicity, we will call these modules packages as well. If you're interested in the difference between packages and modules, Real Python has a nice article on the topic. Here is an extensive list of all the packages that <code>Python</code> ships with.</p> <p>Let's use the <code>random</code> package to generate some random numbers.  First, we have to import the package with the following command:</p> <pre><code>import random\n\n# with the package imported we can use its functions\n# e.g., random integer (between 1 and 100)\nprint(random.randint(1, 100))\n</code></pre> &gt;&gt;&gt; Output<pre><code>42\n</code></pre> <p>Note, the output will be different when you run the code, since it is random.</p> <p>The corresponding documentation is available here. Generally speaking, almost all packages offer an online documentation page. It is good practice, to consult these documentation sites as they offer a lot of information on how to use their package and which methods/functions are available. Usually, functionalities are illustrated with examples that can be a good starting point for your project.</p> Info <p>If you remember, <code>random</code> was used in one of the previous  sections on control structures to generate  passwords of variable length.</p> Calculate the median <p>Use the built-in <code>statistics</code> package to calculate the median of  the below given values. Use Google to search for the <code>statistics</code>  documentation page and try to find the appropriate function.</p> <pre><code>values = [13, 58, 90, 34, 49, 41]\n</code></pre> <p>We will continue with another exercise.</p> Variance of random values <p>Generate a list of random values (can be integers and/or floats) and  calculate the variance. Hint: Use both the <code>random</code> and  <code>statistics</code> package.</p>"},{"location":"python-extensive/packages/#installing-packages","title":"Installing packages","text":"<p>To get access to all the packages available online, we need to install them using a package manager. One such manager is <code>pip</code> which is  automatically installed alongside <code>Python</code>. To check if <code>pip</code> is available  on your system open a new terminal within VSC by navigating in the menu bar  <code>Terminal</code> <code>New Terminal</code> </p> VSC Terminal <p>and execute the following command:</p> <pre><code>pip\n</code></pre> <p>... you should see a list of commands and their description:</p> <pre><code>Usage:   \n  pip &lt;command&gt; [options]\n\nCommands:\n  install                     Install packages.\n  download                    Download packages.\n  uninstall                   Uninstall packages.\n  freeze                      Output installed packages in requirements format.\n  inspect                     Inspect the python environment.\n  list                        List installed packages.\n  show                        Show information about installed packages.\n...\n</code></pre> Info <p>You can run shell commands directly from your notebook by using an  exclamation mark (<code>!</code>) as a prefix (e.g., <code>!pip</code>). However, in some cases, such as when uninstalling a package, this approach may cause issues.  Therefore, it's often recommended to use the terminal instead.</p> <p>Now, we'll install our first package, called <code>seaborn</code>. To install a package use pip's <code>install</code> command followed by the package name  (<code>pip install &lt;package-name&gt;</code>). Don't worry, it might take a couple of seconds.</p> <pre><code>pip install seaborn\n</code></pre> <p><code>seaborn</code> is a quite common package to visualize data. Now, run the following code to create your first plot. The code snippet was copied from the <code>seaborn</code> documentation here.</p> <pre><code># taken from https://seaborn.pydata.org/examples/grouped_boxplot.html\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\n\n# Load the example tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Draw a nested boxplot to show bills by day and time\nsns.boxplot(x=\"day\", y=\"total_bill\",\n            hue=\"smoker\", palette=[\"m\", \"g\"],\n            data=tips)\nsns.despine(offset=10, trim=True)\n</code></pre> <p>You don't have to fully understand the code snippet. It's more about the successful usage of a package. You might have noticed, that you didn't solely install <code>seaborn</code>. Among <code>seaborn</code>, <code>pip</code> also installed <code>pandas</code> (for data handling). We can 'verify' that by checking the type of <code>tips</code> (from the code snippet above).</p> <pre><code>print(type(tips))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> <p>Most of the time, a package does not 'stand on its own'. It uses the functionalities of other packages as well. In our case, <code>seaborn</code> also needs <code>pandas</code> to properly function. Hence, a lot of packages are dependent on each other.</p> Remove a package <p>Remove the <code>seaborn</code> package. Like above, use <code>pip</code> within a terminal to  list all commands and find the appropriate one. Execute the command  to remove the package.</p>"},{"location":"python-extensive/packages/#pypi","title":"PyPI","text":"<p>You might wonder where <code>pip</code> downloads the packages?! In short, all packages are downloaded from the Python Package Index (PyPI). That's where the open-source community (usually) publishes their packages. Simply put, if you type <code>pip install seaborn</code>, <code>pip</code> looks for a package called <code>seaborn</code> on PyPI and downloads it. <code>PyPI</code> is a valuable resource if you're searching for packages, certain versions, etc.</p>"},{"location":"python-extensive/packages/#virtual-environments","title":"Virtual environments","text":"<p>Previously, we have installed the package <code>seaborn</code>. The package itself was available system-wide as we did not create a virtual environment beforehand. This means, if you open a new folder/project and you select the same python kernel (typically the global python installation), the package will be available and you do not need to install it again. That might not sound too bad, but it's actually considered bad practice. But what is good practice and what the heck is a virtual environment?</p>"},{"location":"python-extensive/packages/#why","title":"Why?","text":"<p>To understand virtual environments, let's use an analogy from everyday life: cooking in a kitchen. Imagine you are baking two different cakes in the same kitchen. One is a regular chocolate cake, and the other must be gluten-free because someone has an allergy.</p> Allergic Reaction (Source: Tenor) <p>Even though both cakes are made in the same kitchen, you would not casually reuse the same bowls, spoons, and surfaces without cleaning them carefully. If flour from the regular cake gets into the gluten-free one, the result is ruined - and potentially harmful. </p> <p>So what do you do? You create separate, clean work areas with the exact ingredients and tools needed for each cake.</p> <p>A virtual environment in Python works the same way.</p> <ul> <li>Your computer is the kitchen.</li> <li>Each project is a different recipe.</li> <li>The packages (like seaborn, numpy, or pandas) are the ingredients.</li> </ul> <p>If all projects share the same global Python installation, it's like throwing all ingredients into one giant bowl. Sooner or later, versions clash, dependencies break, and one project can accidentally ruin another.</p> <p>A virtual environment gives each project its own clean workspace, with its own set of packages and versions, completely separated from other projects - even though everything still runs on the same computer.</p> Virtual Environment (Source: kelvininc) <p>That is why using virtual environments is considered best practice.</p> <p>To summarize, the <code>pip</code>/virtual environment combination facilitates:</p> <ul> <li>Dependency management: You can keep track of the packages that your   project needs to function. Packages are typically built on top of other packages.    For example, <code>seaborn</code> is built on top of <code>pandas</code> and <code>matplotlib</code>. If you want to use <code>seaborn</code>, you need to install <code>pandas</code> and <code>matplotlib</code> first and sometimes in a specific version.</li> <li>Version management: You can specify the exact versions of a package that   your project needs. This is important, because different versions of a    package may have different functionalities or bugs.</li> <li>Environment management: It's easier to work on multiple projects on a   single machine as you can install multiple versions of a package on a   per-project basis.</li> <li>Shareable: Your projects will be shareable with other developers as they   can easily install all dependencies with a single command. No more \"it worked   on my machine\" excuses!</li> </ul>"},{"location":"python-extensive/packages/#how","title":"How?","text":"<p>To work with virtual environments, you need to follow three steps: </p> The three steps to work with virtual environments <ol> <li>Create a virtual environment</li> <li>Activate the virtual environment</li> <li>Select the virtual environment as your Jupyter or Python kernel</li> </ol>"},{"location":"python-extensive/packages/#create-a-virtual-environment","title":"Create a virtual environment","text":"<p>To create a virtual environment, open a new command prompt within VSCode (you can use the shortcut Ctrl + Shift + <code>\u00f6</code>). Check if the terminal is opened in the correct folder. This should be your current project folder. If not, you can change the folder by typing <code>cd &lt;path/folder-name&gt;</code> in the terminal or by right clicking in the file explorer and selecting \"Open in Integrated Terminal\".</p> <p>Then execute the following command:</p> <pre><code>python -m venv .venv\n</code></pre> <p>This command creates a new folder called <code>.venv</code> within your project folder. Instead of <code>.venv</code> you can choose any name you want. However, this section assumes that you named it <code>.venv</code>.</p> Warning <p>The virtual environment folder should never be touched by the user. Initially a clean copy of your global Python installation will be created.  This includes absolute paths to the Python installation and the Python executable.  Therefore the virtual environment folder cannot be moved or sent to another machine.</p> <p>Furthermore, your Jupyter or Python files should always be in the project folder. NEVER in the virtual environment folder. A typical structure of a project folder might look like this:</p> <pre><code>project_folder/\n\u251c\u2500\u2500 .venv/\n\u251c\u2500\u2500 data/\n\u2514\u2500\u2500 my_script.ipynb\n</code></pre>"},{"location":"python-extensive/packages/#activate-an-environment","title":"Activate an environment","text":"<p>So far we have created the virtual environment. But that is not enough. We need to activate it in order to use it. Depending on your operating system, the command to activate the environment is slightly different.</p> Windows macOS/Linux / <p>As a Windows user type</p> <pre><code>.venv\\Scripts\\activate\n</code></pre> If an error occurs <p>If an error occurs (\"the execution of scripts is deactivated on this  system\") run</p> <pre><code>Set-ExecutionPolicy Unrestricted -Scope Process\n</code></pre> <p>... and use the previous command again.</p> <p>Type</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>to activate your environment.</p> <p>Once the environment is activated, you can see the name of the environment (here <code>.venv</code>) in the terminal.</p> <p></p> <p>From now on, every package you install from this activated terminal (for example with <code>pip install</code>) will be installed into the virtual environment. In VS Code or Jupyter you also need to select this virtual environment as the Python/Jupyter kernel for your notebook or script (see next section); this kernel selection is separate from activating the environment in the terminal. Use activation whenever you run terminal commands that should use the virtual environment. Note that once you close the terminal or VS Code, the environment in that terminal will be deactivated, but files that use the virtual-environment kernel will still run with the packages from that environment.</p> <p>Deactivating the environment is the same on all operating systems. To deactivate it, simply use</p> <pre><code>deactivate\n</code></pre> <p>in your command prompt/terminal.</p>"},{"location":"python-extensive/packages/#select-the-virtual-environment","title":"Select the virtual environment","text":"<p>So now we have created the virtual environment and activated it in order to install packages. Now the third and last step is to select the virtual environment for your file as Jupyter or Python kernel.</p> <p></p> Fit a machine learning model <p>Assuming your virtual environment is activated, try to get the following code cell running. </p> <pre><code>from matplotlib import pyplot  # (1)!\n\nfrom sklearn.datasets import fetch_california_housing  #(2)!\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\n\n# load data\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\n\n# fit a decision tree\ntree = DecisionTreeRegressor(\n    random_state=784, max_depth=2, min_samples_leaf=15\n)\ntree.fit(X, y)\n\n# visualize the tree\nplot_tree(tree, filled=True, feature_names=X.columns, proportion=True)\npyplot.show()\n</code></pre> <ol> <li><code>pyplot</code> is a submodule of <code>matplotlib</code> and can be directly imported      with the <code>from</code> statement.</li> <li>Or you can import functions (like <code>fetch_california_housing()</code>) directly     from its submodule <code>datasets</code>.</li> </ol> <p>Install the packages <code>matplotlib</code> and <code>scikit-learn</code> with <code>pip</code>. Then try to execute the code cell.</p> <p>Congratulations \ud83c\udf89, you've just fitted a machine learning model (simple decision tree) on a data set and visualized the model. That's the power of <code>Python</code> - easily accessible packages with a lot of functionality ready to use. \ud83e\uddbe</p> <p>Don't worry too much about the actual code lines above. Again, the important thing is to get the code running. With the above exercise, you've  reproduced the result from the motivational section.</p>"},{"location":"python-extensive/packages/#requirementstxt","title":"<code>requirements.txt</code>","text":"<p>As we have mentioned before, virtual environments are a great way to isolate project dependencies. However, sharing the whole virtual environment folder (e.g. <code>.venv</code>) is impractical. It often contains thousands of files, OS-specific binaries and absolute paths, so copying it to another machine or location usaually breaks. A better approach is to export the environment's installed packages to a simple text file that others can use to recreate the environment no matter if they are working on MacOS, Linux or Windows.</p> Export dependencies <p>Assume you want to share the code snippet from the previous task with  someone. First, your colleague might not know which packages you used to  get the code running. With no more information, one has to read the code  and manually determine which packages are necessary. To circumvent such situations, you export all your packages to a file.  Open a command prompt/terminal and execute</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>A <code>requirements.txt</code> is written which contains all your used  packages.</p> <p>Your colleague can now take the file and install all packages needed, at once.</p> <p><pre><code>pip install -r requirements.txt\n</code></pre> ... is the corresponding command.</p> Info <p>A <code>requirements.txt</code> file is a common way to share project dependencies. However, it will also help you, to restore your environment, in case  something goes wrong. Hence, keep your requirements file up-to-date.</p>"},{"location":"python-extensive/packages/#other-choicesoutlook","title":"Other choices?/Outlook","text":"<p>Apart from <code>pip</code> there are a couple of other package managers available. For example, there are</p> <ul> <li><code>uv</code></li> <li><code>pipenv</code></li> <li><code>poetry</code></li> <li><code>miniconda</code></li> </ul> <p>... and this is by no means an extensive list. All of these tools let you install and manage packages. Nevertheless, they have their differences. In the end, it is up to you, the developer which tool fits best. <code>pip</code> is always a solid choice (and the go-to choice to get the hang of package/virtual environment management). However, if you're working on larger scale projects with a couple of other developers, one of these package managers might offer some functionalities which facilitates the development workflow.</p>"},{"location":"python-extensive/packages/#recap","title":"Recap","text":"<p>In this section, you have learned how to install packages and manage them  within virtual environments. The topics covered:</p> <ul> <li><code>pip</code></li> <li>How to install/uninstall packages</li> <li>PyPI - the package hub</li> <li>Concept and benefits of virtual environments</li> <li>Creation and basic usage of a virtual environment</li> </ul>"},{"location":"python-extensive/pandas/","title":"Pandas","text":""},{"location":"python-extensive/pandas/#pandas","title":"<code>pandas</code>","text":"Info <p>This section is heavily based on the excellent 10 minutes to pandas  guide.</p>"},{"location":"python-extensive/pandas/#the-data-set","title":"The data set","text":"<p>We will use a custom Spotify data set, containing the current<sup>1</sup> top 50  songs in Austria. You can find the corresponding playlist here.</p> Info <p>If you're interested in the creation of the data set, you can find the code  below. Note, <code>pandas</code> was the only package needed, and we will cover some of  the used functionalities in this section.</p> Create Spotify data set <pre><code># Create a Spotify data set, containing the top 50 tracks in Austria\n\n# Original data (Top Spotify Songs in 73 Countries (Daily Updated)) from:\n# https://www.kaggle.com/datasets/asaniczka/top-spotify-songs-in-73-countries-daily-updated/data\nfrom pathlib import Path\n\nimport pandas as pd\n\n# read initial data set\ndata = pd.read_csv(Path(\"data/universal_top_spotify_songs.csv\"))\n\n# only Austrian chart topping songs\ndata = data[data[\"country\"] == \"AT\"]\n\n# subset by latest snapshot date\nlatest_snapshot = data[\"snapshot_date\"].max()\ndata = data[data[\"snapshot_date\"] == latest_snapshot]\n# sort by daily_rank\ndata = data.sort_values(by=\"daily_rank\").reset_index(drop=True)\n\n# write data to csv\ndata.to_csv(Path(\"data/spotify-top50.csv\"), index=False)\n\n# excerpt of the data set for inclusion in markdown\nwith Path(\"data/spotify-top50.md\").open(\"w\", encoding=\"UTF-8\") as f:\n    prefix = (\n        f\"Excerpt of the data set (snapshot date: **{latest_snapshot}**):\\n\\n\"\n    )\n    _data = data[\n        [\n            \"daily_rank\",\n            \"name\",\n            \"artists\",\n            \"popularity\",\n            \"is_explicit\",\n            \"energy\",\n        ]\n    ]\n    # only top 5 songs\n    _data = _data.head(5)\n    markdown = _data.to_markdown(index=False)\n    f.write(prefix + markdown)\n</code></pre> <p>Excerpt of the data set (snapshot date: 2024-09-25):</p> daily_rank name artists popularity is_explicit energy 1 The Emptiness Machine Linkin Park 93 True 0.872 2 Rote Flaggen Berq 76 True 0.336 3 Bauch Beine Po Shirin David 80 True 0.746 4 Die With A Smile Lady Gaga, Bruno Mars 100 False 0.592 5 BIRDS OF A FEATHER Billie Eilish 99 False 0.507 <p>Download the whole data set to follow this section:</p> <p>Spotify Top 50 Austria </p>"},{"location":"python-extensive/pandas/#prerequisites","title":"Prerequisites","text":"<p>For this section, we recommend, to make a new project folder with a  Jupyter notebook. Additionally, create a new virtual environment and  activate it. Please refer to the previous section on packages and virtual  environments if you're having trouble. Lastly, install <code>pandas</code>.</p> <p>You should end up with a project structure similar to the following:</p> <pre><code>\ud83d\udcc1 pandas-course/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc4 spotify-top50.csv\n\u2514\u2500\u2500 \ud83d\udcc4 pandas-course.ipynb\n</code></pre>"},{"location":"python-extensive/pandas/#tabular-data","title":"Tabular data","text":"<p>Before we dive into <code>pandas</code>, let's briefly discuss tabular data. At its simplest, tabular data consists of rows and columns. Looking at the Spotify  table above; each row contains information about a specific track  (e.g., <code>name</code>, <code>artists</code>), while each column represents a specific attribute  (e.g., <code>popularity</code>, <code>energy</code>).</p> <p>Tabular data has a clear structure which makes it easy to work with. On the other hand sources for tabular data can be manifold. However, one of the most  common format is the XLSX ( - Excel) or CSV  ( - Comma Separated Values) format which is the one we are working with in this chapter. Nevertheless, tabular data is  also present in various other text based formats like TXT, TSV or even in  databases (e.g. MySQL, PostgreSQL).</p> <p>No matter the source, <code>pandas</code> is the go-to tool to work with tabular data.</p>"},{"location":"python-extensive/pandas/#getting-started","title":"Getting started","text":"<p>Let's explore some of <code>pandas</code> functionalities on the example of the Spotify data set. First, we need to import the package.</p> <pre><code>import pandas as pd\n</code></pre> <p>The <code>as</code> statement is used to create an alias for the package in  order to quickly reference it within our next code snippets. An alias  simply reduces the amount of characters you have to type. Moreover, the  alias <code>pd</code> is commonly used for <code>pandas</code>. Therefore, you can more  easily  employ code snippets you find online.</p>"},{"location":"python-extensive/pandas/#reading-files","title":"Reading files","text":"<p>With the package imported, we can already read the data set (given as <code>.csv</code>).</p> <pre><code>data = pd.read_csv(\"spotify-top50.csv\")\n</code></pre> <p>The above code snippet assumes, that both data set and notebook are located at the same directory level. Else, you have to adjust the path  <code>\"spotify-top50.csv\"</code> accordingly.</p> <p>Besides <code>.csv</code> files, <code>pandas</code> supports reading from various other file types like Excel, text files or a SQL database. The <code>pandas</code> documentation  provides a comprehensive overview of different file types and their corresponding function. Have a look, to get an idea which file formats are supported not  only for reading but also for writing. </p>"},{"location":"python-extensive/pandas/#displaying-data","title":"Displaying data","text":"<p>With a data set at hand, we will most likely want to view it. To view the  first rows of our data frame use the <code>head()</code> method.</p> <pre><code>print(data.head())\n</code></pre> &gt;&gt;&gt; Output<pre><code>               spotify_id                   name  ...    tempo  time_signature\n0  2PnlsTsOTLE5jnBnNe2K0A  The Emptiness Machine  ...  184.115               4\n1  7bkUa9kDFGxgCC7d36dzFI           Rote Flaggen  ...  109.940               3\n2  64f3yNXsi2Vk76odcHCOnw         Bauch Beine Po  ...  123.969               4\n3  2plbrEY59IikOBgBGLjaoe       Die With A Smile  ...  157.969               3\n4  6dOtVTDdiauQNBQEDOtlAB     BIRDS OF A FEATHER  ...  104.978               4\n</code></pre> <p>To display the last rows of the data frame, use the <code>tail()</code> method.</p> <pre><code>print(data.tail())\n</code></pre> &gt;&gt;&gt; Output<pre><code>                spotify_id  ... time_signature\n45  6leQi7NakJQS1vHRtZsroe  ...              4\n46  5E4jBLx4P0UBji68bBThSw  ...              4\n47  6qzetQfgRVyAGEg8QhqzYD  ...              4\n48  3WOhcATHxK2SLNeP5W3v1v  ...              4\n49  7xLbQTeLpeqlxxTPLSiM20  ...              4\n</code></pre> <p>Columns can be viewed with:</p> <pre><code>print(data.columns)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Index(['spotify_id', 'name', 'artists', 'daily_rank', 'daily_movement',\n       'weekly_movement', 'country', 'snapshot_date', 'popularity',\n       'is_explicit', 'duration_ms', 'album_name', 'album_release_date',\n       'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n       'time_signature'],\n      dtype='str')\n</code></pre> <p>Similarly, we can print the row indices.</p> <pre><code>print(data.index)\n</code></pre> <p>&gt;&gt;&gt; Output<pre><code>RangeIndex(start=0, stop=50, step=1)\n</code></pre> This data set is consecutively indexed from <code>0</code> to <code>49</code>. If  you recall, a range does not include its <code>stop</code> value (<code>50</code>).</p> <p>By default, (if not otherwise specified) <code>pandas</code> will assign a range index to a data set in order to label the rows.</p> <p>The data set dimensions are accessed with the <code>shape</code> attribute.</p> <pre><code>print(data.shape)\n</code></pre> &gt;&gt;&gt; Output<pre><code>(50, 25)\n</code></pre> <p>The data set has <code>50</code> rows and <code>25</code> columns.</p>"},{"location":"python-extensive/pandas/#data-structures","title":"Data structures","text":"<p><code>pandas</code> has two main data structures: <code>Series</code> and <code>DataFrame</code>. As you would expect, a <code>DataFrame</code> is a two-dimensional data structure such as  our whole Spotify data set assigned to the variable <code>data</code>:</p> <pre><code>print(type(data))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> <p>Whereas, a single column of a <code>DataFrame</code> is referred to as a <code>Series</code>.  Generally, selections of the <code>DataFrame</code> can be accessed with square  brackets (<code>[]</code>). To get a column, you can simply use its name.</p> <pre><code>print(data[\"artists\"])\n\nprint(type(data[\"artists\"]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>0                                       Linkin Park\n1                                              Berq\n2                                      Shirin David\n3                             Lady Gaga, Bruno Mars\n...                                             ...\n\n&lt;class 'pandas.core.series.Series'&gt;\n</code></pre> <p>A <code>DataFrame</code> is composed of at least one <code>Series</code>.</p>"},{"location":"python-extensive/pandas/#detour-series-and-dataframe-from-scratch","title":"Detour: <code>Series</code> and <code>DataFrame</code> from scratch","text":"<p>It's not always the case that you have a data set (in form of a file) at hand. Sometimes you have to create a <code>Series</code> or <code>DataFrame</code> yourself.</p> <p>A <code>Series</code> can be easily created from a list.</p> <pre><code>austrian_artists = [\"Bibiza\", \"Wanda\", \"Bilderbuch\"]\naustrian_artists = pd.Series(austrian_artists)\n</code></pre> <p>To initiate a <code>DataFrame</code>, you can use a dictionary (among others).</p> <pre><code>austrian_artists = {\n    \"name\": [\"Bibiza\", \"Wanda\", \"Bilderbuch\"],\n    \"album\": [\"bis einer weint\", \"Amore\", \"mea culpa\"],\n    \"release_year\": [2024, 2014, 2019]\n}\naustrian_artists = pd.DataFrame(austrian_artists)\n</code></pre> <p>Dictionary keys are used as column names and the corresponding values as the column values.</p> Info <p>Apart from a <code>dict</code>, a <code>DataFrame</code> can be created from multiple other data structures like a <code>list</code> or <code>tuple</code>. For an  extensive guide, visit the <code>pandas</code> documentation on Intro to data  structures (specifically the  section DataFrame). </p>"},{"location":"python-extensive/pandas/#selecting-data","title":"Selecting data","text":"<p>Let's dive deeper into selecting data. To access specific rows, you can use  a slice (just like with lists).</p> <pre><code># rows 5 and 6\nprint(data[5:7])\n</code></pre> &gt;&gt;&gt; Output<pre><code>               spotify_id        name  ...    tempo  time_signature\n5  0io16MKpbeDIdYzmGpQaES  Embrace It  ...  114.933               4\n6  3aJT51ya8amzpT3TKDVipL         FTW  ...   91.937               4\n</code></pre> <p>Select multiple columns by passing a list of column names.</p> <pre><code>print(data[[\"name\", \"artists\"]])\n</code></pre> &gt;&gt;&gt; Output<pre><code>                                  name    artists\n0                The Emptiness Machine    Linkin Park\n1                         Rote Flaggen    Berq\n2                       Bauch Beine Po    Shirin David\n...\n</code></pre>"},{"location":"python-extensive/pandas/#boolean-indexing","title":"Boolean indexing","text":"<p>Most of the time, we want to filter the data based on criterias.  For example, we can select the tracks with a tempo higher than <code>120</code> beats per minute (BPM).</p> <pre><code>high_tempo = data[data[\"tempo\"] &gt; 120]\n</code></pre> <p>Let's break the example down:</p> <ul> <li>First, we select the column <code>tempo</code> from the data set with <code>data[\"tempo\"]</code>.</li> <li>Next, we expand our expression to <code>data[\"tempo\"] &gt; 120</code>.   This will return a <code>Series</code> of boolean values.</li> <li>Lastly, we wrap the expression in another set of square brackets to    filter the whole data set based on our boolean values.</li> </ul> <p>We end up with 27 tracks that meet the criteria. <code>high_tempo</code> is a new  <code>DataFrame</code> containing entries that exceed <code>120</code> BPM.</p> Danceable tracks <p> Saturday Night Fever </p> <p>We assume that tracks with a danceability score higher than <code>0.8</code> are danceable. </p> <p>How many of the tracks are danceable?</p> <p>Danceability describes how suitable a track is for dancing based on a  combination of musical elements including tempo, rhythm stability, beat  strength, and overall regularity. A value of 0.0 is least danceable and  1.0 is most danceable.</p> <p>-- Spotify for Developers</p>"},{"location":"python-extensive/pandas/#mathematical-operations","title":"Mathematical operations","text":"<p><code>pandas</code> supports mathematical operations on both <code>Series</code> and <code>DataFrame</code>. For instance, we weigh the popularity of a track by its energy level.</p> <p>Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. </p> <p>-- Spotify for Developers</p> <pre><code>weighted_popularity = data[\"popularity\"].mul(data[\"energy\"])\nprint(type(weighted_popularity))\n\n# assign the Series to the DataFrame\ndata[\"weighted_popularity\"] = weighted_popularity\n</code></pre> <p>The <code>mul()</code> method is used to multiply the <code>popularity</code> and <code>energy</code> columns. The resulting <code>Series</code> is assigned to <code>data</code> as a new column.</p> &gt;&gt;&gt; Output<pre><code>&lt;class 'pandas.core.series.Series'&gt;\n</code></pre> Track length <p> </p> <p>Since songs are getting shorter and shorter, we want to know how long the tracks in our data set are.  To do so, calculate the length in minutes.</p> <ul> <li>Explore the given data set to find the appropriate column (which    holds information on the song length). </li> <li>Calculate the length in minutes (hint: use the <code>pandas</code> documentation    or Google.)</li> <li>Assign the result to the data frame.</li> <li>Use boolean indexing, to check if there are any tracks longer than    <code>4</code> minutes.</li> <li>Lastly, calculate the average track length in minutes.</li> </ul>"},{"location":"python-extensive/pandas/#basic-statistics","title":"Basic statistics","text":"<p><code>pandas</code> provides a variety of methods to calculate basic statistics. For  instance, <code>min()</code>, <code>max()</code>, <code>mean()</code> can be easily retrieved for a numeric  <code>Series</code> in the data set.</p> <pre><code>print(data[\"tempo\"].min())\n</code></pre> &gt;&gt;&gt; Output<pre><code>80.969\n</code></pre> <p>Conveniently, statistics can be calculated for each column at once using  the <code>DataFrame</code>. In this example, we calculate the standard deviation.</p> <pre><code>print(data.std())\n</code></pre> <p>If you execute the above snippet, a <code>TypeError</code> is raised.</p> Fix the error <p>Try to determine, why the error was raised in the first place. Now, circumvent/fix the error.</p> <p>Hint: Look at the documentation of the <code>std()</code> method and its  parameters.</p> <p>If you want to calculate multiple statistics, you can call the <code>describe()</code>  method.</p> <pre><code>stats = data.describe()\nprint(stats)\n</code></pre> &gt;&gt;&gt; Output<pre><code>       daily_rank  daily_movement  ...       tempo  time_signature\ncount    50.00000        50.00000  ...   50.000000       50.000000\nmean     25.50000         1.04000  ...  125.087260        3.920000\nstd      14.57738         8.14902  ...   26.751323        0.340468\nmin       1.00000       -22.00000  ...   80.969000        3.000000\n25%      13.25000        -3.00000  ...  104.990750        4.000000\n50%      25.50000         1.00000  ...  123.981500        4.000000\n75%      37.75000         3.00000  ...  137.487250        4.000000\nmax      50.00000        29.00000  ...  184.115000        5.000000\n</code></pre> <p><code>describe()</code> provides descriptive statistics for each column. The result of  <code>describe()</code> is a <code>DataFrame</code> itself.</p>"},{"location":"python-extensive/pandas/#other-functionalities","title":"Other functionalities","text":"<p><code>pandas</code> offers a plethora of functionalities. There's simply too much to  cover in a brief introductory section. Still, there are some common  <code>DataFrame</code> methods/properties that are worth mentioning:</p> <ul> <li><code>sort_values()</code>: Sort the data frame by a specific column.</li> <li><code>groupby()</code>: Group the data frame by a column.</li> <li><code>merge()</code>: Merge two data frames.</li> <li><code>T</code>: Transpose of the data frame.</li> <li><code>drop_duplicates()</code>: Remove duplicates.</li> <li><code>dropna()</code>: Remove missing values.</li> </ul> <p>All methods are linked to its corresponding documentation with examples  that help you get started.</p>"},{"location":"python-extensive/pandas/#recap","title":"Recap","text":"<p>We covered <code>pandas</code> and some selected functionalities which should provide  you with a solid foundation to work with tabular data sets. Moreover, you  should be able to follow the code portions in the upcoming courses more easily.</p> <ol> <li> <p>The full data set is available on Kaggle and contains the most streamed songs for multiple different countries. For our purpose, the data was subset.\u00a0\u21a9</p> </li> </ol>"},{"location":"python-extensive/plotting/","title":"Plotting","text":""},{"location":"python-extensive/plotting/#plotting","title":"Plotting","text":"<p>With a couple of practical examples, we will discover tips on how to generate a plot, and outline the various plotting methods and format styles you can explore.</p> Info <p>If you're interested in the creation of the sine-graph above, you can find the code  below. Note, for the data generation the <code>numpy</code> package is used as well as the  <code>matplotlib</code> package for data visualization. </p> <p>We will cover some of the used functionalities and formatting styles in this section.</p> Create Visualization <pre><code># import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definition of variables\nphi_min = 0                # definition of starting angle in degrees\nphi_max = 360              # definition of final angle in degrees\nn = 100                    # number of points\n\n# Calculations and data generation with numpy\n# Gererate time-vector\nt = np.linspace(np.radians(phi_min), np.radians(phi_max), n, endpoint=True)  \ny = np.sin(t)  \n\n# Visualization with matplotlib\nplt.rcParams['text.usetex'] = False    # if True use LATEX font type\n\nplt.figure()\nplt.plot(t, y, 'k')\n\n# Labels for the x- and y-axis\nplt.xlabel(r'Angle $\\theta$ in degrees', fontsize=12) \nplt.ylabel(r'Sine($\\theta$)', fontsize=12)\n\n# Change axes\nstartx, endx = np.radians(phi_min), np.radians(phi_max)\nstarty, endy = -1.1, 1.1\nplt.axis([startx, endx, starty, endy])\n\n# Add grid\nplt.grid()\n\n# Change scale of axes\nax = plt.gca()\naxis_x = np.array([0, 90, 180, 270, 360])\naxis_x = np.radians(axis_x)\nplt.xticks(axis_x, [360, 450, 540, 630, 720])\n\n# Add legend\nax.legend([r\"Sine($\\theta$)\"], loc=\"lower left\", fontsize=13) \n\n# Add title\nplt.title(r\"$sin(\\theta) = cos(\\theta - 90^\\circ)$\", fontsize=24)\n\n# Add text using x- and y-coordinates\nplt.text(3.5, 0.35, r'$1^\\circ=\\frac{2\\pi}{360}~rad$', fontsize=13)\n\n# Show graph\nplt.show()\n</code></pre>"},{"location":"python-extensive/plotting/#introduction","title":"Introduction","text":"Info <p>We will give a brief introduction on plotting data with <code>pandas</code>, which is built on the package <code>matplotlib</code>(the corresponding documentation is available here).</p> <p>This chapter is an extension to the previous <code>pandas</code>  chapter. Therefore, we will use the Spotify data set and we assume that you have imported the data already (to download the data set and reading the file see <code>pandas</code>). </p> <p>This chapter should equip you with the necessary skills to generate various visualizations for your data analysis.  </p>"},{"location":"python-extensive/plotting/#getting-started","title":"Getting started","text":"<p>First, install <code>matplotlib</code> as it is required for <code>pandas</code>' plotting  functionalities. Additionally, you can use <code>matplotlib</code> to customize your  figures, but more on that later. Import both packages to start plotting.</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"python-extensive/plotting/#2-d-plots","title":"2-D plots","text":"<p>Figures can be generated directly using a <code>DataFrame</code>. Simply call its <code>plot()</code> method. The <code>x</code> and <code>y</code> attributes refer to the values of the x and  y-axis. </p> <pre><code>data.plot(x='daily_rank', y='popularity')\n</code></pre> <p>If you visualize a DataFrame, you plot all columns as multiple lines. If the  <code>x</code> values are not explicitly stated, the index of the <code>DataFrame</code> is  utilized. In our case, the index and hence the x-values start with zero and  end with the number of rows minus one (<code>range(0, number of rows)</code>). </p> <pre><code># Mathematical operations (see pandas)\ndata[\"weighted_popularity\"] = data[\"popularity\"].mul(data[\"energy\"]) \ndata_plot = data[['popularity', 'weighted_popularity']]\ndata_plot.plot()\n</code></pre> <p>Note: The <code>plt.show()</code> function in a <code>py</code> script opens one or more interactive windows to show the graphs. For jupyter notebooks the command is not necessary because the graph is enbedded in the document.</p>"},{"location":"python-extensive/plotting/#formatting","title":"Formatting","text":"<p><code>pandas</code> offers a a range of pre-configured plotting styles. You can use plot style arguments to perform format changes by simply adding it to the <code>plot</code> function. There are some common <code>plot</code> arguments that are worth mentioning (for more detail see also the <code>pandas</code> documentation or Google.):</p> <ul> <li><code>style</code>: Color and style of lines or markers (see table below).</li> <li><code>linewidth</code>: Changing thickness of the line. </li> <li><code>legend</code>: Description of the elements in a plot (<code>loc</code> for the location using <code>plt.legend()</code>).</li> <li><code>grid</code>: Adding a grid.</li> <li><code>xlabel</code>, <code>ylabel</code>: Labeling the x- and y-axis.</li> <li><code>xticks</code>, <code>yticks</code>: Changig the annotation of the axis.</li> <li><code>axis</code>: Changing the range of the axis (<code>xlim</code>, <code>ylim</code> individually). </li> <li><code>secondary_y</code>: Adding additional plot.</li> <li><code>subplots</code>: Generating individual plots for each column (<code>layout</code> for the <code>subplots</code>).</li> <li><code>title</code>: Adding a title (set <code>fontsize</code>).</li> <li><code>figsize</code>: Changing the size of the plot.</li> </ul> <p>The following table shows additional arguments for different colors, line styles and marker types.</p> Initials Description Initials Description Initials Description <code>y</code> yellow <code>-</code> solid line <code>+</code> plus-marker <code>m</code> magenta <code>--</code> dashed <code>o</code> circle <code>c</code> cyan <code>:</code> dotted <code>*</code> asterisk <code>r</code> red <code>-.</code> dotdashed <code>.</code> point <code>g</code> green <code>x</code> cross <code>b</code> blue <code>s</code> square <code>w</code> white <code>d</code> diamant <code>k</code> black <code>&gt;</code>, <code>&lt;</code>, <code>^</code>, <code>v</code> triangle"},{"location":"python-extensive/plotting/#mci-wing-formatting-standards","title":"MCI | WING: Formatting standards","text":"Info <p>For laboratory reports and final papers formatting standards exist (see <code>Academic Walkthrough - Formakriterien f\u00fcr schrifliche Abgaben - Abbildungen und Diagramme</code>).  For example:</p> <ul> <li>The line colors are usually set to black or gray with different line styles for black/white printing. </li> <li>The legend is necessary to identify different data series in one graph.</li> <li>For axis labelling the following information is mandatory: the variable name and unit.</li> </ul> <pre><code>data_plot.plot(style=['k-','k--'], xlim = (0,49), ylim=(0, 110), linewidth=0.8, \n               grid=True, xlabel='daily rank', ylabel='popularity points')\nplt.legend(loc='lower right')\n</code></pre> Formatting line plots <p>We want to analyse the tempo of our tracks.</p> <p>Generate a line plot of the tempo (<code>y</code> argument) and the <code>daily_rank</code> as the horizontal axis (<code>x</code> argument). Change the format to the following:</p> <ul> <li>Set the line color to black</li> <li>Delete the legend (hint: set legend to <code>False</code>)</li> <li>Set the labels of the x- and y-axis</li> <li>Set the range of the x-axis from 1 to 50</li> </ul>"},{"location":"python-extensive/plotting/#statistical-plots","title":"Statistical plots","text":"<p><code>pandas</code> supports statistical plots, which present results of the statistical data analysis.  The following table shows some of these plotting methods, which are provided with the <code>kind</code> argument in the <code>plot()</code> function or using the mehod <code>DataFrame.plot.&lt;kind&gt;</code> instead. </p> Initials Description <code>hist</code> histogram (<code>bins</code> change number, <code>density</code> for probability?) <code>scatter</code> scatter plot (two variables representing the x- and y-values) <code>bar</code> bar plot for labeled, not time-series data (<code>stacked</code> for multiple bars/columns) <code>barh</code> horizontal bar plots (also used for gantt charts) <code>box</code> boxplot shows distribution of values within each column (<code>by</code> for grouping) <code>kde</code>, <code>density</code> density plot <code>area</code> area plot <code>pie</code> pie chart (percent of categorical data) <p>Some of the formatting arguments can be used for statistical plots. For a detailed description see the <code>pandas</code>documentation.</p>"},{"location":"python-extensive/plotting/#histogram","title":"Histogram","text":"<p>The histogram counts the number of values in each bin. The range of the bin and the number of bins can be changed using the <code>range</code> and <code>bins</code> argument. Histograms of multiple columns can be drawn at once using <code>subplots</code>. </p> <pre><code>data_plot = data[['liveness', 'acousticness']]\ndata[['liveness', 'acousticness']].plot(kind='hist', layout=(1,2), figsize=(10, 4), subplots=True, \n                                        color='k', alpha=0.5)\n</code></pre> Change arguments <p>Make the following changes to the histogram above and see what happens.</p> <ul> <li>Generate a probability density function (hint: use the argument <code>density</code>).</li> <li>Add the argument <code>bins</code> and change the number of bins to 20.</li> </ul>"},{"location":"python-extensive/plotting/#scatter-plot","title":"Scatter plot","text":"<p>The scatter plot can be used to show correlations between two variables. Therefore, the horizontal (<code>x</code> argument) and vertical (<code>y</code> argument) coordinates are defined by two columns of the <code>DataFrame</code>.</p> <pre><code>data.plot.scatter(x='loudness', y='energy', color='k')\n</code></pre> Scatter plot  <p>Generate a scatter plot to show, if there is a relationship between the variabel <code>speechiness</code>and the variable <code>tempo</code>.</p>"},{"location":"python-extensive/plotting/#detour-data-categorisation-for-statistical-analysis","title":"Detour: Data categorisation for statistical analysis","text":"<p><code>Categorical</code> data in <code>pandas</code> correspond to categorical variables in statistics. This data type has a limited number of possible values, which are called <code>categories</code>. </p> <p>For example, some artists have more than one song in this list. The calculation of the maximum number of tracks one artist has, can be generated as follows.</p> <pre><code>number_artists = pd.Categorical(data['artists']).value_counts()\nprint(number_artists.max())\nprint(number_artists[number_artists == 3])\n</code></pre> &gt;&gt;&gt; Output<pre><code>3\nLACAZETTE    3\nName: count, dtype: int64\n</code></pre> <p>Let's break the example down:</p> <ul> <li>The datatype <code>Categorical</code> shows a list of unique artists (output: 46 different artists).</li> <li>The method <code>value_counts()</code> creates a <code>Series</code> with the number of counts for each artist.</li> <li>The method <code>max()</code> calculates the maximum number of tracks for one artist in the data set.</li> <li>Lastly, we use boolean indexing to show the name of the artist.</li> </ul> <p>The <code>cut()</code> method discretizes data according to intervals (<code>bins</code>) and chosen names (<code>labels</code>).  The <code>DataFrame</code> can easily be extended by the <code>Series</code> with <code>categorical</code> data.</p> <pre><code>data['tempo_cat'] = pd.cut(x=data['tempo'], bins=[0, 110, 140, 200], \n                           labels=['slow', 'medium', 'fast'])\n</code></pre> <p><code>Categorical</code> data can be used for grouping in box plots, as we will see below.</p> <p>Further we can generate a <code>DataFrame</code> which contains the different categories in the first column and the number of counts in the second column using the <code>value_counts()</code> method. </p> <pre><code>data_count = pd.DataFrame(data['tempo_cat'].value_counts())\ndata_count\n</code></pre> &gt;&gt;&gt; Output<pre><code>tempo_cat   count\nmedium      24\nslow        15\nfast        11\n</code></pre> <p>We will see, that this <code>DataFrame</code> can be used for statistical graphs like <code>bar</code> plots or <code>pie</code> charts.</p>"},{"location":"python-extensive/plotting/#box-plot","title":"Box plot","text":"<p>The boxplot is generated to visualize the statistical values for each column of a <code>DataFrame</code>.  <code>pandas</code> also supports many arguments of the <code>matplotlib</code> package for boxplots (look at the <code>matplotlib</code> documentation). </p> <pre><code>data.plot.box(column=['popularity','weighted_popularity'], by='tempo_cat', \n              color='k', ylabel='points', figsize=(10,4))\n</code></pre> Box plot  <p>Generate a box plot to show the difference between the variables <code>acousticness</code>, <code>speechiness</code> and <code>liveness</code>.</p>"},{"location":"python-extensive/plotting/#bar-plot","title":"Bar plot","text":"<p>The bar plot presents rectangular bars, which represent the values of the <code>DataFrame</code> for different categories (<code>x</code> axis).</p> <pre><code>data_count.plot.bar(color='k', alpha=0.5, legend=False, xlabel='tempo', \n                    ylabel='count')\n</code></pre>"},{"location":"python-extensive/plotting/#pie-chart","title":"Pie chart","text":"<p>The pie chart shows the percentage of each category from the absolute values of the count table.  The different formatting styles for the pie chart can be done using the <code>autopct</code> argument (for more information see the <code>matplotlib</code> documentation).  </p> <pre><code>data_count.plot.pie(y='count', autopct=\"%1.0f%%\")\n</code></pre> Danceable tracks <p> Saturday Night Fever </p> <p>We assume that tracks with a danceability score higher than <code>0.8</code> are most danceable and the tracks less than <code>0.7</code> are less danceable. </p> <ul> <li>Categorize the data using the categories <code>less_danceable</code>, <code>danceable</code> and <code>most_danceable</code>.</li> <li>Generate a boxplot to explore the relationship between the <code>tempo</code> grouped by the different categories for <code>danceability</code>.</li> <li>Display the number of tracks for each category in a <code>DataFrame</code> (hint: use the <code>value_counts()</code> method).</li> <li>Visualize the number of tracks for each category with a <code>bar</code> chart.</li> <li>Visualize the number of tracks for each category in a <code>pie</code> chart.</li> </ul>"},{"location":"python-extensive/plotting/#recap","title":"Recap","text":"<p>We provided the basis to generate good looking visualization of your data analysis using <code>pandas</code>.  The introduced functionalities and arguments can be used to change the format of your graphs to your liking. </p>"},{"location":"python-extensive/streamlit/","title":"Building Web Applications with Streamlit","text":""},{"location":"python-extensive/streamlit/#introduction","title":"Introduction","text":"<p>As the final topic in our <code>Python</code>  course, we introduce <code>streamlit</code>,  a powerful framework for building web applications. It allows you to turn your Python scripts into  interactive web apps with minimal effort, handling most of the complexities of web development in the background. This knowledge will prove invaluable when showcasing your data analysis or machine learning projects.</p> Info <p>At the time of writing, <code>streamlit</code> version <code>1.41.0</code> was used. Keep in mind that <code>streamlit</code> is actively developed and some functionalities might change in the future. However, as always, we try to keep the content up-to-date.</p> <p>This section is based on the excellent Streamlit documentation.</p>"},{"location":"python-extensive/streamlit/#prerequisites","title":"Prerequisites","text":"<p>For this section, it's best to create a new project folder and a fresh virtual environment, then activate it.  Refer back to the packages and virtual environments section if you need a refresher. Next, create a file named <code>app.py</code>.</p> <p>You should end up with a project structure similar to the following:</p> <pre><code>\ud83d\udcc1 streamlit/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u2514\u2500\u2500 \ud83d\udcc4 app.py\n</code></pre> <p>Finaly, install <code>streamlit</code>:</p> <pre><code>pip install streamlit\n</code></pre> <p>To verify the installation, run:</p> <pre><code>streamlit hello\n</code></pre> <p>This command launches Streamlit's built-in demo app in your default web browser. To stop the app, go back to the terminal window and press: Ctrl+C.</p> Switch from Jupyter to standalone Python? <p>Up until now, we\u2019ve primarily used Jupyter Notebooks (<code>.ipynb</code>) for experimentation and data analysis. However, to build shareable, user-friendly applications, standalone Python scripts (<code>.py</code>) are typically more suitable. Streamlit enables you to create interactive web apps directly from Python files, adding a professional edge and new level of interactivity to your projects.</p>"},{"location":"python-extensive/streamlit/#your-first-streamlit-app","title":"Your First Streamlit App","text":"<p>Let\u2019s create a basic Streamlit application. In app.py, add the following code:</p> <pre><code>import streamlit as st\n\nst.title(\"My First Streamlit App\")\nst.write(\"Welcome to Streamlit!\")\n</code></pre> <p>To run the app, execute the following command in your terminal:</p> <pre><code>streamlit run app.py\n</code></pre> <p>Streamlit will open a new tab in your web browser and display your app.</p> Hot Reloading <p>Streamlit automatically detects changes in your source code and updates the web  application in real time. If you notice the hot reloading feature isn\u2019t working as expected, use the \"Rerun\" button in the upper-right corner of the Streamlit app or enable the \"Always rerun\" option.</p>"},{"location":"python-extensive/streamlit/#basic-elements","title":"Basic Elements","text":"<p>Streamlit provides various elements to build your web interface. Let's explore some of the most  commonly used ones.</p>"},{"location":"python-extensive/streamlit/#text-elements","title":"Text Elements","text":"<pre><code>import streamlit as st\n\n# Display text\nst.title(\"Main Title\")\nst.header(\"Header\")\nst.subheader(\"Subheader\")\nst.text(\"Simple text\")\nst.markdown(\"**Bold** and *italic* text\")\n\n# Information boxes\nst.info(\"Info message\")\nst.warning(\"Warning message\")\nst.error(\"Error message\")\nst.success(\"Success message\")\n</code></pre>"},{"location":"python-extensive/streamlit/#input-widgets","title":"Input Widgets","text":"<p>Streamlit offers multiple widgets for capturing user input:</p> <pre><code># Text input\nname = st.text_input(\"Enter your name\")\nif name:\n    st.write(f\"Hello, {name}!\")\n\n# Numeric input\nage = st.number_input(\"Enter your age\", min_value=0, max_value=120, value=25)\n\n# Slider\ntemperature = st.slider(\"Select temperature\", min_value=-10.0, max_value=40.0, value=20.0)\n\n# Checkbox\nagree = st.checkbox(\"I agree to the terms\")\nif agree:\n    st.write(\"Thank you for agreeing!\")\n\n# Select box\noption = st.selectbox(\n    \"What's your favorite color?\",\n    [\"Red\", \"Green\", \"Blue\"]\n)\n</code></pre> Temperature Converter <p>Create a simple temperature converter application that:</p> <ol> <li>Accepts temperature input in Celsius via <code>st.number_input</code></li> <li>Converts this temperature to Fahrenheit using \u00b0F = (\u00b0C \u00d7 9/5) + 32</li> <li>Displays the result with <code>st.write</code></li> </ol> <p>The Ultimate temperature conversion guide.... byu/noobmaster69_is_hela inmemes</p>"},{"location":"python-extensive/streamlit/#data-display","title":"Data Display","text":"<p>Streamlit makes it easy to display <code>pandas</code> dataframes and other data structures:</p> <pre><code>import streamlit as st\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\ndata = pd.DataFrame({\n    'Name': ['John', 'Anna', 'Peter'],\n    'Age': [28, 22, 35],\n    'City': ['New York', 'Paris', 'London']\n})\n\n# Display data\nst.dataframe(data)  # Interactive dataframe\nst.table(data)      # Static table\n</code></pre> <p>Furthermore, Streamlit allows us to edit dataframe right in the application: </p> <pre><code># Display data and allow user to edit it\nedited_table = st.data_editor(data)\n\noldest_name = edited_table.loc[edited_table[\"Age\"].idxmax()][\"Name\"]\nst.markdown(f\"The oldest Person is **{oldest_name}** \ud83c\udf88\")\n</code></pre>"},{"location":"python-extensive/streamlit/#charts-and-plots","title":"Charts and Plots","text":"<p>Streamlit seamlessly integrates with plotting libraries such as Matplotlib, Seaborn, or Plotly (make sure to install these libraries first).</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Generate sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Matplotlib\nfig, ax = plt.subplots()\nax.plot(x, y)\nst.pyplot(fig)\n\n# Seaborn\nsns.set_theme()\ntips = sns.load_dataset(\"tips\")\nfig = plt.figure(figsize=(10, 6))\nsns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\")\nst.pyplot(fig)\n</code></pre> Interactive Plots <p>Streamlit also supports interactive plotting libraries like Plotly:</p> <pre><code>import plotly.express as px\n\nfig = px.scatter(tips, x=\"total_bill\", y=\"tip\", color=\"size\")\nst.plotly_chart(fig)\n</code></pre>"},{"location":"python-extensive/streamlit/#layout-and-containers","title":"Layout and Containers","text":"<p>Streamlit provides several options to manage your layout and organize the user interface.</p>"},{"location":"python-extensive/streamlit/#columns","title":"Columns","text":"<pre><code>col1, col2 = st.columns(2)\n\nwith col1:\n    st.header(\"Column 1\")\n    st.write(\"This is the first column\")\n\nwith col2:\n    st.header(\"Column 2\")\n    st.write(\"This is the second column\")\n</code></pre>"},{"location":"python-extensive/streamlit/#tabs","title":"Tabs","text":"<pre><code>tab1, tab2 = st.tabs([\"Tab 1\", \"Tab 2\"])\n\nwith tab1:\n    st.header(\"Tab 1 Content\")\n    st.write(\"This is the first tab\")\n\nwith tab2:\n    st.header(\"Tab 2 Content\")\n    st.write(\"This is the second tab\")\n</code></pre>"},{"location":"python-extensive/streamlit/#expanders","title":"Expanders","text":"<pre><code>with st.expander(\"Click to expand\"):\n    st.write(\"This content is hidden by default\")\n    st.image(\"https://upload.wikimedia.org/wikipedia/commons/7/7d/Seal_point_Sphynx_Kitten.jpg\")\n</code></pre> Did You Know? <p>Streamlit automatically adjusts its layout to fit different screen sizes, ensuring your app looks great on both desktop and mobile devices.</p>"},{"location":"python-extensive/streamlit/#file-upload-and-download","title":"File Upload and Download","text":"<p>Streamlit makes it easy to handle file uploads and downloads:</p> <pre><code># File upload\nuploaded_file = st.file_uploader(\"Choose a Excel file\", type=\"xlsx\")  # (1)!\n\nif uploaded_file is not None:\n    data = pd.read_excel(uploaded_file)\n    st.write(data)\n\n    # File download (display only if a file is uploaded)\n    st.download_button(\n        label=\"Download data as CSV\",\n        data=data.to_csv(index=False),\n        file_name='sample_data.csv',\n        mime='text/csv',\n    )\n</code></pre> <ol> <li>The <code>type</code> parameter restricts the file types that can be uploaded. To only    accept text files, use <code>type=\"txt\"</code>. For multiple file types, use a    list: <code>type=[\"txt\", \"csv\"]</code>.</li> </ol> Info <p>For the above snippet to work, you need to install an additional package to read the Excel file. Look at the error message to determine which one.</p> Data Analyzer <p>Create a Streamlit application that:</p> <ol> <li>Lets users upload the 'Student Data' XLSX file from before.</li> <li>Displays basic statistics using <code>df.describe()</code></li> <li>Creates a line chart based on the 'Student Data' dataset using ploty and visualize it in the streamlit application</li> </ol> <p>Bonus: Add error handling for invalid file formats and empty data frames.</p>"},{"location":"python-extensive/streamlit/#optional-session-state","title":"Optional: Session State","text":"<p>Streamlit is stateless by default, meaning it reruns your entire script on any user interaction.  Session state allows you to persist data between reruns:</p> <pre><code>if 'counter' not in st.session_state:\n    st.session_state.counter = 0\n\nif st.button('Increment'):\n    st.session_state.counter += 1\n\nst.write('Counter:', st.session_state.counter)\n</code></pre> When to Use Session State <p>Use session state when you need to:</p> <ul> <li>Persist data between reruns</li> <li>Share data between different parts of your app</li> <li>Implement counters or progress tracking</li> <li>Store user preferences</li> </ul>"},{"location":"python-extensive/streamlit/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Performance</p> <ul> <li>Cache expensive computations using <code>@st.cache_data</code></li> <li>Use appropriate container widgets to organize your layout</li> <li>Minimize the use of heavy computations in the main thread</li> </ul> </li> <li> <p>User Experience</p> <ul> <li>Add proper error handling</li> <li>Include loading indicators for long operations</li> <li>Provide clear instructions and feedback</li> <li>Use appropriate input validation</li> </ul> </li> <li> <p>Code Organization</p> <ul> <li>Split your code into logical functions</li> <li>Use config files for constants</li> <li>Follow Python naming conventions</li> </ul> </li> </ol>"},{"location":"python-extensive/streamlit/#example-data-dashboard","title":"Example: Data Dashboard","text":"<p>Let's create a simple dashboard that combines various Streamlit features:</p> <pre><code>import streamlit as st\nimport pandas as pd\nimport plotly.express as px\nimport seaborn as sns\n\n# Page config\nst.set_page_config(page_title=\"Data Dashboard\", layout=\"wide\")\n\n# Title\nst.title(\"\ud83d\udcca Data Dashboard\")\n\n# Sidebar\nst.sidebar.header(\"Settings\")\ndataset = st.sidebar.selectbox(\n    \"Select Dataset\",\n    [\"Iris\", \"Diamonds\", \"Tips\"]\n)\n\n# Load data\n@st.cache_data\ndef load_data(dataset_name):\n    if dataset_name == \"Iris\":\n        return sns.load_dataset(\"iris\")\n    elif dataset_name == \"Diamonds\":\n        return sns.load_dataset(\"diamonds\")\n    else:\n        return sns.load_dataset(\"tips\")\n\ndata = load_data(dataset)\n\n# Display data overview\ncol1, col2 = st.columns(2)\n\nwith col1:\n    st.subheader(\"Data Preview\")\n    st.dataframe(data.head())\n\nwith col2:\n    st.subheader(\"Basic Statistics\")\n    st.dataframe(data.describe())\n\n# Visualizations\nst.subheader(\"Data Visualization\")\ntab1, tab2 = st.tabs([\"Distribution\", \"Relationships\"])\n\nwith tab1:\n    numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n    col = st.selectbox(\"Select Column\", numeric_cols)\n    fig = px.histogram(data, x=col)\n    st.plotly_chart(fig, use_container_width=True)\n\nwith tab2:\n    x_col = st.selectbox(\"Select X axis\", numeric_cols, key=\"x\")\n    y_col = st.selectbox(\"Select Y axis\", numeric_cols, key=\"y\")\n    fig = px.scatter(data, x=x_col, y=y_col)\n    st.plotly_chart(fig, use_container_width=True)\n</code></pre>"},{"location":"python-extensive/streamlit/#deployment","title":"Deployment","text":"<p>Streamlit applications can be deployed in various ways:</p> <ol> <li> <p>Streamlit Cloud (Recommended for small projects) </p> <ul> <li>Push your code to a GitHub repository.</li> <li>Log in to Streamlit Cloud.</li> <li>Connect your repository and deploy the app.</li> </ul> </li> <li> <p>Docker </p> </li> <li> <p>Kubernetes </p> </li> </ol> Production Deployment <p>When deploying to production:</p> <ul> <li>Use requirements.txt or Poetry for dependency management</li> <li>Set up proper environment variables</li> <li>Configure authentication if needed</li> <li>Monitor application performance</li> <li>Set up error logging</li> </ul>"},{"location":"python-extensive/streamlit/#recap","title":"Recap","text":"<p>In this chapter, we covered the fundamentals of building web applications with Streamlit. We  explored:</p> <ul> <li>Basic Streamlit elements and widgets</li> <li>Data visualization and interaction</li> <li>Layout management</li> <li>File handling</li> <li>Session state</li> <li>Best practices</li> <li>Deployment options</li> </ul> <p>With these skills, you can now transform your Python scripts into interactive web applications  that can be shared with others.</p> \ud83c\udf89 <p>Congratulations! You've completed the Streamlit chapter. You're now equipped to create interactive web applications using Python.</p> <p> </p>"},{"location":"python-extensive/tkinter/","title":"Graphical User Interfaces with Tkinter","text":""},{"location":"python-extensive/tkinter/#introduction","title":"Introduction","text":"<p>A Graphical User Interface (GUI) allows users to interact with a program through visual elements like buttons, text fields, and menus, rather than typing commands in a terminal. GUIs thus make applications more accessible to non-technical users.</p> <p></p>"},{"location":"python-extensive/tkinter/#how-guis-relate-to-oop","title":"How GUIs relate to OOP?","text":"<p>Each GUI element (widget), like a button or text field, can be thought of as an object of the corresponding class. These objects:    </p> <ul> <li>have attributes (e.g., size, color, labels),</li> <li>have methods (e.g., click, enable/disable), and</li> <li>interact with each other through events,</li> </ul> <p>following the principles of object-oriented programming (OOP).</p> <p>Tkinter is the standard <code>Python</code>  library for creating graphical user interfaces (GUIs) for desktop applications. It serves as an excellent starting point for those new to GUI development and event-driven programming. By using Tkinter, interactive applications such as calculators, form-based tools, or simple games, can be build. The Tkinter documentation can be found here: Tkinter documentation.  Nevertheless, Tkinter is not the only option for developing desktop applications.While it's lightweight and simple, there are many other GUI libraries and frameworks that offer more advanced features, modern aesthetics, and flexibility, depending on your needs. The <code>Python</code>  wiki lists several alternative GUI frameworks and tools.</p>"},{"location":"python-extensive/tkinter/#creating-a-basic-tkinter-application","title":"Creating a Basic Tkinter Application","text":""},{"location":"python-extensive/tkinter/#the-tkinter-main-loop","title":"The Tkinter Main Loop","text":"<p>The root window is responsible for running the main event loop (<code>mainloop()</code>), which keeps the GUI application running, listens for events (e.g., clicks, keystrokes), and updates the interface. The main event loop ends when the window is closed. Without the main loop, the GUI application would close immediately after being displayed. <pre><code>import tkinter as tk\n\n# Create root window\nroot = tk.Tk()\nroot.title(\"My First GUI Program\")\nroot.minsize(width=300, height=400)\n\n# Running the main loop event\nroot.mainloop()\n</code></pre></p>"},{"location":"python-extensive/tkinter/#widgets-and-controls","title":"Widgets and Controls","text":"<p>Widgets and controls are the building blocks of a Tkinter GUI. Widgets are structured in a hierarchy, where some widgets serve as containers (parent widgets - e.g., windows or frames) that hold other widgets (child widgets). This hierarchical arrangement defines the relationships and organization of widgets, influencing their layout, behavior, and interaction.   They are placed using a layout manager (place, pack, or grid).   Widgets feature attributes like <code>text</code>, <code>font</code>, <code>bg</code> (background color), <code>fg</code> (foreground color), and <code>width</code>, which can be set during creation or updated later. </p> <p>Collection of Different Widgets:  The following code provides a collection of different widgets: <code>Label</code>, <code>Input</code>, <code>Button</code>, <code>Text</code>, <code>Spinbox</code>, <code>Scale</code>, <code>Checkbutton</code>, <code>Radiobutton</code>, and <code>Listbox</code>.  Each widget is an object of its class (e.g., button, label) and is part of the same parent widget (<code>root</code> window).</p> <p>Widget Callbacks:  Some widgets support event handling. Each widget's behavior is managed by a specific method:    </p> <ul> <li>Button: <code>button_clicked()</code></li> <li>Spinbox: <code>spinbox_used()</code></li> <li>Scale: <code>scale_used(value)</code></li> <li>Checkbutton: <code>checkbutton_used()</code></li> <li>Radiobutton: <code>radio_used()</code></li> <li>Listbox: <code>listbox_used(event)</code></li> </ul> <p>Dynamic Behavior:   Widgets like the spinbox, scale, and listbox dynamically print or process user interactions.</p> <p>Event Binding:   The <code>listbox</code> widget uses the <code>bind</code> method to associate the selection event with the <code>listbox_used</code> method.</p> Task: Run the Code <p>Interact with the widgets to see their behavior in action (check the console for printed outputs). Don't worry if you don't understand every single line of code right away, just focus on  getting familiar with the structure.</p> <pre><code>import tkinter as tk\n\n# Set window attributes\nroot = tk.Tk()\nroot.title(\"My First GUI Program and Widgets\")\nroot.geometry(\"1024x768\")\n\n# Label\nmy_label = tk.Label(\n    text=\"My First Label\", font=(\"Arial\", 24, \"bold\")\n)\nmy_label.pack()\n\n# Input\nmy_input = tk.Entry(width=10)\nmy_input.pack()\n\n# Button callback\ndef button_clicked():\n    print(\"Do Something Cool\")\n# Button\nmy_button = tk.Button(\n    text=\"Click Me\", command=button_clicked\n)\nmy_button.pack()\n\n# Text\ntext = tk.Text(height=5, width=30)\ntext.focus()\ntext.insert(tk.END, \"My first try on a multi-line text entry.\")\n# Prints the current text starting from line 1, character 0\nprint(text.get(\"1.0\", tk.END)) \ntext.pack()\n\n# Spinbox callback\ndef spinbox_used():\n    print(spinbox.get())\n# Spinbox\nspinbox = tk.Spinbox(from_=0, to=3, width=5, command=spinbox_used)\nspinbox.pack()\n\n# Scale callback\ndef scale_used(value):\n    print(value)\n# Scale\nscale = tk.Scale(from_=0, to=1000, command=scale_used)\nscale.pack()\n\n# Checkbutton callback\ndef checkbutton_used():\n    print(checked_state.get())\n# Checkbutton\nchecked_state = tk.IntVar()  # Variable to hold the checked state\ncheckbutton = tk.Checkbutton(\n    text=\"Is This On?\", variable=checked_state, command=checkbutton_used\n)\ncheckbutton.pack()\n\n# Radiobutton callback\ndef radio_used():\n    print(radio_state.get())\n# Radiobutton\nradio_state = tk.IntVar()  # Variable to hold the radio button selection\nradiobutton1 = tk.Radiobutton(\n    text=\"Answer1\", value=1, variable=radio_state, command=radio_used\n)\nradiobutton2 = tk.Radiobutton(\n    text=\"Answer2\", value=2, variable=radio_state, command=radio_used\n)\nradiobutton1.pack()\nradiobutton2.pack()\n\n# Listbox callback\ndef listbox_used(event):\n    print(listbox.get(listbox.curselection()))\n# Listbox\nlistbox = tk.Listbox(height=4)\nfor item in [\"Red\", \"Green\", \"Blue\", \"Yellow\"]:\n    listbox.insert(tk.END, item)\nlistbox.bind(\"&lt;&lt;ListboxSelect&gt;&gt;\", listbox_used)\nlistbox.pack()\n\nroot.mainloop()\n</code></pre>"},{"location":"python-extensive/tkinter/#tkinter-layout-manager","title":"Tkinter Layout Manager","text":"<p>The layout of a graphical user interface is largely determined by the arrangement and design of the GUI components. Tkinter provides three different layout managers that automatically generate the layout based on the settings defined in the program. Without using a layout manager, the GUI element will not appear on the screen.</p> <ol> <li>Place Manager: Positions components based on specific coordinates (x, y), offering precise control over placement.</li> <li>Pack Manager: Automatically arranges components in a container, either vertically or horizontally, based on the order they are added.</li> <li>Grid Manager: Organizes components in a grid with rows and columns, allowing more complex layouts with alignment options.</li> </ol> <p>Key Concepts of the Grid Layout Manager:   </p> <ol> <li> <p>Rows and Columns:</p> <ul> <li>The grid is organized into rows and columns. You can place widgets in a specific row and column using the <code>grid(row, column)</code> method.</li> <li>You can also span multiple rows or columns using <code>rowspan</code> and <code>columnspan</code> attributes.</li> </ul> </li> <li> <p>Control Placement:</p> <ul> <li>You can specify the row, column, and sticky options to control widget alignment (e.g., top, bottom, left, right).</li> </ul> </li> <li> <p>Column and Row Configuration:</p> <ul> <li>Tkinter allows you to configure the weight of rows and columns to define how they should expand when the window is resized. You can do this using <code>grid_columnconfigure()</code> and <code>grid_rowconfigure()</code>.</li> </ul> </li> </ol> <pre><code>import tkinter as tk\n\nroot = tk.Tk()\nroot.title(\"Grid Layout Example\")\n\n# Label in the first row and first column\nname_label = tk.Label(text=\"Name:\")\nname_label.grid(row=0, column=0)\n\n# Entry in the first row and second column\nname_entry = tk.Entry()\nname_entry.grid(row=0, column=1)\n\n# Label in the second row and first column\nage_label = tk.Label(text=\"Age:\")\nage_label.grid(row=1, column=0)\n\n# Entry in the second row and second column\nage_entry = tk.Entry()\nage_entry.grid(row=1, column=1)\n\n# Button in the third row, spanning both columns\nsubmit_button = tk.Button(text=\"Submit\")\nsubmit_button.grid(row=2, columnspan=2)  # Spanning both columns\n\nroot.mainloop()\n</code></pre> Tkinter Hints and Suggestions <ol> <li> <p>Modularize Code</p> <ul> <li>Break the GUI into smaller classes or methods, particularly for large application.</li> <li>Use reusable components: e.g., set an <code>InputForm</code> class for repeated input forms.</li> </ul> </li> <li> <p>Use Meaningful Variable and Method Names</p> <ul> <li>Name widgets and methods clearly to reflect their purpose. Avoid generic names like <code>button1</code> or <code>label1</code>, use for example <code>submit_button</code> instead.</li> </ul> </li> <li> <p>Avoid Hardcoding Layouts</p> <ul> <li>Use layout managers (<code>pack</code> or <code>grid</code>) instead of coding absolute positions (<code>place(x, y)</code>).</li> <li>Avoid mixing layout manager in the same container.</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Add errror handling for user input or unexpected behavior.</li> <li>Provide clear instructions and feedback.</li> </ul> </li> </ol>"},{"location":"python-extensive/tkinter/#example-create-your-own-to-do-list-application","title":"Example: Create Your Own To-Do List Application","text":"<p>The following code provides the basis for a simple To-Do List application with the following key features:</p> <ul> <li>Add Task: Enter tasks using the <code>Entry</code> widget and add them to the <code>Listbox</code>.</li> <li>Cross Out Task: Mark the selected task with a \"\u2714\" symbol to indicate completion.</li> </ul> <p>Think of this task as a practical summary of building an app\u2014a great starting point if you want to dive  deeper into GUI development. The key is to experiment, explore, and have fun with it. You don\u2019t need to grasp every detail at first glance\u2014just focus on getting familiar with the  structure and making small improvements step by step. Keep going, and you'll see progress before you know it! \ud83d\ude80        </p> Task: To-Do List Enhancements <p>Customize and enhance the To-Do List app with the following features:       1. Update the Color Scheme: Use hexadecimal color codes to personalize the app\u2019s appearance (you can find color codes online).        2. Add a <code>Delete Button</code>: Create a button that removes a task from the <code>Listbox</code>. Refer to the existing \"Cross Task\" function and button for guidance.        3. Position the <code>Delete Button</code> next to the <code>Cross Out Button</code> and use a red color. If needed, review the grid layout example for positioning help.      </p> <p>Bonus: Highlight important tasks by adding features like colored text or checkboxes to mark tasks as important.</p> <pre><code>import tkinter as tk\n\n# Create the main window\nroot = tk.Tk()\nroot.title(\"To-Do List\")\nroot.minsize(height=400, width=400) # Set minimum window size\nroot.configure(bg=\"#d0ebff\")  # Light blue background\n\n# Add new task\ndef add_task():\n    task = task_entry.get() # Get task input from entry box\n    if task.strip(): # Check if input is not empty or just spaces\n        task_listbox.insert(tk.END, task) # Add task to the listbox at the end\n        task_entry.delete(0, tk.END) # Clear entry box after adding task\n\n# Cross out selected task\ndef cross_task():\n    selected = task_listbox.curselection() # Get selected task from listbox\n    if selected:\n        current_task = task_listbox.get(selected) # Get selected text to modify\n        task_listbox.delete(selected) # Remove the selected task from the list\n        task_listbox.insert(selected, f\"\u2714 {current_task}\") # Mark as complete\n\n# Entry box for adding tasks\ntask_entry = tk.Entry(\n    width=30, font=(\"Helvetica\", 14)\n)\ntask_entry.grid(row=0, column=1, columnspan=2, padx=10, pady=10)\n\n# Add Task button\nadd_button = tk.Button(\n    text=\"Add Task\", font=(\"Helvetica\", 12), \n    bg=\"#b0e0e6\", fg=\"black\", command=add_task # Trigger add_task\n)\nadd_button.grid(row=0, column=0, padx=10, pady=10)\n\n# Listbox to display tasks\ntask_listbox = tk.Listbox(\n    width=40, height=15, font=(\"Helvetica\", 12), \n    selectbackground=\"#b3e5fc\", selectforeground=\"black\"\n)\ntask_listbox.grid(row=1, column=0, columnspan=3, padx=10, pady=10)\n\n# Cross Out Task button\ncross_out_button = tk.Button(\n    text=\"Cross Out Task\", font=(\"Helvetica\", 12), \n    bg=\"#00264d\", fg=\"white\", command=cross_task # Trigger cross_task\n)\ncross_out_button.grid(row=2, column=0, padx=10, pady=10)\n\n# Start the main loop to display GUI\nroot.mainloop()\n</code></pre>"},{"location":"python-extensive/tkinter/#optional-storing-data-and-running-your-app-on-the-desktop","title":"Optional: Storing Data and Running Your App on the Desktop","text":""},{"location":"python-extensive/tkinter/#storing-app-data","title":"Storing App Data","text":"<p>To keep your data (e.g., from your To-Do List) available when reopening the app, use JSON for saving and loading\u2014just like we explored in the previous chapter, \"Data Acquisition and Export\".</p> <p>Steps to store and retrieve data from the To-Do List using JSON:  </p> <ul> <li>save_tasks(): This function saves the current tasks from the listbox to the <code>tasks.json</code> file whenever a task is added or deleted.        </li> <li>load_tasks(): This function attempts to read a file called <code>tasks.json</code> and load the tasks from it. If the file doesn't exist, it returns an empty list.      </li> <li>Loading and Saving: The app will load the tasks when it starts and save them whenever a task is added or removed.</li> </ul> <p>If you'd like to store tasks from your To-Do List, the updated code is provided below.</p> Save To-Do List Data <pre><code>import tkinter as tk\nimport json\n\n# Load tasks from the file\ndef load_tasks():\n    try:\n        with open('tasks.json', 'r') as file:\n            return json.load(file) # Load task from the JSON file\n    except FileNotFoundError:\n        return []  # If the file doesn't exist, return an empty list\n\n# Save tasks to the file\ndef save_tasks():\n    tasks = task_listbox.get(0, tk.END)  # Get all tasks from the Listbox\n    with open('tasks.json', 'w') as file:\n        json.dump(tasks, file) # Save tasks as JSON in the file\n\n# Create the main window\nroot = tk.Tk()\nroot.title(\"To-Do List\")\nroot.minsize(height=400, width=400) # Set minimum window size\nroot.configure(bg=\"#d0ebff\")  # Light blue background\n\n# Add new task\ndef add_task():\n    task = task_entry.get() # Get task input from entry box\n    if task.strip(): # Check if input is not empty or just spaces\n        task_listbox.insert(tk.END, task) # Add task to the listbox at the end\n        task_entry.delete(0, tk.END) # Clear entry box after adding task\n        save_tasks() # Save the updated task list to the JSON file\n\n# Cross out selected task\ndef cross_task():\n    selected = task_listbox.curselection() # Get selected task from listbox\n    if selected:\n        current_task = task_listbox.get(selected) # Get selected text to modify\n        task_listbox.delete(selected) # Remove the selected task from the list\n        task_listbox.insert(selected, f\"\u2714 {current_task}\") # Mark as complete\n        save_tasks() # Save the updated task list to the JSON file\n\n# Entry box for adding tasks\ntask_entry = tk.Entry(\n    width=30, font=(\"Helvetica\", 14)\n)\ntask_entry.grid(row=0, column=1, columnspan=2, padx=10, pady=10)\n\n# Add Task button\nadd_button = tk.Button(\n    text=\"Add Task\", font=(\"Helvetica\", 12), \n    bg=\"#b0e0e6\", fg=\"black\", command=add_task # Trigger add_task\n)\nadd_button.grid(row=0, column=0, padx=10, pady=10)\n\n# Listbox to display tasks\ntask_listbox = tk.Listbox(\n    width=40, height=15, font=(\"Helvetica\", 12), \n    selectbackground=\"#b3e5fc\", selectforeground=\"black\"\n)\ntask_listbox.grid(row=1, column=0, columnspan=3, padx=10, pady=10)\n\n# Cross Out Task button\ncross_out_button = tk.Button(\n    text=\"Cross Out Task\", font=(\"Helvetica\", 12), \n    bg=\"#00264d\", fg=\"white\", command=cross_task # Trigger cross_task\n)\ncross_out_button.grid(row=2, column=0, padx=10, pady=10)\n\n# Load tasks at the start of the app\ntasks = load_tasks() # Load saved tasks when app starts\nfor task in tasks:\n    task_listbox.insert(tk.END, task) # Insert each task into the Listbox\n\n# Start the main loop to display GUI\nroot.mainloop()\n</code></pre>"},{"location":"python-extensive/tkinter/#converting-your-script-into-a-desktop-app","title":"Converting Your Script into a Desktop App","text":"<p>Once you've built your Tkinter app (e.g., the To-Do List), you can convert it into a standalone desktop application by turning your <code>.py</code> script into an executable (<code>.exe</code>). While we mostly used Jupyter notebooks for convenience in this course, it's recommended to use a <code>.py</code> script for creating desktop applications. To make your To-Do List app runnable without opening Python, you can use a tool like PyInstaller to convert your script into an executable.</p> <p>Steps to Convert to <code>.exe</code>:</p> <ol> <li> <p>Create a Python Script:     Create a new file with <code>.py</code>extension (e.g., <code>your_script.py</code>) and write or paste your code into this file - save it.</p> </li> <li> <p>Install PyInstaller:    <pre><code>pip install pyinstaller\n</code></pre></p> </li> <li> <p>Navigate to your script\u2019s folder in VS Code and run the following command in your terminal:     <pre><code>pyinstaller --onefile --windowed your_script.py\n</code></pre></p> <ul> <li><code>--onefile</code>: Bundles everything into a single <code>.exe</code> file.</li> <li><code>--windowed</code>: Prevents the terminal from appearing (useful for GUI applications).</li> </ul> </li> <li> <p>Locate your <code>.exe</code> in the <code>dist/</code> folder inside your project directory and execute it.</p> </li> </ol> <p>\ud83d\udccc Tip: Place <code>tasks.json</code> (if not done automatically) in the same folder as the <code>.exe</code> file to ensure data is stored persistently.</p> PyInstaller and macOS <p>PyInstaller can also be used to convert your Python script into a standalone application on macOS. However, instead of generating a <code>.exe</code> file (for Windows), PyInstaller will create a <code>.app</code> file for macOS. The process is the same. \ud83d\ude0e</p> <p></p>"},{"location":"python-extensive/tkinter_streamlit/","title":"Tkinter vs Streamlit","text":"<p>In the previous sections, you explored two different approaches to building graphical user interfaces: Tkinter and Streamlit.  </p> <p>On one side, Tkinter provided a foundation for creating desktop applications. While its design may feel somewhat outdated, it remains a powerful and widely used tool for GUI programming, sharing core principles with many other frameworks.  </p> <p>On the other side, you worked with Streamlit, a modern, web-based framework designed for rapid prototyping and interactive data applications. Its ease of use and built-in interactivity make it particularly appealing for data-driven projects.  </p> <p>To round off this chapter on \"Graphical User Interfaces,\" the following comparison highlights key similarities and differences between Tkinter and Streamlit.  </p>"},{"location":"python-extensive/tkinter_streamlit/#tkinter","title":"Tkinter","text":"<ul> <li>GUI framework for building desktop applications.  </li> <li>Event-driven programming, requiring more code for UI interactivity.  </li> <li>Highly customizable, though complex for advanced UIs.  </li> <li>Desktop-only deployment, requiring a Python environment.  </li> <li>Class-based approach, where windows, widgets, and events are manually structured.  </li> </ul>"},{"location":"python-extensive/tkinter_streamlit/#streamlit","title":"Streamlit","text":"<ul> <li>Web-based framework, ideal for rapid prototyping of data-driven apps.  </li> <li>Real-time interactivity with minimal setup and coding.  </li> <li>Simplified interface, offering limited customization but excellent ease of use.  </li> <li>Easy web deployment, requiring no manual environment setup.  </li> <li>Function-based design, though class-based structures can be incorporated for reusability.  </li> </ul> When to Use What <ul> <li>Use Tkinter if your focus is on building desktop applications with a traditional GUI.    </li> <li>Use Streamlit if your goal is to rapidly prototype web apps, particularly in data science or machine learning, where real-time updates and interactivity are crucial.</li> </ul>"},{"location":"python-extensive/variables/","title":"Variables","text":""},{"location":"python-extensive/variables/#variables","title":"Variables","text":"Info <p>The structure of following sections is based on the official Python tutorial.</p> <p>Many thanks to @jhumci for providing the initial resource materials!</p>"},{"location":"python-extensive/variables/#getting-started","title":"Getting started","text":"<p>    Important    </p> <p>We encourage you to execute all upcoming code snippets on your machine. You can easily copy each code snippet to your clipboard, by clicking the icon in the top right corner. By doing so, you will be prepared for all upcoming tasks within the sections. Tasks are indicated by a -icon.</p> <p>We recommend to create a new notebook for each chapter, e.g. create <code>variables.ipynb</code> for this chapter. Doing so, your notebooks will follow the structure of this crash course.</p> <p>You will encounter multiple info boxes   throughout the course. They provide additional information, tips, and tricks.  Be sure to read them thoroughly.</p> <p>Let's start with the first task.</p> Notebook &amp; first code cell <p>Create a new Jupyter notebook and name it <code>variables.ipynb</code>. Paste the following code snippet into a new cell and execute it.</p> <pre><code>print(\"Hello World!\")\n</code></pre> <p>The output should be:</p> &gt;&gt;&gt; Output<pre><code>Hello World!\n</code></pre> Info <pre><code>print(1+1)\n</code></pre> <p>The upcoming content contains a lot of code snippets. They are easily  recognizable due to their colourful syntax highlighting, such as:</p> <p>Code snippets are an integral part, to illustrate concepts, which are  introduced and explained along the way. Commonly, these code snippets are accompanied by an output block to display the result, for instance:</p> &gt;&gt;&gt; Output<pre><code>2\n</code></pre> <p>Nevertheless note, output blocks can be missing as there is not always  an explicit result.</p> <p>Again, execute and experiment with all code snippets on your machine to  verify the results and get familiar with <code>Python</code> !</p>"},{"location":"python-extensive/variables/#variable","title":"Variable","text":"<p>Computers can store lots of information. To do so, in <code>Python</code>  we use variables. A variable is a name that refers to a value. The following code snippet, assigns the value <code>4</code> to the variable <code>number</code>. In  general, you pick the variable name on the left hand side, assign a value  with <code>=</code> and the value itself is on the right hand side.</p> <pre><code>number = 4\n</code></pre> <p>You can change the value of a variable in your program at any time, and Python will always keep track of its current value.</p> <pre><code>number = 4\nnumber = 4000\n</code></pre> <p>You will notice that none of the cells had any output. To display the value of a variable we use the <code>print()</code> function.</p>"},{"location":"python-extensive/variables/#print","title":"<code>print()</code>","text":"<pre><code>number = 4\nprint(number)\n</code></pre> &gt;&gt;&gt; Output<pre><code>4\n</code></pre> <p>Now, we can also verify that in the above snippet the value of <code>number</code> was actually changed.</p> <pre><code>number = 4\nprint(number)\nnumber = 4000\nprint(number)\n</code></pre> &gt;&gt;&gt; Output<pre><code>4\n4000\n</code></pre> Info <p>Within a notebook, the variables are stored in the background and can be  overwritten at any time. Therefore, it is good practice to execute all  cells from top to bottom of the notebook in the right order so that  nothing unexpected is stored in a variable.</p>"},{"location":"python-extensive/variables/#comments","title":"Comments","text":"<p>Comments exist within your code but are not executed. They are used to describe your code and are ignored by the <code>Python</code> interpreter. Comments are prefaced by a <code>#</code>.</p> <pre><code># this is a comment\nprint(\"Hello World!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello World!\n</code></pre> Info <p>Comments help you and others understand what your code is doing. It is good practice to use comments as a tool for documentation.</p>"},{"location":"python-extensive/variables/#variable-naming","title":"Variable naming","text":"<p>When you\u2019re using variables in <code>Python</code>, you need to adhere to a few rules and guidelines. Breaking some of these rules will cause errors; other guidelines just help you write code that\u2019s easier to read and understand. Be sure to keep the following rules in mind:</p> <ul> <li>Variable names are lower case and can contain only letters, numbers, and   underscores.   They can start with a letter or an underscore, but not with a number.   For instance, you can call a variable <code>message_1</code> but not <code>1_message</code>.</li> <li>Whitespace is not allowed in variable names, but an underscores <code>_</code> can be   used to separate words in variable names. For example, <code>greeting_message</code>   works, but <code>greeting message</code> won't.</li> <li>Avoid using <code>Python</code> keywords and function names as variable names;   that is, do not use words that <code>Python</code> has reserved for a particular   programmatic purpose, such as the word <code>print</code>.</li> <li>Variable names should be short but descriptive. For example, <code>name</code> is better   than <code>n</code>, <code>student_name</code> is better than <code>s_n</code>, and <code>name_length</code> is better   than <code>length_of_persons_name</code>.</li> </ul>"},{"location":"python-extensive/variables/#errors-nameerror","title":"Errors (<code>NameError</code>)","text":"<p>Every programmer makes mistakes and even after years of experience, mistakes are part of the process. With time, you get more efficient in debugging  (=process of finding and fixing errors).</p> Debugging tactics byu/0ajs0jas inProgrammerHumor <p>Let\u2019s look at an error you\u2019re likely to make early on and learn how to fix it. We\u2019ll write some code that throws an error message on purpose. Copy the code and run your cell.</p> <pre><code>message = \"Hello Python Crash Course reader!\"\nprint(mesage)\n</code></pre> <p>Which should result in:</p> <pre><code>NameError: name 'mesage' is not defined\n</code></pre> <p>When an error occurs in your program, the <code>Python</code> interpreter does its best to help you figure out where the problem is. The interpreter provides a traceback which is a record of where the interpreter ran into trouble when trying to execute your code. Here\u2019s an example of the traceback that <code>Python</code> provides, after you\u2019ve accidentally misspelled a variable\u2019s name:</p> <pre><code>Traceback (most recent call last):\n  File \"C:\\\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-35-c8f2adeaed02&gt;\", line 2, in &lt;module&gt;\n    print(mesage)\n          ^^^^^^\nNameError: name 'mesage' is not defined. Did you mean: 'message'?\n</code></pre> <p>The output reports that an error occurs in line 2. The interpreter shows this line to help us spot the error quickly and tells us what kind of error it found. In this case, it found a <code>NameError</code> and reports that the variable <code>mesage</code> has not been defined. A name error usually means we made a spelling mistake when entering the variable\u2019s name or that the variable simply does not exist.</p> Your first fix <p>Fix the <code>NameError</code> in your code cell.</p>"},{"location":"python-extensive/variables/#recap","title":"Recap","text":"<p>In this section, we have covered variables in <code>Python</code> .</p> <p>You have learned (about):</p> <ul> <li>To create and assign a value to a variable</li> <li><code>print()</code> to display the value of a variable</li> <li>Comments</li> <li>Naming conventions for variables</li> <li>How to fix a <code>NameError</code></li> </ul>"},{"location":"python-extensive/containers/dict/","title":"Dictionaries","text":""},{"location":"python-extensive/containers/dict/#dictionaries","title":"Dictionaries","text":"<p>In this section, you\u2019ll learn how to use dictionaries which allow you to connect pieces of related information. Dictionaries let you model a variety of real-world objects more accurately. We will create, modify and  access elements of a dictionary.</p>"},{"location":"python-extensive/containers/dict/#creating-a-dictionary","title":"Creating a dictionary","text":"<pre><code>experiment = {\"description\": \"resource optimization\"}\nprint(type(experiment))\n</code></pre> <p>Above code snippet creates a simple dictionary and prints its type:</p> &gt;&gt;&gt; Output<pre><code>&lt;class 'dict'&gt;\n</code></pre> <p>In <code>Python</code>, a dictionary is wrapped in curly braces (<code>{}</code>), with a series  of key-value pairs inside the braces. Each key is connected to a value, and  you can use a key to access the value associated with that key.  A key\u2019s value can be any type, like a string, integer, list, or even  another dictionary. In the above example, the key is <code>\"description\"</code>  and its value <code>\"resource optimization\"</code></p> <p>Every key is connected to its value by a colon. Individual key-value pairs are separated by commas. You can store as many key-value pairs as you want in a dictionary.</p> <pre><code>experiment = {\n    \"description\": \"resource optimization\",\n    \"sample_weight_in_grams\": 5,\n}\n</code></pre>"},{"location":"python-extensive/containers/dict/#accessing-values","title":"Accessing values","text":"<p>To get the value associated with a key, give the name of the dictionary and then place the key inside a set of square brackets.</p> <pre><code>experiment = {\"sample_weight_in_grams\": 5}\nprint(experiment[\"sample_weight_in_grams\"])\n</code></pre> &gt;&gt;&gt; Output<pre><code>5\n</code></pre> Create a dictionary <p>Manage the cost of raw materials in a dictionary. The dictionary should  contain the following key-value pairs:</p> <ul> <li><code>\"steel\"</code>: <code>100</code></li> <li><code>\"aluminium\"</code>: <code>150</code></li> <li><code>\"copper\"</code>: <code>200</code></li> <li><code>\"plastic\"</code>: <code>50</code></li> </ul> <p>Create the dictionary and print the price of copper.</p>"},{"location":"python-extensive/containers/dict/#adding-key-value-pairs","title":"Adding key-value pairs","text":"<p>You can add new key-value pairs to a dictionary at any time. For example,  to add a new key-value pair, you would give the name of the dictionary followed by the new key in square brackets along with the new value.</p> <pre><code>experiment = {}\nexperiment[\"description\"] = \"resource optimization\"\nprint(experiment)\n</code></pre> <p>In the above example, we start with an empty dictionary and add a key-value pair to it.</p> &gt;&gt;&gt; Output<pre><code>{'description': 'resource optimization'}\n</code></pre> <p>However, we can't add the same key a second time to the dictionary. Every key is unique within the dictionary.</p>"},{"location":"python-extensive/containers/dict/#modifying-values","title":"Modifying values","text":"<p>Values can be overwritten:</p> <pre><code>experiment = {\"sample_weight_in_grams\": 10}\nexperiment[\"sample_weight_in_grams\"] = 10.2\n</code></pre>"},{"location":"python-extensive/containers/dict/#removing-key-value-pairs","title":"Removing key-value pairs","text":"<p>We can remove key-value-pairs using the key and the <code>del</code> statement:</p> <pre><code>experiment = {\n    \"supervisor\": \"Alex\",\n    \"sample_weight_in_grams\": 10,\n}\n\nprint(experiment)\n\ndel experiment[\"supervisor\"]\n\nprint(experiment)\n</code></pre> &gt;&gt;&gt; Output<pre><code>{'supervisor': 'Alex', 'sample_weight_in_grams': 10}\n{'sample_weight_in_grams': 10}\n</code></pre> Modify a dictionary <p>Remember that a value can hold any data type? You are given a dictionary with production data. <pre><code>production = {\n    \"singapore\": {\"steel\": 100, \"aluminium\": 150},\n    \"taipeh\": {\"steel\": 200, \"aluminium\": 250},\n    \"linz\": {\"steel\": 300, \"aluminium\": 350, \"copper\": 100},\n}\n</code></pre></p> <p>Each key represents a location and has another dictionary as value. This dictionary contains the production quantity of different materials.</p> <ul> <li>Remove <code>linz</code> from the dictionary.</li> <li>Add a new location <code>vienna</code> with the production of 200 steel  and 250 aluminium.</li> <li>Print the <code>aluminium</code> value of <code>taipeh</code> (try accessing it step by step and use variables for each step).</li> </ul> Info <p>At first, the above example might seem a bit too overcomplicated. However,  nesting (in this example: storing a dictionary within a dictionary) is  common practice and as already discussed, lets you represent more  complex data structures. Even databases like Redis and MongoDB are at it's core key value stores, just like our dictionary above.</p>"},{"location":"python-extensive/containers/dict/#recap","title":"Recap","text":"<p>We have covered following topics in this section:</p> <ul> <li>Dictionaries store key-value pairs</li> <li>How to get and modify values</li> <li>Adding key-value pairs</li> <li>Removing key-value pairs with <code>del</code></li> </ul> <p>Lastly, as part of the <code>Containers</code> topic, we will have a look at tuples.</p>"},{"location":"python-extensive/containers/list/","title":"Lists","text":""},{"location":"python-extensive/containers/list/#lists","title":"Lists","text":""},{"location":"python-extensive/containers/list/#introduction","title":"Introduction","text":"<p>In <code>Python</code>, a container is a type of data structure that holds and organizes multiple values or objects. Containers are used to store collections of elements, allowing you to group related data together for easier management and manipulation. <code>Python</code> provides several built-in container types, each with its own characteristics and use cases. In this first section, we cover  <code>list</code> objects, followed by dictionaries and tuples.</p>"},{"location":"python-extensive/containers/list/#what-is-a-list","title":"What is a <code>list</code>?","text":"<p>A <code>list</code> is a collection of items. You can make a <code>list</code>  including the letters of the alphabet or the digits from <code>0</code> to  <code>9</code>. You can put anything you want into a <code>list</code> and the items in your <code>list</code> don\u2019t have to be related in any particular way.</p> <p>Because a <code>list</code> usually contains more than one element, it\u2019s a good  idea to use the plural for a variable of type <code>list</code>, such as  <code>letters</code>, <code>digits</code>, or <code>names</code>.</p> <p>A <code>list</code> is created with an opening and closing square bracket  <code>[]</code>. Individual elements in the <code>list</code> are separated by commas. Here\u2019s a  simple example of a <code>list</code>:</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['John', 'Paul', 'George', 'Ringo']\n</code></pre> <pre><code>print(type([]))  # print the type of an empty list\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'list'&gt;\n</code></pre>"},{"location":"python-extensive/containers/list/#accessing-elements","title":"Accessing elements","text":"<p>You can access any element in a <code>list</code> by using the <code>index</code> of the desired item. To access an element in a <code>list</code>, write the name of the <code>list</code> followed by the index of the item enclosed in square brackets. For example, let\u2019s pull out  the first Beatle in <code>beatles</code>:</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles[0])\n</code></pre> &gt;&gt;&gt; Output<pre><code>John\n</code></pre>"},{"location":"python-extensive/containers/list/#indexerror","title":"<code>IndexError</code>","text":"Info <p>In Python, index positions start at 0, not 1. This is true for most  programming languages. If you\u2019re receiving unexpected results, determine  whether you are making a simple off-by-one error.</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles[4])\n</code></pre> <p>... results in</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;ipython-input-7-68dd8df4c868&gt;\", line 2, in &lt;module&gt;\n    print(beatles[4])\n          ~~~~~~~^^^\nIndexError: list index out of range\n</code></pre> <p>... since there is no official 5<sup>th</sup> Beatle. </p> <p>There is a special syntax for accessing the last element in a <code>list</code>. Use the index <code>-1</code> to access the last element, <code>-2</code> to access the second-to-last element, and so on.</p> <pre><code>beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\nprint(beatles[-1])  # Ringo\nprint(beatles[-2])  # George\n</code></pre> Indexing <p>Define a list that stores following programming languages:</p> <ul> <li>R</li> <li>Python</li> <li>Julia</li> <li>Java</li> <li>C++</li> </ul> <p>and use <code>print()</code> to output: <code>\"My favourite language is Python!\"</code></p>"},{"location":"python-extensive/containers/list/#list-manipulation","title":"List manipulation","text":"<p><code>Python</code> provides several ways to add or remove data to existing lists.</p>"},{"location":"python-extensive/containers/list/#adding-elements","title":"Adding elements","text":"<p>The simplest way to add a new element to a <code>list</code>, is to append it.  When you append an item to a <code>list</code>, the new element is added at  the end.</p> <pre><code>numbers = [1, 2, 3]\nprint(numbers)\nnumbers.append(4)\nprint(numbers)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1, 2, 3]\n[1, 2, 3, 4]\n</code></pre> <p>The <code>append()</code> method makes it easy to build lists dynamically. For example, you can start with an empty <code>list</code> and then add items by  repeatedly calling <code>append()</code>.</p> <pre><code>numbers = [1.0, 2.0, 0.5]\nnumbers.append(4.0)\nnumbers.append(3.0)\nnumbers.append(\"one hundred\")\n\nprint(numbers)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1.0, 2.0, 0.5, 4.0, 3.0, 'one hundred']\n</code></pre> <p>Up until now, our lists contained only one type of elements -  strings or integers. However, as in the example above, you can store  multiple different types of data in a <code>list</code>. Moreover, you can do  nesting (for example, you can store a <code>list</code> within a <code>list</code> - more on that later). Hence, lists can represent complex data structures. Nevertheless, don't mix and match every imaginable data type within a single  <code>list</code> (just because you can) as it makes the handling of your  <code>list</code> quite difficult.</p> Info <p>Later, we will learn how to perform the same task without repeatedly  calling the same <code>append()</code> method over and over.</p>"},{"location":"python-extensive/containers/list/#inserting-elements","title":"Inserting elements","text":"<p>You can add a new element at any position in your <code>list</code> by using the <code>insert()</code> method. You do this by specifying the index of the new element and the value of the new item.</p> <pre><code>pokemon = [\"Charmander\", \"Charizard\"]\npokemon.insert(1, \"Charmeleon\")\nprint(pokemon)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['Charmander', 'Charmeleon', 'Charizard']\n</code></pre>"},{"location":"python-extensive/containers/list/#removing-elements","title":"Removing elements","text":"<p>To remove an item from a <code>list</code>, you can use the <code>remove()</code>  method. You need to specify the value which you want to remove. However,  this will only remove the first occurrence of the item.</p> <pre><code>pokemon = [\"Charmander\", \"Squirtle\", \"Charmeleon\", \"Charizard\", \"Squirtle\"]\npokemon.remove(\"Squirtle\")\nprint(pokemon)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['Charmander', 'Charmeleon', 'Charizard', 'Squirtle']\n</code></pre>"},{"location":"python-extensive/containers/list/#popping-elements","title":"Popping elements","text":"<p>Sometimes you\u2019ll want to use the value of an item after you remove it from a <code>list</code>. The <code>pop()</code> method removes a specified element of a  <code>list</code>. Additionally, the item is returned so you can work with that  item after removing it. </p> <p>The term pop comes from thinking of a <code>list</code> as a stack of items and popping one item off the top of the stack. In this analogy, the top of a stack  corresponds to the end of a <code>list</code>.</p> <pre><code>pokemon = [\"Charmander\", \"Charmeleon\", \"Bulbasaur\", \"Charizard\"]\nbulbasaur = pokemon.pop(2)\nprint(pokemon)\nprint(bulbasaur)\n</code></pre> &gt;&gt;&gt; Output<pre><code>['Charmander', 'Charmeleon', 'Charizard']\nBulbasaur\n</code></pre> List manipulation <p>Define a <code>list</code> with a couple of elements (of your choice). Play around with the methods <code>append()</code>, <code>insert()</code>,  <code>remove()</code> and <code>pop()</code>. Print the <code>list</code> after  each operation to see the changes.</p>"},{"location":"python-extensive/containers/list/#organizing-a-list","title":"Organizing a <code>list</code>","text":"<p>For various reasons, often, your lists will be unordered. If you want to  present your <code>list</code> in a particular order, you can use the method  <code>sort()</code>, or the function <code>sorted()</code>.</p>"},{"location":"python-extensive/containers/list/#sort","title":"<code>sort()</code>","text":"<p>The <code>sort()</code> method operates on the <code>list</code> itself and  changes its order.</p> <pre><code>numbers = [5, 4, 1, 3, 2]\nnumbers.sort()  # sort in ascending order\nprint(numbers)\n\nnumbers.sort(reverse=True)  # sort in descending order\nprint(numbers)\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1, 2, 3, 4, 5]\n[5, 4, 3, 2, 1]\n</code></pre>"},{"location":"python-extensive/containers/list/#sorted","title":"<code>sorted()</code>","text":"<p>The <code>sorted()</code> function maintains the original order of a  <code>list</code> and returns a sorted <code>list</code> as well.</p> <pre><code>numbers = [5, 4, 1, 3, 2]\nsorted_numbers = sorted(numbers)\n\nprint(f\"Original list: {numbers}; Sorted list: {sorted_numbers}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Original list: [5, 4, 1, 3, 2]; Sorted list: [1, 2, 3, 4, 5]\n</code></pre>"},{"location":"python-extensive/containers/list/#length","title":"Length","text":"<p>You can easily find the length of a <code>list</code> with <code>len()</code>.</p> <pre><code>print(len([3.0, 1.23, 0.5]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>3\n</code></pre>"},{"location":"python-extensive/containers/list/#slicing","title":"Slicing","text":"<p>To make a slice (part of a <code>list</code>), you specify the index of the  first and last elements you want to work with. Elements up until the second index are  included. To output the first three elements in a <code>list</code>, you would request indices 0 through 3, which would return elements 0, 1, and 2.</p> <pre><code>players = [\"charles\", \"martina\", \"michael\", \"florence\", \"eli\"]\nprint(players[0:3])\n</code></pre> &gt;&gt;&gt; Output<pre><code>['charles', 'martina', 'michael']\n</code></pre> Slicing <p>Define a <code>list</code> of your choice with at least <code>5</code>  elements. </p> <ul> <li>Now, perform a slice from the second up to and including the fourth  element.</li> <li>Next, omit the first index in the slice (only omit the number!). What  happens?</li> <li>Lastly, re-add the first index and omit the second index of your  slice. Print the result.</li> </ul>"},{"location":"python-extensive/containers/list/#copy","title":"Copy","text":"<p>To copy a <code>list</code>, you can use the <code>copy()</code> method.</p> <pre><code>original_list = [1, 2, 3]\ncopied_list = original_list.copy()\n\n# perform some changes to both lists\noriginal_list.append(4)\ncopied_list.insert(0, \"zero\")\n\nprint(f\"Original list: {original_list}, Copied list: {copied_list}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Original list: [1, 2, 3, 4], Copied list: ['zero', 1, 2, 3]\n</code></pre>"},{"location":"python-extensive/containers/list/#be-careful","title":"Be careful!","text":"<p>You might wonder why we can't simply do something along the lines of  <code>copied_list = original_list</code>. With lists, we have to be careful,  as this syntax simply creates a reference to the original <code>list</code>. Let's look at an example:</p> <pre><code>original_list = [1, 2, 3]\ncopied_list = original_list\n\n# perform some changes to the original list\noriginal_list.append(4)\n\nprint(f\"Original list: {original_list}, Copied list: {copied_list}\")\n</code></pre> <p>which leaves us with: &gt;&gt;&gt; Output<pre><code>Original list: [1, 2, 3, 4], Copied list: [1, 2, 3, 4]\n</code></pre></p> <p>As you can see, the changes to the original <code>list</code> are reflected in  the copied one. You can read about this in more detail  here.</p> Note <p>We can actually check whether both lists point to the same object in memory by using <code>id()</code> which returns the memory address of an object.  Just remember, to be careful when copying lists and check if your program  behaves as intended!</p> <pre><code>original_list = [1, 2, 3]\ncopied_list = original_list\n\nprint(id(original_list) == id(copied_list))  # True\n</code></pre> Unknown method <p>Use the given <code>list</code> (don't worry about the syntax, it's just a  short expression to create a huge <code>list</code>):</p> <pre><code>long_list = [True] * 1000\n</code></pre> <ul> <li>Check the length of the <code>list</code>.</li> <li>Apply a method that deletes all elements in the <code>list</code> and  returns an empty list <code>[]</code>. You might need to use Google,  since it is a method not previously discussed.</li> <li>Check the length of the <code>list</code> again.</li> </ul>"},{"location":"python-extensive/containers/list/#recap","title":"Recap","text":"<p>We extensively covered lists and their manipulation.</p> <ul> <li>Accessing elements with indices (including slicing)</li> <li>The <code>IndexError</code></li> <li>Adding elements with <code>append()</code> and <code>insert()</code></li> <li>Removing elements with <code>remove()</code> and <code>pop()</code></li> <li>Sorting with <code>sort()</code> and <code>sorted()</code></li> <li>Length of a <code>list</code> with <code>len()</code></li> <li>Make a copy with <code>copy()</code></li> </ul>"},{"location":"python-extensive/containers/tuple/","title":"Tuples","text":""},{"location":"python-extensive/containers/tuple/#tuples","title":"Tuples","text":"<p>Lists and dictionaries work well for storing and manipulating data during the execution of a program. Both lists and dictionaries are mutable. However, sometimes you\u2019ll want to create a collection of elements that are immutable (can't change). Tuples allow you to do just that.</p> <pre><code># coordinates of MCI IV\ncoordinates = (47.262996862335854, 11.393082185178823)\nprint(type(coordinates))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'tuple'&gt;\n</code></pre> <p>A <code>tuple</code> is created with round brackets (<code>()</code>). As with lists and dictionaries, the elements are separated by commas. Tuples can hold any type of data.</p>"},{"location":"python-extensive/containers/tuple/#accessing-elements","title":"Accessing elements","text":"<p>With indexing, the individual elements of a <code>tuple</code> can be retrieved.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\nprint(coordinates[0])\nprint(coordinates[1])\n</code></pre> &gt;&gt;&gt; Output<pre><code>47.262996862335854\n11.39308218517882\n</code></pre>"},{"location":"python-extensive/containers/tuple/#immutability","title":"Immutability","text":"<p>Let's try to change the value of an element in a <code>tuple</code>.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\ncoordinates[0] = 50.102\n</code></pre> <p>we will encounter following error:</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;ipython-input-29-d74dc80ea879&gt;\", line 2, in &lt;module&gt;\n    coordinates[0] = 50.102\n    ~~~~~~~~~~~^^^\nTypeError: 'tuple' object does not support item assignment\n</code></pre> <p>As a <code>tuple</code> is immutable, you can only redefine the entire  <code>tuple</code>.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\n# redefine the entire tuple\ncoordinates = (5.513615392318705, 95.2060492604128)\n</code></pre>"},{"location":"python-extensive/containers/tuple/#tuple-unpacking","title":"<code>tuple</code> unpacking","text":"<p>Tuples can be unpacked, to use them separately.</p> <pre><code>coordinates = (47.262996862335854, 11.393082185178823)\nlatitude, longitude = coordinates\n</code></pre> Info <p>Tuples are often used for constants. In the above examples, we used  coordinates. As these coordinates are not going to change, a  <code>tuple</code> is a fitting data type.</p> Tuple unpacking <p>Use the following <code>tuple</code> with cities. <pre><code>cities = (\"New York\", \"Los Angeles\", \"Chicago\")\n</code></pre></p> <ul> <li>Print the first city.</li> <li>Use <code>tuple</code> unpacking and print the resulting variables.</li> </ul>"},{"location":"python-extensive/containers/tuple/#recap","title":"Recap","text":"<p>In this rather short section, we introduced tuples and covered:</p> <ul> <li>Mutability vs. immutability</li> <li>How to define a <code>tuple</code></li> <li>Access elements with indexing</li> <li>... and <code>tuple</code> unpacking</li> </ul>"},{"location":"python-extensive/control-structures/for/","title":"Loops","text":""},{"location":"python-extensive/control-structures/for/#loops-for","title":"Loops - <code>for</code>","text":""},{"location":"python-extensive/control-structures/for/#introduction","title":"Introduction","text":"<p>In this section, you\u2019ll learn how to loop through elements using just a few lines of code. Looping allows you to take the same action, or set of actions with every item in an iterable. Among iterables are for example, lists or dictionaries. As a result, you'll be able to streamline tedious tasks. First, we'll loop over lists.</p>"},{"location":"python-extensive/control-structures/for/#looping-over-lists","title":"Looping over lists","text":"<p>You\u2019ll often want to run through all entries in a <code>list</code>, performing the same task with each item. In the below example, we loop through a <code>list</code> of passwords and print the length of each one.</p> <pre><code>passwords = [\"1234\", \"password\", \"admin\", \"123456\"]\nfor password in passwords:\n    print(f\"Password: {password} is {len(password)} characters long.\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Password: 1234 is 4 characters long.\nPassword: password is 8 characters long.\nPassword: admin is 5 characters long.\nPassword: 123456 is 6 characters long.\n</code></pre> <p>A loop is written with the <code>for</code> statement. The <code>password</code> is a temporary variable that holds the current item in the <code>list</code>. You can  choose any name you want for the temporary variable that holds each value.  However, it\u2019s helpful to choose a meaningful name that represents a single item from the <code>list</code>. For example:</p> <pre><code>for experiment in experiments:\n    ...\nfor user in users:\n    ...\n</code></pre> Info <p>When you\u2019re using loops for the first time, keep in mind that the set  of steps is repeated once for each item in the <code>list</code>, no matter  how many items are in the <code>list</code>. If you have a million items  in your <code>list</code>, <code>Python</code> repeats these steps a million times.</p>"},{"location":"python-extensive/control-structures/for/#scope","title":"Scope","text":"<p>Python uses indentation (whitespace) to indicate, what is part of the loop.  With an indentation being four characters of whitespace. For a faster way to  intend, use the tab key Tab.</p> <p>Let's extend the example from above:</p> <pre><code>passwords = [\"1234\", \"password\", \"admin\", \"123456\"]\nfor password in passwords:\n    print(f\"Password: {password} is {len(password)} characters long.\")\n\nprint(\"All passwords have been checked.\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Password: 1234 is 4 characters long.\nPassword: password is 8 characters long.\nPassword: admin is 5 characters long.\nPassword: 123456 is 6 characters long.\nAll passwords have been checked.\n</code></pre> <p>You can easily see that only the first <code>print</code> statement is part of  the loop, simply because it is indented. The second <code>print</code> statement is executed after the loop has finished as it is outside the loop.</p>"},{"location":"python-extensive/control-structures/for/#indentationerror","title":"<code>IndentationError</code>","text":"<p>In longer programs, you\u2019ll notice blocks of code indented at a few different levels. These indentation levels help you gain a general sense of the overall program\u2019s organization.</p> <p>As you begin to write code that relies on proper indentation, you\u2019ll need to watch for a few common indentation errors.</p>"},{"location":"python-extensive/control-structures/for/#expected-indentation","title":"Expected indentation","text":"<pre><code>for number in [1, 2, 3]:\nprint(number)\n</code></pre> <pre><code>  Cell In[4], line 2\n    print(number)\n    ^\nIndentationError: expected an indented block after 'for' statement on line 1\n</code></pre> <p>As the <code>IndentationError</code> states, <code>Python</code> expects an indented  block of code after the <code>for</code> statement.</p>"},{"location":"python-extensive/control-structures/for/#unexpected-indentation","title":"Unexpected indentation","text":"<pre><code>message = \"Hello\"\n    print(message)\n</code></pre> <pre><code>  Cell In[9], line 2\n    print(message)\n    ^\nIndentationError: unexpected indent\n</code></pre> <p>In this case, the code snippet contains an unnecessary indentation.</p> Square numbers <p>Square each number in a given list and print the result. First, initialize a list of numbers from 1 to 10. Square each number and <code>print</code> it. Use a <code>for</code> loop.</p>"},{"location":"python-extensive/control-structures/for/#range","title":"<code>range()</code>","text":"<p>The <code>range()</code> function makes it easy to generate a series of numbers.  For example, you can use <code>range()</code> to print a series of numbers like this:</p> <pre><code>for value in range(3):\n  print(value)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0\n1\n2\n</code></pre> <p>Remember, that <code>Python</code> 'starts counting at <code>0</code>'. <code>3</code> is not  included in the output, as <code>range()</code> generates a sequence up to, but not including, the number you provide. You can also pass two arguments to <code>range()</code>, the first and the last number of the sequence. In this case, the sequence will start at the first number and end at the last number minus one.</p> <pre><code>for value in range(3, 6):\n    print(value)\n</code></pre> &gt;&gt;&gt; Output<pre><code>3\n4\n5\n</code></pre> <code>range()</code> <p>Use <code>range()</code> to build a <code>list</code> which holds the numbers from 15 to 20 - including 20.</p> Savings account growth <p>Write a <code>for</code> loop to calculate the growth of savings over a  period of time. Use following formula to calculate the future value of  savings in year \\(t\\):</p> \\[ \\text{A} = \\text{P} \\times \\left(1 + \\frac{\\text{r}}{100} \\right)^{\\text{t}} \\] <p>where:</p> <ul> <li>\\(\\text{A}\\) is the future value of the savings account or investment.</li> <li>\\(\\text{P}\\) is the present value of the savings account or investment.</li> <li>\\(\\text{r}\\) is the annual interest rate.</li> <li>\\(\\text{t}\\) is the number of years the money is invested for.</li> </ul> <p>Given values:</p> <ul> <li>\\(\\text{P} = 1000\\)</li> <li>\\(\\text{r} = 5\\)</li> </ul> <p>Print the future value of the savings account over a period of 10 years.  Skip each second year. Use  Python's documentation on range() as a starting point.</p>"},{"location":"python-extensive/control-structures/for/#detour-simple-statistics-on-lists-with-numbers","title":"Detour: Simple statistics on lists with numbers","text":"<p>A few functions are specific to lists of numbers. For example, you can easily find the minimum, maximum, and sum of a list of numbers:</p> <pre><code>numbers = [1.0, 8.38, 3.14, 7.0, 2.71]\nprint(\n    f\"Minimum: {min(numbers)}\",\n    f\"Maximum: {max(numbers)}\",\n    f\"Sum: {sum(numbers)}\", sep=\"\\n\"\n)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Minimum: 1.0\nMaximum: 8.38\nSum: 22.23\n</code></pre> Calculate the average <p>Calculate the average of the following list: <pre><code>numbers = [4.52, 3.14, 2.71, 1.0, 8.38]\n</code></pre></p>"},{"location":"python-extensive/control-structures/for/#list-comprehensions","title":"List comprehensions","text":"<p>... are a concise way to create lists.</p> <p>A list comprehension combines a <code>for</code> loop to create a new list in a single line.</p> Rewrite a list comprehension <p>Rewrite the following list comprehension in a regular for-loop to  achieve the same result: <pre><code>squares = [value**2 for value in range(1,11)]\nprint(squares)\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n</code></pre>"},{"location":"python-extensive/control-structures/for/#looping-over-dictionaries","title":"Looping over dictionaries","text":"<p>As previously discussed, you can not only loop over a <code>list</code>, but  also iterate over a variety of different data types, such as dictionaries. You can loop over a dictionary\u2019s key-value pairs, solely over the keys  or just the values.</p>"},{"location":"python-extensive/control-structures/for/#items","title":"<code>items()</code>","text":"<p>Using the <code>.items()</code> method, we can loop over the key-value pairs. Take note, that the method returns two values, which we store in two separate variables (<code>key</code> and <code>value</code>).</p> <p>We can freely choose the variable names in the <code>for</code>-loop. It does  not have to be <code>key</code> and <code>value</code> respectively.</p> <pre><code>parts = {\n    \"P100\": \"Bolt\",\n    \"P200\": \"Screw\",\n    \"P300\": \"Hinge\",\n}\n\nfor key, value in parts.items():\n    print(key, value)\n</code></pre> &gt;&gt;&gt; Output<pre><code>P100 Bolt\nP200 Screw\nP300 Hinge\n</code></pre>"},{"location":"python-extensive/control-structures/for/#values-keys","title":"<code>values()</code>, <code>keys()</code>","text":"Dictionary methods <p>Define a (non-empty) dictionary of your choice and use both methods <code>.values()</code> and <code>.keys()</code> to access solely values and keys respectively.</p>"},{"location":"python-extensive/control-structures/for/#recap","title":"Recap","text":"<p>With the introduction of the <code>for</code> loop, you can now start to  automate re-occurring tasks. We have covered:</p> <ul> <li>Looping over lists</li> <li>Indentation and possible resulting <code>IndentationError</code></li> <li><code>range()</code> to generate a series of numbers</li> <li>Simple statistics on lists of numbers</li> <li>List comprehensions</li> <li>Specific methods to loop over dictionaries</li> </ul>"},{"location":"python-extensive/control-structures/if/","title":"More Control Structures","text":""},{"location":"python-extensive/control-structures/if/#more-control-structures","title":"More Control Structures","text":""},{"location":"python-extensive/control-structures/if/#introduction","title":"Introduction","text":"<p>In this section, we will cover additional control structures. First, we  discuss the <code>if</code> statement, which allows us to execute code based on a  condition. Followed by the <code>elif</code>, <code>else</code> and  <code>while</code> statements.</p>"},{"location":"python-extensive/control-structures/if/#if","title":"<code>if</code>","text":"<p>The <code>if</code> statement lets you evaluate conditions. The simplest kind of  <code>if</code> statement has one condition and one action. Here is some  pseudocode:</p> <pre><code>if condition is True:\n    do something\n</code></pre> <p>You can put any condition in the first line and just about any action in the  indented block following the test. If the condition evaluates to <code>True</code>, <code>Python</code> executes the indented code following the <code>if</code> statement.  If the test evaluates to <code>False</code>, the indented code block (following the  <code>if</code>) is ignored.</p> <pre><code>user = \"admin\"\n\nif user == \"admin\":\n  print(f\"Welcome {user}!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Welcome admin!\n</code></pre> <p>First, the condition <code>user == \"admin\"</code> is evaluated. If it  evaluates to <code>True</code>, the indented print is executed. If the condition evaluates to <code>False</code>, the indented code block is ignored.</p> <p>Indentation plays the same role in <code>if</code> statements as it did in  <code>for</code> loops (see the previous section).</p> Password strength: Part 1 <p> </p> <p>In the section on comparisons and logical operators, you had to check whether a password meets certain criterias. The following example expands on this task as you are  given a list of passwords. You have to check if each password exceeds a length of 12 characters.</p> <p>Execute the first code cell to generate some random passwords  (note every time you rerun the code snippet, different passwords will be  generated).</p> <pre><code># generate passwords - simply execute the code to generate some random\n# passwords\nimport random\nimport string\n\npasswords = []\nfor i in range(10):\n    length = random.randint(3, 25)\n    password = \"\".join(random.choices(string.ascii_letters, k=length))\n    passwords.append(password)\n</code></pre> <p>The <code>list</code> <code>passwords</code> should look something like this: <pre><code>['PWgOYxQgnxgXm',\n 'gpOMVTmCSjAcndowkUd',\n 'ADKIEthzsGBr',\n 'VRLzOIZtEz',\n 'uOckmTJjeonUyMlnG',\n 'gjOpWuHrIbG',\n 'doxIylbRkNLdvdLNgVgYsDGzd',\n 'KvUdsgZhPIrS',\n 'LrdpffEqlBVQYr',\n 'ncyqXNLnVstVxlx']\n</code></pre></p> <p>Now, loop over the passwords and check if each password exceeds the  character limit of 12. If so, print the password.</p>"},{"location":"python-extensive/control-structures/if/#else","title":"<code>else</code>","text":"<p>Previously, every time the condition in the <code>if</code> statement  evaluated to <code>False</code>,  no action was taken. Hence, the <code>else</code> clause is introduced which  allows you to define a set of actions that are executed when the conditional test fails.</p> <pre><code>user = \"random_user\"\n\nif user == \"admin\":\n  print(f\"Welcome {user}!\")\nelse:\n  print(\"Only admins can enter this area!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Only admins can enter this area!\n</code></pre> Password strength: Part 2 <p>Let's expand on our previous example. Re-use your code to check the length of the generated passwords. Now, we would like to store all passwords that did not meet our criteria in the empty list <code>invalid_passwords</code>.</p> <p>Hint: Introduce an <code>else</code> statement to save the invalid passwords.</p>"},{"location":"python-extensive/control-structures/if/#elif","title":"<code>elif</code>","text":"<p>Often, you\u2019ll need to test more than two possible situations, and to evaluate these, you can use an <code>if-elif-else</code> syntax. <code>Python</code> executes only one block in an <code>if-elif-else</code> chain. It runs each conditional test in order until one passes. When a test passes, the code following that test is executed and the rest is skipped.</p> <pre><code>user = \"xX_user_Xx\"\nregistered_users = [\n    \"admin\",\n    \"guest\",\n    \"SwiftShark22\",\n    \"FierceFalcon66\",\n    \"BraveWolf11\"\n]\n\nif user == \"admin\":\n  print(f\"Welcome {user}!\")\nelif user not in registered_users:\n  print(\"Please create an account first!\")\nelse:\n  print(\"Only admins can enter this area!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Please create an account first!\n</code></pre> Info <p>As you might have noticed, you can use a single <code>if</code> statement or <code>if</code> in combination with <code>else</code>. For multiple conditions  you can add as many <code>elif</code> parts as you wish.</p>"},{"location":"python-extensive/control-structures/if/#while","title":"<code>while</code>","text":"<p>The <code>for</code> loop takes an iterable and executes a block of  code once for each element. In contrast, the <code>while</code> loop runs as  long as a certain condition is <code>True</code>.</p> <p>For instance, you can use a <code>while</code> loop to count up through a  series of numbers. Here is an example:</p> <pre><code># set a counter\ncurrent_number = 1\n\nwhile current_number &lt;= 5:\n  print(current_number)\n  # increment the counter value by one\n  current_number += 1\n</code></pre> &gt;&gt;&gt; Output<pre><code>1\n2\n3\n4\n5\n</code></pre> <p>Note, that the variable, that is checked in the <code>while</code>-condition  must be defined prior to the loop, otherwise we will encounter a  <code>NameError</code>.</p> Infinite loops <p>Moreover, the variable must be updated within the loop to avoid an infinite loop. For example, if <code>current_number</code> is not incremented by one, the condition <code>current_number &lt;= 5</code> will always evaluate to <code>True</code>, leaving us stuck in an infinite loop. In such cases, simply click the <code>Stop</code> button (on the left-hand side of the  respective code cell) to interrupt the execution.</p> Addition assignment <p>In the above example, we used the <code>+=</code> operator, referred to as  addition assignment. It is a shorthand for incrementing a variable by a  certain value.</p> <pre><code>a = 10\na += 5\nprint(a)\n</code></pre> &gt;&gt;&gt; Output<pre><code>15\n</code></pre> <p>The above code is equivalent to <code>a = a + 5</code>. This shorthand assignment can be used with all arithmetic operators, such as subtraction <code>-=</code> or division <code>/=</code>.</p> While loop <p>Write some code, to print all even numbers up to 42 using a while  loop.</p>"},{"location":"python-extensive/control-structures/if/#detour-user-input","title":"Detour: User input","text":"<p>Most programs are written to solve an end user\u2019s problem. To do so, usually  we need to get some information from the user. For a simple example, let\u2019s  say someone wants to enter a username.</p> <p>You can store the user input in the variable <code>user_name</code> like in the example  below.</p> <pre><code>user_name = input(\"Please enter your username:\")\n</code></pre> <p>However, the <code>input()</code> function always returns a string.</p> <pre><code>age = input(\"Please enter your age:\")\nprint(type(age))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'str'&gt;\n</code></pre> <p>... use casting to  convert the input to the desired type.</p>"},{"location":"python-extensive/control-structures/if/#break","title":"<code>break</code>","text":"<p>To exit any loop immediately without running any remaining 'loop code', use  the <code>break</code> statement. The <code>break</code> statement directs the flow of your program; you can use it to control which lines of code are executed and  which aren\u2019t, so the program only executes code that you want it to, when you want it to.</p> <pre><code>for i in range(5):\n    if i == 3:\n        break\n    print(i)\n\nprint(\"Continue running the program...\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>0\n1\n2\nContinue running the program...\n</code></pre>"},{"location":"python-extensive/control-structures/if/#continue","title":"<code>continue</code>","text":"<p>Rather than breaking out of a loop entirely, you can use the <code>continue</code>  statement to return to the beginning of the loop based on the result of a  condition.</p> <pre><code>for i in range(5):\n    if i == 3:\n        continue\n    print(i)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0\n1\n2\n4\n</code></pre>"},{"location":"python-extensive/control-structures/if/#recap","title":"Recap","text":"<p>In this section we have expanded on control structures. We discussed:</p> <ul> <li><code>if</code> statements and how to use them</li> <li><code>else</code> clauses</li> <li><code>elif</code> statements for multiple conditions</li> <li><code>while</code> loops</li> <li><code>break</code> and <code>continue</code> statements for more 'fine-grained'   control</li> </ul>"},{"location":"python-extensive/data/api/","title":"API","text":"<p>Another source of data is an application programming interface (API). An API  consists of a set of rules and protocols that allows one software application  to interact with another. In other words, it is a way to communicate with a server. Some of these servers are openly available and host data that can be accessed by anyone. Others require authentication and are therefore paid services.</p> <p>To illustrate the practical interaction with APIs, we will retrieve data from multiple different APIs, namely {JSON} Placeholder, GitHub and NASA.</p>"},{"location":"python-extensive/data/api/#first-example","title":"First example","text":"Open a website  <p>Open the {JSON} Placeholder website  here.</p> <p>As the site says {JSON} Placeholder is a public API which serves some fake  data. It is the perfect starting place, to try and retrieve some data. Without any prior knowledge you can start to send your first request using your browser.</p> Send your first request <p>Open following link within your browser to send your first request: https://jsonplaceholder.typicode.com/comments</p> <p>Observe the response. The structure of the resulting data should already look familiar.</p> # <p>Which Python type does the output of your request most closely resemble?</p> A pandas DataFrameA simple listA simple dictionary <p>The server response you got, was actually in the form of a <code>JSON</code> file. This is a common format for APIs to return data. Later, we can easily read the <code>JSON</code> with <code>Python</code> and convert it to a dictionary.</p> <p>In this case, you have sent a request to the <code>/comments</code> endpoint to  retrieve some fake data containing comments. </p>"},{"location":"python-extensive/data/api/#endpoints","title":"Endpoints","text":"<p>An endpoint is a specific URL that the API uses to perform a specific action. To send a request to the server, we need to specify the endpoint in the URL. The server will then respond with the requested data (if everything went  smoothly).</p> <p>In our example, <code>https://jsonplaceholder.typicode.com</code> is simply the URL of  the API and <code>/comments</code> is the endpoint of our interest. </p> Explore different endpoints <p>Again, visit the {JSON} Placeholder website and scroll down to the  Resources section. This specific API has 6 different endpoints. Try other  endpoints of your choice and observe the response. Specifically look at  the URL!</p>"},{"location":"python-extensive/data/api/#apis-python","title":"APIs &amp; Python","text":"<p>Since we don't want to manually use the browser anytime we want to retrieve  data, we now replicate a request in <code>Python</code> .  To send requests we can make use of the appropriately named <code>requests</code> package.</p> Setup <p>Within a virtual environment install the <code>requests</code> package.</p>  Windows MacOS <pre><code>pip install requests\n</code></pre> <pre><code>pip3 install requests\n</code></pre> <p>To send the same request to the <code>/comments</code> endpoint we can use following code snippet.</p> <pre><code>import requests\n\nresponse = requests.get(\"https://jsonplaceholder.typicode.com/comments\")\n</code></pre> <p>Since <code>response</code> does not explicitly return the data we have to access it with</p> <pre><code>data = response.json()\n</code></pre> Validate the above quiz question <p>What type is returned by the <code>response.json()</code> method?  Check the <code>type</code> of the <code>data</code> variable.</p> <p>We can easily convert the data into a tabular format with <code>pandas</code>.</p> <pre><code>import pandas as pd\n\ndata = pd.DataFrame(data)\n</code></pre> Info <p>Looking at <code>data.shape</code>, you'll notice that we retrieved 500 rows of data with a single request. With a <code>pandas.DataFrame</code> you can utilize all your knowledge you gained so far to further handle and plot the data.</p> Send a request with Python <p>Pick one of the 6 available endpoints. Write code to</p> <ol> <li>Send a request</li> <li>Access the data</li> </ol>"},{"location":"python-extensive/data/api/#methods","title":"Methods","text":"<p>In the above code snippet, we applied <code>requests</code> <code>get()</code> method. The <code>get</code> method solely retrieves data from the server, that is no data is  sent to or modified on the server. In fact, all endpoints of {JSON} Placeholder  can be accessed using the <code>GET</code> method.</p> <p>Nevertheless, <code>GET</code> is not the only method, there are also <code>POST</code>, <code>PUT</code>, <code>DELETE</code>, and <code>PATCH</code>. Following table provides a brief overview:</p> Method Description <code>requests</code> method GET Retrieve data from the server <code>requests.get()</code> POST Create data on the server <code>requests.post()</code> PUT Update data on the server <code>requests.put()</code> DELETE Delete data on the server <code>requests.delete()</code> PATCH Partially update data on the server <code>requests.patch()</code> <p>We will continue solely with <code>GET</code> methods.</p> <p></p> Info <p>If you need to revisit the topic of HTTP methods or simply want to dive  deeper, here's a great article.</p>"},{"location":"python-extensive/data/api/#github-api","title":"GitHub API","text":"<p>Since APIs serve different purposes, we switch to another real world example, the GitHub API.</p> Info <p> </p> <p>GitHub (github.com) is a popular web-based platform that serves as:</p> <ul> <li>A hosting service for software development and version control</li> <li>A place where developers store, share, and collaborate on code projects</li> </ul> <p>Think of GitHub as a social network for programmers - like a combination of  Google Drive, LinkedIn, and Instagram, but for code. Developers can:</p> <ul> <li> Store their code in \"repositories\" (like folders)</li> <li> Track changes to their code over time</li> <li> Collaborate with others on projects</li> <li> Share their work with the global developer community</li> </ul> <p>Besides browsing GitHub on the web, we can also access information  programmatically using their API. You can find more information  here.</p> <p>The base URL of the Github API is</p> <pre><code>https://api.github.com\n</code></pre> <p>For example, we can retrieve the information of a specific user with the  <code>/users</code> endpoint (official documentation).</p> <pre><code>https://api.github.com/users/{username}\n</code></pre> <p>Using the above URL as is won't work, as we need to specify the specific  user we are looking for. Thus, we have to replace <code>{username}</code> with the actual user. This concept is known as a path parameter.</p> <pre><code>username = \"mciwing\"\nurl = f\"https://api.github.com/users/{username}\"\n\nresponse = requests.get(url)\ndata = response.json()\n</code></pre> Tip <p>With an f-string you can easily set the path parameter using a variable.</p> <p>You accessed some public information on the <code>mciwing</code> user who manages the code for this website and content you're currently reading.</p> Not found <p>Use the same endpoint, this time around you intentionally have to look for  a non-existent username. Use this nonsensical one:</p> <pre><code>username = \"iwo2j\u00f6iojfnvjlkhsnkjdvn\"\n</code></pre> <p>What's the response?</p>"},{"location":"python-extensive/data/api/#http-status-codes","title":"HTTP Status Codes","text":"<p>When making requests to an API, the server responds with a status code  indicating how the request was handled. Common status codes include:</p> Code Meaning Description 200 OK Request succeeded 404 Not Found Requested resource doesn't exist 403 Forbidden Server understood but refuses to authorize 429 Too Many Requests You've exceeded the rate limit 500 Internal Server Error Something went wrong on the server Status codes <p>You can find a complete list of status codes at the  MDN Web Docs.</p> <p>You can access the status code of any response using the <code>status_code</code>  attribute. Let's look at another example. With the</p> <pre><code>https://api.github.com/repos/{owner}/{repo}/contributors\n</code></pre> <p>endpoint, we can retrieve all contributors of a specific project. We access information of this site's project (repository):</p> <pre><code>owner = \"mciwing\"\nrepo = \"code-campus\"\n\nurl = f\"https://api.github.com/repos/{owner}/{repo}/contributors\"\n\nresponse = requests.get(url)\nprint(response.status_code)  # (1)!\n</code></pre> <ol> <li>If everything went smoothly the status code is <code>200</code></li> </ol> <p>We can now easily sum up the contributions of all authors.</p> <pre><code>n_contributions = 0\nfor contributor in response.json():\n    n_contributions = n_contributions + contributor[\"contributions\"]  # (1)!\nprint(n_contributions)\n</code></pre> <ol> <li>Remember, we are dealing with a <code>dict</code> hence, we can easily access      the number of contributions using the corresponding key      <code>\"contributions\"</code>.</li> </ol> <p>At the time of writing <code>813</code> contributions were made by all authors  to build this site. If you execute the code, the number has changed  as we are continually working on the site. </p> Tip <p>That's the power of APIs, you're able to access the up-to date information.  If you want to have the latest data, all you have to do is execute your  code again.</p>"},{"location":"python-extensive/data/api/#limits","title":"Limits","text":"<p>Although we have not encountered any limitations so far, most APIs come with  various limitations to ensure fair usage:</p> <ul> <li> <p> Rate Limits</p> <p>Rate limits restrict the number of requests you can make within a specific time frame. For example:</p> <ul> <li>200 requests per minute</li> <li>1000 requests per day</li> <li>5 requests per second</li> </ul> <p>If you exceed these limits, the server typically responds with a  <code>429 Too Many Requests</code> status code.</p> </li> <li> <p> Authentication</p> <p>Many APIs require authentication for:</p> <ul> <li>Tracking usage</li> <li>Controlling access</li> <li>Billing purposes</li> </ul> </li> </ul> <p>Depending on the API, you may experience rate limits or have to authenticate your requests or simply have to pay for the service. Either way, the specific API documentation will guide you through the process.</p>"},{"location":"python-extensive/data/api/#nasa-api","title":"NASA API","text":"<p>To conclude the chapter you will look at another real world example and  get the current astronomy picture of the day (APOD) from NASA.  Here is the one from the 6th of February 2026:</p> <p>Now it's your turn to read the documentation and retrieve the picture of the  day.</p> Figure out the URL path <p>Open the NASA API, scroll down to the Browse APIs  section and expand the APOD container. Figure out the URL path. Send a  request in Python.</p> Info <p>Although most NASA APIs require an API key, the APOD API is free to use and tasks can be performed without an API key.</p> <p>Obviously we would like to visualize the picture. To do so, use following  helper function:</p> <pre><code>from io import BytesIO\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef plot_image_from_url(url):\n    \"\"\"Plot an image from a URL.\"\"\"\n    # Retrieve the image from the URL with requests\n    img_response = requests.get(url)\n    img = Image.open(BytesIO(img_response.content))\n\n    # Display the image\n    plt.imshow(img)\n    plt.axis(\"off\")  # Hide the axes\n    plt.tight_layout()\n    plt.show()\n</code></pre> Info <p>The above code uses requests to retrieve the image from the URL and then utilizes the <code>Pillow</code> and <code>maptlotlib</code> packages to plot it.</p> Plot the image <p>Use the above function to plot the image you retrieved from the NASA API.</p> <ol> <li>Install any missing packages.</li> <li>Access the appropriate key from the response data to get the URL of      the image.</li> <li>Pass the URL to the <code>plot_image_from_url</code> function.</li> </ol> More information <p>The APOD API provides more information than just the image.  Since we want to have further information on the picture, look at the  response data and print a description of your picture.</p>"},{"location":"python-extensive/data/api/#conclusion","title":"Conclusion","text":"<p>In our examples, we have seen how to retrieve information from an different  APIs, work with the data and visualize it. </p> <p>Despite our specific use case, concepts like rate limits, endpoints, request methods and status codes were introduced along the way which are universal to  APIs.</p> Apply your knowledge <p>You should be able to apply your knowledge to other APIs as well. Here  are just a couple of other APIs<sup>1</sup>:</p> <ul> <li>OpenWeatherMap for weather data</li> <li>NASA has a couple APIs for various use cases</li> <li>Google Search      access search results programmatically</li> <li>Spotify access      Spotify's music data</li> <li>Instagram      access Instagram's data</li> </ul> <p>The possibilities are endless. \ud83d\ude80</p> <p>If you want to dive deeper into APIs, we recommend the following resources:</p> <ul> <li>HTTP Status Codes:      Dive deeper into the server responses and their various status codes.</li> <li>FastAPI: Build an API yourself with Python</li> </ul> <ol> <li> <p>Some of these APIs require authentication or are paid services.\u00a0\u21a9</p> </li> </ol>"},{"location":"python-extensive/data/tabular/","title":"Tabular Data","text":"<p>With a couple of practical examples, we will discover tips on how to work with tabular data, where to find it, and the various sources and formats you can explore. This chapter will guide you through working with different types of structured data, enabling you to extract, transform, and analyze information from multiple sources.</p> Info <p>This chapter is an extension to the previous <code>pandas</code>  chapter. It should equip you with the necessary skills to acquire data  from various different sources.</p> <p>Our journey will cover a selection of following topics:</p> <ol> <li>Excel: Learn how to read spreadsheets</li> <li>Web Scraping: Extract tables directly from online sources</li> <li>File Writing: Save your data to disk</li> <li>Various Sources: An incomplete list of further data sources</li> </ol>"},{"location":"python-extensive/data/tabular/#prerequisites","title":"Prerequisites","text":"<p>For this chapter we recommend to initialize a new project. Additionally,  create a new virtual environment and activate it.</p> <p>You should end up with a project structure similar to:</p> <pre><code>\ud83d\udcc1 tabular/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u2514\u2500\u2500 \ud83d\udcc4 tabular_data.ipynb\n</code></pre>"},{"location":"python-extensive/data/tabular/#excel","title":"Excel","text":"<p>Let's start off with arguably the most common data source: Excel spreadsheets.</p> <p>Reading Excel files can be straightforward, if they are properly  structured! However, if you see files like these...</p> <p></p> <p>... run, or it will take you several days to parse the file. </p> <p>Although, the example might exaggerate, it is not uncommon to encounter  spreadsheets that are easily readable by humans but hard to parse by machines. Like in the example, the title, empty rows and columns, merged cells,  column names spanning multiple lines, pictures and other formatting can  make it difficult to extract the data in a structured manner.</p>"},{"location":"python-extensive/data/tabular/#reading-excel-files","title":"Reading Excel files","text":"<p>Download the following (structured) file to get started:</p> <p>Student data </p> <p>Data source: Statistik Austria - data.statistik.gv.at<sup>1</sup></p> <p>Place the file within your project directory. The data set contains the  number of students enrolled at universities of applied sciences in Austria  per semester. </p> <p>If you are an MCI student, you are part of this data set!</p> Interested in the creation of the Excel? <p>Since, Statistik Austria provides the data across three files, a <code>Python</code>  script was used to merge everything  into a single Excel. Below you can find the code snippet:</p> <pre><code># Data from:\n# https://data.statistik.gv.at/web/meta.jsp?dataset=OGD_fhsstud_ext_FHS_S_1\nimport pandas as pd\n\nstudents = pd.read_csv(\"OGD_fhsstud_ext_FHS_S_1.csv\", sep=\";\")\nsemester = pd.read_csv(\n    \"OGD_fhsstud_ext_FHS_S_1_C-SEMESTER-0.csv\",\n    sep=\";\",\n    usecols=[\"code\", \"name\"],\n)\nheader = pd.read_csv(\n    \"OGD_fhsstud_ext_FHS_S_1_HEADER.csv\", sep=\";\", usecols=[\"code\", \"name\"]\n)\n\n# replace column codes with their corresponding names\nstudents = students.rename(\n    columns={row[\"code\"]: row[\"name\"] for _, row in header.iterrows()}\n)\n# replace semester codes with their descriptions\nstudents.Berichtssemester = students.Berichtssemester.replace(\n    {row[\"code\"]: row[\"name\"] for _, row in semester.iterrows()}\n)\n\n# get term\nstudents[\"Semester\"] = students.Berichtssemester.str.split(\" \").str[0]\n\n# write Excel\nstudents.to_excel(\"fhsstud.xlsx\", index=False)\n</code></pre> <p>To read the Excel file, we will use <code>pandas</code> in conjunction with <code>openpyxl</code>  (to read and write Excel files):</p> <pre><code>pip install pandas openpyxl\n</code></pre> Tip <p>You can install multiple packages with a single command. Simply  separate the package name with a space.</p> <p>To read the file, it's as simple as:</p> <pre><code>import pandas as pd\n\ndata = pd.read_excel(\"fhsstud.xlsx\")\n</code></pre> <p>As the file is structured, the data loads without any issues. </p>"},{"location":"python-extensive/data/tabular/#reading-specific-sheets","title":"Reading specific sheets","text":"<p>By default, <code>pd.read_excel()</code> loads the first sheet. If you want to read another sheet, you can specify it  with the <code>sheet_name</code> parameter:</p> Create and read a new sheet <ol> <li>Open <code>fhsstud.xlsx</code> within Excel. </li> <li>Manually create a new sheet and fill it with some data of your choice.</li> <li>Save the file.</li> <li>Read the new sheet with <code>pd.read_excel()</code>.</li> </ol>"},{"location":"python-extensive/data/tabular/#detour-visualize-enrolled-students","title":"Detour: Visualize enrolled students","text":"<p>To further consolidate your visualization skills, obtained in the  Plotting chapter, we create a simple plot to visualize the  total of newly enrolled students per winter term in Austria.</p> <p>On a side note, it's quite interesting that the numbers are steadily rising,  with a dip in the winter term 2022/23.</p> Create a static version of the plot <p>Recreate the above plot with <code>pandas</code> and <code>matplotlib</code> as backend.  It does not have to be the same colors, background, title etc.</p> <ol> <li>Subset the data by winter term.</li> <li>Create a suitable plot (e.g., line plot, area plot).</li> </ol>"},{"location":"python-extensive/data/tabular/#writing-excel-files","title":"Writing Excel files","text":"<p>You can't just easily read Excel files, but also write them.</p> <pre><code>data.to_excel(\"fhsstud_copy.xlsx\", index=False)  # (1)!\n</code></pre> <ol> <li>The <code>index=False</code> parameter omits the index to be written to the     file. Have a look at your <code>DataFrame</code>'s index with <code>data.index</code>.</li> </ol> <p>Or you can write multiple sheets:</p> <pre><code>with pd.ExcelWriter(\"fhsstud_multiple.xlsx\") as writer:\n    data.to_excel(writer, sheet_name=\"Students\", index=False)\n    data.to_excel(writer, sheet_name=\"Students-Copy\", index=False)\n</code></pre> <p>Although the same data is written to two different named sheets, you should  get the idea.</p>"},{"location":"python-extensive/data/tabular/#with-statement","title":"<code>with</code> statement","text":"<p>The <code>with</code> statement is used to wrap the execution of a block of code. It is commonly used for resource management, such as opening files or managing database connections, ensuring that resources are properly cleaned up after use</p> <p>In the above example, the <code>with</code> statement is used to open an Excel file for writing and ensures that the Excel writer is properly closed after writing the data.</p>"},{"location":"python-extensive/data/tabular/#web-scraping","title":"Web Scraping","text":"<p>Web scraping is a technique to extract data from websites. It can be used to extract structured data from HTML pages, such as tables.</p> <p>To illustrate web scraping, we pick an example from Wikipedia as our  HTML. We use the english article of the  ATX (Austrian Traded Index) to retrieve a data set with all companies listed in the ATX.</p> Visit the article <ol> <li>Open a new browser tab and visit the  ATX Wikipedia article.</li> <li>Open the source code of the page, the HTML code. To do so, right-click on the page and select <code>View page source</code>.  Alternatively, use the shortcut Ctrl+U. Simply scroll through the HTML code a bit.</li> </ol> <p>You might have noticed that the HTML code is quite complex. Nevertheless,  we can easily extract all the tables on the page with <code>pandas</code>:</p> <pre><code>tables = pd.read_html(\"https://en.wikipedia.org/wiki/Austrian_Traded_Index\")\nprint(type(tables))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'list'&gt;\n</code></pre> <p>Simply by passing the URL to <code>pd.read_html()</code>, we get a list of <code>DataFrame</code> objects. Each <code>DataFrame</code> corresponds to a table found on the page.</p> <p>The second table on the page contains the companies listed in the ATX.  Let's have a look:</p> <pre><code>atx_companies = tables[1]\nprint(atx_companies.head())\n</code></pre> Company Industry Sector Erste Bank Financials Banking Verbund Utilities Electric Utilities OMV Basic Industries Oil &amp; Gas BAWAG Financials Banking Andritz Industrial Goods &amp; Services Industrial Engineering &amp; Machinery <p>The resulting <code>DataFrame</code> <code>atx_companies</code> can be perfectly used as is,  without any further data cleaning.</p> Tip <p>The <code>pd.read_html()</code> function is a powerful tool to extract  tables from HTML pages. However, it might not always work out of the  box by simply passing a URL. Most websites have a complex structure, and the tables might not be directly accessible. In such cases, you might need to use a more sophisticated web scraping packages like</p> <ul> <li><code>BeautifulSoup</code></li> <li><code>Scrapy</code></li> <li><code>Selenium</code></li> </ul> Info <p>Be nice to the websites you scrape! Always check the website's <code>robots.txt</code> file to see if web scraping is allowed. For example,  Wikipedia's <code>robots.txt</code> file can be found at  https://en.wikipedia.org/robots.txt.</p> <p>Respect the website's terms of service and don't overload the  server with requests.</p>"},{"location":"python-extensive/data/tabular/#file-writing","title":"File Writing","text":"<p>So far we have written data solely to Excel files. However, <code>pandas</code> supports a variety of file formats for writing data. Using the <code>atx_companies</code> data  set, we explore two more useful file formats.</p>"},{"location":"python-extensive/data/tabular/#csv","title":"CSV","text":"<p>Writing to a CSV (Comma Separated Values) file is as simple as:</p> <pre><code>atx_companies.to_csv(\"atx_companies.csv\", index=False)\n</code></pre> Tip <p>If you are dealing with large data sets, you might want to consider  compressing the output file and thus reducing the file size.</p> <p>Simply pass a compression algorithm as <code>str</code> (e.g.,<code>\"gzip\"</code>), to the <code>compression</code> parameter:</p> <pre><code>atx_companies.to_csv(\"atx_companies.csv.gz\", index=False, compression=\"gzip\")\n</code></pre> <p>To read a compressed file:</p> <pre><code>_atx_companies = pd.read_csv(\"atx_companies.csv.gz\", compression=\"gzip\")\n</code></pre> <p>Have a look at the documentation of  <code>DataFrame.to_csv()</code> for further options.</p>"},{"location":"python-extensive/data/tabular/#latex","title":"LaTeX","text":"<p>Exporting your data to a LaTeX table can be easily achieved with:</p> <pre><code>atx_companies.to_latex(\"atx_companies.tex\", index=False)\n</code></pre> Additional dependency <p>To use the LaTeX export functionality, <code>Jinja2</code> is required.</p> <pre><code>pip install Jinja2\n</code></pre> <p>Since CSV and LaTeX formats are just a fraction of the supported file formats, navigate to panda's Input/Output section for further reference. </p>"},{"location":"python-extensive/data/tabular/#data-sources","title":"Data Sources","text":"<p>Apart from Excel files and web scraping, there are numerous other  online sources where you can find structured data. Here are a  couple of further links to explore:</p> <ul> <li>data.gv.at - Open government data from Austria    covering various topics like economy, environment, and society.</li> <li>data.statistik.gv.at -    Statistics Austria portal for open data ranging from population,    environment to economy and tourism.</li> <li>UCI Machine Learning Repository -   Popular data repository for machine learning, hosting classic data sets    from various domains.</li> <li>Kaggle - A platform owned by Google to    share data sets, models and code. Although kaggle is free to access, you need   to create an account to download data sets.</li> <li>Google Dataset Search -    Google's search engine for data sets. It helps you find data sets stored    across the web.</li> </ul>"},{"location":"python-extensive/data/tabular/#recap","title":"Recap","text":"<p>This chapter equipped you with the necessary skills to work with Excel  files in <code>Python</code> . Furthermore, we have dipped  our toes into web scraping and learned how to extract structured data from a simple HTML page.</p> <p>Two brief sections on writing data to different file formats and a list of data sources rounded off the chapter.</p> <p>Next up, we will introduce APIs (Application Programming Interfaces) as another possible source of data.</p> <ol> <li> <p>Studien an Fachhochschulen At the time of writing (December 2024), the data was last updated on  2024-07-25.\u00a0\u21a9</p> </li> </ol>"},{"location":"python-extensive/types/bool_and_none/","title":"Boolean & None","text":""},{"location":"python-extensive/types/bool_and_none/#boolean-none","title":"Boolean &amp; None","text":"<p>In this section, we introduce two more data types, namely boolean (<code>bool</code>) and None (<code>NoneType</code>). Let's start with the latter one,  <code>NoneType</code>.</p>"},{"location":"python-extensive/types/bool_and_none/#none-nonetype","title":"None (<code>NoneType</code>)","text":"<p><code>NoneType</code> is a special data type in Python that represents the absence of a value.</p> <pre><code>nothing = None\nprint(type(nothing))\n</code></pre> <p>... which outputs:</p> &gt;&gt;&gt; Output<pre><code>&lt;class 'NoneType'&gt;\n</code></pre> <p><code>None</code> can be used as a placeholder for a variable which will be assigned a value later on. Furthermore, if a program was not able to return  a value, <code>None</code> can be used as a return value. </p> <p>Later, <code>None</code> will play a bigger role. For now, we simply keep in mind that <code>None</code> is a thing.</p>"},{"location":"python-extensive/types/bool_and_none/#detour-typeerror","title":"Detour: <code>TypeError</code>","text":"... yet another error. <p>Often, you\u2019ll want to use a variable\u2019s value within a message. For example, say you want to wish someone a happy birthday. You might write code like this:</p> <pre><code>age = 23\nmessage = \"Happy \" + age + \"rd Birthday!\"\nprint(message)\n</code></pre> <p>... results in.</p> <pre><code>Traceback (most recent call last):\n  File \"C:\\\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-34-80a141e301d6&gt;\", line 2, in &lt;module&gt;\n    message = \"Happy \" + age + \"rd Birthday!\"\n              ~~~~~~~~~^~~~~\nTypeError: can only concatenate str (not \"int\") to str\n</code></pre> <p>You might expect this code to print the simple birthday greeting, <code>Happy 23rd birthday!</code>. But if you run this code, you\u2019ll see that it generates an  error.</p> <p>This is a <code>TypeError</code>. It means Python encounters an unexpected  type in <code>age</code>, as strings were mixed with integers in the expression. We will  easily fix the <code>TypeError</code> in the next section.</p>"},{"location":"python-extensive/types/bool_and_none/#casting","title":"Casting","text":"<p>When you use integers within strings like this, you need to specify explicitly  that you want Python to use the integer as a string of characters.  You can do this by wrapping the variable in the <code>str()</code> function, which tells <code>Python</code> to represent non-string values as strings:</p> <pre><code>age = 23\nmessage = \"Happy \" + str(age) + \"rd Birthday!\"\nprint(message)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Happy 23rd Birthday!\n</code></pre> <p>Changing the type of <code>age</code> to string is called casting.</p> Info <p>A <code>TypeError</code> can stem from various 'sources'. This is just one  common example.</p> <p>In the following example the integers <code>3</code> and <code>2</code> were  implicitly cast to floating point numbers, to calculate the result,  which is a floating point number.</p> <pre><code>print(type(3 / 2))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'float'&gt;\n</code></pre> <p>With the function <code>int()</code> any value can be explicitly cast into an  integer, if possible. The value to be cast is passed as the input parameter.</p> <pre><code>number = 3.0\nprint(type(number))\n\n# casting\nnumber = int(number)\nprint(type(number))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'float'&gt;\n&lt;class 'int'&gt;\n</code></pre> <p>To explicitly cast a value into a float, use the function <code>float()</code>.</p> Casting <pre><code># variables\nfirst = \"12\"\nsecond = \"1.2\"\nthird = 12\n</code></pre> <p>For each of the given variables, check whether you can cast them to another type. First, print the type of each variable. Then, use <code>int()</code>, and <code>float()</code> to cast the variables.</p> Note <p>Remember the f-string (<code>f\"...\"</code>) from the previous section? Try a slightly modified example from above.</p> <pre><code>age = 23\nmessage = f\"Happy {age}rd Birthday!\"\nprint(message)\n</code></pre> <p>You'll notice, that there's no need for any explicit casting of <code>age</code>.</p> <p>Whenver, you want to include a variable in a string, remember that  f-strings might be more convenient. \ud83d\ude09</p>"},{"location":"python-extensive/types/bool_and_none/#booleans","title":"Booleans","text":"<p>Computers work with binary (e.g. <code>True</code> or <code>False</code>). Such information can be stored in a single bit. A boolean is either <code>True</code> or <code>False</code>. Booleans are often used to keep  track of certain conditions, such as whether a game is running or whether a  user can edit certain content on a website:</p> <pre><code>game_active = True\ncan_edit = False\n\nprint(type(True))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'bool'&gt;\n</code></pre>"},{"location":"python-extensive/types/bool_and_none/#recap","title":"Recap","text":"<p>In this section, we have introduced two more data types in Python:</p> <ul> <li>None (<code>NoneType</code>)</li> <li>and Booleans (<code>bool</code>)</li> </ul> <p>Now, we have covered all basic types! \ud83c\udf89 With that knowledge, we can already  start to do comparisons and logical operations.</p>"},{"location":"python-extensive/types/numbers/","title":"Integers & Floats","text":""},{"location":"python-extensive/types/numbers/#integers-floats","title":"Integers &amp; Floats","text":""},{"location":"python-extensive/types/numbers/#integers","title":"Integers","text":"<p><code>Python</code>  treats numbers in several different ways, depending on how they are used. Let\u2019s first look at how <code>Python</code> manages whole  numbers called integers (<code>int</code>).</p> <p>Any number without decimal places is automatically interpreted as an integer.</p> <pre><code>number = 10176\n</code></pre> <p>We can verify the type of the variable <code>number</code> with <code>type()</code>.</p> <pre><code>number = 10176\nprint(type(number))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'int'&gt;\n</code></pre>"},{"location":"python-extensive/types/numbers/#arithmetic-operators","title":"Arithmetic operators","text":"<p>Of course, with integers, you can perform basic arithmetic operations. These are:</p> Operator Description <code>+</code> Addition <code>-</code> Subtraction <code>*</code> Multiplication <code>/</code> Division <code>**</code> Exponentiation <code>%</code> Modulo (Used to calculate the remainder of a division) <code>//</code> Floor division <pre><code># Modulo\n20 % 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>2\n</code></pre> <pre><code># Floor division\n20 // 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>6\n</code></pre> Info <p>The floor division <code>//</code> is often referred to as integer division. It rounds down to the nearest whole number.</p> Integer division<pre><code>20 // 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>6\n</code></pre> <p>Contrary, the divison operator <code>/</code> does not round the result.</p> Float division<pre><code>20 / 3\n</code></pre> &gt;&gt;&gt; Output<pre><code>6.666666666666667\n</code></pre> <p>Hence, <code>/</code> is referred to as float division as it always returns  a  <code>float</code> (a number with decimal places). More on floats in a  second.</p> <p>Moreover, you can use multiple operations in one expression. You can also use parentheses to modify the order of operations so Python can evaluate your  expression in the order you specify. For example:</p> <pre><code>2 + 3 * 4\n</code></pre> &gt;&gt;&gt; Output<pre><code>14\n</code></pre> <pre><code>(2 + 3) * 4\n</code></pre> &gt;&gt;&gt; Output<pre><code>20\n</code></pre>"},{"location":"python-extensive/types/numbers/#floats","title":"Floats","text":"<p>Any number with decimal places is automatically interpreted as a <code>float</code>.</p> <pre><code>number = 10176.0\nprint(type(number))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'float'&gt;\n</code></pre> <p>All previously introduced arithmetic operations can be used for floats as well.</p> <pre><code>3.0 + 4.5\n# operations with floats and integers\n3.0 * (4 / 2)\n</code></pre>"},{"location":"python-extensive/types/numbers/#limitations","title":"Limitations","text":"Info <p>Floats are not 100% precise. \"[...] In general, the decimal floating-point  numbers you enter are only approximated by the binary floating-point  numbers actually stored in the machine.\" (The Python Tutorial, 2024)<sup>1</sup></p> <p>So be aware that these small numerical errors could add up in complex  calculations.</p> <pre><code>34.3 + 56.4\n</code></pre> <p>... results in</p> &gt;&gt;&gt; Output<pre><code>90.69999999999999\n</code></pre> Calculate the BEP <p>Use variables and arithmetic operations to calculate the break-even point (the number of units that need to be sold to cover the costs) for a product. The break-even point is given as:</p> \\[ \\text{BEP (units)} = \\frac{\\text{Fixed Costs}}{\\text{Price per Unit} - \\text{Variable Cost per Unit}} \\] <p>Calculate the \\(\\text{BEP}\\) for the following values:</p> <ul> <li>Fixed Costs: 30000</li> <li>Variable Cost per Unit: 45</li> <li>Price per Unit: 75</li> </ul> <p>Assign each given value to a variable. Print the result in a sentence, e.g.  <code>\"The break-even point is X units.\"</code></p>"},{"location":"python-extensive/types/numbers/#recap","title":"Recap","text":"<p>This section was all about numbers in Python. We have covered:</p> <ul> <li>Whole numbers  <code>int</code></li> <li>Decimal numbers  <code>float</code></li> <li>Floating-point arithmetic issues and limitations</li> </ul> <p>Next up, we will introduce the <code>bool</code> and <code>NoneType</code> type in Python.</p> <ol> <li> <p>The Python Tutorial \u21a9</p> </li> </ol>"},{"location":"python-extensive/types/strings/","title":"Strings","text":""},{"location":"python-extensive/types/strings/#strings","title":"Strings","text":"<p>So far, we have already stored some text in a variable. For example  <code>\"Hello World!\"</code> which is called a string. A string is a primitive data type. Integer, float, boolean and None are also primitive data types which we  will cover later.</p> <p>A string is simply a series of characters. Anything inside quotes is considered a string in <code>Python</code>, and you can use single (<code>'</code>) or double  quotes (<code>\"</code>) around your strings like this:</p> <pre><code>text = \"This is a string.\"\nanother_text = 'This is also a string.'\n</code></pre> <p>This flexibility allows you to use quotes and apostrophes within your strings:</p> <pre><code>text = \"One of Python's strengths is its diverse and supportive community.\"\nprint(text)\n</code></pre> &gt;&gt;&gt; Output<pre><code>One of Python's strengths is its diverse and supportive community.\n</code></pre> <pre><code>text = 'I told my friend, \"Python is my favorite language!\"'\nprint(text)\n</code></pre> &gt;&gt;&gt; Output<pre><code>I told my friend, \"Python is my favorite language!\"\n</code></pre>"},{"location":"python-extensive/types/strings/#type","title":"<code>type()</code>","text":"<p>Let's check the type of the variable <code>text</code>.</p> <pre><code>text = \"The language 'Python' is named after Monty Python, not the snake.\"\nprint(type(text))\n</code></pre> &gt;&gt;&gt; Output<pre><code>&lt;class 'str'&gt;\n</code></pre> <p><code>type()</code> comes in handy to check the type of variables. In this  case, we can verify that <code>text</code> is indeed a string. Just like  <code>print()</code>, <code>type()</code>  is an important tool in your programming arsenal.</p> Info <p>It is advisable to consistently enclose your strings with either single  <code>'...'</code> or double quotes <code>\"...\"</code>. This will make your  code easier to read and maintain.</p>"},{"location":"python-extensive/types/strings/#string-methods","title":"String methods","text":"<p>One of the simplest string manipulation, is to change the case of  the words in a string.</p> <pre><code>name = \"paul atreides\"\nprint(name.title())\n</code></pre> &gt;&gt;&gt; Output<pre><code>Paul Atreides\n</code></pre> <p>A method is an action performed on an object (in our case the  variable). The dot (.) in <code>name.title()</code> tells <code>Python</code> to  make the <code>title()</code> method act on the variable <code>name</code> which holds  the value <code>\"paul atreides\"</code>.</p> <p>Every method is followed by a set of parentheses, because methods often need additional information to do their work. That information is provided inside the parentheses. The <code>title()</code> method doesn\u2019t need any additional information, so its parentheses are empty.</p>"},{"location":"python-extensive/types/strings/#methods-vs-functions","title":"Methods vs. functions","text":"<p>We have already encountered functions like <code>print()</code> and <code>type()</code>. Functions are standalone entities that perform a specific task.</p> <p>On the other hand, methods are associated with objects. In this case, the  <code>title()</code> method is associated with the string object <code>name</code>.</p> String methods <p>You start with the variable <code>input_string</code> that holds the value  <code>\"fEyD rAuThA\"</code>. </p> <pre><code>input_string = \"fEyD rAuThA\"\n</code></pre> <p>Experiment and apply a combination of the following methods:</p> <ul> <li><code>capitalize()</code></li> <li><code>title()</code></li> <li><code>istitle()</code></li> <li><code>isupper()</code></li> <li><code>upper()</code></li> <li><code>lower()</code></li> </ul> <p>Eventually you should end up with the string <code>\"Feyd Rautha\"</code>,  print it.</p>"},{"location":"python-extensive/types/strings/#concatenation","title":"Concatenation","text":"<p>It\u2019s often useful to combine strings. For example, you might want to store first and last name in separate variables, and then combine them when you want to display someone\u2019s full name.</p> <p>Python uses the plus symbol (<code>+</code>) to combine strings. In this  example, we use <code>+</code> to create a full name by combining a  <code>first_name</code>, a space, and a <code>last_name</code>:</p> <pre><code>first_name = \"paul\"\nlast_name = \"atreides\"\nfull_name = first_name + \" \" + last_name\nprint(full_name)\n</code></pre> &gt;&gt;&gt; Output<pre><code>paul atreides\n</code></pre> <p>Here, the full name is used in a sentence that greets the user, and the <code>title()</code> method is used to format the name appropriately. This code returns a simple but nicely formatted greeting:</p> <pre><code>full_name = \"paul atreides\"\nprint(\"Hello, \" + full_name.title() + \"!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello, Paul Atreides!\n</code></pre> <p>Another way to nicely format strings is by using f-strings. To achieve the same result as above, simply put an <code>f\"...\"</code> in front of the string and use  curly braces <code>{}</code> to insert the variables. </p> <pre><code>full_name = \"Alia Atreides\"\n\nprint(f\"Hello, {full_name}!\")\n\n# you can even apply methods directly to the variables \n# (within the curly braces)\nprint(f\"Hello, {full_name.lower()}!\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Hello, Alia Atreides!\nHello, alia atreides!\n</code></pre> A quote <p>Find a quote from a famous person you admire. Store the  quote and name in variables and print both with an f-string.</p> <p>Your output should look something like the following,  including the quotation marks: </p> <p><code>Frank Herbert (Dune opening quote): \"I must not fear. Fear is the  mind-killer.\"</code></p>"},{"location":"python-extensive/types/strings/#recap","title":"Recap","text":"<p>This section was all about strings, we have covered:</p> <ul> <li>How to create strings</li> <li>Use <code>type()</code> to check a variable's type</li> <li>String methods, such as <code>title()</code> and <code>lower()</code></li> <li>Distinction between methods and functions</li> <li>String concatenation with <code>+</code> and f-strings (<code>f\"...\"</code>)</li> </ul> <p>Next, up we will introduce numbers in Python, namely integers and floats. </p>"},{"location":"statistics/","title":"Home","text":"<p>In today's data-driven world, the importance of statistics cannot be overstated. Statistics offers the foundational tools for making sense of the vast amount of information surrounding us, helping us draw meaningful conclusions from data and guiding decision-making across a wide array of fields. Whether it's in business, healthcare, engineering, or social sciences, the ability to analyze data effectively has become a crucial skill in the modern age. This chapter aims to introduce the fundamental concepts of statistics, from basic descriptive techniques to more advanced inferential methods.</p> <p>The motivation behind studying statistics lies in its dual role: providing tools for summarizing and understanding data (descriptive statistics), and equipping us with methodologies to infer patterns and make predictions about broader populations from samples (inferential statistics). By mastering these concepts, we enhance our ability to interpret data accurately and make informed decisions, thus fostering a more analytical and evidence-based approach to problem-solving.</p> DID YOU KNOW? byu/hardik_borana_Pro instatisticsmemes <p>The content of this chapter is structured to provide a comprehensive overview of both descriptive and inferential statistics. We begin with univariate and bivariate methods in descriptive statistics, which cover frequency distribution, measures of central tendency, and measures of dispersion. These provide the basic tools to summarize data. We then explore bivariate methods to examine relationships between two variables.</p> <p>Moving into inferential statistics, we delve into probability theory, discussing the essential concepts of probability, random variables, the Law of Large Numbers, and the Central Limit Theorem. These foundational theories pave the way for more complex topics such as hypothesis testing, where we cover hypothesis metrics, T-tests, confidence intervals, and ANOVA. </p> <p>Finally, the chapter concludes with regression analysis, exploring various regression techniques such as linear, multiple, polynomial, and logistic regression. These methods enable us to model relationships between variables, offering deeper insights into data trends and predictions for future outcomes.</p> <p>With this chapter, the goal is to equip you with a solid understanding of statistical principles, empowering you to apply these concepts in practical scenarios and appreciate the critical role statistics plays in our increasingly data-centric world.</p>"},{"location":"statistics/bivariate/Correlation/","title":"Measures of Correlation","text":"<p>This chapter covers the measures of linear relationships, specifically covariance and correlation. These measures help in characterizing the linear relationship between two variables, if such a relationship exists.</p>"},{"location":"statistics/bivariate/Correlation/#covariance","title":"Covariance","text":"<p>In the previous univariate parts, the metrics focused only on a single variable. Covariance allows us to determine the relationship between two variables. </p> Definition: Covariance \\[ \\text{cov}(X, Y) = \\frac{\\sum_{i=1}^{N}(x_i - \\bar{x}) \\cdot (y_i - \\bar{y})}{N} \\] <p>where \\(X\\) and \\(Y\\) are variables consisting of \\(N\\) values, and \\( \\bar{x} \\) and \\( \\bar{y} \\) are their respective means.  </p> <p>The name suggests that it is a type of 'variance,' as \\( \\text{cov}(X, X) = \\sigma^2(X) \\).</p> <pre><code>import plotly.express as px\ndf = px.data.tips()\n\nCovariance = df['total_bill'].cov(df['tip'], ddof=0)\nprint(f\"Covariance: {Covariance}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>8.29\n</code></pre> <p>By default, the degree of freedom <code>ddof=1</code>. In this case the population formular will be used. For the sample formular, the <code>ddof=0</code> needs to be set.</p> Example: Covariance of House Price <p>Given is a table with the size and price of houses. Determine the covariance.</p> <pre><code>size = [60, 72, 111, 67, 90]\nprice = [330000, 490000, 600000, 400000, 455000]\n</code></pre> <p>Solution </p> \\[ \\bar{x} = 80 \\, \\text{m}^2 \\quad \\bar{y} = 455,000 \\, \\text{\u20ac} \\] \\[ \\begin{eqnarray*}                     cov(X,Y)&amp;=&amp;\\frac{[(60-80)\\cdot(330k-455k)]+[(72-80)\\cdot(490k-455k)]+[(111-80)\\cdot(600k-455k)]}{5}\\\\                     &amp;&amp;+\\frac{[(67-80)\\cdot(400k-455k)]+[(90-80)\\cdot(455k-455k)]}{5}\\\\                     &amp;=&amp;1,486,000 \\, \\text{m}^2\\text{\u20ac}             \\end{eqnarray*} \\] <p>The covariance between \\(X\\) and \\(Y\\) is \\(1,486,000 \\, \\text{m}^2\\text{\u20ac}\\).</p> Code <pre><code>import pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame([(60, 330000), (72, 490000), (111, 600000), (67, 400000) , (90, 455000)], columns=['size', 'price'])\n\n# Calculate covariance\nhouse_cov = df['size'].cov(df['price'], ddof=0) # ddof=0 --&gt; Formlar for sample | ddof=1 --&gt; Formlar for population (default)\nprint(f\"Covariance: {house_cov}\")\n</code></pre> <p>Interpretation of Covariance: </p> <ul> <li>Covariance indicates the direction of the relationship:<ul> <li>\\(&gt;0\\): Positive relationship (if \\(X\\) increases, \\(Y\\) increases)</li> <li>\\(=0\\): No relationship</li> <li>\\(&lt;0\\): Negative relationship (if \\(X\\) increases, \\(Y\\) decreases)</li> </ul> </li> <li>Covariance does not provide information about the strength of the relationship since it is not dimensionless.</li> <li>To quantify the strength, we use a normalized measure called the correlation coefficient.  </li> </ul>"},{"location":"statistics/bivariate/Correlation/#correlation-coefficient","title":"Correlation Coefficient","text":""},{"location":"statistics/bivariate/Correlation/#pearson","title":"Pearson","text":"<p>The Pearson correlation coefficient expresses both the direction and strength of the linear relationship between two variables. It is a normalized form of covariance and is symmetric: \\( \\rho_{X,Y} = \\rho_{Y,X} \\).</p> Definition: Pearson  Correlation Coefficient \\[ \\rho = \\frac{\\text{cov}(X, Y)}{\\sigma_x \\cdot \\sigma_y} \\] <p>where \\(X\\) and \\(Y\\) are metric variables, \\( \\sigma_x \\) and \\( \\sigma_y \\) are their standard deviations, and \\( \\text{cov}(X, Y) \\) is the covariance. </p> <pre><code>Pearson = df['total_bill'].corr(df['tip'],method='pearson')\nprint(f\"Pearson Correlation Coefficient: {Pearson}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.68\n</code></pre> Example: Pearson Correlation Cofficient of House Price <p>Given is a table with the size and price of houses.  <pre><code>size = [60, 72, 111, 67, 90]\nprice = [330000, 490000, 600000, 400000, 455000]\n</code></pre> Additionally, we already know:</p> \\[ \\text{cov}(X, Y) = 1,486,000 \\quad \\sigma_x = 18.4 \\quad \\sigma_y = 90,443 \\] <p>Determine the Pearson correlation coefficient.</p> <p>Solution </p> \\[ \\rho = \\frac{1,486,000}{18.4 \\cdot 90,443} = 0.89 \\] <p>The correlation coefficient between \\(X\\) and \\(Y\\) is \\(0.89\\).</p> Code <pre><code>import pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame([(60, 330000), (72, 490000), (111, 600000), (67, 400000) , (90, 455000)], columns=['size', 'price'])\n\n# Calculate pearson correlation coefficient\nhouse_pearson = df['size'].corr(df['price'],method='pearson')\nprint(f\"Pearson Correlation Coefficient: {house_pearson}\")\n</code></pre> <p>Interpretation of the Pearson Correlation Coefficient : </p> <ul> <li>The Pearson correlation coefficient has no dimesion.</li> <li>Its value ranges between \\([-1 \\dots 1]\\) (according to the standard terminology by Jacob Cohen, 1988):<ul> <li>\\(0\\): No correlation</li> <li>\\(&gt; \\pm 0.1\\): Weak correlation</li> <li>\\(&gt; \\pm 0.3\\): Moderate correlation</li> <li>\\(&gt; \\pm 0.5\\): Strong correlation</li> <li>\\(\\pm 1\\): Perfect positive/negative correlation (one variable can be derived from the other)</li> </ul> </li> </ul>"},{"location":"statistics/bivariate/Correlation/#spearman","title":"Spearman","text":"<p>Covariance and the Pearson correlation coefficient require variables to be at least metric. When one of the variables is ordinal, the rank correlation should be calculated, with Spearman's method being a common choice. </p> <pre><code>Spearman = df['total_bill'].corr(df['tip'],method='spearman')\nprint(f\"Spearman Correlation Coefficient: {Spearman}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.68\n</code></pre> <p>In order for <code>df.corr()</code> to deal with ordinal data, the input needs to be numeric. This means that for ordinal data consisting of letters, the data needs to be mapped: </p> <pre><code>day_order = {\n    'Thur' : 4, \n    'Fri'  : 5, \n    'Sat'  : 6,\n    'Sun'  : 7\n    }\ndf['day_ord'] = df['day'].map(day_order)\n\nSpearman = df['day_ord'].corr(df['size'],method='spearman')\nprint(f\"Spearman Correlation Coefficient: {Spearman}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.24\n</code></pre> <p>The interpretation of Spearman's rank correlation is similar to Pearson's.</p> Definition: Spearman Rank Correlation Coefficient \\[ \\rho_s = 1 - \\frac{6 \\cdot \\sum_{i=1}^{N} d_i^2}{N^3 - N} \\quad \\text{where } d_i = R(x_i) - R(y_i) \\] <p>with \\(X\\) and \\(Y\\) as variables consisting of \\(N\\) values and ranks \\(R(x_i)\\) and \\(R(y_i)\\).  </p> <p>Rank Formation</p> <ul> <li> <p>The rank corresponds to the position a value holds when all values are arranged in order.</p> Example: Ranking of Values \\(x_i\\) 2.17 8.00 1.09 2.01 \\(R(x_i)\\) 3 4 1 2 </li> <li> <p>For identical values, the average rank (mean of the relevant ranks) is used.</p> Example: Ranking of Equal Values \\(x_i\\) 1.09 2.17 2.17 2.17 3.02 4.50 \\(R(x_i)\\) 1 3 3 3 5 6 <p>with \\( (2+3+4)/3 = 3 \\)</p> </li> </ul> Example: Spearman Correlation Coefficient of House Price <p>Given is a table with the size and price of houses.  <pre><code>size = [60, 72, 111, 67, 90]\nprice = [330000, 490000, 600000, 400000, 455000]\n</code></pre> Determine the Spearman correlation coefficient.</p> <p>Solution </p> Size R(Size) Price R(Price) \\( d_i \\) 60 1 330k 1 0 72 3 490k 4 -1 111 5 600k 5 0 67 2 400k 2 0 90 4 455k 3 1 \\[ \\rho_s = 1 - \\frac{6 \\cdot [0^2 + 0^2 + (-1)^2 + 1^2 + 0^2]}{5^3 - 5} = 0.9 \\] <p>The Spearman correlation coefficient between \\(X\\) and \\(Y\\) is \\(0.9\\).</p> Code <pre><code>import pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame([(60, 330000), (72, 490000), (111, 600000), (67, 400000) , (90, 455000)], columns=['size', 'price'])\n\n# Calculate spearman correlation coefficient\nhouse_spearman = df['size'].corr(df['price'],method='spearman')\nprint(f\"Spearman Correlation Coefficient: {house_spearman}\")\n</code></pre>"},{"location":"statistics/bivariate/Correlation/#scatter-plot","title":"Scatter Plot","text":"<p>A scatter plot provides a graphical representation of the relationship between two metric variables on a Cartesian coordinate system. </p> <pre><code>import plotly.express as px\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\")\nfig.show()\n</code></pre> <p>Each data pair is treated as a coordinate and is represented by a point on the plot. This allows for identifying relationships, patterns, or trends between the variables.</p> Different Types of Correlation (Source: https://www.geeksforgeeks.org/what-is-correlation-analysis/)  Example: Scatter Plot of House Prices <p></p> Code <pre><code>import pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame([(60, 330000), (72, 490000), (111, 600000), (67, 400000) , (90, 455000)], columns=['size', 'price'])\n\nimport plotly.express as px\n\n# Create a scatter plot\nfig = px.scatter(df, x=\"size\", y=\"price\")\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Size',\n    yaxis_title_text='Price',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;House Prices: Scatter Plot&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Variables: size, price&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre>"},{"location":"statistics/bivariate/Correlation/#recap","title":"Recap","text":"<ul> <li>Measures of linear relationships describe the relationship between two variables.</li> <li>Covariance identifies the direction of the relationship between two metrically scaled variables but not the strength.</li> <li>The Pearson correlation coefficient measures both the direction and strength of the relationship between two metrically scaled variables.</li> <li>If at least one variable is ordinal, the Spearman correlation coefficient is used.</li> <li>A scatter plot can visually represent the relationship between two metric variables.  </li> </ul>"},{"location":"statistics/bivariate/Correlation/#tasks","title":"Tasks","text":"Task: Attribute Correlation <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ncars = fetch_ucirepo(id=9) \n# https://archive.ics.uci.edu/dataset/9/auto+mpg\n\n# data (as pandas dataframes) \ndata = cars.data.features\n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following task: </p> <ol> <li>Analyze the correlation between the variables <code>horsepower</code> and <code>cylinders</code>. Therefore calculate the covariance, pearson correlation coefficient and spearman correlation coefficient. Interpret the results.</li> <li>Generate a scatter plot for the variabels <code>horsepower</code> and <code>cylinders</code>. Compare the before result with the calculated measures. </li> <li>Take a closer look on the different variables and the corresponding attribute type. Is there a variable, where the calculation of the correlation makes no sense? </li> </ol> Task: Income vs. Expenditures <p>Given below are the incomes and weekly consumption expenditures of four households (sample), each measured in euros: <pre><code># \nimport pandas as pd\n\nincome = [150, 250, 175, 165]\nexpenditure = [135, 150, 140, 150]\n\n# convert the lists to pandaframe\ndf = pd.DataFrame({'income': income, 'expenditure': expenditure})\ndf.head()\n</code></pre> Work on the following task: </p> <ol> <li>Calculate the covariance between <code>income</code> and <code>expenditure</code>. Interpret the result.</li> <li>Calculate the covariance when <code>income</code> is measured in Euro cents. How does this affect the interpretation?</li> <li>Switch back to <code>income</code> in Euro. Calculate the Pearson correlation coefficient. Interpret the results.</li> <li>Calculate the correlation coefficient when income is measured in Euro Cents. How does this affect the interpretation?</li> </ol>"},{"location":"statistics/bivariate/Frequency/","title":"Frequency Distribution","text":"<p>Two variables, \\(X\\) and \\(Y\\), consist of \\(n\\) elements. The raw data list consists of the tuples \\((x_1, y_1), \\dots, (x_n, y_n)\\). Possible values are \\(a_1, \\dots, a_k\\) for \\(X\\) and \\(b_1, \\dots, b_m\\) for \\(Y\\). The absolute frequency refers to how often a combination \\((a_i, b_j)\\) occurs.</p> <pre><code>drinks = ['small', 'small', 'small', 'medium', 'medium', 'medium', 'large']\nkcal =   [    123,     154,     123,      201,      201,      201,     434]\n</code></pre> <p>In this example, </p> <ul> <li> <ul> <li>\\( X \\): <code>drinks</code></li> <li>\\( k \\): 3</li> <li>\\( a_1, \\dots, a_k \\): <code>['small', 'medium', 'large']</code></li> </ul> </li> <li> <ul> <li>\\( Y \\): <code>kcal</code></li> <li>\\( m \\): 4</li> <li>\\( b_1, \\dots, b_m \\): <code>[123, 154, 201, 434]</code></li> </ul> </li> </ul> <ul> <li>\\( n \\): <code>7</code></li> <li>\\((x_1, y_1), \\dots, (x_n, y_n)\\): <code>('small', 123), ('small', 154) ... ('large', 434)</code></li> </ul> <p>Representation of frequencies can be done in the form of a table or a graphic. In tabular form, the so-called crosstab (or contingency table) is commonly used. For graphical representation, a histogram (or 2D bar chart) is suitable.  It is important that the data remain the focal point and are presented as accurately and objectively as possible, avoiding distortions like 3D effects or shadows. Titles, axis labels, legends, the data source, and the time of data collection should always be clearly indicated.</p> Definition: Bivariate Frequency <p>Absolute frequency of the combination \\( (a_i, b_j) \\):</p> \\[ h_{ij} = h(a_i, b_j) \\] <p>Relative frequency of the combination \\( (a_i, b_j) \\):</p> \\[ f_{ij} = f(a_i, b_j) = \\frac{h_{ij}}{n} \\]"},{"location":"statistics/bivariate/Frequency/#histogram","title":"Histogram","text":"<p>Histograms are also suitable for bivariate data to represent frequency. A specific type of representation is the density heatmap, which can also be generated using plotly.</p> <pre><code>import plotly.express as px\ndf = px.data.tips()\n\nfig = px.density_heatmap(df, x=\"total_bill\", y=\"tip\")\nfig.show()\n</code></pre> <p>Both absolute and relative frequencies can be visualized using this method (by adding the parameter <code>histnorm='percent'</code>). Multidimensional histograms can be created using the Python package matplotlib.</p> Example: Plotly Heatmap <p></p> Code <pre><code>from ucimlrepo import fetch_ucirepo \nimport plotly.express as px\n\n# fetch dataset \ndrugs = fetch_ucirepo(id=468) \n# https://archive.ics.uci.edu/dataset/462\n\n# data (as pandas dataframes) \ndata = drugs.data.features\n\n# Create a density heatmap\nfig = px.density_heatmap(data, x=\"Region\", y=\"VisitorType\")\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Region',\n    yaxis_title_text='Visitor Type',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Densitiy Heatmap&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: drug_reviews_drugs_com; variable: Region, VisitorType&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre>"},{"location":"statistics/bivariate/Frequency/#crosstab-contingency-table","title":"Crosstab (Contingency Table)","text":"<p>The representation of the joint distribution of discrete features with few categories (if there are many categories, they need to be grouped into categories) can be done using contingency tables. </p> <pre><code>import pandas as pd\nimport plotly.express as px\n\ndf = px.data.tips()\npd.crosstab( df['sex'], df['day'])\n</code></pre> <p>These tables can display both absolute and relative frequencies (by adding the parameter <code>normalize=True</code>). It is important to note that contingency tables use only the nominal scale level, even if a variable could be measured at a higher level (ordinal or numerical).</p> <p>Marginal Frequencies refer to the row and column totals added to a table. The row totals are the marginal frequencies of the variable \\(X\\), calculated as \\( h_{i.} = h_{i1} + \\dots + h_{im} \\) for \\( i = 1, \\dots, k \\). The column totals are the marginal frequencies of the variable \\(Y\\), given by \\( h_{.j} = h_{1j} + \\dots + h_{kj} \\) for \\( j = 1, \\dots, m \\). In <code>Python</code> you just need to add the parameter <code>margins=True</code>.</p> <p>Marginal Distribution refers to the marginal frequencies of a variable, which are the simple frequencies without considering the second variable. The collection of all marginal frequencies for a variable gives the marginal distribution of \\(X\\) (\\(h_{1.}, h_{2.}, \\dots, h_{k.}\\)) or \\(Y\\)(\\(h_{.1}, h_{.2}, \\dots, h_{.m}\\)) in absolute frequencies:</p>"},{"location":"statistics/bivariate/Frequency/#absolute-frequency","title":"Absolute Frequency","text":"Definition: Absolute Crosstab <p>Crosstab of the absolute Frequencies</p> <ul> <li> \\[ \\begin{array}{c|ccc|c}     &amp; b_1 &amp; \\dots &amp; b_m &amp; \\sum \\\\ \\hline     a_1 &amp; h_{11} &amp; \\dots &amp; h_{1m} &amp; h_{1.} \\\\     a_2 &amp; h_{21} &amp; \\dots &amp; h_{2m} &amp; h_{2.} \\\\     \\vdots &amp; \\vdots &amp;  &amp; \\vdots &amp; \\vdots \\\\     a_k &amp; h_{k1} &amp; \\dots &amp; h_{km} &amp; h_{k.} \\\\ \\hline     \\sum &amp; h_{.1} &amp; \\dots &amp; h_{.m} &amp; n \\end{array} \\] </li> <li> <ul> <li>\\( a_i \\): Values of \\( X \\) with \\( i = 1, \\dots, k \\)</li> <li>\\( b_j \\): Values of \\( Y \\) with \\( j = 1, \\dots, m \\)</li> <li>\\( h_{ij} \\): The absolute frequency of the combination \\( (a_i, b_j) \\)</li> <li>\\( h_{1.}, \\dots, h_{k.} \\): The marginal frequencies of \\( X \\)</li> <li>\\( h_{.1}, \\dots, h_{.m} \\): The marginal frequencies of \\( Y \\)</li> <li>\\( n \\): Total number of elements </li> </ul> </li> </ul> Example: Absolute Crosstab Website Visitors <p>Crosstab of the absolute Frequencies </p> \\[ \\begin{array}{r|ccccccccc|c}  &amp; &amp; &amp; &amp; &amp; Region &amp; &amp; &amp; &amp; &amp; \\\\ Visitor Types   &amp; \\textbf{1}  &amp; \\textbf{2}  &amp; \\textbf{3}  &amp; \\textbf{4}  &amp; \\textbf{5}  &amp;\\textbf{ 6}  &amp; \\textbf{7}  &amp; \\textbf{8}  &amp;\\textbf{9}  &amp;  \\sum\\\\\\hline \\textbf{New_Visitor} &amp; 657 &amp;    149&amp;    312&amp;    139&amp;    50&amp; 121&amp;    100&amp;    74&amp; 92&amp; 1694 \\\\ \\textbf{Returning_Visitor} &amp; 4115   &amp;982    &amp;2083   &amp;1038   &amp;268    &amp;683&amp;   659&amp;    359&amp;    364&amp;    10551 \\\\ \\textbf{Other} &amp; 8  &amp;5  &amp;8  &amp;5  &amp;0  &amp;1  &amp;2  &amp;1  &amp;55 &amp;85 \\\\ \\hline \\sum &amp; 4780 &amp;1136   &amp;2403   &amp;1182   &amp;318    &amp;805    &amp;761    &amp;434    &amp;511    &amp;12330\\\\ \\end{array} \\] Code <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ndrugs = fetch_ucirepo(id=468) \n# https://archive.ics.uci.edu/dataset/462\n\n# data (as pandas dataframes) \ndata = drugs.data.features\n\nimport pandas as pd\n\n# Create a crosstab\npd.crosstab( data['VisitorType'],data['Region'], margins=True)\n</code></pre>"},{"location":"statistics/bivariate/Frequency/#relative-frequency","title":"Relative Frequency","text":"Definition: Relative Crosstab <p>Crosstab of the relative Frequencies</p> <ul> <li> \\[ \\begin{array}{c|ccc|c}     &amp; b_1 &amp; \\dots &amp; b_m &amp; \\sum \\\\ \\hline     a_1 &amp; f_{11} &amp; \\dots &amp; f_{1m} &amp; f_{1.} \\\\     a_2 &amp; f_{21} &amp; \\dots &amp; f_{2m} &amp; f_{2.} \\\\     \\vdots &amp; \\vdots &amp;  &amp; \\vdots &amp; \\vdots \\\\     a_k &amp; f_{k1} &amp; \\dots &amp; f_{km} &amp; f_{k.} \\\\ \\hline     \\sum &amp; f_{.1} &amp; \\dots &amp; f_{.m} &amp; 1 \\end{array} \\] </li> <li> <ul> <li>\\( a_i \\): Values of \\( X \\) with \\( i = 1, \\dots, k \\)</li> <li>\\( b_j \\): Values of \\( Y \\) with \\( j = 1, \\dots, m \\)</li> <li>\\( f_{ij} = \\frac{h_{ij}}{n} \\): The relative frequency of the combination \\( (a_i, b_j) \\)</li> <li>\\( f_{i.} = \\frac{h_{i.}}{n} \\): The relative marginal frequencies of \\( X \\)</li> <li>\\( f_{.j} = \\frac{h_{.j}}{n} \\): The relative marginal frequencies of \\( Y \\) </li> </ul> </li> </ul> Example: Relative Crosstab Website Visitors <p>Crosstab of the relative Frequencies in [%]</p> \\[ \\begin{array}{r|ccccccccc|c}  &amp; &amp; &amp; &amp; &amp; Region &amp; &amp; &amp; &amp; &amp; \\\\ Visitor Types   &amp; \\textbf{1}  &amp; \\textbf{2}  &amp; \\textbf{3}  &amp; \\textbf{4}  &amp; \\textbf{5}  &amp;\\textbf{ 6}  &amp; \\textbf{7}  &amp; \\textbf{8}  &amp;\\textbf{9}  &amp;  \\sum\\\\\\hline \\textbf{New_Visitor}        &amp;   5.3 &amp;     1.2 &amp;   2.5 &amp;   1.1 &amp;   0.4 &amp;   1.0 &amp;   0.8 &amp;   0.6 &amp;   0.7 &amp;  13.7 \\\\ \\textbf{Returning_Visitor}  &amp;  33.4 &amp;     8.0 &amp;  16.9 &amp;   8.4 &amp;   2.2 &amp;   5.5 &amp;   5.3 &amp;   2.9 &amp;   3.0 &amp;  85.6 \\\\ \\textbf{Other}              &amp;   0.1 &amp;     0.0 &amp;   0.1 &amp;   0.0 &amp;   0.0 &amp;   0.0 &amp;   0.0 &amp;   0.0 &amp;   0.4 &amp;   0.7 \\\\ \\hline \\sum                        &amp;  38.8 &amp;     9.2 &amp;  19.5 &amp;   9.6 &amp;   2.6 &amp;   6.5 &amp;   6.2 &amp;   3.5 &amp;   4.1 &amp; 100.0\\\\ \\end{array} \\] Code <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ndrugs = fetch_ucirepo(id=468) \n# https://archive.ics.uci.edu/dataset/462\n\n# data (as pandas dataframes) \ndata = drugs.data.features\n\nimport pandas as pd\n\n# Create a crosstab\npd.crosstab( data['VisitorType'],data['Region'], margins=True, normalize='all')\n</code></pre>"},{"location":"statistics/bivariate/Frequency/#conditional-frequency","title":"Conditional frequency","text":"<p>Absolute and relative frequencies are not suitable for determining the relationship between variables. For example, the frequency of regions for <code>New_Visitors</code> and <code>Returning_Visitors</code> cannot be directly compared because the sizes of both groups are different. The conditional relative frequency allows for this comparison by accounting for the differences in group sizes. Therefore, in <code>pd.crosstab()</code> you need to add <code>normalize='index'</code> or <code>normalize='columns'</code></p> Definition: Conditional Frequency <p>Conditional Frequency Distribution of \\( Y \\) given \\( X = a_i \\):</p> \\[ f_{Y,ij} = \\frac{f_{ij}}{f_{i.}} = \\frac{h_{ij}}{h_{i.}} \\] <p>Conditional Frequency Distribution of \\( X \\) given \\( Y = b_j \\):</p> \\[ f_{X,ij} = \\frac{f_{ij}}{f_{.j}} = \\frac{h_{ij}}{h_{.j}} \\] Example: Condtional Frequency of Website Visitors <p>Crosstab of the Conditional Frequencies for given Visitor Types in [%]</p> \\[ \\begin{array}{r|ccccccccc|c}  &amp; &amp; &amp; &amp; &amp; Region &amp; &amp; &amp; &amp; &amp; \\\\ Visitor Types   &amp; \\textbf{1}  &amp; \\textbf{2}  &amp; \\textbf{3}  &amp; \\textbf{4}  &amp; \\textbf{5}  &amp;\\textbf{ 6}  &amp; \\textbf{7}  &amp; \\textbf{8}  &amp;\\textbf{9}  &amp;  \\sum\\\\\\hline \\textbf{New_Visitor}        &amp; 38.8 &amp; 8.8 &amp; 18.4 &amp; 8.2 &amp; 3.0 &amp; 7.1 &amp; 5.9 &amp; 4.4 &amp;  5.4 &amp; 100 \\\\ \\textbf{Returning_Visitor}  &amp; 39.0 &amp; 9.3 &amp; 19.7 &amp; 9.8 &amp; 2.5 &amp; 6.5 &amp; 6.2 &amp; 3.4 &amp;  3.4 &amp; 100 \\\\ \\textbf{Other}              &amp;  9.4 &amp; 5.9 &amp;  9.4 &amp; 5.9 &amp; 0.0 &amp; 1.2 &amp; 2.4 &amp; 1.2 &amp; 64.7 &amp; 100 \\\\ \\hline \\sum                        &amp; 38.8 &amp; 9.2 &amp; 19.5 &amp; 9.6 &amp; 2.6 &amp; 6.5 &amp; 6.2 &amp; 3.5 &amp;  4.1 &amp; 100\\\\ \\end{array} \\] Code <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ndrugs = fetch_ucirepo(id=468) \n# https://archive.ics.uci.edu/dataset/462\n\n# data (as pandas dataframes) \ndata = drugs.data.features\n\nimport pandas as pd\n\n# Create a crosstab\nprint(pd.crosstab( data['VisitorType'],data['Region'], margins=True, normalize='index'))\n</code></pre> Example: Condtional Frequency of Website Visitors <p>Crosstab of the Conditional Frequencies for given Region in [%]</p> \\[ \\begin{array}{r|ccccccccc|c}  &amp; &amp; &amp; &amp; &amp; Region &amp; &amp; &amp; &amp; &amp; \\\\ Visitor Types   &amp; \\textbf{1}  &amp; \\textbf{2}  &amp; \\textbf{3}  &amp; \\textbf{4}  &amp; \\textbf{5}  &amp;\\textbf{ 6}  &amp; \\textbf{7}  &amp; \\textbf{8}  &amp;\\textbf{9}  &amp;  \\sum\\\\\\hline \\textbf{New_Visitor}        &amp; 13.7 &amp; 13.1 &amp; 13.0 &amp; 11.8 &amp; 15.7 &amp; 15.0 &amp; 13.1 &amp; 17.1 &amp; 18.0 &amp; 13.7  \\\\ \\textbf{Returning_Visitor}  &amp; 86.1 &amp; 86.4 &amp; 86.7 &amp; 87.8 &amp; 84.3 &amp; 84.8 &amp; 86.6 &amp; 82.7 &amp; 71.2 &amp; 85.6  \\\\ \\textbf{Other}              &amp;  0.2 &amp;  0.4 &amp;  0.3 &amp;  0.4 &amp;  0.0 &amp;  0.1 &amp;  0.3 &amp;  0.2 &amp; 10.8 &amp;  0.7  \\\\ \\hline \\sum                        &amp; 100&amp; 100&amp; 100&amp; 100&amp; 100&amp; 100&amp; 100&amp; 100&amp; 100&amp; 100\\\\ \\end{array} \\] Code <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ndrugs = fetch_ucirepo(id=468) \n# https://archive.ics.uci.edu/dataset/462\n\n# data (as pandas dataframes) \ndata = drugs.data.features\n\nimport pandas as pd\n\n# Create a crosstab\nprint(pd.crosstab( data['VisitorType'],data['Region'], margins=True, normalize='columns'))\n</code></pre>"},{"location":"statistics/bivariate/Frequency/#recap","title":"Recap","text":"<ul> <li>Frequencies in the bivariate case describe how often a combination of two values occurs.</li> <li>As in the univariate case, a distinction between absolute and relative frequency is made.</li> <li>2D histograms or contingency tables can be used for representation.</li> <li>Relationships between variables are not easily identified in either absolute or relative contingency tables.</li> <li>The conditional frequency examines the frequency distribution of one variable while fixing the second variable.</li> </ul>"},{"location":"statistics/bivariate/Frequency/#tasks","title":"Tasks","text":"Task: Bivariate Frequency <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ncars = fetch_ucirepo(id=9) \n# https://archive.ics.uci.edu/dataset/9/auto+mpg\n\n# data (as pandas dataframes) \ndata = cars.data.features\n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following task: </p> <ol> <li>Generate a 2D Histogram for the variables <code>origin</code> and <code>horsepower</code> (think about attribute types, title, labeling of the axes). Interpret the results. </li> <li>Calculate the crosstab for the absolute frequencies of the variables <code>origin</code> and <code>cylinders</code></li> <li>Calculate the conditional crosstab for the relative frequencies to answer the following question: Whats the cylinder distributed within each origin? </li> </ol>"},{"location":"statistics/hypothesis/ANOVA/","title":"ANOVA","text":"<p>ANOVA, which stands for Analysis of Variance, is a statistical method used to determine whether there are statistically significant differences between the means of three or more groups. Unlike the t-test, which examines whether there is a difference between two groups, ANOVA is used to assess differences among multiple group means simultaneously by comparing the variances within each group to the variances between the groups. The primary goal of ANOVA is to determine whether any of those differences are statistically significant.</p> (Source: Imgflip Meme Generator)"},{"location":"statistics/hypothesis/ANOVA/#types-of-anova","title":"Types of ANOVA","text":"<ul> <li> <p>One-Way ANOVA</p> <p>analyzes the impact of a single independent variable (factor) with three or more levels on a continuous dependent variable.</p> Example One-Way ANOVA <p>A factory manager uses One-Way ANOVA to compare the average production times of widgets produced by Machine A, Machine B, and Machine C.</p> </li> <li> <p>Two-Way ANOVA</p> <p>evaluates the effects of two independent variables (factors) simultaneously and examines the interaction between them on a continuous dependent variable.</p> Example Two-Way ANOVA <p>A researcher employs Two-Way ANOVA to study the effects of different fertilizers and watering schedules on plant growth.</p> </li> <li> <p>Repeated Measures ANOVA</p> <p>assesses the effects of one or more factors when the same subjects are measured multiple times under different conditions.</p> Example Repeated Measures ANOVA <p>A psychologist uses Repeated Measures ANOVA to evaluate participants' stress levels before, during, and after a meditation intervention.</p> </li> <li> <p>MANOVA (Multivariate ANOVA)</p> <p>extends ANOVA by analyzing the effects of one or more independent variables on two or more dependent variables simultaneously.</p> Example MANOVA <p>An educator applies MANOVA to investigate how teaching methods and class sizes influence both student performance and engagement levels.</p> </li> </ul> (Source: Imgflip Meme Generator)"},{"location":"statistics/hypothesis/ANOVA/#one-way-anova","title":"One-Way ANOVA","text":"<p>One-Way ANOVA is a statistical method used to determine whether there are any statistically significant differences between the means of three or more independent (unrelated) groups. Unlike the t-test, which compares the means of two groups, One-Way ANOVA can handle multiple groups simultaneously, making it a powerful tool for analyzing variations within and between groups.</p> Example One-Way ANOVA in Production Scenario <p>In a factory, there are three different machines used to assemble electronic components: Machine A, Machine B, and Machine C. The production manager wants to know if the type of machine impacts the production time of the components.</p> <p>The key concepts of the One-Way ANOVA includes: </p> <ul> <li>Factor: The independent variable that categorizes the data. In One-Way ANOVA, there is only one factor.</li> <li>Levels: The different categories or groups within the factor.</li> <li>Dependent Variable: The outcome or response variable that is measured.</li> <li>Null Hypothesis (H<sub>0</sub>): Assumes that all group means are equal.</li> <li>Alternative Hypothesis (H<sub>1</sub>): Assumes that at least one group mean is different.</li> </ul> Example One-Way ANOVA in Production Scenario <ul> <li>Factor: Machine Types</li> <li>Levels: Machine A, Machine B, Machine C</li> <li>Dependent Variable: Production Time</li> <li> <p>Null Hypothesis (H<sub>0</sub>): there is no difference in the average assembly time among the three machines.</p> \\[     H_0 : \\mu_A = \\mu_B = \\mu_C \\] </li> <li> <p>Alternative Hypothesis (H<sub>1</sub>): at least one group mean is different.</p> </li> </ul>"},{"location":"statistics/hypothesis/ANOVA/#when-to-use-one-way-anova","title":"When to Use One-Way ANOVA","text":"<p>One-Way ANOVA is appropriate when you want to:</p> <ul> <li>Compare the means of three or more independent groups.</li> <li>Assess the impact of a single categorical factor on a continuous dependent variable.</li> <li>Determine if at least one group mean significantly differs from the others.</li> </ul>"},{"location":"statistics/hypothesis/ANOVA/#approach","title":"Approach","text":"<p>At its core, ANOVA partitions the total variance in the data into components attributable to different sources. Here's a simplified breakdown:</p> <ol> <li> <p>Hypothesis Definition</p> <ul> <li>Null Hypothesis (\\( H_0 \\)): All group means are equal.</li> <li>Alternative Hypothesis (\\( H_a \\)): At least one group mean is different.</li> </ul> </li> <li> <p>Sum of Squares (SS)</p> <ol> <li> <p>Total Sum of Squares (SS Total)</p> Definition: SS Total <p>Measure of the total variability in the data</p> \\[ SS_{Total} = \\sum_{i=1}^{N} (Y_i - \\overline{Y})^2 \\] <p>with: </p> <ul> <li>\\( Y_i \\) = individual observations  </li> <li>\\( \\overline{Y} \\) = grand mean</li> <li>\\( N \\) = total number of observations  </li> </ul> </li> <li> <p>Between-Group Sum of Squares (SS Between)</p> Definition: SS Between <p>Measure of the variability due to the factor (e.g., different treatments).</p> \\[ SS_{Between} = \\sum_{j=1}^{k} n_j (\\overline{Y}_j - \\overline{Y})^2 \\] <p>with:</p> <ul> <li>\\( \\overline{Y}_j \\) = mean of group \\( j \\) </li> <li>\\( n_j \\) = number of observations in group \\( j \\)</li> <li>\\( k \\) = number of groups  </li> </ul> </li> <li> <p>Within-Group Sum of Squares (SS Within)</p> Definition: SS Between <p>Measure of the variability within each group.</p> \\[ SS_{Within} = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (Y_{ij} - \\overline{Y}_j)^2 \\] </li> </ol> </li> <li> <p>Degrees of Freedom (df)</p> Definition: Degree of Freedom \\[ df_{Total} = N-1 \\quad | \\quad df_{Between} = k-1 \\quad | \\quad df_{Within} = N-k \\] <p>with:</p> <ul> <li>\\( N \\) = total number of observations  </li> <li>\\( k \\) = number of groups </li> </ul> </li> <li> <p>Mean Squares (MS)</p> Definition: Mean Squares \\[ MS_{Between} = \\frac{SS_{Between}}{df_{Between}} \\quad | \\quad MS_{Within} =  \\frac{SS_{Within}}{df_{Within}} \\] </li> <li> <p>F-Statistic</p> Definition: ANOVA F-Statistic \\[ F = \\frac{MS_{Between}}{MS_{Within}} \\] <p>Interpretation: A higher F-value indicates greater evidence against the null hypothesis. The p-value is derived from the F-distribution and determines the statistical significance of the results.</p> <p>Constructing the ANOVA Table</p> Source of Variation Sum of Squares (SS) Degrees of Freedom (df) Mean Square (MS) F-Statistic p-Value Between Groups \\( SS_{Between} \\) \\( k - 1 \\) \\( MS_{Between} \\) \\( F \\) Within Groups \\( SS_{Within} \\) \\( N - k \\) \\( MS_{Within} \\) Total \\( SS_{Total} \\) \\( N - 1 \\) </li> <li> <p>Interpretation of Results     As we have have seen before in the T-Test and F-Test, if the p-value is smaller than \\( \\alpha\\) (commonly 0.05) we reject H<sub>0</sub>. If \\(p&gt; \\alpha\\) we fail to reject \\( H_0 \\).</p> <p>If the ANOVA is significant, determine which specific groups differ using post-hoc tests like the Tukey HSD test to control for multiple comparisons.</p> </li> </ol> Example One-Way ANOVA in Production Scenario <p>Let\u2019s consider a realistic production scenario. A factory uses three different machines (Machine A, Machine B, and Machine C) to assemble electronic components. The production manager wants to know if the machine type affects the assembly time.</p> <p>The Production times (in minutes) for 5 widgets from each machine:</p> <pre><code># Production Time in Minutes\nA = [12, 14, 13, 15, 14] # Machine A\nB = [16, 18, 17, 19, 18] # Machine B\nC = [11, 10, 12, 11, 10] # Machine C\n</code></pre> Manual Calculation <ul> <li> <p>Calculate Group Means:     <pre><code>k = 3\n\nY_mean_A = np.mean(A)\nY_mean_B = np.mean(B)\nY_mean_C = np.mean(C)\n\nY_mean = (Y_mean_A + Y_mean_B + Y_mean_C) / k\n\nprint(f\"Mean of Machine A: {Y_mean_A} minutes\")\nprint(f\"Mean of Machine B: {Y_mean_B} minutes\")\nprint(f\"Mean of Machine C: {Y_mean_C} minutes\")\n\nprint(f\"Mean of Production Time: {Y_mean} minutes\")\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>Mean of Machine A: 13.6 minutes\nMean of Machine B: 17.6 minutes\nMean of Machine C: 10.8 minutes\nMean of Production Time: 14.0 minutes\n</code></pre> </li> </ul> <ul> <li> <p>Sum of Squares (SS) <pre><code># Calculate Sum of Squares Betwen Groups\n\nSSB = len(A)*(Y_mean_A - Y_mean)**2 + len(B)*(Y_mean_B - Y_mean)**2 + len(C)*(Y_mean_C - Y_mean)**2\nprint(f\"Sum of Squares Between Groups: {SSB}\")\n\n# Calculate Sum of Squares Within Groups\n\nSSW = sum((np.array(A) - Y_mean_A)**2) + sum((np.array(B) - Y_mean_B)**2) + sum((np.array(C) - Y_mean_C)**2)\nprint(f\"Sum of Squares Within Groups: {SSW}\")\n\n# Calculate Sum of Squares Total\n\nSST = SSB + SSW\nprint(f\"Sum of Squares Total: {SST}\")\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>Sum of Squares Between Groups: 116.8\nSum of Squares Within Groups: 13.2\nSum of Squares Total: 130\n</code></pre> </li> </ul> <ul> <li> <p>Degrees of Freedom <pre><code># Degrees of Freedom\nN = len(A) + len(B) + len(C)\n\ndf_B = k - 1\ndf_W = N - k\ndf_T = N - 1\n\nprint(f\"df Between Groups: {df_B}\")\nprint(f\"df Within Groups: {df_W}\")\nprint(f\"df Total: {df_T}\")\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>df Between Groups: 2\ndf Within Groups: 12\ndf Total: 14\n</code></pre> </li> </ul> <ul> <li> <p>Mean Squares <pre><code># Mean Squares\n\nMSB = SSB / df_B\nMSW = SSW / df_W\n\nprint(f\"Mean Squares Between Groups: {MSB}\")\nprint(f\"Mean Squares Within Groups: {MSW}\")\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>Mean Squares Between Groups: 58.4\nMean Squares Within Groups: 1.1\n</code></pre> </li> </ul> <ul> <li> <p>F-Statistic <pre><code># F-Statistic\n\nF = MSB / MSW\nprint(f\"F-Statistic: {F}\")\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>F-Statistic: 53.1\n</code></pre> </li> </ul> <ul> <li>Determine p-Value     Using an F-distribution table or statistical software with \\( df_1 = df_{Between} = 2 \\) and \\( df_2 = df_{Within} = 12 \\), an F-value of 53.1 is highly significant (p &lt; 0.001).</li> </ul> Automatic Calculation <p>For calculation the p-value and the f-statistics of the ANOVA we can use the 'f_oneway' method of the 'scipy.stats' library: </p> <pre><code>from scipy.stats import f_oneway\n\nf,p = f_oneway(A, B, C)\n\nprint(f\"F-Statistic: {f}\")\nprint(f\"P-Value: {p}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>F-Statistic: 53.09090909090907\nP-Value: 1.0959316602384747e-06\n</code></pre> <ul> <li> <p>Interpretation</p> <p>Since the p-value is less than 0.05, we reject the null hypothesis. There are significant differences in production times among the three machines.</p> </li> </ul> <ul> <li> <p>Post-Hoc Analysis     To identify which specific machines differ we can perform a Tukey HSD Test revealing that:</p> <pre><code>from scipy.stats import tukey_hsd\n\ntukey_results = tukey_hsd(A,B,C)\nprint(tukey_results)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Tukey's HSD Pairwise Group Comparisons (95.0% Confidence Interval)\nComparison  Statistic  p-value  Lower CI  Upper CI\n(0 - 1)     -4.000     0.000    -5.770    -2.230\n(0 - 2)      2.800     0.003     1.030     4.570\n(1 - 0)      4.000     0.000     2.230     5.770\n(1 - 2)      6.800     0.000     5.030     8.570\n(2 - 0)     -2.800     0.003    -4.570    -1.030\n(2 - 1)     -6.800     0.000    -8.570    -5.030\n</code></pre> <ul> <li>Machine A vs. Machine B: Significant difference</li> <li>Machine A vs. Machine C: Significant difference</li> <li>Machine B vs. Machine C: Significant difference</li> </ul> </li> </ul> <ul> <li>Conclusion: All three machines have significantly different production times, with Machine B (<code>Y_mean_A = 17.6</code>) being the slowest and Machine C (<code>Y_mean_A = 10.8</code>) being the fastest.</li> </ul>"},{"location":"statistics/hypothesis/ANOVA/#assumptions-of-one-way-anova","title":"Assumptions of One-Way ANOVA","text":"<p>Before performing One-Way ANOVA, ensure that your data meet the following assumptions:</p> <ul> <li>Independence of Observations: The samples are independent of each other.</li> <li>Normality: The data in each group are approximately normally distributed.</li> <li>Homogeneity of Variances: The variance among the groups should be approximately equal.</li> </ul>"},{"location":"statistics/hypothesis/ANOVA/#task","title":"Task","text":"Task: Student Performance <p>Download the following dataset and load it into your notebook. Therefore use the python package <code>openml</code>. </p> <pre><code># Dataset: https://www.openml.org/search?type=data&amp;status=active&amp;id=43098\nimport openml\n\ndataset = openml.datasets.get_dataset(43098)\ndf ,_ ,_ ,_  = dataset.get_data(dataset_format=\"dataframe\", target=None)\ndf.head()\n</code></pre> <p>It contains the academic performance of 1000 students in different subjects. Answer the following questions using Python:</p> <ol> <li>Are the <code>math.score</code> results of different <code>race.ethnicity</code> groups significantly different (\\(\\alpha = 5\\%\\))?</li> <li>Are the <code>reading.score</code> results of different <code>lunch</code> groups significantly different?</li> </ol> <p>For both questions, proceed as follows:</p> <ul> <li>Perform an ANOVA to answer both questions. Formulate an H<sub>0</sub> and perform:<ul> <li>Manual calculation of the ANOVA</li> <li>Automatic calculation of the ANOVA using <code>scipiy.stats</code>.</li> </ul> </li> <li>Are both results matching? Interpret the results.</li> </ul>"},{"location":"statistics/hypothesis/General/","title":"Hypothesis","text":""},{"location":"statistics/hypothesis/General/#independent-variables","title":"(In)dependent Variables","text":"<p>Let's begin with the basic concepts of independent variables (IVs) and dependent variables (DVs). </p> <ul> <li>The dependent variable is what you're aiming to explain or predict in your study (the target). </li> <li>The independent variables are the factors you believe will influence or cause changes in the dependent variable (the features).</li> </ul> <p>In experimental research, independent variables are what you manipulate or control to observe their effect on the dependent variable. For example, if you're studying the effect of water on a plant, the amount of water is the independent variable, and the size or number of leaves are the dependent variable. </p> Controlable Variables <p>Not all independent variables are under your control; some are observed variables that you believe have an impact on the dependent variable.</p> (Source: Sciencenotes.org)  <p>Let's explore a few examples.</p> Examples: IV/DV Easy <ol> <li> <p>The effect of sleep duration on cognitive function.</p> <ul> <li>IV: Sleep duration\u2014it's what you might manipulate or measure to see its effect.</li> <li>DV: Cognitive function\u2014the outcome you're trying to explain or predict.</li> </ul> </li> <li> <p>The relationship between socioeconomic status and health outcomes.</p> <ul> <li>IV: Socioeconomic status\u2014it potentially influences health outcomes.</li> <li>DV: Health outcomes\u2014the aspect you're studying in relation to socioeconomic status.</li> </ul> </li> <li> <p>Does the amount of screen time affect children's attention spans?</p> <ul> <li>IV: Amount of screen time - this is the variable you might adjust or observe.</li> <li>DV: Children's attention spans - the outcome you're assessing.</li> </ul> </li> </ol> <p>In some cases, distinguishing between the IV and DV can be ambiguous. For instance:</p> Examples: IV/DV Difficult <p>The correlation between stress levels and physical activity.</p> <p>It's not immediately clear which variable influences the other. Does increased stress lead to less physical activity, or does less physical activity contribute to higher stress levels? Depending on your research focus, you might assign stress levels as the IV and physical activity as the DV, or vice versa.</p>"},{"location":"statistics/hypothesis/General/#models-and-modeling","title":"Models and Modeling","text":""},{"location":"statistics/hypothesis/General/#definition","title":"Definition","text":"<p>Models are simplified representations of reality, designed to explain or predict phenomena by highlighting essential features while ignoring less critical details. The real world is incredibly complex, and models help us make sense of it by focusing on key variables.</p> Imagination Examples: Model <p>Consider, for example, modeling what influences a person's academic performance. A simple model might include:</p> <ul> <li>Study Time: More hours spent studying could lead to better performance.</li> <li>Class Attendance: Regular attendance might improve understanding of the material.</li> <li>Access to Resources: Availability of textbooks and learning materials could enhance learning.</li> </ul> <p>Our model might look something like this:</p> \\[ \\text{Academic Performance} = \\beta_1 (\\text{Study Time}) + \\beta_2 (\\text{Class Attendance}) + \\beta_3 (\\text{Access to Resources}) + \\epsilon \\] <p>Here, \\( \\beta_1, \\beta_2, \\beta_3 \\) are coefficients representing the influence of each independent variable, and \\( \\epsilon \\) is the residual or error term. The residual captures all other factors affecting academic performance that aren't included in the model.</p>"},{"location":"statistics/hypothesis/General/#residuals","title":"Residuals","text":"<p>Residuals are a key component in evaluating the effectiveness of a statistical model. They represent the difference between the observed data points and the values predicted by the model</p> Interpretation: Residuals <p>Residuals essentially capturing what the model fails to explain. </p> <p>For example, suppose you're modeling the impact of study time (independent variable) on test scores (dependent variable). After applying your model, you find that some students scored higher or lower than predicted. These differences are the residuals. </p> <ul> <li>Small residuals imply that the model accurately captures the relationship between study time and test scores. </li> <li>On the other hand, large residuals indicate that there are other factors - like prior knowledge or test anxiety - not accounted for in the model. </li> </ul> <p>Analyzing residuals helps you identify shortcomings in your model and areas where it can be refined for better accuracy.</p>"},{"location":"statistics/hypothesis/General/#fitting","title":"Fitting","text":"<p>In modeling, there's a crucial balance between simplicity and accuracy:</p> <ul> <li>Underfitting: A model that's too simple may not capture important patterns in the data, leading to large residuals and poor predictions.</li> <li>Overfitting: A model that's too complex may fit the training data too closely, including the noise, and may not generalize well to new data.</li> <li>Optimal Model: A model with an optimal fit captures the underlying trend without fitting the noise.</li> </ul> (Source: Medium)  <p>The goal is to develop a model that's as simple as possible but still effectively explains the data - a concept known as Occam's Razor in philosophy.</p>"},{"location":"statistics/hypothesis/General/#models-vs-hypothesis","title":"Models vs Hypothesis","text":"<p>Let's touch on hypothesis testing. In statistics, hypothesis testing is essentially about comparing models to see which one better fits the data. A hypothesis might propose that one model is superior to another in explaining a particular phenomenon.</p> <p>For example:</p> <ul> <li>Null Hypothesis (\\( H_0 \\)): There is no effect of study time on exam scores - the simpler model without the study time variable fits the data just as well.</li> <li>Alternative Hypothesis (\\( H_1 \\)): Study time does affect exam scores - the model including study time provides a better fit.</li> </ul> <p>By testing these hypotheses, we're evaluating whether adding a variable (making the model more complex) significantly improves our ability to explain the dependent variable.</p>"},{"location":"statistics/hypothesis/General/#hypothesis_1","title":"Hypothesis","text":"<p>In this section, we'll delve into the concept of hypothesis testing, a cornerstone of statistical analysis. Understanding what a hypothesis is and how to formulate a strong one is essential for designing experiments and interpreting data effectively.</p> (Source: Imgflip Meme Generator)"},{"location":"statistics/hypothesis/General/#what-is-a-hypothesis","title":"What Is a Hypothesis?","text":"<p>Hypothesis: an idea or explanation for something that is based on known facts but has not yet been proved</p> <p>-- Cambridge Dictionary</p> <p>So in other words, a hypothesis is a testable statement that predicts a relationship between variables. It is an educated guess based on prior knowledge and observation, which can be rejected or not through experimentation or further observation. Crucially, a hypothesis must be falsifiable, meaning there must be a possible negative answer to the hypothesis.</p> <p>Hypotheses serve several critical functions in research and data analysis:</p> <ul> <li>Enhance Experimental Design: They provide a clear focus and direction for designing experiments.</li> <li>Promote Critical Thinking: Formulating hypotheses encourages logical reasoning and careful consideration of potential outcomes.</li> <li>Guide Data Analysis: They help determine which statistical tests to use and how to interpret the results.</li> <li>Advance Knowledge: Hypotheses enable the development and refinement of theories, contributing to scientific progress.</li> <li>Test Theories: They allow researchers to challenge existing theories and potentially replace them with more accurate ones.</li> </ul> <p>By converting abstract ideas into specific, testable predictions, hypotheses facilitate meaningful research and discoveries.</p>"},{"location":"statistics/hypothesis/General/#characteristics-of-a-hypothesis","title":"Characteristics of a Hypothesis","text":"<p>A Hypothesis should fullfill some requirements to be considered as strong. Those requirement are:</p> <ul> <li>Clear and Specific: It precisely states the expected relationship between variables.</li> <li>Testable and Falsifiable: It can be supported or refuted through experimentation or observation.</li> <li>Based on Existing Knowledge: It relies on prior research or established theories.</li> <li>Predictive: It makes definite predictions about outcomes.</li> <li>Relevant: It has implications for understanding broader phenomena, not just a single dataset.</li> <li>Directional (when appropriate): It specifies the expected direction of the relationship (e.g., increases, decreases).</li> </ul> Not a Hypothesis <p>\"Technology is changing rapidly.\"</p> <p>Explanation: This is a general observation, not a testable hypothesis. It lacks specificity and does not predict a measurable outcome.</p> Weak Hypothesis <p>\"Eating fruits affects health.\"</p> <p>Explanation: While somewhat testable, it's vague. It doesn't specify which fruits, what aspect of health, or the nature of the effect.</p> Strong Hypothesis <p>\"Adults who eat an apple a day have lower cholesterol levels than those who do not.\"</p> <p>Explanation*: This hypothesis is specific, testable, and based on prior knowledge about the health benefits of apples. It predicts a measurable outcome (cholesterol levels) in a defined group (adults).</p> <p>To explore the concept of hypothesis characteristics we can practice on some examples</p> Task: Hypothesis Characteristics <p>Let's practice by classifying some statements (no/weak/strong hypothesis):</p> <ol> <li>\"Does exercise improve mental health?\"</li> <li>\"Drinking green tea leads to weight loss.\"</li> <li>\"College students who sleep at least 7 hours per night have higher GPAs than those who sleep less.\"</li> <li>\"Reading improves language skills.\"</li> <li>\"Children exposed to bilingual education from an early age will perform better on cognitive flexibility tests than those who are not.\"</li> </ol>"},{"location":"statistics/hypothesis/General/#the-null-hypothesis","title":"The Null Hypothesis","text":"<p>In statistical testing, we often work with two hypotheses:</p> Definition: Null &amp; Alternative Hypothesis <ul> <li>Null Hypothesis (\\( H_0 \\)): Assumes no effect or no difference between groups or variables.</li> <li>Alternative Hypothesis (\\( H_1 \\) or \\( H_a \\)): Proposes that there is an effect or a difference.</li> </ul> Examples: \\( H_0 \\) &amp; \\( H_1 \\) <ul> <li>Null Hypothesis \\( H_0 \\): \"Listening to classical music while studying has no effect on memory retention in high school students.\"</li> <li>Alternative Hypothesis \\( H_1 \\): \"Listening to classical music while studying improves memory retention in high school students.\"</li> </ul> <p>In statistical analyses, we test the null hypothesis to determine whether there is sufficient evidence to reject it in favor of the alternative hypothesis. Rejecting the null hypothesis suggests that the data support the alternative hypothesis.</p> (Source: Imgflip Meme Generator)  <p>But why Focus on the Null Hypothesis?</p> <ul> <li>Statistical Simplicity: It's mathematically simpler to test for no effect than to prove a specific effect.</li> <li>Avoiding Bias: It prevents researchers from seeing effects that aren't there due to expectations.</li> <li>Falsifiability: It's easier to disprove (falsify) a universal negative (no effect) than to prove a universal positive.</li> </ul>"},{"location":"statistics/hypothesis/General/#recap","title":"Recap","text":"<ul> <li>Independent Variables (IVs): Factors you believe influence the outcome.</li> <li>Dependent Variables (DVs): The outcomes you're trying to explain or predict.</li> <li>Models: Simplified representations of reality using equations to explain relationships between variables.</li> <li>Residuals: Differences between the observed data and what the model predicts; they capture unexplained variability.</li> <li>Balance in Modeling: Aim for models that are simple yet sufficiently complex to accurately capture the underlying patterns in the data.</li> <li>Hypothesis Testing: A method of comparing models to determine which one better explains the data.</li> </ul> <p>Understanding how to formulate and test hypotheses is essential for conducting rigorous research and making meaningful contributions to knowledge. A strong hypothesis guides the research process, informs experimental design, and provides a basis for interpreting results. By mastering hypothesis testing, you enhance your ability to analyze data critically and draw valid conclusions.</p>"},{"location":"statistics/hypothesis/Metrics/","title":"Metrics","text":""},{"location":"statistics/hypothesis/Metrics/#p-values","title":"P-Values","text":"<p>P-values are a central measure of modern inferential statistics, often mentioned in research studies and statistical analyses. You might frequently hear statements like \"The p-value is less than 0.05,\" highlighting their importance in determining statistical significance.</p> (Source: xkcd)"},{"location":"statistics/hypothesis/Metrics/#definition","title":"Definition","text":"<p>A p-value is a probability metric that helps us determine the significance of our results. </p> Definition: P-Value <p>The p-Value quantifies the likelihood of obtaining a test statistic at least as extreme as the one observed, assuming the null hypothesis is true. </p> <p>In simpler terms, it answers the question: If there is no real effect or difference, what is the probability of observing the data we have collected?</p> Code <pre><code>import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\n# Generate the data for the normal distribution\nx = np.linspace(-4, 4, 1000)\ny = norm.pdf(x, 0, 1)\n\n# Data for filling the upper 5% region\nx_fill = np.linspace(norm.ppf(0.95, 0, 1), 4, 100)\ny_fill = norm.pdf(x_fill, 0, 1)\n\n# Create DataFrame for Plotly\ndf = pd.DataFrame({'x': x, 'y': y})\ndf_fill = pd.DataFrame({'x': x_fill, 'fill': y_fill})\n\n# Create the plot\nfig = px.line(df, x='x', y='y')\nfig.add_trace(px.line(df_fill, x='x', y='fill').data[0])\n\n# Adjust the plot\nfig.data[0].update(line=dict(color='#00416E', width=2))\nfig.data[1].update(\n    fill='tozeroy', \n    fillcolor='rgba(0, 65, 110, 0.4)',\n    line=dict(width=0),\n)\n\n# Add layout modifications\nfig.update_layout(\n    xaxis_title_text='z',\n    yaxis_title_text='Probability Density',\n    title=dict(\n        text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Null Distribtion vs Alternativ Value &lt;/span&gt;&lt;/b&gt;',\n    ),\n    showlegend=False,\n)\n\n# Add vertical line and annotations\nlimit_down = norm.ppf(0.95, 0, 1)\nfig.add_vline(x=limit_down, line_dash=\"dash\", annotation_text=\"H_A value\", annotation_position=\"top right\", line_color=\"#E87F2B\", annotation_font_color=\"#E87F2B\")\nfig.add_annotation(x=-0, y=0.2, text=\"Null Distribution\", showarrow=False, font=dict(color=\"#00416E\"))\n\n# Show the plot\nfig.show()\n</code></pre> Reminder <ul> <li> <p>Null Hypothesis (H<sub>0</sub>): This is the default assumption that there is no effect or no difference. For example, when testing a new medication, the null hypothesis might state that the medication has no effect on patients compared to a placebo.</p> </li> <li> <p>Alternative Hypothesis (H<sub>1</sub> or H<sub>A</sub>): This hypothesis suggests that there is an effect or a difference. In the medication example, the alternative hypothesis would state that the medication does have an effect on patients.</p> </li> </ul> <p>In statistical testing, we often visualize these hypotheses using distributions:</p> <ul> <li> <p>Null Hypothesis Distribution: This represents the expected distribution of the test statistic if the null hypothesis is true. It is typically derived from theoretical probability distributions (like the normal distribution) and calculated using statistical formulas based on assumptions and degrees of freedom.</p> </li> <li> <p>Alternative Hypothesis Distribution: This represents the distribution if the alternative hypothesis is true. However, in practice, we usually don't have a theoretical distribution for the alternative hypothesis because it's based on the actual effect we're trying to detect, which is unknown.Therefore, we collect data from experiments or studies, resulting in an observed test statistic (like a sample mean or proportion).</p> </li> </ul>"},{"location":"statistics/hypothesis/Metrics/#interpretation","title":"Interpretation","text":"<p>The method for calculating a p-value depends on the statistical test being used (e.g., t-test, chi-squared test, ANOVA). While the computational approaches vary, the interpretation of the p-value remains consistent: the p-value answers the  fundamental question about the probability of observing the data under the null hypothesis.</p> <ul> <li> <p>Low P-Value (e.g., p &lt; 0.05): Indicates that the observed data is unlikely under the null hypothesis. This leads us to consider rejecting the null hypothesis in favor of the alternative hypothesis.     </p> </li> <li> <p>High P-Value (e.g., p &gt; 0.05): Suggests that the observed data is consistent with the null hypothesis. We do not have enough evidence to reject the null hypothesis.     </p> </li> </ul> Note <p>A low p-value does not prove that the alternative hypothesis is true; it merely indicates that the null hypothesis may not fully explain the observed data.</p> Limitations <ul> <li> <p>Cannot Prove Hypotheses: P-values do not provide proof but rather evidence against the null hypothesis.</p> </li> <li> <p>Dependence on Sample Size: Large samples can produce small p-values for trivial effects, while small samples might not detect significant effects.</p> </li> <li> <p>Not Measures of Effect Size: A p-value does not indicate the magnitude of an effect.</p> </li> </ul>"},{"location":"statistics/hypothesis/Metrics/#significance","title":"Significance","text":"<p>To determine statistical significance, we compare the p-value to a predetermined significance level, often denoted as alpha (\\(\\alpha\\)). If the p-value is less than \\(\\alpha\\), we declare the result statistically significant. Common choices for \\(\\alpha\\) are 0.05 or 0.01, representing a 5% or 1% threshold for significance.</p> <p>It's crucial to recognize that the choice of \\(\\alpha\\) is somewhat arbitrary and can influence whether a result is deemed significant. For instance, if we obtain a p-value of 0.03, setting \\(\\alpha\\) at 0.05 would lead us to consider the result significant. However, if we set \\(\\alpha\\) at 0.01, the same p-value would not be considered significant.</p> <p>In essence, p-values help us assess how compatible our data is with the null hypothesis. By establishing a significance threshold, we can make informed decisions about whether our findings are likely due to random chance or if they indicate a genuine effect.</p>"},{"location":"statistics/hypothesis/Metrics/#one-tailed-vs-two-tailed-tests","title":"One-Tailed vs. Two-Tailed Tests","text":"<p>One-Tailed Test: Used when the research hypothesis predicts the direction of the effect (e.g., a new drug is expected to lower blood pressure). The critical region for rejecting the null hypothesis is entirely on one side of the distribution.</p> <ul> <li>Example: Testing if a new study technique increases test scores. The null hypothesis states there is no improvement, and the alternative hypothesis states there is an improvement. Only high test scores (one tail) are considered evidence against the null hypothesis.</li> <li> <p>In a one-tailed test with an alpha level of 0.05, the entire 5% significance level is in one tail.</p> <p></p> </li> </ul> <p>Two-Tailed Test: Used when the research hypothesis does not predict the direction (e.g., the drug affects blood pressure but could either raise or lower it). The critical regions are on both ends of the distribution.</p> <ul> <li>Example: Testing if a new fertilizer affects plant growth. The null hypothesis states it has no effect, while the alternative hypothesis states it has an effect (could be an increase or decrease). Both unusually high and low plant growth measurements (both tails) are evidence against the null hypothesis.</li> <li>In a two-tailed test with the same alpha level, the 5% is split between both tails (2.5% in each).     </li> </ul>"},{"location":"statistics/hypothesis/Metrics/#common-misconceptions","title":"Common Misconceptions","text":"<p>Understanding what p-values do not represent is crucial to avoid misinterpretations.</p> <ol> <li> <p>Misconception: A p-value of 0.02 means there's a 2% chance the effect is present in the population.</p> <ul> <li>Correction: A p-value of 0.02 means there's a 2% probability of observing the test statistic (or something more extreme) assuming the null hypothesis is true. It does not indicate the proportion of the population exhibiting the effect.</li> </ul> </li> <li> <p>Misconception: A p-value of 0.02 means there's a 98% chance that the sample statistic equals the population parameter.</p> <ul> <li>Correction: The p-value does not provide the probability that the sample statistic equals the population parameter. It only assesses the likelihood of the observed data under the null hypothesis.</li> </ul> </li> <li> <p>Misconception: A p-value smaller than the threshold confirms the effect is real.</p> <ul> <li>Correction: A small p-value suggests that the observed data is unlikely under the null hypothesis, but it does not prove the effect is real. Other factors like sample size, data quality, and experimental design also play significant roles.</li> </ul> </li> <li> <p>Misconception: A large p-value means the null hypothesis is true.</p> <ul> <li>Correction: A large p-value indicates that the data is consistent with the null hypothesis, but it doesn't prove that the null hypothesis is true. It might also be due to insufficient sample size or variability in the data.</li> </ul> </li> </ol>"},{"location":"statistics/hypothesis/Metrics/#z-score","title":"Z-Score","text":"<p>Every statistical distribution can be associated with a set of p-values because they can all be interpreted in terms of probability. However, some distributions are more prominent due to their frequent use in statistical analyses. The most significant of these is the normal distribution, also known as the bell curve or Gaussian distribution. Its importance is so profound that it's fundamental in various fields, including statistics, physics, and social sciences.</p> <p>Let's begin by exploring z-scores and the proportion of data within a normal distribution. The normal distribution is symmetrical around the mean, and its standard deviation determines the spread of the data. This property allows us to make precise statements about the proportion of data falling within certain ranges.</p> Example: Heights of Adult Women <p>Suppose the average height of adult women in a country is 165 cm, with a standard deviation of 6.35 cm.</p> <ul> <li> <p>Within 1 Standard Deviation (\u00b11 SD): Heights between 158.65 cm and 171.35 cm (165 cm \u00b1 6.35 cm). Approximately 68.3% of women have heights within this range.</p> </li> <li> <p>Within 2 Standard Deviations (\u00b12 SD): Heights between 152.3 cm and 177.7 cm (165 cm \u00b1 12.7 cm). About 95.5% of women fall within this range.</p> </li> <li> <p>Within 3 Standard Deviations (\u00b13 SD): Heights between 145.95 cm and 184.05 cm (165 cm \u00b1 19.05 cm). Approximately 99.7% of women are within this range.</p> </li> </ul> <p>These percentages are known as the Empirical Rule or the 68-95-99.7 rule and are fundamental properties of the normal distribution. Memorizing these values can greatly enhance your understanding and efficiency when working with normally distributed data.</p> Definition: Empirical Rule \\[ \\begin{aligned}\\Pr(\\mu -1\\sigma \\leq X\\leq \\mu +1\\sigma )&amp;\\approx 68.27\\%\\\\\\Pr(\\mu -2\\sigma \\leq X\\leq \\mu +2\\sigma )&amp;\\approx 95.45\\%\\\\\\Pr(\\mu -3\\sigma \\leq X\\leq \\mu +3\\sigma )&amp;\\approx 99.73\\%\\end{aligned} \\]"},{"location":"statistics/hypothesis/Metrics/#common-p-value-and-z-score-pairs","title":"Common P-Value and Z-Score Pairs","text":"<p>Understanding the relationship between p-values and z-scores is crucial for hypothesis testing. Here are some commonly used pairs that are worth committing to memory.</p> <ul> <li> <p>One-Tailed Test</p> P-Value Z-Score 0.05 1.645 0.01 2.33 0.001 3.09 <p>Example: P-Value = 0.05: Z-Score \u2248 1.645</p> <p>This means there's a 5% chance of observing a test statistic at least as extreme as 1.645 standard deviations above the mean, assuming the null hypothesis is true.</p> </li> <li> <p>Two-Tailed Test</p> P-Value Z-Score 0.05 \u00b11.96 0.01 \u00b12.576 0.001 \u00b13.291 <p>Example: P-Value = 0.05: Z-Score \u2248 \u00b11.96</p> <p>There's a 5% chance of observing a test statistic more than 1.96 standard deviations away from the mean in either direction.</p> </li> </ul> Example: Exam Scores <p>Imagine a standardized test where the average score is 500, with a standard deviation of 100.</p> <p>In a one-tailed test, we're interested in deviations in one direction - for example, scores significantly above the mean. The Critical Z-Score for a one-tailed test at the 5% significance level, the critical z-score is approximately 1.645.</p> <ul> <li>Critical Score Calculation:</li> </ul> \\[ \\begin{aligned} \\text{Critical Score} &amp; =  \\text{Mean} + (Z \\cdot \\text{Standard Deviation})\\\\                       &amp; =   500 + (1.645 \\cdot 100) \\\\                       &amp; =  664.5 \\end{aligned} \\] <ul> <li>Interpretation: Any score above 664.5 is considered statistically significant at the 5% level in a one-tailed test.</li> </ul>"},{"location":"statistics/hypothesis/Metrics/#z-table","title":"z-Table","text":"<p>However, these common p-value and z-score pairs are not always sufficient for every analysis. For more precise probability calculations or for confidence levels not listed in standard tables, you can use z-tables. A z-table, also known as the standard normal distribution table, provides the cumulative probabilities associated with z-scores in a standard normal distribution. By using the z-table, you can determine the probability that a data point falls below a specific z-score or find the z-score corresponding to a particular cumulative probability. This table is essential for various statistical analyses, including hypothesis testing and constructing confidence intervals, as it allows for the translation of z-scores into meaningful probability values. Mastery of the z-table enhances the ability to interpret and apply statistical findings accurately.</p> <p>There are many sources for z-tables around the internet. One well-organized version is available on Wikipedia.</p> <p>The table basically consists of two areas:</p> <ul> <li>z-values (divided into rows and columns): need to be summed up (row + column)</li> <li>Cumulative probabilities: the values in the table</li> </ul> <p>The structure of the table can be read as follows:</p> <p></p> Using the z-table <p>It is not sufficient to know the significance level \\(\\alpha = 0.05\\) for determining the z-value; we also need to know if we are going to perform a one-tailed or two-tailed test. For a one-tailed test, we need to look up the cumulative probability of \\(95\\%\\). For a two-tailed test, we need to split the significance level of \\(5\\%\\) and look for the critical values corresponding to \\(2.5\\%\\) and \\(97.5\\%\\). Since the normal distribution is symmetric around zero, both critical values for the two-tailed test will be the same.</p>"},{"location":"statistics/hypothesis/Metrics/#confidence-interval","title":"Confidence Interval","text":"<p>While z-scores are instrumental in understanding the position of a data point within a distribution, confidence intervals provide a range within which we expect a population parameter (like the mean) to lie, based on our sample data. Integrating the concept of confidence intervals with z-scores enhances our ability to make informed statistical inferences.</p>"},{"location":"statistics/hypothesis/Metrics/#what-is-a-confidence-interval","title":"What is a Confidence Interval?","text":"<p>A confidence interval (CI) is a range of values derived from sample data that is likely to contain the true population parameter. The confidence level, typically expressed as a percentage (e.g., 95%), represents the degree of certainty that the interval captures the parameter.</p> <ul> <li>Confidence Level: The probability that the confidence interval contains the true parameter in repeated sampling.</li> <li>Interval Width: Reflects the precision of the estimate; narrower intervals indicate higher precision.</li> <li>Dependence on Sample Size and Variability: Larger sample sizes and lower variability lead to narrower confidence intervals.</li> </ul>"},{"location":"statistics/hypothesis/Metrics/#calculating-confidence-intervals-using-z-scores","title":"Calculating Confidence Intervals Using Z-Scores","text":"<p>In the context of the normal distribution, z-scores play a crucial role in constructing confidence intervals for the population mean when the population standard deviation is known. Specifically, the z-score determines the number of standard deviations to extend from the sample mean to achieve the desired confidence level.</p> Definition: Confidence Interval <p>When the population standard deviation (\\(\\sigma\\)) is known, the confidence interval for the population mean (\\(\\mu\\)) can be calculated using the following formula:</p> \\[ \\left[\\bar{x} - z_{(1-\\frac{\\alpha}{2})} \\frac{\\sigma}{\\sqrt{n}}; \\quad \\bar{x} + z_{(1-\\frac{\\alpha}{2})} \\frac{\\sigma}{\\sqrt{n}} \\right] \\] <p>with</p> <ul> <li>\\(\\bar{x}\\): Sample mean</li> <li>\\(z\\): Critical value from the standard normal distribution corresponding to the desired confidence level</li> <li>\\(\\sigma\\): Population standard deviation</li> <li>\\(n\\): Sample size</li> </ul> Exmple: Estimating Average Height with Confidence Interval <p>Using the earlier example of adult women\u2019s heights:</p> <ul> <li>Sample Size (\\(n\\)): 100</li> <li>Sample Mean (\\(\\bar{x}\\)): 165 cm</li> <li>Population Standard Deviation (\\(\\sigma\\)): 6.35 cm</li> <li>Confidence Level: 95% \u2192 \\(\\alpha = 0.05\\)</li> </ul> <p>Calculate the confidence interval: </p> <ol> <li>Critical Value (\\(z_{(1-\\frac{\\alpha}{2})}\\)): For a 95% confidence level, \\(z_{(1-\\frac{\\alpha}{2})} \\approx 1.96\\).</li> <li> <p>Standard Error: </p> \\[      \\frac{\\sigma}{\\sqrt{n}}= \\frac{6.35}{\\sqrt{100}} = 0.635 cm  \\] </li> <li> <p>Margin of Error: </p> \\[      z_{(1-\\frac{\\alpha}{2})} \\frac{\\sigma}{\\sqrt{n}} = 1.96 \\times 0.635 \\approx 1.245 cm \\] </li> <li> <p>Confidence Interval: \\(165 \\pm 1.245\\)</p> \\[     \\left[163.755 cm, 166.245 cm\\right] \\] </li> </ol> <p>Interpretation: We are 95% confident that the true average height of adult women in the country lies between 163.755 cm and 166.245 cm.</p>"},{"location":"statistics/hypothesis/Metrics/#factors-affecting-confidence-interval-width","title":"Factors Affecting Confidence Interval Width","text":"<p>Several factors influence the width of a confidence interval:</p> <ol> <li> <p>Sample Size (\\(n\\)):</p> <ul> <li>Larger Sample Size: Leads to a smaller standard error, resulting in a narrower confidence interval.</li> <li>Smaller Sample Size: Increases the standard error, leading to a wider confidence interval.</li> </ul> </li> <li> <p>Variability in Data (\\(\\sigma\\)):</p> <ul> <li>Lower Variability: Decreases the standard error, narrowing the confidence interval.</li> <li>Higher Variability: Increases the standard error, widening the confidence interval.</li> </ul> </li> <li> <p>Confidence Level:</p> <ul> <li>Higher Confidence Level (e.g., 99%): Requires a larger critical value, resulting in a wider interval.</li> <li>Lower Confidence Level (e.g., 90%): Uses a smaller critical value, leading to a narrower interval.</li> </ul> </li> </ol>"},{"location":"statistics/hypothesis/Metrics/#degree-of-freedom","title":"Degree of Freedom","text":"Example: DoF Mean <p>Given are four variables: \\(A\\), \\(B\\), \\(C\\), and \\(D\\) with an average (mean) of \\(50\\). Is it possible to determine the exact values of \\(A\\), \\(B\\), \\(C\\), and \\(D\\)?</p> <p>No, it's not. Knowing only the mean and the number of variables, there are infinitely many combinations of values that satisfy the condition. The mean is calculated as:</p> \\[ \\text{Mean} = \\frac{A + B + C + D}{4} = 50 \\] <p>This equation alone isn't enough to solve for the individual variables because there are infinitely many solutions.</p> <p>Now, suppose the values for \\(A\\), \\(B\\) and \\(C\\) are given.</p> <ul> <li>A = 40</li> <li>B = 55</li> <li>C = 60</li> </ul> <p>Can we now calculate \\(D\\)? Yes, by substituting the values into the variables</p> \\[ 50 = \\frac{40 + 55 + 60 + D}{4} \\] \\[ D = (50 \\cdot 4)-(40+55+60) = 45 \\] <p>Once we know the mean and three of the four variables, the fourth variable is completely determined. It has no freedom to vary independently; it's dependent on the others. So, in this scenario, we have three degrees of freedom. That's because we can freely choose any values for \\(A\\), \\(B\\) and \\(C\\), but \\(D\\) is constrained by the mean.</p> <p>Now, Suppose we have a population where the true mean (\\( \\mu \\)) is known to be \\(50\\). We take a random sample of four observations: \\(A\\), \\(B\\), \\(C\\), and \\(D\\). What are the degrees of freedom in our sample?</p> <p>Answer: We have four degrees of freedom. </p> <p>Even though we know the population mean, the sample mean (\\( \\bar{x} \\)) calculated from A, B, C, and D may not be exactly 50 due to sampling variability. The sample values can vary freely; knowing the population mean does not impose a constraint on the individual sample values. Therefore, all four sample observations have the freedom to vary independently.</p> <p>So, based on this example, the Degree of Freedom (DoF) can be defined as: </p> Definition: Degree of Freedom <p>Degrees of freedom are the number of independent values that a statistical analysis can estimate. You can also think of it as the number of values that are free to vary as you estimate parameters. </p> <p>-- Jim Frost - Degrees of Freedom in Statistics</p> <p>In statistics the degrees of freedom are necessary to provide unbiased estimates of population parameters which is crucial for reliable statistical inference. Furthermore it affects the shape of statistical distributions used in hypothesis testing, influencing critical values and p-values.</p> <p>In following test we will conduct, the degree of freedom will depend on the sample size and can be calculated as: </p> \\[dof = n-1\\]"},{"location":"statistics/hypothesis/Metrics/#errors","title":"Errors","text":"<p>In statistical hypothesis testing, we make decisions based on sample data, which can lead to two types of errors due to randomness and variability.</p> Definition: Types of Error <p>In statistical testing, there are two types of errors: </p> <ol> <li>Type I Error (False Positive): Rejecting the null hypothesis (\\( H_0 \\)) when it is actually true.</li> <li>Type II Error (False Negative): Failing to reject the null hypothesis when it is actually false.</li> </ol> Example: Error Type <p>A factory produces electronic components that must meet specific quality standards before they are shipped to customers. Each component is tested to determine whether it is defective or not.</p> <ul> <li>Null Hypothesis (\\( H_0 \\)): The component is not defective; it meets quality standards.</li> <li>Alternative Hypothesis (\\( H_A \\)): The component is defective; it does not meet quality standards.</li> </ul> <p>Type I Error (False Positive)</p> <p>A component that functions perfectly is tested and, due to a measurement error or overly strict testing criteria, is labeled as defective.</p> <p>Consequence:</p> <ul> <li>Increased Costs: Good components are unnecessarily scrapped or reworked, leading to wasted materials and labor.</li> <li>Reduced Efficiency: Production slows down due to re-inspection and additional quality checks.</li> </ul> <p>Type II Error (False Negative)</p> <p>A component with a subtle flaw passes the quality test and is shipped to customers.</p> <p>Consequence:</p> <ul> <li>Customer Dissatisfaction: Defective products may fail during use, leading to returns and complaints.</li> <li>Reputation Damage: Consistent quality issues can harm the company's reputation.</li> <li>Safety Risks: In critical applications, defective components could cause equipment failure or safety hazards.</li> </ul>"},{"location":"statistics/hypothesis/Metrics/#error-probability","title":"Error Probability","text":"<p>The probability of a Type I Error is equal to the significance level (\\( \\alpha \\)), while on the other side, the probability of the Type II Error is denoted as (\\( \\beta \\)). Reducing the probability of a Type I error (\\( \\alpha \\)) increases the probability of a Type II error (\\( \\beta \\)), and vice versa.</p> Table of Error Types Null Hypothesis (H<sub>0</sub>) is True False Decision about Null Hypothesis (H<sub>0</sub>) Not Reject Correct Decision (True Negative)(Probability = 1\u00a0\u2212\u00a0\u03b1) Type II Error (False Negative)(Probability = \u03b2) Reject Type I Error (False Positive)(Probability = \u03b1) Correct Decision (True Positive)(Probability = 1\u00a0\u2212\u00a0\u03b2) (Source: Graduatetutor)"},{"location":"statistics/hypothesis/Testing/","title":"Testing Principals","text":"<p>Before diving into hypothesis testing, let's briefly revisit the central limit theorem (CLT). The CLT states that the sampling distribution of the sample mean will approximate a normal distribution as the sample size becomes large, regardless of the population's distribution. Because measuring the entire population is often impractical, we rely on samples. Each sample provides an estimate, and repeated sampling yields a distribution of these estimates.</p> Examples: Are Apples as Heavy as Apples <p>Consider a seemingly nonsensical question:</p> <p>\"Are apples as heavy as apples?\"</p> <p>At first glance, this question doesn't make sense. We're comparing apples to apples! However, this example illustrates sampling variability under the null hypothesis.</p> <p> </p> <p>Imagine you have a large bucket of apples, and you want to test whether one group of apples is heavier than another group from the same bucket. Here's how you might proceed:</p> <ol> <li>Sample Group A: Randomly select 10 apples and measure their weights.</li> <li>Sample Group B: Randomly select another 10 apples and measure their weights.</li> <li>Calculate the Mean Weights: Compute the average weight for each group.</li> <li>Compute the Difference: Find the difference between the two average weights.</li> </ol> <p>Under the null hypothesis (no difference in weights), we expect the difference in mean weights to be zero. However, due to sampling variability, the difference is unlikely to be exactly zero. You might find that Group A has a mean weight of 150 grams, while Group B has a mean weight of 152 grams - a difference of 2 grams.</p> <p>Does this mean that apples in Group B are inherently heavier? Probably not. The observed difference is likely due to random variation.</p>"},{"location":"statistics/hypothesis/Testing/#null-alternative-distribution","title":"Null &amp; Alternative Distribution","text":""},{"location":"statistics/hypothesis/Testing/#null-distribution","title":"Null Distribution","text":"<p>When comparing samples from the same population the expected difference is Zero (no difference under the null hypothesis). But due to sampling variability, the observed differences are small. By repeatedly sampling and calculating differences, we can create a null distribution - a distribution of differences expected under the null hypothesis \\( H_0 \\). The null distribution will center around zero and display the variability expected due to random sampling. </p> <p>In statistical hypothesis testing, the null distribution is the probability distribution of the test statistic when the null hypothesis is true.</p> <p>-- Kent W. Staley - An Introduction to the Philosophy of Science</p> <p>To build the null distribution:</p> <ol> <li>Repeat Sampling: Take many pairs of samples from the population.</li> <li>Calculate Differences: For each pair, compute the difference in sample means.</li> <li>Plot the Differences: Create a histogram of these differences.</li> </ol> <p>So if we stick with our <code>age</code> example, and compare samples from the whole population, we will receive a distribution of age differences:</p> Code <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \nadult = fetch_ucirepo(id=2) \n\n# data (as pandas dataframes) \ndata = adult.data.features\n\nimport plotly.express as px\nimport random\nimport numpy as np\nimport pandas as pd\n\nsample_means_dif = []\nfor i in range(10000):\n    sample_means_dif.append(np.mean(random.sample(list(data.age), 20))-np.mean(random.sample(list(data.age), 20)))\n\n\nfig = px.histogram(x=sample_means_dif)\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.8)'))\n\n# Add a horizontal line for the population mean\nfig.add_vline(x=0, line_dash=\"dash\", annotation_text=\"Expected Value: 0\", annotation_position=\"top right\", line_color=\"#E87F2B\")\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Absolute Frequency',\n        xaxis_title_text='Sample Means Difference',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Null Distribution&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Age vs. Age&lt;/span&gt;&lt;span style=\"font-size: 5pt\"&gt; Number of Samples: 10.000 | Data: UCIML Repo: Adult; variable: age&lt;/span&gt; &lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <p>In this case we see, that the distribution looks like we would imagine if the null hypothesis cannot be rejected (is true). </p>"},{"location":"statistics/hypothesis/Testing/#alternative-distribution","title":"Alternative Distribution","text":"<p>The alternative hypothesis posits that there is a genuine difference between groups. Therefore, under the alternative hypothesis, the distribution of differences shifts away from zero. The alternative distribution represents the expected differences if the alternative hypothesis is true.</p> <p>Again we stick with the <code>age</code> example and compare the age differences of mutliple samples for <code>male</code> and <code>female</code> in our dataset:</p> Code <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \nadult = fetch_ucirepo(id=2) \n\n# data (as pandas dataframes) \ndata = adult.data.features\n\nimport plotly.express as px\nimport random\nimport numpy as np\nimport pandas as pd\n\nsample_means_dif = []\nfor i in range(10000):\n    sample_means_dif.append(np.mean(random.sample(list(data[data['sex']==\"Male\"].age), 20))-np.mean(random.sample(list(data[data['sex']==\"Female\"].age), 20)))\n\n\nfig = px.histogram(x=sample_means_dif)\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.8)'))\n\n# Add a horizontal line for the population mean\nfig.add_vline(x=np.mean(sample_means_dif), line_dash=\"dash\", annotation_text=\"Mean Difference: \"+str(np.mean(sample_means_dif)), annotation_position=\"top right\", line_color=\"#E87F2B\")\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Absolute Frequency',\n        xaxis_title_text='Sample Means Difference',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Alternative Distribution&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Age (Male) vs. Age (Female) &lt;/span&gt;&lt;span style=\"font-size: 5pt\"&gt; Number of Samples: 10.000 | Data: UCIML Repo: Adult; variable: age&lt;/span&gt; &lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <p>What we can see now is, that the peak (mean) of the distribution ist not around zero and therefore the null hypothesis is false and can be rejected against an alternative hypothesis. But is this difference significant?</p>"},{"location":"statistics/hypothesis/Testing/#quantifying-differences-between-distributions","title":"Quantifying Differences Between Distributions","text":"<p>To assess whether an observed difference is significant, we need to analyze the center of the two peaks of the null distribution and the alternative distribution. However, to make this comparison meaningful, we need to scale or normalize this difference based on the spread (or width) of the distributions. This normalization accounts for the units and ensures that the comparison is independent of the scale. For example, if we're measuring something in centimeters, both the numerator and denominator will be in the same unit, effectively canceling the units out.</p> Code <pre><code>import plotly.graph_objects as go\nfig = go.Figure()\n\nfig.add_trace(go.Histogram(x=sample_means_dif, marker=dict(color='rgba(0, 65, 110, 0.8)'),name='Null Distribution'))\n\n# Zweite Spur hinzuf\u00fcgen\nfig.add_trace(go.Histogram(x=sample_means_difa, marker=dict(color='rgba(232, 127, 43, 0.8)'),name='Alternative Distribution'))\n\n# Add a horizontal line for the population mean\nfig.add_vline(x=0, line_dash=\"dash\", annotation_text=\"H_0 Center\", annotation_position=\"top left\", line_color=\"#00416E\")\nfig.add_vline(x=np.mean(sample_means_difa), line_dash=\"dash\", annotation_text=\"H_a Center\", annotation_position=\"top right\", line_color=\"#E87F2B\")\n\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Absolute Frequency',\n        xaxis_title_text='Sample Means Difference',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Differences in Distributions&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 5pt\"&gt; Number of Samples: 10.000 | Data: UCIML Repo: Adult; variable: age&lt;/span&gt; &lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <p>It\u2019s important to note that different statistical methods may define this \"width\" differently. So, we shouldn't get too caught up in formal definitions here. The key is to keep things flexible and understand this as a general approach that various statistical tests can adapt in different ways.</p> <p>In essence, what we\u2019re doing is taking the difference between two central points (which we'll call \"signal\") and dividing it by some measure of the distribution\u2019s width (which represents \"noise\"). This idea of comparing signal to noise is at the core of hypothesis testing. Whether we\u2019re dealing with null hypothesis testing or alternative methods, the goal is to quantify how strong the signal is relative to the noise.</p> \\[ \\text{Signal-to-Noise Ratio} = \\frac{\\text{Difference of Means}}{\\text{Measure of Variability}} \\] <p>This concept of comparing signal to noise is the cornerstone of inferential statistics. Regardless of the specific statistical test, the underlying principle is the same:</p> <ul> <li>Assess the Effect: Measure the difference or relationship you're interested in.</li> <li>Account for Variability: Consider the variability in the data.</li> <li>Determine Significance: Use the signal-to-noise ratio to infer whether the effect is statistically significant.</li> </ul> <p>Examples of Statistical Tests:</p> <ul> <li>t-test: Compares the means of two groups relative to the variability within the groups.</li> <li>ANOVA: Analyzes differences among group means in a sample.</li> <li>Regression Analysis: Assesses the relationship between variables, considering residual variability.</li> </ul>"},{"location":"statistics/hypothesis/Ttest/","title":"T-Test","text":""},{"location":"statistics/hypothesis/Ttest/#introduction-to-t-tests","title":"Introduction to T-Tests","text":"<p>The t-test is a fundamental statistical method widely used in various fields to compare the means of two groups and determine if they are statistically different from each other. At its core, the t-test evaluates whether the difference between the means of two groups is significant or if it could have occurred by chance due to variability in the data. This is crucial when testing hypotheses in research studies.</p> Example: Math Score <p>Suppose you're investigating whether a new tutoring program improves students' math test scores compared to a standard curriculum. Here, the two groups are:</p> <ul> <li>Group A: Students using the new tutoring program.</li> <li>Group B: Students following the standard curriculum.</li> </ul> <p>Your alternative hypothesis is that the mean test score of Group A is different from that of Group B. The null hypothesis states that there is no difference in the mean test scores between the two groups.</p> <p>The t-test formula provides a standardized way to measure the difference between group means relative to the variability in the data:</p> Definition: T-Test \\[ t_k = \\frac{\\bar{x}-\\bar{y}}{s/\\sqrt{n}} = \\frac{\\text{Difference of Means}}{\\text{Standard Deviations}} \\] <p>Where:</p> <ul> <li>\\(\\bar{x}\\) = average of group x</li> <li>\\(\\bar{y}\\) = average of group y</li> <li>\\(s\\) = standard error of the arithmetic mean = \\(\\sigma\\sqrt{n}\\)</li> <li>\\(n\\) = number of samples</li> <li>\\(k\\) = degree of freedom</li> </ul> <p>This formula essentially calculates how many standard deviations the difference between the two means is away from zero. A larger absolute value of \\(t\\) indicates a more significant difference between the groups. The degree of freedom is equal to the sample size minus 1:</p> \\[dof = n-1\\]"},{"location":"statistics/hypothesis/Ttest/#t-distribution","title":"T-Distribution","text":"<p>In the following graph you see the probability of a certain t-value occurring given that the null hypothesis is true. This curve is called the t-distribution (or sometimes Student's t-distribution).</p> Code <pre><code>import numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom scipy.stats import norm\n\n# Generate the data for the normal distribution\nx = np.linspace(-4, 4, 1000)\ny = norm.pdf(x, 0, 1)\n\n# Create DataFrame for Plotly\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Create the plot\nfig = px.line(df, x='x', y='y')\n\n# Adjust the plot\nfig.data[0].update(line=dict(color='#00416E', width=2))\n\n# Add layout modifications\nfig.update_layout(\n    xaxis_title_text='t-Value',\n    yaxis_title_text='P(t|H0)',\n    title=dict(\n        text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;t-Distribution&lt;/span&gt;&lt;/b&gt;',\n    ),\n    showlegend=False,\n)\n\n# Show the plot\nfig.show()\n</code></pre> <p>If the null hypothesis is true, we expect the difference between the means to be zero \\(\\bar{x} - \\bar{y} = 0\\), resulting in a t-value of zero. This corresponds to the highest probability and the peak of the distribution. However, due to sampling variability and noise, the means will not be exactly equal (even if the null hypothesis is true), leading to t-values around zero and therefore to the t-distribution.</p> <p>In Python we can use the <code>scipy.stats</code> package to work easily with the t-distribution</p> <p><pre><code>import scipy.stats as stats\n</code></pre> There are different methods available for the t-distribution <code>stats.t</code> which can be very helpful. </p> <pre><code>stats.t.pdf(1.5, df=10) # probability density function\nstats.t.cdf(1.5, df=10) # cumulative distribution function\n</code></pre>"},{"location":"statistics/hypothesis/Ttest/#dependency-on-the-degree-of-freedom","title":"Dependency on the Degree of Freedom","text":"<p>The t-distribution depends on the degree of freedom.</p> <pre><code>stats.t.pdf(x, df=dof)\n</code></pre> <p>The higher the DoF, the more the curve converge to a standard normal distribution. Because the DoF depends on the sample size n, the following rule of thumb can be stated: for a sample size &gt;30 the standard normal distribution can be used for calculating the p-value. </p> Code <pre><code># Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport scipy.stats as stats\n\n# Define the x-axis values (t-values)\nx = np.arange(-5, 5.01, 0.01)\n\n# Define the degrees of freedom you want to plot, including 'infinity' for Normal distribution\ndegrees_of_freedom = [1, 3, 5, 30, np.inf]  # np.inf represents infinite degrees of freedom (Normal distribution)\n\n# Create an empty DataFrame to store the data for all curves\ndf_all = pd.DataFrame()\n\n# Loop through each degree of freedom and compute the t-distribution\nfor df in degrees_of_freedom:\n    if df == np.inf:\n        y = stats.norm.pdf(x)  # Normal distribution for df = infinity\n        df_label = 'df=\u221e (Normal)'\n    else:\n        y = stats.t.pdf(x, df=df)\n        df_label = f'df={df}'\n    df_temp = pd.DataFrame({'x': x, 'y': y, 'df': df_label})\n    df_all = pd.concat([df_all, df_temp])\n\n# Create the plot using Plotly Express\nfig = px.line(df_all, x='x', y='y', color='df', \n            title=\"&lt;b&gt;t-Distribution for Different Degrees of Freedom&lt;/b&gt;\",\n            labels={'x': 't-Value', 'y': 'P(t|H0)', 'df': 'Degrees of Freedom'})\n\n# Update layout\nfig.update_layout(\n    title=dict(\n        text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;t-Distribution for Different Degrees of Freedom&lt;/span&gt;&lt;/b&gt;',\n    ),\n    xaxis_title_text='t-Value',\n    yaxis_title_text='P(t|H0)',\n    showlegend=True,\n)\n\n# Show the plot\nfig.show()\n</code></pre>"},{"location":"statistics/hypothesis/Ttest/#t-table","title":"T-Table","text":"<p>As described in the previous chapter, the p-value represents the cumulative probability of obtaining a certain t-value or more extreme values. </p> <p>One-Tailed Test</p> <pre><code>stats.t.cdf(1.812, df=10)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.95\n</code></pre> <p>Two-Tailed Test</p> <pre><code>stats.t.cdf(2.228, df=10)-stats.t.cdf(-2.228, df=10)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.95\n</code></pre> <p>Graphically, the p-value is the area under the t-distribution starting from a given t-value. To fasten the evaluation of a t-test and avoid the need for constant calculations, precomputed values are available in what are known as t-tables.</p> <p>There are a lot of sources for t-tables around the internet. One very neatly is available on Wikipedia.</p> <p>The table basically consists of four areas: </p> <ul> <li>degree of freedom, </li> <li>significance level \\(\\alpha\\)</li> <li>Information about one oder two sided test</li> <li>t-value</li> </ul> <p>The structure of the table can be read as follows:</p> <p></p> Using the t-table <p>At least three of the four pieces of information must therefore be available in order to use the table. </p> Task: t-table <p>Take a closer look at the above shown t-table and compare it to the two examples shown above. Can you see the connection? </p>"},{"location":"statistics/hypothesis/Ttest/#calculating-the-p-value","title":"Calculating the p-Value","text":"<p>The p-value helps determine the statistical significance of your results. It represents the probability of observing a t-value as extreme as the one calculated, assuming the null hypothesis is true.</p> <p>There are two ways to use the p-value in the T-Test: </p> <p>Calculate Critical t-Value</p> <p>The first approach is to calculate the critical t-value. This value depends on the chosen significance level (\\(\\alpha\\)), the degrees of freedom, and whether the test is one-tailed (e.g., alpha = 5% on one side) or two-tailed (alpha = 5%, meaning 2.5% on each side). Based on these factors, the critical t-values can be determined, for example by using the t-table shown above. </p> <p>Next, the t-value of the sample can be calculated (using the before mentioned formula) and compared to the critical t-values in order to make a statement about the validity of the null hypothesis.</p> Example: Math Score <p>Let's stick with the example from before. Imagine you conduct the tutoring program study and calculate a t-value of \\(2.5\\). Because we are only interested in the fact that the grades get better, we can use a one-tailed test. Our significance level \\(\\alpha = 5\\%\\). The sample size was \\(11\\) and therefore the degree of freedom is \\(10\\). </p> <p>We can determine the critical t-value using a t-table or by using python</p> <pre><code>stats.t.ppf(0.95, df=10)\n</code></pre> &gt;&gt;&gt; Output<pre><code>1.812\n</code></pre> <p></p> <p>Since the sampled t-value of 2.5 is extremer than the critical t-value of 1.8, the null hypothesis can be rejected and you conclude that the tutoring program has a statistically significant effect on test scores.</p> <p> (Source: Memecreator)  </p> <p>Calculate p-Value of the Sample</p> <p>The second approach tackles the problem from the opposite side. In this case, we start with the sample's t-value and calculate the corresponding p-value. If the p-value is below the significance level alpha, we can reject the null hypothesis.</p> Example: Math Score <p>Now we use the second approach and start from the sample t-value. We calcualte the corresponding p-value</p> <pre><code>(1-stats.t.cdf(2.5, df=10))\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.016\n</code></pre> <p>The calculated p-value (\\(1.6\\%\\)) is lower than the significance level (\\(\\alpha = 5\\%\\)) and the null hypothesis can therefore be rejectet. </p> <p></p>"},{"location":"statistics/hypothesis/Ttest/#strategies-to-maximize-the-t-value","title":"Strategies to Maximize the t-value","text":"<p>To increase the likelihood of detecting a true effect, consider the following approaches:</p> <ol> <li> <p>Increase the Difference Between Means (\\(\\bar{x} - \\bar{y}\\)):</p> <ul> <li>Action: Enhance the impact of the treatment or condition.</li> <li>Example: If testing a new drug, use a dosage that is expected to produce a noticeable effect compared to the placebo.</li> </ul> </li> <li> <p>Decrease the Variability (Reduce \\(s\\)):</p> <ul> <li>Action: Control external factors to minimize data dispersion.</li> <li>Example: In an agricultural study measuring crop yield, ensure that soil quality, irrigation, and sunlight are consistent across test plots.</li> </ul> </li> <li> <p>Increase the Sample Size (\\(n\\)):</p> <ul> <li>Action: Collect data from more subjects to reduce the standard error.</li> <li>Example: Survey a larger number of participants in a market research study to obtain more reliable results.</li> </ul> </li> </ol>"},{"location":"statistics/hypothesis/Ttest/#one-sample-t-test","title":"One-Sample T-Test","text":"<p>The one-sample t-test is the simplest form of the t-test family and serves as an excellent introduction to understanding t-tests in general. It is used when you have a single sample and want to determine whether its mean is significantly different from a known or hypothesized population mean. So in this case, we do not have two different samples or groups, but one sample from a population. </p> Definition: One-Sample T-Test <p>The formula for calculating the t-value in a one-sample t-test is:</p> \\[ t = \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\] <p>Where:</p> <ul> <li>\\(\\bar{x}\\) = Sample mean</li> <li>\\(\\mu\\) = Hypothesized population mean (the value you're testing against)</li> <li>\\(s\\) = Sample standard deviation</li> <li>\\(n\\) = Sample size</li> </ul> <p>The degrees of freedom (df) for this test are calculated as \\(df = n - 1\\).</p>"},{"location":"statistics/hypothesis/Ttest/#applying-the-test","title":"Applying the Test","text":"<ol> <li>Set the Hypothesis</li> <li>Collect Data: Measure the stress levels of the sample employees.</li> <li>Calculate the Sample Mean (\\(\\bar{x}\\)): Find the average stress level from your data.</li> <li>Compute the Sample Standard Deviation (s).</li> <li>Calculate the t-value using the formula above.</li> <li>Determine Degrees of Freedom: \\(df = n - 1\\).</li> <li>Find the p-value: Use the t-distribution table or statistical software.</li> <li>Make a Decision: If the p-value is less than your significance level (e.g., 0.05), reject the null hypothesis.</li> </ol> Robustness <p>The t-test is relatively robust to violations of normality with larger sample sizes (n &gt; 30).</p> Example: Thickness Testing <p>A factory produces metal sheets that are supposed to have an average thickness of \\(2.5 mm\\). The quality control team wants to ensure that the production process is meeting this specification. They randomly sample 30 sheets from the production line and measure their thickness.</p> <p>They want to determine if the average thickness of the sampled sheets is statistically different from the target mean of \\(2.5 mm\\).</p> <p>Assumptions: </p> <ul> <li>Significance level \\( \\alpha = 0.05 \\)</li> <li>Two-Tailed Tests: There can be positive or negative deviations </li> </ul> <ul> <li>Set the hypotheses:<ul> <li>Null hypothesis (H<sub>0</sub>): The mean thickness of the sheets is \\(2.5 mm\\) (\u03bc = \\(2.5 mm\\)).</li> <li>Alternative hypothesis (H<sub>1</sub>): The mean thickness of the sheets is not \\(2.5 mm\\) (\u03bc \u2260 \\(2.5 mm\\)).</li> </ul> </li> </ul> <ul> <li>Collect sample data <pre><code># Generate Data\nimport numpy as np\n\n# Simulate a dataset for the example\nnp.random.seed(46)  # for reproducibility\n\n# Given parameters\nsample_size = 30\nsample_mean = 2.45  # as found in the test example\nstd_dev = 0.1  # standard deviation\n\n# Generate random sample data\ndata = np.random.normal(loc=sample_mean, scale=std_dev, size=sample_size)\n</code></pre></li> </ul> Manual Calculation <ul> <li> <p>Calculate the \\(\\bar{x}\\) and \\(s\\):      <pre><code>print(\"Mean:\", np.mean(data))\nprint(\"Standard Deviation:\", np.std(data, ddof=1))\n</code></pre></p> &gt;&gt;&gt; Output<pre><code>Mean: 2.448088453428328\nStandard Deviation: 0.08337699472325459\n</code></pre> <p>The sample of 30 sheets has an average thickness of \\(2.45 mm\\) and a standard deviation of \\(0.08 mm\\).</p> </li> </ul> <ul> <li> <p>Calculate the t-value:</p> \\[ t = \\frac{2.45 - 2.5}{\\frac{0.08}{\\sqrt{30}}} \u2248 -3.41 \\] <pre><code>true_mean = 2.5\nt_statistic = (np.mean(data) - true_mean) / (np.std(data, ddof=1) / np.sqrt(sample_size))\nprint(\"t-statistic:\", t_statistic)\n</code></pre> &gt;&gt;&gt; Output<pre><code>t-statistic: -3.4101882835499286\n</code></pre> </li> </ul> <ul> <li> <p>Determine Degrees of Freedom:</p> <pre><code>dof = sample_size - 1\nprint(\"Degrees of Freedom:\", dof)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Degrees of Freedom: 29\n</code></pre> </li> </ul> <ul> <li> <p>Find the p-value:</p> <pre><code>p_val = stats.t.cdf(t_statistic, df=dof)*2\nprint (\"p-value:\", p_val)\n</code></pre> &gt;&gt;&gt; Output<pre><code>p-value: 0.0019285965194208732\n</code></pre> </li> </ul> Automatic Calculation <p>For calculating the p-value, the t-statistics and the degree of freedom we can use the <code>ttest_1samp</code> method of the <code>scipy.stats</code> library:</p> <pre><code>res = stats.ttest_1samp(data, popmean=true_mean, alternative='two-sided')\nprint(\"t-statistic:\", res.statistic)\nprint(\"p-value:\", res.pvalue)\nprint(\"Degrees of Freedom:\", res.df)\n</code></pre> &gt;&gt;&gt; Output<pre><code>t-statistic: -3.4101882835499286\np-value: 0.0019285965194208732\nDegrees of Freedom: 29\n</code></pre> <ul> <li>Make a Decision     The calculated p-value of the sample data is lower than the signifance level \\( \\alpha = 0.05 \\). Therefore, we reject the null hypothesis.So, there is significant evidence at the 5% level to conclude that the average thickness of the metal sheets is not 2.5 mm. The production process may need to be adjusted to ensure the thickness specification is met.</li> </ul>"},{"location":"statistics/hypothesis/Ttest/#assumptions-of-the-one-sample-t-test","title":"Assumptions of the One-Sample t-test","text":"<p>For the test results to be valid, the following assumptions should be met:</p> <ol> <li>Independence: Observations are independent of one another.</li> <li>Normality: The data should be approximately normally distributed, especially important for small sample sizes.</li> <li>Scale of Measurement: The data are continuous and measured on an interval or ratio scale.</li> </ol> Task: Weight of Euro Coins <p>  Download the following dataset from this page and load it into your notebook.</p> <pre><code># Website: https://jse.amstat.org/v14n2/datasets.aerts.html\n# Dataset: https://jse.amstat.org/datasets/euroweight.dat.txt\n# Description: https://jse.amstat.org/datasets/euroweight.txt\n\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\ndata = pd.read_csv('Daten/euroweight.dat.txt', sep='\\t', header=None, index_col=0, names=['Weight', 'Batch'])\n\n# Display the first few rows\ndata.head()\n</code></pre> <p>It contains information about the weight of a sample of specially issued euro coins. Answer the following questions using Python.  Assume a significance level \\(\\alpha = 5 \\%\\) and a two-sided test:</p> <ol> <li>Determine the average weight and the standard deviation of the sample.</li> <li>Formulate the hypothesis (Null and Alternative)</li> <li>Test the hypothesis that the population mean weight is \\(7.5\\) g.</li> <li>Test the hypothesis that the population mean weight is \\(7.51\\) g and \\(7.52\\) g, respectively. </li> <li>Interpret the results.</li> </ol>"},{"location":"statistics/hypothesis/Ttest/#two-sample-t-test","title":"Two-Sample T-Test","text":"<p>The two-sample t-test, also known as the independent samples t-test, is used to determine whether there is a statistically significant difference between the means of two independent groups. Unlike the one-sample t-test, which compares a sample mean to a known population mean, the two-sample t-test compares the means from two separate groups to see if they come from the same population.</p> Equal vs. Unequal Variances <p>There are two versions of the two-sample t-test:</p> <ul> <li>Student's t-test: Assumes equal variances between the two groups.</li> <li>Welch's t-test: Does not assume equal variances and is more robust when the variances are unequal.</li> </ul> <p>In practice, Welch's t-test is often preferred due to its robustness.</p> Definition: Two-Sample T-Test - Welch's T-Test <p>The formula for calculating the t-value in a two-sample t-test is:</p> \\[ t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\] <p>Where:</p> <ul> <li>\\(\\bar{x}_1\\), \\(\\bar{x}_2\\) = Sample means of group 1 and group 2</li> <li>\\(s_1^2\\), \\(s_2^2\\) = Sample variances of group 1 and group 2</li> <li>\\(n_1\\), \\(n_2\\) = Sample sizes of group 1 and group 2</li> </ul> <p>The degrees of freedom (DoF) for the test can be approximated using the Welch-Satterthwaite equation:</p> \\[ DoF = \\frac{\\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{\\left( \\frac{s_1^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_2^2}{n_2} \\right)^2}{n_2 - 1}} \\]"},{"location":"statistics/hypothesis/Ttest/#applying-the-test_1","title":"Applying the Test","text":"<ol> <li>Set the Hypotheses:<ul> <li>Null hypothesis (H<sub>0</sub>): The means of the two groups are equal (\\(\\mu_1 = \\mu_2\\)).</li> <li>Alternative hypothesis (H<sub>1</sub>): The means of the two groups are not equal (\\(\\mu_1 \\ne \\mu_2\\)).</li> </ul> </li> <li>Collect Data: Obtain independent random samples from both groups.</li> <li>Calculate the Sample Means (\\(\\bar{x}_1\\) and \\(\\bar{x}_2\\)).</li> <li>Compute the Sample Variances (\\(s_1^2\\) and \\(s_2^2\\)).</li> <li>Calculate the t-value using the formula above.</li> <li>Determine Degrees of Freedom using the Welch-Satterthwaite equation.</li> <li>Find the p-value: Use the t-distribution table or statistical software.</li> <li>Make a Decision: If the p-value is less than your significance level (e.g., 0.05), reject the null hypothesis.</li> </ol> Example: Comparing Teaching Methods <p>A researcher wants to determine if two different teaching methods lead to different student performance levels. They randomly assign students to two groups: one uses Method A, and the other uses Method B. After a semester, both groups take the same standardized test.</p> <p>Assumptions:</p> <ul> <li>Significance level \\( \\alpha = 0.05 \\)</li> <li>Two-tailed test: Testing for any difference in means</li> </ul> <ul> <li>Set the hypotheses:<ul> <li>Null hypothesis (H<sub>0</sub>): \\(\\mu_1 = \\mu_2\\) (no difference in mean scores)</li> <li>Alternative hypothesis (H<sub>1</sub>): \\(\\mu_1 \\ne \\mu_2\\) (difference in mean scores)</li> </ul> </li> </ul> <ul> <li> <p>Collect sample data:</p> <pre><code># Generate Data\nimport numpy as np\n\nnp.random.seed(42)  # for reproducibility\n\n# Group sizes\nn1 = 30  # Method A\nn2 = 35  # Method B\n\n# Simulate test scores\nmean1, std1 = 75, 10  # Method A\nmean2, std2 = 80, 12  # Method B\n\nscores1 = np.random.normal(mean1, std1, n1)\nscores2 = np.random.normal(mean2, std2, n2)\n</code></pre> </li> </ul> Manual Calculation <ul> <li> <p>Calculate sample means and variances:</p> <pre><code>mean1 = np.mean(scores1)\nmean2 = np.mean(scores2)\nvar1 = np.var(scores1, ddof=1)\nvar2 = np.var(scores2, ddof=1)\n\nprint(f\"Mean of Method A: {mean1:.2f}\")\nprint(f\"Mean of Method B: {mean2:.2f}\")\nprint(f\"Variance of Method A: {var1:.2f}\")\nprint(f\"Variance of Method B: {var2:.2f}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Mean of Method A: 73.12\nMean of Method B: 78.01\nVariance of Method A: 81.00\nVariance of Method B: 119.45\n</code></pre> </li> <li> <p>Calculate the t-value:</p> \\[ t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} = \\frac{73.12- 78.01}{\\sqrt{\\frac{81}{30} + \\frac{119.45}{35}}} \\approx -1.98 \\] <pre><code>numerator = mean1 - mean2\ndenominator = np.sqrt((var1 / n1) + (var2 / n2))\nt_statistic = numerator / denominator\nprint(f\"t-statistic: {t_statistic:.2f}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>t-statistic: -1.98\n</code></pre> </li> <li> <p>Determine degrees of freedom (df) using Welch's formula:</p> \\[ df = \\frac{\\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{\\left( \\frac{s_1^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_2^2}{n_2} \\right)^2}{n_2 - 1}} \\approx 62.91 \\] <pre><code>s1_squared_over_n1 = var1 / n1\ns2_squared_over_n2 = var2 / n2\n\nnumerator_df = (s1_squared_over_n1 + s2_squared_over_n2) ** 2\ndenominator_df = ((s1_squared_over_n1 ** 2) / (n1 - 1)) + ((s2_squared_over_n2 ** 2) / (n2 - 1))\ndf = numerator_df / denominator_df\nprint(f\"Degrees of Freedom: {df:.2f}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Degrees of Freedom: 62.91\n</code></pre> </li> <li> <p>Find the p-value:</p> <pre><code>from scipy import stats\n\np_value = 2 * stats.t.cdf(-abs(t_statistic), df)\nprint(f\"p-value: {p_value:.4f}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>p-value: 0.0520\n</code></pre> </li> </ul> Automatic Calculation <p>Alternatively, use <code>ttest_ind</code> from <code>scipy.stats</code> with <code>equal_var=False</code> for Welch's t-test (you can use <code>equal_var=True</code> for Student's t-test):</p> <pre><code>from scipy import stats\n\nt_statistic, p_value = stats.ttest_ind(scores1, scores2, equal_var=False)\nprint(f\"t-statistic: {t_statistic:.2f}\")\nprint(f\"p-value: {p_value:.4f}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>t-statistic: -1.98\np-value: 0.0520\n</code></pre> <ul> <li> <p>Make a Decision:</p> <p>The p-value (0.0520) is higher than the significance level (\\(\\alpha = 0.05\\)), so we cannot reject the null hypothesis. There is no significant evidence to suggest a difference in mean test scores between the two teaching methods.</p> </li> </ul>"},{"location":"statistics/hypothesis/Ttest/#assumptions-of-the-two-sample-t-test","title":"Assumptions of the Two-Sample T-Test","text":"<p>For the results of the two-sample t-test to be valid, the following assumptions must be met:</p> <ol> <li>Independence: Observations are independent both within and between groups.</li> <li>Normality: The data in each group are approximately normally distributed.</li> <li>Homogeneity of Variances:<ul> <li>Student's t-test: Assumes equal variances between groups.</li> <li>Welch's t-test: Does not assume equal variances.</li> </ul> </li> <li>Scale of Measurement: The dependent variable is measured on a continuous scale (interval or ratio).</li> </ol> Testing for Equal Variances <p>Before deciding between Student's t-test and Welch's t-test, you can perform an F-test or Levene's Test (robust against non-normal distribution) to assess the equality of variances. </p> <pre><code>from scipy import stats\n\nstat, p_value = stats.levene(scores1, scores2)\nprint(f\"Levene's Test Statistic: {stat:.2f}\")\nprint(f\"p-value: {p_value:.4f}\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Levene's Test Statistic: 2.00\np-value: 0.1623\n</code></pre> <p>Since, the p-value is 0.1623, which is greater than the significance level of 0.05, we do not have sufficient evidence to conclude that the variances are different, and therefore we cannot reject the null hypothesis (H<sub>0</sub>: equal variance) </p> <p>So it's appropriate to use Student's t-test assuming equal variances and not the Welch's t-test. which is above the  of 0.05. This suggests that the variances are equal.</p> Task: Firefighter Test Results <p> (Source: Wikipedia | Copyright: Steven Terblanche)  </p> <p>To become a Captain or Lieutenant in the New Haven Fire Department, both a written and oral test must be passed.  Download the following dataset and load it into your notebook. Therefore the python package <code>liac-arff</code> needs to be installed first. </p> <pre><code>#Website: https://www.openml.org/search?type=data&amp;sort=runs&amp;id=42665&amp;status=active\n#Download: https://www.openml.org/data/download/22044446/ricci_processed.arff\n\nimport arff #Installation: pip install liac-arff\nimport pandas as pd\n\n# Load the .arff dataset\nwith open('ricci_processed.arff', 'r') as file:\n    data = arff.load(file)\n\n# Convert data into dataframe\ndf = pd.DataFrame(data['data'], columns=[attr[0] for attr in data['attributes']])\n\n# Show the first 5 rows\ndf.head()\n</code></pre> <p>It contains the results of 118 exams. Answer the following questions using Python:</p> <ol> <li>Are the exam results of Captains significantly different (\\(\\alpha = 5\\%\\)) from those of Lieutenants?</li> <li>Are the exam results of candidates belonging to a minority (<code>Race == H</code> or <code>B</code>) significantly different (\\(\\alpha = 5\\%\\)) from those of candidates who do not belong to a minority?</li> </ol> <p>For both questions, proceed as follows:</p> <ul> <li>The analysis should focus on the total exam scores (attribute <code>Combine</code>).</li> <li>Perform an F-test to ensure that both samples have equal variance.</li> <li>Conduct a two-sided, two-sample t-test (Student or Welch; depending on the F-test results).</li> <li>Interpret the results.</li> </ul>"},{"location":"statistics/probability/CentralLimitTheorem/","title":"Central Limit Theorem","text":"<p>In this section, we'll explore the Central Limit Theorem (CLT), a cornerstone of probability and statistics that works hand in hand with the Law of Large Numbers (LLN) you've previously learned about. The CLT is fascinating because it explains why normal (Gaussian) distributions are so prevalent in statistics, even when the underlying data doesn't seem to fit that mold. </p> (Source: Pinterest)"},{"location":"statistics/probability/CentralLimitTheorem/#definition-interpretation","title":"Definition &amp; Interpretation","text":"Interpretation: Central Limit Theorem <p>The distribution of sample means approaches a normal distribution as the number of samples becomes large, regardless of the shape of the population distribution.</p> <p>But what does this mean?</p> <ul> <li> <p>Population Distribution: This could be any distribution\u2014uniform, skewed, bimodal, or even a highly irregular distribution.</p> <p></p> Code <pre><code>import plotly.express as px\n\nfig = px.histogram(data, x=\"age\")\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.8)'))\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Absolute Frequency',\n        xaxis_title_text='Age',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Age Distribution&lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> </li> <li> <p>Sample Means: If you take multiple samples from this population and calculate their means, those means will form their own distribution.     <pre><code>sample_mean = np.mean(random.sample(list(data.age), 20))\n</code></pre></p> <p></p> Code <pre><code>sample_means = []\nfor i in range(100):\n    sample_means.append(np.mean(random.sample(list(data.age), 20)))\n\nfig = px.histogram(x=sample_means)\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.8)'))\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Absolute Frequency',\n        xaxis_title_text='Sample Mean',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample Mean Distribution&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Sample Size: 20 | Number of Samples: 100&lt;/span&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n\n        ),\n    )\nfig.show()\n</code></pre> </li> <li> <p>Result: As the number of samples increases, the distribution of these sample means will tend toward a normal distribution.     </p> Code <pre><code>sample_means = []\nfor i in range(100000):\n    sample_means.append(np.mean(random.sample(list(data.age), 20)))\n\nfig = px.histogram(x=sample_means)\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.8)'))\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Absolute Frequency',\n        xaxis_title_text='Sample Mean',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample Mean Distribution&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Sample Size: 20 | Number of Samples: 100000&lt;/span&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n\n        ),\n    )\nfig.show()\n</code></pre> </li> </ul> <p>This demonstrates that even if the original data is not normally distributed, the distribution of the sample means will be approximately normal if the sample size is large enough.</p>"},{"location":"statistics/probability/CentralLimitTheorem/#relation-to-the-law-of-large-numbers","title":"Relation to the Law of Large Numbers","text":"<p>The Law of Large Numbers states that as the size of a sample increases, the sample mean will get closer to the population mean. When combined with the CLT, we understand not only that the sample mean converges to the population mean but also that the distribution of those sample means becomes normal.</p>"},{"location":"statistics/probability/CentralLimitTheorem/#why-is-the-central-limit-theorem-important","title":"Why Is the Central Limit Theorem Important?","text":"<p>Simplifies Statistical Analysis</p> <ul> <li>Normality Assumption: Many statistical tests assume normality. Thanks to the CLT, we can often safely make this assumption for sample means or sums.</li> <li> <p>Parameterization: Normal distributions are fully described by their mean and variance, making them easy to work with.</p> <p> (Source: Wikipedia)  </p> </li> </ul> <p>Practical Applications</p> <ul> <li> <p>Confidence Intervals: The CLT allows us to construct confidence intervals around sample means.      (Source: Ainali on Wikipedia)  </p> </li> <li> <p>Hypothesis Testing: Facilitates hypothesis testing by enabling the use of z-scores and t-scores.</p> </li> <li>Quality Control: In manufacturing, the CLT helps in monitoring process variations.</li> </ul>"},{"location":"statistics/probability/CentralLimitTheorem/#recap","title":"Recap","text":"<p>The Central Limit Theorem is a powerful reminder that, regardless of the original data distribution, the process of sampling and aggregation tends to produce a normal distribution. This universality is why we often say, \"All roads lead to Gauss.\" Understanding the CLT not only deepens your grasp of statistical principles but also equips you with the tools to make accurate inferences from data in a wide array of fields.</p>"},{"location":"statistics/probability/CentralLimitTheorem/#tasks","title":"Tasks","text":"Task: Central Limit Theorem <p>We use the biased die example from before: </p> <pre><code>die_biased = [1, 2, 4, 5, 6, 6]\n</code></pre> <p>Work on the following tasks: </p> <ol> <li>Calculate the expected value of the biased die (mean).</li> <li>Visualize the probability of each side in a histrogram.</li> <li>Now we perform some experiments:<ul> <li>Choose one random side (<code>sample_size = 1</code>) of the biased die. Repeat this <code>100.000</code> times and visualize the frequency in a histogram. </li> <li>Compare the result with the visualization of the probability. </li> <li>Now increase the <code>sample_size</code> and inspect the change in histogram. </li> <li>Does the calculated expected value match with the mean value of the normal distribution? </li> </ul> </li> </ol>"},{"location":"statistics/probability/General/","title":"Probability","text":"<p>In this section we trasit from the exploration of descriptive statistics to the domain of inferential statistics. Inferential statistics plays an important role in interpreting data, making predictions, and drawing conclusions about broader populations based on sample data. At the heart of inferential statistics lies the concept of probability - an essential tool for calculating, interpreting, and applying likelihoods to real-world situations. This section delves into the fundamental principles of probability, providing a foundation for understanding the processes behind statistical inference and its practical applications.</p>"},{"location":"statistics/probability/General/#what-is-probability","title":"What is Probability?","text":"<p>We'll begin by defining probability and exploring some common misconceptions. For now, let's start with a basic definition.</p> <p>Probability is the level of possibility of something happening or being true</p> <p>-- Cambridge Dictionary</p> <p>The probability values range from 0 to 1, where </p> <ul> <li>0 indicates impossibility and </li> <li>1 represents certainty. </li> </ul> <p>A key point to note is that the sum of all probabilities for a set of outcomes must equal 1. In other words, the total likelihood of all possible outcomes must add up to certainty.</p> Example: Rolling a Die <p>Imagine you are rolling a standard six-sided die. The die has six possible outcomes: </p> \\[ 1, 2, 3, 4, 5, 6 \\] <p>Each of these outcomes has an equal probability of occurring. The probability of rolling any specific number, say a 1, is 1 out of 6, or </p> \\[ P(X = 1) = 1/6. \\] <p>Now, if we sum the probabilities of all possible outcomes (rolling a 1, 2, 3, 4, 5, or 6), we get:</p> \\[ P(X = 1) + P(X = 2) + \\text{...} + P(X = 6) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = 1 \\] <p>This shows that the total probability of all possible outcomes equals 1, meaning that one of the six outcomes is certain to occur when you roll the die.</p> <p>Let's make this concept more tangible by looking at an example from a common smartphone weather app. Imagine the app shows </p> <p>\"On Tuesday there is a 30% chance of rain.\" This statement can be interpreted in different ways: Does it mean that it will rain for 30 percent of the day? Or that it will rain over 30 percent of the area at a given location? Or is there a 3 in 10 chance it will rain? </p> <p>The correct interpretation, however, is that there is a 3 in 10 chance of rain at any given point during the day. This aligns with how probability works - it's not about how long it will rain or the confidence level in the prediction, but the likelihood of rain occurring at all. We'll explore more examples like this to clarify these ideas as we move forward.</p> <p>Why is probability so important? Probability plays a central role in fields like weather forecasting, actuarial science, and medical research. In medicine, for instance, probability is used to evaluate the effectiveness of treatments and to model the spread of diseases. It's also essential in physics, chemistry, biology, and even machine learning and artificial intelligence, where it helps make predictions based on data.</p> (Source: sandserifcomics)  <p>We rely on probability whenever uncertainty (e.g. randomness) exists about the outcome of an event. For example, determining whether men are generally taller than women requires probability because it isn't universally true that all men are taller than all women. Similarly, probability helps assess the accuracy of medical tests, as even highly reliable tests can produce false positives. In contrast, for questions with known answers, such as the speed of light versus the speed of sound, probability isn't needed.</p>"},{"location":"statistics/probability/General/#random-variables","title":"Random Variables","text":"<p>A random variable differs slightly from traditional mathematical variables. In mathematics, variables can:</p> <ol> <li>Represent values that may vary.</li> <li>Act as unknowns to be solved.</li> </ol> <p>For random variables, the first property holds, but the second does not. Random variables allow us to perform calculations with the outcomes of a random experiment, making it possible to model and work with uncertainty. They are typically represented by capital letters, and their specific values are denoted by lowercase letters.</p> Definition: Random Variable <p>A random variable is a function that assigns numbers to the outcomes of a random process (mapping).</p> <p>For example, consider the result of rolling a die:</p> \\[ X = \\text{The number shown when a fair die is rolled} \\] <pre><code>import random\nrandom.randint(1, 6)\n</code></pre> <p>Or the result of flipping a coin:</p> \\[ X =  \\begin{cases} 1, &amp; \\text{if heads} \\\\ 0, &amp; \\text{if tails} \\end{cases} \\] <pre><code>random.choices([\"Heads\", \"Tails\"])\n</code></pre> <p>In both cases, the random variable maps the outcome of a random process to a numerical value. In the context of a coin flip:</p> <ul> <li>The random process is the coin flip itself.</li> <li>Each experiment refers to an individual flip.</li> <li>An event is the outcome, such as getting heads or tails.</li> <li>The sample space is the set of all possible outcomes (heads or tails).</li> <li>The random variable assigns a numerical value (e.g., 1 for heads, 0 for tails) to each event.</li> </ul> <p>Thus, random variables enable us to quantify the outcomes of random processes, allowing for further analysis and interpretation.</p> Task: Rolling the Dice <p>Now it's your turn to create a random variable. There are several packages in Python that we can use for this purpose. Use the <code>random</code> package we already used in the package management section. </p> <p> (Source: imgflip)  </p> <ol> <li>Now generate your own random number. Use the commands <code>randint</code>, <code>random</code> and <code>choices</code>. A good documentation can be found here</li> <li>Are those numbers really random? Do some research about the <code>random.seed</code> command</li> <li>Now create the following experiments: <ul> <li>Fair Die: Perform a virtual 'rolling of the die' for a fair (normal) die by using the <code>choices</code> command     <pre><code>die_fair = [1, 2, 3, 4, 5, 6]\n</code></pre></li> <li>Biased Die: Now use a biased die with no 3 but two times the side 6     <pre><code>die_biased = [1, 2, 4, 5, 6, 6]\n</code></pre></li> </ul> </li> </ol>"},{"location":"statistics/probability/General/#probability-vs-proportion","title":"Probability vs. Proportion","text":"<p>Two concepts that students frequently mix up in statistics are probability and proportion. Here\u2019s the key distinction:</p> <ul> <li>Probability refers to the likelihood of an event occurring and is based on theoretical outcomes.</li> <li>Proportion reflects how often an event has actually occurred, relying on observed data.</li> </ul> <p>In simpler terms, probability is typically used to discuss the likelihood of future events, while proportion is used to describe the frequency of events that have already happened. The following examples highlight the differences between these two concepts in various situations.</p> Example: Flip a Coin <p>When flipping a fair coin, the probability of it landing on heads is 0.5, or 50%, which is based on theory. However, if we flip the coin 20 times, we can calculate the proportion of times it actually lands on heads. For instance, it might land on tails 40% of the time in those 20 flips. In this case, probability is a theoretical expectation, while proportion is based on real, observable outcomes that we can count.</p> <p></p> Code <pre><code>import random\nimport plotly.express as px\n\nmylist = [\"Heads\", \"Tails\"]\nrandom.seed(23) # Set seed for reproducibility\nflips = random.choices(mylist, k=20)\n\n# Create a histogram\nfig = px.histogram(x=flips, nbins=2)\n\n# Change bar mode\nfig.update_traces(marker=dict(color='#00416E', line=dict(color='#00416E', width=0.5)))\n\n# Set overlay mode\nfig.update_layout(\n    xaxis_title_text='Result',\n    yaxis_title_text='Count',\n    title=dict(\n            text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Experiment: Flipping 20 Coins &lt;/span&gt;&lt;/b&gt;',\n        ),\n    bargap=0.1,\n    showlegend=False,)\n\nfig.show()\n</code></pre> Fun Fact: A Coin Toss is not 50/50 <p>The term \"coin toss\" is often used as a symbol of randomness, but mathematicians have long suspected that even a fair coin has a slight tendency to land more often on one side. To investigate this bias, Ph.D. candidate Franti\u0161ek Barto\u0161 gathered 47 volunteers who flipped coins over multiple weekends, eventually conducting 350,757 tosses. Their findings showed that coins landed with the same side facing upward as before the toss 50.8% of the time, confirming a small but significant bias in coin flips. (Arxiv, derStandard)</p> Task: Roll a Biased Die <p>Let's stick with the example from before and perform further experiments. We use a fair die and a biased die and perform the following task:  </p> <ol> <li>Roll each die <ul> <li>5 </li> <li>50  </li> <li>500 times. </li> </ul> </li> <li>Visualize the results in histogram (one for the fair die, one for the biased die). </li> </ol>"},{"location":"statistics/probability/General/#calculation-of-probability","title":"Calculation of Probability","text":""},{"location":"statistics/probability/General/#prerequisites","title":"Prerequisites","text":"<p>Before we start calculating probability, it's important to note that certain types (scales) of data (see section Attribute Types), such as nominal and ordinal are suitable for probability calculations, while interval and ratio scaled data are not directly valid for such computations. Interval and ratio data must first be converted into a discrete scale (like creating bins in a histogram) before probability can be applied, as their values have infinite precision, making them unsuitable for exact probability calculations.</p> Example: Length of Wooden Beams <p>An example involving the length of wooden beams can illustrate this point. Asking for the probability of a beam being exactly a certain length, down to a microscopic precision, is not a practical question. Instead, engineers or builders focus on the probability of a beam\u2019s length falling within a certain range, such as between 3.0 and 3.5 meters, or even within more precise intervals like 3.2 to 3.25 meters. This approach allows for meaningful analysis while accounting for slight variations in manufacturing or cutting processes.</p> <p>Another key requirement for valid probability calculations is that the data categories must be mutually exclusive. For instance, when flipping a coin, the events \"heads\" and \"tails\" are mutually exclusive since both cannot happen at the same time. Similarly, if a wooden beam is measured to be between 3.5 and 4 m in length, it cannot also be between 4 and 4.5 m.</p> (Source: MakeAMeme)  <p>However, some situations, like getting news from multiple sources, do not have mutually exclusive categories (a person can receive news from both TV and the internet). In such cases, probability cannot be computed unless the question is reformulated with exclusive categories.</p>"},{"location":"statistics/probability/General/#calculation","title":"Calculation","text":"<p>For discrete random variables, each outcome of an experiment can be assigned a probability. The probability of a specific outcome \\(X = x\\) is calculated using the formula:</p> Definition: Probability <p>Probability of a specific outcome</p> \\[ P(X = x) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of possible outcomes}} \\] <pre><code>p_head = 1/2 # favorable outcomes = head / total outcomes = head + tail\n</code></pre> <p>The probability distribution of a discrete random variable shows the likelihood of various outcomes occurring. While this distribution helps us understand the chances of different events, it doesn\u2019t allow us to predict the result of any single experiment. However, if the experiment is repeated many times, the overall pattern becomes clearer, following predictable rules.</p> Example: Flip a Coin <p></p> Code <pre><code>import random\nimport plotly.express as px\n\nmylist = [\"Heads\", \"Tails\"]\nrandom.seed(23) # Set seed for reproducibility\nflips = random.choices(mylist, k=20)\n\n# Create a histogram\nfig = px.histogram(x=flips, nbins=2, histnorm='probability')\n\n# Change bar mode\nfig.update_traces(marker=dict(color='#00416E', line=dict(color='#00416E', width=0.5)))\n\n# Set overlay mode\nfig.update_layout(\n    xaxis_title_text='Result',\n    yaxis_title_text='Probability/Proportion',\n    title=dict(\n            text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Experiment: Flipping 20 Coins &lt;/span&gt;&lt;/b&gt;',\n        ),\n    bargap=0.1,\n\nfor i in range(0, 2):\n    fig.add_shape(\n        type='line',\n        x0=i-0.46,\n        x1=i+0.46,\n        y0=0.5,\n        y1=0.5,\n        line=dict(color='#E87F2B', width=5),\n        xref='x',\n        yref='y',\n        name='Probability' if i == 0 else None,\n        showlegend=(i == 0) \n    )\nfig.show()\n</code></pre> Task: Probability of the Die <p>We will continue with our example of the fair and biased die. </p> <ol> <li>Calculate the probability for each side of the fair/biased die by using the <code>pandas</code> <code>value_counts</code> function</li> <li>Visualize the experiment from before in a histogram and overlay the calculated probabilty. Use the <code>plotly express</code> <code>add_shape</code> function.</li> </ol>"},{"location":"statistics/probability/General/#odds","title":"Odds","text":"<p>When dealing with probabilities, you'll often encounter the term odds. So, what does odds mean? It\u2019s frequently used in everyday language, especially in fields like medicine and gambling. While probability and odds are related, they are not the same thing. This explanation will clarify the meaning of odds and show how they differ from probability, as well as how to convert between them.</p> <p>Odds typically describe the ratio between two possibilities: the chance of something not happening compared to it happening. For example, when you hear \"the odds are five to one,\" this means the odds ratio is 5:1, or 5 divided by 1. In mathematical terms, odds represent the ratio of the probability of an event not happening to the probability of it happening. It can be written as:</p> Definition: Odds \\[ \\text{Odds ratio (r)} = \\frac{p}{1 - p} \\] <p>where \\(p\\) is the probability of the event happening, and \\(1 - p\\) is the probability of the event not happening. To convert odds into probability, you can solve for \\(p\\) using the equation:</p> \\[ p = \\frac{r}{1 + r} \\] <p>where \\(r\\) is the odds ratio. </p> Example: Flip a Coin <p>When flipping a fair coin, there are two possible outcomes: heads or tails. Each outcome has an equal chance of occurring. Understanding the concept of odds in this simple scenario can help clarify the difference between probability and odds.</p> <ul> <li>Probability of getting heads (P): \\( \\frac{1}{2} \\) or 0.5 (50%)</li> <li>Probability of getting tails: \\( 1 - P = \\frac{1}{2} \\) or 0.5 (50%)</li> </ul> <p>Calculating Odds:</p> \\[ \\text{Odds in favor of heads} = \\frac{P(\\text{heads})}{P(\\text{not heads})} = \\frac{0.5}{0.5} = \\frac{1}{1} = 1 \\] <p>This means the odds in favor of getting heads are 1 to 1, often written as 1:1. This indicates an equal chance of getting heads or tails.</p> Task: Odds of the Die <ol> <li>Calculate the odds for the fair die to roll 6</li> <li>Now calculate the same thing for the biased die</li> </ol>"},{"location":"statistics/probability/General/#mass-density-function","title":"Mass &amp; Density Function","text":"<p>In statistics, we often encounter the concepts of probability mass functions (PMF) and probability density functions (PDF). These functions help describe the probabilities of different types of events\u2014whether they are discrete or continuous.</p>"},{"location":"statistics/probability/General/#probability-mass-function-pmf","title":"Probability Mass Function (PMF)","text":"<p>A probability mass function is used to describe probabilities for discrete events. Discrete events are those that occur in distinct, countable states, such as flipping a coin, rolling a die, or drawing a card. For example, if we roll a die, each face (1, 2, 3, 4, 5, or 6) is a discrete event. A PMF assigns probabilities to each possible outcome. In this case, each side of a fair die has a probability of 1/6, and these probabilities are represented in a bar plot or histogram.</p> <pre><code>random.choices([1,2,3,4,5,6])\n</code></pre> <p>Let\u2019s say we have a biased die, and the probability of rolling a 6 is twice as hig than other numbers and there is no 3. </p> <pre><code>random.choices([1,2,3,4,5,6], weights=[1/6, 1/6, 0, 1/6, 1/6, 2/6])\n</code></pre> <p>In this case, the PMF would show different probabilities for each number, but the probabilities are still discrete values\u2014there\u2019s no such thing as rolling a 4.5 on a die.</p> Example: Rolling the Die <ul> <li> <p>Fair Die</p> <p></p> Code <pre><code>import numpy as np\nimport plotly.express as px\nimport pandas as pd\n\nx = [1,2,3,4,5,6]\n\ndf = pd.DataFrame(x, columns=['fair'])\n\nfig = px.histogram(df, x='fair', nbins=6, histnorm='probability density')\n\n# Adjust the plot\nfig.update_layout(\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Rolling a Fair Die&lt;/span&gt;&lt;/b&gt;',\n        ),\n    xaxis_title_text='Number',\n    yaxis_title_text='Probability',\n    bargap=0.1,\n)\n\n# Scale the axis\nfig.update_layout(yaxis_range=[0,1])\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Biased Die</p> <p></p> Code <pre><code>import numpy as np\nimport plotly.express as px\nimport pandas as pd\n\nx = [1,2,4,5,6,6]\n\ndf = pd.DataFrame(x, columns=['unfair'])\n\nfig = px.histogram(df, x='unfair', nbins=6, histnorm='probability density')\n\n# Adjust the plot\nfig.update_layout(\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Rolling a Biased Die&lt;/span&gt;&lt;/b&gt;',\n        ),\n    xaxis_title_text='Number',\n    yaxis_title_text='Probability',\n    bargap=0.1,\n)\n\n# Scale the axis\nfig.update_layout(yaxis_range=[0,1])\n\n# Show the plot\nfig.show()\n</code></pre> </li> </ul> <p>For the Biased Die</p> <p>The probability of rolling a 6 would be:</p> \\[ P(X = 6) = \\frac{2}{6} = 33.3\\% \\] <p>Similarly, the probability of rolling a number greater than 4 would be:</p> \\[ P(X &gt; 4) = \\frac{3}{6} = 50\\% \\] <p>The probability of rolling a number between 1 and 4 is:</p> \\[ P(1 &lt; X &lt; 4) = \\frac{1}{6} = 16.7\\% \\] <p>One key rule of PMFs is that the sum of all probabilities must equal 1. For example, in a deck of cards, the sum of the probabilities for drawing any card must equal 1, since you're certain to draw some card from the deck.</p>"},{"location":"statistics/probability/General/#probability-density-function-pdf","title":"Probability Density Function (PDF)","text":"<p>In contrast, a probability density function is used for continuous events. Continuous events don\u2019t have discrete outcomes; instead, they can take on any value within a range. For example, if we\u2019re measuring the height of a person, we can\u2019t pinpoint an exact value (down to the atom). Instead, we look at the probability of the height falling within a range, such as between 180 cm and 190 cm. Unlike PMFs, PDFs are represented by smooth curves, showing how the probability is distributed over a range of values.</p> <p>With continuous data, we can\u2019t assign a probability to a specific value (such as someone being exactly 165.432 cm tall). Instead, we compute the probability of a value falling within a range using the area under the curve of the PDF. For instance, we might ask, \"What\u2019s the probability that someone\u2019s height is between 180 cm and 190 cm?\" This probability is calculated by integrating the PDF over that range.</p> Example: Height of a Person <p></p> Code <pre><code>import numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom scipy.stats import norm\n\n# Initialization\nlimit_up = 190\nlimit_down = 180\n\n# Generate data for the normal distribution\nx = np.arange(130, 210, 0.1)\ny = norm.pdf(x, 170, 10)\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Create a column for the fill area\ndf['fill'] = np.where((df['x'] &gt;= limit_down) &amp; (df['x'] &lt;= limit_up), df['y'], 0)\n\n\n# Create the plot\nfig = px.line(df, x='x', y='y')\nfig.add_trace(px.line(df, x='x', y='fill').data[0])\n\n# Adjust the plot\nfig.data[0].update(line=dict(color='#00416E', width=2))\n\nfig.data[1].update(\n    fill='tozeroy', \n    fillcolor='rgba(0, 65, 110, 0.4)',\n    line=dict(width=0),)\n\nfig.update_layout(\n    xaxis_title_text='x',\n    yaxis_title_text='Density',\n    title=dict(\n            text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Probability Density Function &lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;\u00b5=170cm, std=10cm&lt;/span&gt; &lt;/b&gt;',\n        ),\n    showlegend=False,\n)\n\nfig.show()\n</code></pre> <p>Calculating the probability, that a person is between 180 and 190 cm:</p> \\[ P(180 \\le X \\le 190) = \\int_{180}^{190} f(X)dx \\] <p>or smaller than 150 cm:</p> \\[ P(X \\le 150) = \\int_{-\\infty}^{150} f(X)dx \\]"},{"location":"statistics/probability/General/#cumulative-distribution-function-cdf","title":"Cumulative Distribution Function (CDF)","text":"<p>A CDF is a function that provides the cumulative probability for a given random variable. In simpler terms, it gives the probability that a random variable will take a value less than or equal to a specific point. </p> <p><pre><code>from scipy.stats import norm\nnorm.cdf(0)\n</code></pre> &gt;&gt;&gt; Output<pre><code>0.5\n</code></pre></p> <p>To build a CDF from a PDF, you essentially sum all the probability values of the PDF up to a certain point on the x-axis. Mathematically, this is equivalent to calculating the integral of the PDF for continuous distributions. The CDF at a particular value \\( x \\) gives you the total probability of the random variable being less than or equal to \\( x \\).</p> <p>For example, imagine a probability density function that describes the heights of people in a population. The CDF at a specific height (say 150 cm) tells you the probability of randomly selecting someone who is 150 cm or shorter. As you move further along the x-axis, the CDF will continue to increase until it reaches 1, which represents 100% probability.</p> Example: Height of a Person <p></p> Code <pre><code>import numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom scipy.stats import norm\n\n# Initialization\nlimit_up = 190\nlimit_down = 180\n\n# Generate data for the normal distribution\nx = np.arange(130, 210, 0.1)\ny = norm.cdf(x, 170, 10)\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Create a column for the fill area\ndf['fill'] = np.where((df['x'] &gt;= limit_down) &amp; (df['x'] &lt;= limit_up), df['y'], 0)\n\n\n# Create the plot\nfig = px.line(df, x='x', y='y')\nfig.add_trace(px.line(df, x='x', y='fill').data[0])\n\n# Adjust the plot\nfig.data[0].update(line=dict(color='#00416E', width=2))\n\nfig.data[1].update(\n    fill='tozeroy', \n    fillcolor='rgba(0, 65, 110, 0.4)',\n    line=dict(width=0),)\n\nfig.update_layout(\n    xaxis_title_text='x',\n    yaxis_title_text='Probability',\n    title=dict(\n            text=f'&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Cumulated Distribution Function &lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;\u00b5=170cm, std=10cm&lt;/span&gt; &lt;/b&gt;',\n        ),\n    showlegend=False,\n)\n\nfig.show()\n</code></pre> <p>Calculating the probability, that a person is between 180 and 190 cm:</p> \\[ P(180 \\le X \\le 190) = \\int_{180}^{190} f(X)dx = 13.6\\% \\] <p>or smaller than 150 cm:</p> \\[ P(X \\le 150) = \\int_{-\\infty}^{150} f(X)dx = 2.3\\% \\] Code <pre><code>print('Between 180cm and 190cm:', (norm.cdf(190, 170, 10) - norm.cdf(180, 170, 10))*100, '%')\nprint('Smaller than 150cm:', (norm.cdf(150, 170, 10))*100, '%')\n</code></pre> <p>Key Properties of CDFs:</p> <ol> <li>CDFs start at 0: The lowest x-value in the distribution will have a cumulative probability of 0.</li> <li>CDFs increase monotonically: As you move along the x-axis, the CDF always increases or stays the same. It never decreases, since probabilities cannot decrease over time.</li> <li>CDFs approach 1: As the x-values increase and encompass all possible outcomes, the cumulative probability approaches 1, representing 100%.</li> </ol> <p>While PDFs describe the probability density, CDFs are the cumulative sum of those probabilities. One important distinction is that summing all the values of a PDF equals 1 (as it represents the total probability), but summing all the values of a CDF does not necessarily give 1. Instead, the CDF gradually approaches 1 as the x-values increase.</p> <p>CDFs provide a powerful way to compute cumulative probabilities, especially when working with continuous distributions. By understanding the relationship between PDFs and CDFs, we can answer practical questions like \"What is the probability of scoring above a certain value?\" or \"What is the probability of a variable falling within a specific range?\"</p> <p>In practical terms, CDFs allow you to compute probabilities up to a certain value on the x-axis, making them essential tools in statistics, probability theory, and real-world applications like exam scores or analyzing biological data.</p> Task: Density Function <p>Assume the heights of individuals in a certain population follow a normal distribution with a mean of 170 cm and a standard deviation of 10 cm (see examples above).</p> <p>Answer the following questions based on this normal distribution:</p> <ol> <li>What percentage of individuals are taller than 190 cm?</li> <li>What percentage of individuals are between 170 cm and 180 cm tall?</li> <li>Plot the Probability Density Function (PDF) for a normal distribution with a mean of 180 cm and a standard deviation of 5 cm.</li> </ol> <p>For the first two questions use the Cumulative Distribution Function (CDF) to calculate the corresponding probabilities.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/","title":"Law of Large Numbers","text":""},{"location":"statistics/probability/LawOfLargeNumbers/#understanding-the-law","title":"Understanding the Law","text":"<p>The Law of Large Numbers states that as you increase the number of times you repeat an experiment, the average of your sample means will more closely approximate the population mean. In other words, if you conduct the same experiment repeatedly and calculate the mean of each sample, then average those means together, this average will converge toward the true population mean as the number of experiments increases. </p> (Source: QuickMeme)  <p>This is crucial because we often don\u2019t know the true population mean, but by conducting multiple trials, we can estimate it more accurately over time.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/#mathematical-representation","title":"Mathematical Representation","text":"<p>Now, let's explore how this law is expressed mathematically.</p> Definition: Law of Large Numbers \\[ \\lim_{n \\to \\infty} P\\left( \\left| \\bar{x}_n - \\mu \\right| \\geq \\epsilon \\right) = 0 \\] <p>Where:</p> <ul> <li>\\( n \\) is the number of trials (or experiment repetitions) - not the sample size,</li> <li>\\( \\bar{x}_n \\) is the average of the sample means from \\( n \\) trials,</li> <li>\\( \\mu \\) is the true population mean,</li> <li>\\( \\epsilon \\) is an arbitrarily small positive number.</li> </ul> <p>The essence of the formula is that the probability of our average deviating from the true population mean by more than \u03b5 approaches zero as n becomes very large. In simpler terms, the more experiments we conduct, the closer our average gets to the true mean, and the less likely it is to be significantly off.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/#importance-of-the-lln","title":"Importance of the LLN","text":"<p>So, why is the Law of Large Numbers so important? Because it tells us that any single sample or experiment can be heavily influenced by randomness, variability, and noise. This means relying on a single experiment might not give us an accurate estimate of the true population mean. We shouldn't place too much trust in just one set of results.</p> (Source: Imgflip Meme Generator)  <p>However, by repeating the experiment multiple times we can obtain an average that closely approximates the true population mean. Even if we can't measure the population mean directly, averaging multiple samples allows us to estimate it with increasing accuracy.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/#how-large-is-large","title":"How Large is 'Large'?","text":"<p>An important question arises: How large does our number of repetitions (\\(n\\)) need to be to get a reasonable estimate? The law doesn't specify a particular number. Unfortunately, there's no one-size-fits-all answer because it depends on various factors like the nature of the data, experimental conditions, and measurement methods.</p> <p>While we can't name an exact number, we now understand that the Law of Large Numbers operates more qualitatively - it tells us that more repetitions lead to better approximations without specifying exactly how many are needed.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/#an-illustrative-example","title":"An Illustrative Example","text":"<p>Let's consider an example. Suppose we take a fair die, </p> <pre><code>die_fair = [1, 2, 3, 4, 5, 6]\n</code></pre> <p>roll it 50 times (sample size)</p> Code <pre><code>fig = px.scatter(y=fair, x=np.arange(50), labels={'y': 'Die Number', 'x': 'Roll'})\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.6)'))\nfig.update_layout(\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Results of 50 Die Rolls&lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: Fair Die&lt;/span&gt;&lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <p>and calculate the average of the results</p> <p><pre><code>fair_mean = np.mean(fair)\nprint('Mean of 50 rolls of a fair die:', fair_mean)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Mean of 50 rolls of a fair die: 3.64\n</code></pre></p> <p>Then, we repeat this entire experiment 500 times (\\(n\\)). Here's what happens:</p> <ul> <li>Individual Experiment Results: Each experiment's average varies. Sometimes it's close to the expected value of 3.5 (the average roll of a fair die), and other times it's further away. These averages fluctuate and don't necessarily converge to 3.5 on their own.</li> </ul> Code <pre><code>def mean_of_n_rolls(sample_size, number_of_samples):\n    return [np.mean(random.choices([1, 2, 3, 4, 5, 6], k=sample_size)) for _ in range(number_of_samples)]\n\nsample_size = 50\nnumber_of_samples = 500\n\nmeans_result = mean_of_n_rolls(sample_size, number_of_samples)\n\nfig = px.line(x=np.arange(number_of_samples), y=means_result,markers=True)\nfig.update_traces(line=dict(color='rgba(0, 65, 110, 0.6)'))\n\n# Add a vertical line for the population mean\nfig.add_hline(y=3.5, line_dash=\"dash\", annotation_text=\"Expected Value: 3.5\", annotation_position=\"top right\", line_color=\"#E87F2B\")\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Sample Mean',\n        xaxis_title_text='# Experiment',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample Means (Sample Size: 50)&lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: Fair Die&lt;/span&gt;&lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <ul> <li>Cumulative Average: Instead of looking at individual averages, we calculate the cumulative average up to each point. The first point is just the average of the first experiment. The second point is the average of the first two experiments, and so on, up to the 500th point, which is the average of all 500 experiments.</li> </ul> Code <pre><code># Cumulative Average\ncumulative_average = np.cumsum(means_result) / np.arange(1, number_of_samples + 1)\n\n# Plot the cumulative average\nfig = px.line(x=np.arange(number_of_samples), y=cumulative_average,markers=True)\nfig.update_traces(line=dict(color='rgba(0, 65, 110, 0.6)'))\n\n# Add a horizontal line for the population mean\nfig.add_hline(y=3.5, line_dash=\"dash\", annotation_text=\"Expected Value: 3.5\", annotation_position=\"top right\", line_color=\"#E87F2B\")\n\nfig.update_layout(\n        barmode='overlay',\n        yaxis_title_text='Cumulative Average',\n        xaxis_title_text='# Experiment',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Cumulative Average (Sample Size: 50)&lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: Fair Die&lt;/span&gt;&lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <p>What's interesting is how quickly the cumulative average converges to the expected value of 3.5. Even after just a few repetitions, the cumulative average is already quite close to 3.5 - much closer than most individual experiment averages. By the time we reach 500 repetitions, the cumulative average is almost exactly 3.5.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/#recap","title":"Recap","text":"<p>In this session, we learned about the Law of Large Numbers. We've explored what it is, examined its mathematical formulation, and seen a demonstration of how it works. This fundamental principle is crucial for statistical analysis and scientific research, emphasizing the importance of repeated experimentation to obtain accurate estimates of population parameters.</p>"},{"location":"statistics/probability/LawOfLargeNumbers/#tasks","title":"Tasks","text":"Task: Law Of Large Numbers <p>Use the following dataset:  <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \nadult = fetch_ucirepo(id=2)\n# https://archive.ics.uci.edu/dataset/2/adult\n\n# data (as pandas dataframes) \ndata = adult.data.features \n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following tasks: </p> <ol> <li>Calculate the population mean of the variable <code>age</code></li> <li>Take one sample of 100 entries from the variable <code>age</code> and visualize them in a scatter plot</li> <li>Repeat this experiment 500 times and plot the means of each sample in a line chart. Add a horizontal line for the true population mean</li> <li>Visualize the cumulative average in a line chart. Does it approach the expected value? </li> </ol>"},{"location":"statistics/probability/Sampling/","title":"Sampling","text":""},{"location":"statistics/probability/Sampling/#sample-distribution","title":"Sample Distribution","text":"<p>In this explanation, we'll talk about creating sample distributions, an essential topic in inferential statistics. This is important for generating confidence intervals and making inferences about a population from a sample, which is a core goal of inferential statistics.</p> Code <p>For the upcoming example, the following data will be used:      <pre><code>from ucimlrepo import fetch_ucirepo \n\nadult = fetch_ucirepo(id=2) # fetch dataset \ndata = adult.data.features # data (as pandas dataframes) \n</code></pre></p>"},{"location":"statistics/probability/Sampling/#creating-a-data-distribution","title":"Creating a Data Distribution","text":"<p>Let\u2019s say we\u2019re interested in finding out the average age of people. While we know there is a wide range of ages, we want to get a more precise understanding. Statistically, this means determining the population parameter for the age of people. However, it\u2019s not feasible to find out the age of every person in the world. Why? There are simply too many people, living in different regions, and new individuals are born while others pass away. Additionally, we might accidentally include the same person more than once, making the task more complicated.</p> <p>Instead of measuring the entire population, we can take a random sample of people. For example, we could collect the ages of 100 individuals. </p> <pre><code>sample = random.sample(list(data.age), 100)\n</code></pre> <p>One person might be 25 years old, another might be 42 years old, and so on. After gathering the data, we can create a distribution of these ages, often visualized as a histogram, and calculate the sample mean (the average age of the people in our sample).</p> <pre><code>sample_mean = np.mean(sample)\n</code></pre> Code <pre><code>import random \nimport numpy as np\nimport plotly.express as px\nimport pandas as pd\n\n# Pick a random sample of 100 from the data and calculate the mean\nsample = random.sample(list(data.age), 100)\nsample_mean = np.mean(sample)\n\n# Count the frequency of each age in the sample\npd.DataFrame(sample)\ns_counts = pd.DataFrame(sample).value_counts().sort_index()\ns_counts = s_counts.to_frame().reset_index()\n\n# Plot the sample data\nfig = px.bar(x = s_counts[0], y=s_counts['count'])\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.8)'))\n\n# Add a vertical line for the population mean\nfig.add_vline(x=sample_mean, line_dash=\"dash\", annotation_text=\"Mean: \"+str(sample_mean), annotation_position=\"top right\", line_color=\"#E87F2B\")\n\n# Adjust the layout\nfig.update_layout(\n    barmode='overlay',\n    xaxis_title_text='Age',\n    yaxis_title_text='Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Age Distribution - Sample 4 (Mean: '+str(sample_mean)+')&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig.show()\n</code></pre> <p>Now, the sample mean we calculate from our 100 people is just one estimate of the population's average age.</p> Outlook <p>How does this sample mean realate to the actual population mean?  And how confident are we, that this sample mean is really close to the population mean?  Those are questions we will learn to answer in the upcomming sections about confidence intervals and hypothesis testing.</p> <p>However, it\u2019s important to understand that this is just one estimate. If we take another random sample of 100 people, we might get a slightly different average because not all individuals are the same age. So, what can we do? </p> Stewie Repetition GIFfrom Stewie Repetition GIFs <p>Right, we repeat the experiment! We measure another sample of 100 people and calculate a new sample mean. Note, that there can be an overlap between those two samples (one person was part of the first and the second sample). This is called sampling with replacement. </p> <p>We notice that the results are similar to the first sample, but not exactly the same. </p> <p>This difference is known as sampling variability - the natural variation that occurs when taking different samples from the same population. More about this topic will be covered in the  Sampling Variability section. </p> Task: Data Distribution <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \nadult = fetch_ucirepo(id=2)\n# https://archive.ics.uci.edu/dataset/2/adult\n\n# data (as pandas dataframes) \ndata = adult.data.features \n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following tasks: </p> <ol> <li>Pick a random sample of 100 values from the <code>hours-per-week</code> attribute</li> <li>Calculate the mean of this sample</li> <li>Plot the sample distribution as a <code>plotly.express</code> bar chart. Add vertical line (<code>fig.add_vline</code>) to indicate the sample mean</li> <li>Calculate the mean of the population (all the entries in the dataset) and compare it with the sample</li> </ol>"},{"location":"statistics/probability/Sampling/#creating-a-sample-distribution-of-means","title":"Creating a Sample Distribution of Means","text":"<p>We can repeat this sampling process multiple times. Let\u2019s say we take N samples. Each sample gives us a new sample mean, and these means will vary slightly due to the natural differences in people's ages. Now, instead of focusing on individual ages (data distribution), we can create a distribution of sample means - a new distribution that shows how the sample estimates themselves vary.</p> Code <pre><code>import random \nimport numpy as np\nimport plotly.express as px\nimport pandas as pd\n\n# Function to calculate the mean of the sample\ndef calculate_sample_mean(sample_size, n_sample):\n    sample_mean = []\n    for i in range(n_sample):\n        sample = random.sample(list(data.age), sample_size)\n        sample_mean.append(np.mean(sample))\n    return sample_mean\n\n# Initialize variables\nsample_size = 100\nnumber_of_samples = 100\n\n# Calculate the sample means\nx1 = calculate_sample_mean(sample_size, number_of_samples)\n\n# Bin the data\nx1_binned = pd.cut(x1, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\nx1_counts = x1_binned.value_counts().sort_index()\n\n# Trim the data to remove the 0.0 values at the start and end\nfirst_non_zero_index = x1_counts.ne(0.0).idxmax() \nlast_non_zero_index = x1_counts[::-1].ne(0.0).idxmax()\nx1_counts_trimmed = x1_counts.loc[first_non_zero_index:last_non_zero_index]\n\n# Convert the index to string\nx1_counts_trimmed.index = x1_counts_trimmed.index.astype(str)\n\n# Plot the data\nfig = px.bar(x = x1_counts_trimmed.index, y=x1_counts_trimmed)\nfig.update_traces(marker=dict(color='rgba(232, 127, 43, 0.8)'))\n\nfig.update_layout(\n    barmode='overlay',\n    xaxis_title_text='Age Category',\n    yaxis_title_text='Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Age Distribution of Sample Means&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Sample Size: 100 | Number of Samples: 100&lt;/span&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig.show()\n</code></pre> <p>This distribution of sample means gives us valuable insights about the population. For example, if most of the sample means are very close to each other, we can be more confident that our sample estimates are close to the true population mean. However, if the sample means are widely spread out, it indicates that we might need larger or more representative samples to get a more accurate estimate of the population's true average age.</p> Code <pre><code>import random \nimport numpy as np\nimport plotly.express as px\nimport pandas as pd\nimport plotly.graph_objects as go\n\n# Initialize variables\nsample_size = [10, 100, 1000]\nnumber_of_samples = [10, 100, 1000]\n\n# Function to calculate the mean of the sample\ndef calculate_sample_mean(sample_size, n_sample):\n    sample_mean = []\n    for i in range(n_sample):\n        sample = random.sample(list(data.age), sample_size)\n        sample_mean.append(np.mean(sample))\n    return sample_mean\n\nfor i in range(3):\n    # Calculate the sample means\n    x1 = calculate_sample_mean(sample_size[0], number_of_samples[i])\n    x2 = calculate_sample_mean(sample_size[1], number_of_samples[i])\n    x3 = calculate_sample_mean(sample_size[2], number_of_samples[i])\n\n    # Bin the data\n    x1_binned = pd.cut(x1, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\n    x2_binned = pd.cut(x2, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\n    x3_binned = pd.cut(x3, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\n\n    x1_counts = x1_binned.value_counts().sort_index()\n    x2_counts = x2_binned.value_counts().sort_index()\n    x3_counts = x3_binned.value_counts().sort_index()\n\n    x1_counts.index = x1_counts.index.astype(str)\n    x2_counts.index = x2_counts.index.astype(str)\n    x3_counts.index = x3_counts.index.astype(str)\n\n    # Plot the data\n    fig = px.bar(x = x1_counts.index, y=x1_counts)\n    fig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.4)'), name='Sample Size = '+str(sample_size[0]), showlegend=True)\n\n    fig.add_trace(go.Bar(x = x2_counts.index, y=x2_counts, name='Sample Size = '+str(sample_size[1]), marker=dict(color='rgba(255, 0, 0, 0.4)')))\n    fig.add_trace(go.Bar(x = x3_counts.index, y=x3_counts, name='Sample Size = '+str(sample_size[2]), marker=dict(color='rgba(0, 255, 0, 0.4)')))\n\n    fig.update_layout(\n        xaxis_range=[150,300],\n        barmode='overlay',\n        xaxis_title_text='Age Category',\n        yaxis_title_text='Frequency',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Age Distribution of Sample Means&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Number of Samples: ' + str(number_of_samples[i]) + '&lt;/span&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n        ),\n    )\n\n    fig.show()\n</code></pre> Different Types of Sample Distribution <p>While we have been using the mean as an example, the same approach can be applied to other statistics as well. For instance, you could look at the variance. If you're interested in knowing how much age varies across a population, you could calculate the variance or standard deviation for each sample.</p> <p>By repeating the process with multiple samples, you can then create a sample distribution of these variance estimates. This allows you to find the average variance across all the samples, giving you a better understanding of how much variability exists in the ages of the overall population.</p> Task: Sample Distribution <p>Use the the dataset from before and work on the following tasks:</p> <ol> <li>Calculate the sample distribution of the standard deviation for a <code>sample_size = 100</code> and <code>number_of_samples = 100</code> and visualize it in bar chart</li> <li>Calculate the standard deviation of the population and compare the results.</li> <li>Again, visualize the sample distribution of the standard deviation now for <code>sample_size = [10, 100, 1000]</code> and <code>number_of_samples = 100</code></li> </ol>"},{"location":"statistics/probability/Sampling/#sample-distribution-of-differences","title":"Sample Distribution of Differences","text":"<p>Let's shift the focus of our example and explore sample estimate differences instead of just looking at one parameter.  The question we want to explore is: are widowed individuals generally older than people who have never been married? While it seems obvious that widowed individuals are likely to be older, since people tend to marry before becoming widowed, let\u2019s frame this question statistically to better understand the process.</p> <p>We start by imagining two populations: one of widowed individuals and one of never-married individuals. </p> <pre><code>data_widowed = data[data['marital-status'] == 'Widowed'].age\ndata_never_married = data[data['marital-status'] == 'Never-married'].age\n</code></pre> <p>Since it\u2019s not practical to measure the age of every person in both groups, we take a random sample from each. For example, we could sample 100 widowed individuals and 100 people who have never been married, </p> <pre><code>sample_widowed = random.sample(list(data_widowed), 100)\nsample_never_married = random.sample(list(data_never_married), 100)\n</code></pre> <p>and then calculate the average age for each group.</p> <pre><code>sample_widowed_mean = np.mean(sample_widowed)\nsample_never_married_mean = np.mean(sample_never_married)\n</code></pre> <p>Let\u2019s say, in the first sample, the average age of the widowed group is 70 years, and the average age of the never-married group is 45 years. The difference between these averages is 25 years. However, this is just one sample, and the difference might vary slightly if we take another random sample. So, we repeat the process with a second random sample and find that the difference this time is 24 years.</p> Code <pre><code>number_of_samples = 100\nsample_size = 100\n\ndef calculate_sample_mean(sample_size, n_sample, data):\n    sample_mean = []\n    for i in range(n_sample):\n        sample = random.sample(list(data), sample_size)\n        sample_mean.append(np.mean(sample))\n    return sample_mean\n\nx1 = calculate_sample_mean(sample_size, number_of_samples, data_widowed)\nx2 = calculate_sample_mean(sample_size, number_of_samples, data_never_married)\n\nx1_binned = pd.cut(x1, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\nx2_binned = pd.cut(x2, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\n\nx1_counts = x1_binned.value_counts().sort_index()\nx2_counts = x2_binned.value_counts().sort_index()\n\nx1_counts.index = x1_counts.index.astype(str)\nx2_counts.index = x2_counts.index.astype(str)\n\nfig = px.bar(x = x1_counts.index, y=x1_counts)\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.4)'), name='Widowed', showlegend=True)\n\nfig.add_trace(go.Bar(x = x2_counts.index, y=x2_counts, name='Never-Married', marker=dict(color='rgba(255, 0, 0, 0.4)')))\n\nfig.update_layout(\n    xaxis_range=[80,460],\n    barmode='overlay',\n    xaxis_title_text='Age Category',\n    yaxis_title_text='Frequency',\n    title=dict(\n        text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Age Distribution of Sample Means&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Sample Size: 100 | Number of Samples: 100&lt;/span&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age, maritial-statur&lt;/span&gt;&lt;/b&gt;',\n    ),\n)\n\nfig.show()\n</code></pre> <p>By repeating this process across many samples, we can build a distribution of the differences in average age between the widowed and never-married groups. This distribution allows us to see how consistent the differences are across various samples. If the differences are relatively consistent, we can be confident that widowed individuals tend to be older than those who have never been married. If the differences vary widely, we may need larger or more representative samples to draw a reliable conclusion.</p> Code <pre><code>number_of_samples = 100\nsample_size = 100\n\n\ndef calculate_sample_mean_diff(sample_size, n_sample, data1, data2):\n    sample_mean_diff = []\n    for i in range(n_sample):\n        sample1 = random.sample(list(data1), sample_size)\n        sample2 = random.sample(list(data2), sample_size)\n        sample_mean_diff.append(np.mean(sample1)-np.mean(sample2))\n    return sample_mean_diff\n\nx1 = calculate_sample_mean_diff(sample_size, number_of_samples, data_widowed, data_never_married)\n\n\nx1_binned = pd.cut(x1, bins=np.arange(data.age.min(),data.age.max(),0.1), right=False)\nx1_counts = x1_binned.value_counts().sort_index()\n\nfirst_non_zero_index = x1_counts.ne(0.0).idxmax() \nlast_non_zero_index = x1_counts[::-1].ne(0.0).idxmax()  \nx1_counts_trimmed = x1_counts.loc[first_non_zero_index:last_non_zero_index]\n\nx1_counts_trimmed.index = x1_counts_trimmed.index.astype(str)\n\nfig = px.bar(x = x1_counts_trimmed.index, y=x1_counts_trimmed)\nfig.update_traces(marker=dict(color='rgba(0, 65, 110, 0.4)'))\n\nfig.update_layout(\n    barmode='overlay',\n    xaxis_title_text='Age Category',\n    yaxis_title_text='Frequency',\n    title=dict(\n        text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Age Distribution of Sample Differences (widowed vs. never-married)&lt;/span&gt;&lt;br&gt;&lt;span style=\"font-size: 8pt\"&gt;Sample Size: 100 | Number of Samples: 100&lt;/span&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age, maritial-statur&lt;/span&gt;&lt;/b&gt;',\n    ),\n)\n\nfig.show()\n</code></pre> Sign (+/-) of the Estimator <p>It\u2019s also important to note that, just like in other statistical comparisons, the sign of the difference (positive or negative) doesn\u2019t affect the actual result. For instance, if we subtract the average age of the never-married individuals from that of the widowed group, we\u2019ll get a positive difference. But if we subtract the widowed individuals' ages from the never-married individuals' ages, the result will be negative. Statistically, both results show the same difference; it\u2019s just a matter of interpretation.</p>"},{"location":"statistics/probability/Sampling/#random-representative-sampling","title":"Random &amp; Representative Sampling","text":"<p>To give another example, suppose we wanted to compare the ages of people from different regions, say those living in urban areas versus rural areas. Even if there\u2019s no significant age difference between the two groups, each sample might show slight variations due to natural differences in the sample. By taking many samples, we could build a distribution of differences and better understand whether any observed difference is consistent.</p> <p>This also emphasizes the importance of random and representative sampling. If we don\u2019t carefully select our samples, we might get misleading results. For instance, imagine comparing life expectancy in two countries, but we only sample older people in one country and younger people in the other. The resulting data would falsely suggest a significant difference in life expectancy between the countries, when in reality, the sampling was biased. To avoid this, we need to ensure that our samples fairly represent the populations we\u2019re studying.</p>"},{"location":"statistics/probability/Sampling/#sampling-varibility","title":"Sampling Varibility","text":"<p>Variability is a crucial concept in statistics and probability and plays a significant role in scientific research. It\u2019s also one of the main sources of frustration for researchers because it can make results less predictable and more complex. Let\u2019s dive into the idea of sampling variability, where this variability comes from, and why understanding it is so important.</p> Representative Sample (Source: Zieffler, A., &amp; Catalysts for Change. (2019). Statistical Thinking: A simulation approach to uncertainty (4.2th ed.). Minneapolis, MN: Catalyst Press. http://zief0002.github.io/statistical-thinking/) <p>Imagine you have a research question: What is the average age of workers in a factory? To answer this, you decide to measure the age of a few workers. Let's say you randomly select one worker who is 45 years old. Is this the average age of all workers in the factory? Most likely not. So, you keep asking more workers and get different ages like 50, 34, 60, etc. This difference in ages between the workers is an example of sampling variability - the natural differences that occur when taking samples from a population.</p> <p>Even though the overall population (all workers in the factory) has a true average age, you will get slightly different values from each sample you take. This is why relying on just one sample to represent an entire population can lead to inaccurate results. The goal is to reduce this variability by taking more samples and averaging them to get a more reliable estimate of the population\u2019s average age like we did before.</p> Definition: Sampling Variability <p>Using the</p> <ul> <li>same measurement (e.g. mean) on </li> <li>different samples (e.g. age of 100 people) from the </li> <li>same population (e.g. age of all people in the world) can lead to </li> <li>different values (e.g. 47 or 52,...)</li> </ul>"},{"location":"statistics/probability/Sampling/#sources-of-sampling-variability","title":"Sources of Sampling Variability","text":"<ol> <li> <p>Natural Variation: In many fields, especially biology and sociology, natural variation occurs. In our factory example, workers come from different backgrounds and generations, which naturally causes variability in their ages.</p> </li> <li> <p>Measurement Noise: In some cases, the tools or methods used to gather data may introduce variability. For instance, if you\u2019re using a tool with low precision, like a scale that only measures in whole kilograms when you need to measure in grams, you introduce errors in your measurements.</p> </li> <li> <p>Complex Systems: Variability can also come from the complexity of the system being studied. In the factory example, factors like different hiring policies, regional differences, or workforce changes can add to the variation in workers\u2019 ages.</p> </li> <li> <p>Uncontrollable Factors: Some sources of variability, like random or unpredictable events, are outside of the researcher's control. For example, economic conditions might affect the hiring or retirement age of workers, adding unpredictability.</p> </li> </ol>"},{"location":"statistics/probability/Sampling/#dealing-with-sampling-variability","title":"Dealing with Sampling Variability","text":"<p>To reduce the impact of sampling variability, one of the most effective strategies is to take multiple samples. Instead of measuring just a few workers' ages, you would gather information from a larger sample, maybe 100 or more workers, and calculate the average age. The more samples you take, the closer you get to the true average age of the entire population. This is based on the law of large numbers, which states that as you increase the number of samples, the average of those samples will get closer to the population mean.</p> <p>Additionally, statistical tools like confidence intervals can help you understand how close your sample estimate is to the actual population parameter. Confidence intervals provide a range within which the true average age of the factory workers is likely to fall, giving a better sense of the precision of your estimate.</p>"},{"location":"statistics/probability/Sampling/#sampling-variability-vs-reliable-estimates","title":"Sampling Variability vs. Reliable Estimates","text":"<p>So as we mentioned before, we can deal with the sampling variablility by using a larger sample. So we can repeat the experiment from before by randomly selecting a group of different people and calculating the mean of their age. We can do that, by looping through different sample sizes</p> <pre><code>samplesizes = np.arange(5,1000)\nfor sampi in range(len(samplesizes)):\n        sample = random.sample(list(data.age), samplesizes[sampi])\n</code></pre> <p>and for each sample, we calculate the mean age and then plot the result, showing how the mean changes with different sample sizes. Now, let\u2019s consider: what do we expect to happen? Intuitively, as the sample size grows, we\u2019d expect the sample mean to get closer and closer to the population mean, which represents the true average. So, as we increase the sample size, the variation in the sample means should decrease, and the results should start to stabilize around the population mean.</p> Code <pre><code>## Repeat for different sample sizes\nsamplesizes = np.arange(5,1000)\nsamplemeans = np.zeros(len(samplesizes))\n\nfor sampi in range(len(samplesizes)):\n    sample = random.sample(list(data.age), samplesizes[sampi])\n    samplemeans[sampi] = np.mean(sample)\n\nfig = px.line(x=samplesizes, y=samplemeans,markers=True)\nfig.update_traces(line=dict(color='rgba(0, 65, 110, 0.6)'))\n\n# Add a vertical line for the population mean\nfig.add_hline(y=np.mean(data.age), line_dash=\"dash\", annotation_text=\"Mean: \"+str(np.mean(data.age)), annotation_position=\"top right\", line_color=\"#E87F2B\")\n\nfig.update_layout(\n        xaxis_range=[samplesizes.min()-10,samplesizes.max()+10],\n        barmode='overlay',\n        yaxis_title_text='Mean Value',\n        xaxis_title_text='Sample Size',\n        title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample Means of Age&lt;/span&gt;&lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: UCIML Repo: Adult; variable: age&lt;/span&gt;&lt;/b&gt;',\n        ),\n    )\n\nfig.show()\n</code></pre> <p>When we run the experiment, we see that the blue line represents the sample means, and they\u2019re fluctuating quite a bit. The orange line is the true population mean, which is known in advance. While the sample means do start to get closer to the orange line as the sample size increases, they still bounce around quite a lot, even with large samples of up to 1,000 people. This variation is normal for sampling and doesn\u2019t indicate a problem. In fact, it\u2019s expected due to the natural randomness in sample selection. </p> <p>To show something interesting, we take the mean of several sample means (in this case 10)</p> <p><pre><code>print(np.mean(samplemeans[:10]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>35.7\n</code></pre></p> <p>By averaging these sample means together, we get a result that\u2019s much closer to the population mean. </p> <p><pre><code>population_mean = np.mean(data.age)\npopulation_mean\n</code></pre> &gt;&gt;&gt; Output<pre><code>38.6\n</code></pre></p> <p>As we increase the number of sample means we average, the average gets even closer to the true population mean.</p> <p><pre><code>print(np.mean(samplemeans[:100]))\nprint(np.mean(samplemeans[:1000]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>38.3\n38.6\n</code></pre></p> <p>This leads to an important discovery: while individual sample means might not perfectly match the population mean, averaging over multiple sample means brings us much closer to the true value. This concept ties into two key statistical principles - the law of large numbers and the central limit theorem.</p> Task: Sampling Variability <p>Use the the dataset from before and work for the variable <code>hours-per-week</code> on the following tasks:</p> <ol> <li>Calculate the standard deviation for <code>sample_size = np.arange(5,1000)</code></li> <li>Calculate the standard deviation of the population</li> <li>Visualize the results as shown above (incl. standard deviation of the population using <code>fig.add_hline</code> )</li> <li>Calculate the mean of the sample standard deviation for the first <code>10</code>, <code>100</code> and <code>1000</code> samples</li> </ol>"},{"location":"statistics/regression/LinearRegression/","title":"Linear Regression","text":"<p>In many cases, simply characterizing the data is not sufficient. Beyond explaining the data, the goal is often to enable predictions. This chapter introduces the basic approach of linear regression, which allows for approximating bivariate data. The topics covered include linear regression and the coefficient of determination. Regression aims to model the relationships between a dependent variable and one or more independent variables.</p>"},{"location":"statistics/regression/LinearRegression/#motivation","title":"Motivation","text":"<p>To understand the motivation behind linear regression we will start this chapter with an example. Consider a mobile plan that costs \u20ac26, including unlimited SMS, calls, and data within the country. Data roaming costs \u20ac0.84 per MB. The bills for the last year show monthly expenses based on roaming usage.</p> Month Roaming [MB] Bill [\u20ac] Month Roaming [MB] Bill [\u20ac] January 25 47.00 July 125 131.00 February 300 278.00 August 62 78.08 March 258 242.72 September 94 104.96 April 135 139.40 October 381 346.04 May 12 36.08 November 12 36.08 June 0 26.00 December 18 41.12 Code <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\n\n# Create a DataFrame\ndf = pd.DataFrame([(25, 47.00), (300, 278.00), (258, 242.72), (135, 139.40), (12, 36.08), \n                (0, 26.00), (125, 131.00), (62, 78.08), (94, 104.96), \n                (381, 346.04), (12, 36.08), (18, 41.12)], \n                columns=['Roaming', 'Price'])\n\n# Linear Regression\nmodel = LinearRegression()\nmodel.fit(df[['Roaming']], df['Price'])\n\nintercept = model.intercept_\nslope = model.coef_[0]\nr_sq = model.score(df[['Roaming']], df['Price'])\n\n# Generate regression line\ndf['Regression Line'] = intercept + slope * df['Roaming']\n\n# Create Plotly Express figure\nfig = px.scatter(df, x='Roaming', y='Price')\nfig['data'][0]['marker'] = {'color':'red', 'size':10}\n\n# Add regression line\nfig.add_traces(px.line(df, x='Roaming', y='Regression Line').data)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Roaming [MB]',\n    yaxis_title_text='Price [\u20ac]',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Smartphone Bill&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Variables: roaming, price&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> <p>A scatter plot of the data reveals a perfect linear relationship, allowing us to describe the relationship with a linear function:</p> \\[ y = 26 + 0.84 \\cdot x \\] <p>This has several advantages. For one, the bill amount can be explained through fixed and variable costs, specifically showing how the MB usage affects the total cost. Additionally, it allows for predictions of the bill amount for any unobserved amount of MB.</p> <p>However, in reality, most relationships are not perfectly linear. Let's consider two samples, each with variables \\(X\\) and \\(Y\\).</p> \\(X_1\\) \\(Y_1\\) \\(X_2\\) \\(Y_2\\) 0.00 0.23 0.14 2.00 0.12 0.31 0.25 2.41 0.18 0.49 0.18 2.69 0.26 1.11 0.27 3.41 0.40 1.03 0.42 3.43 0.51 1.32 0.50 3.82 0.60 1.58 0.62 4.18 0.68 1.66 0.70 4.36 0.80 1.65 0.79 4.45 0.80 1.85 0.85 4.75 0.99 1.69 1.00 4.69 <p>When analyzing these samples, we find:</p> <ul> <li>Sample 1 has a Pearson correlation coefficient of \\( \\rho_1 = 0.938 \\).</li> <li>Sample 2 has a Pearson correlation coefficient of \\( \\rho_2 = 0.942 \\).</li> </ul> <p>These values are very similar and suggest a strong correlation.</p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul> Code <pre><code>import pandas as pd\nimport plotly.express as px\n\nx1 = [0.00, 0.12, 0.18, 0.26, 0.40, 0.51, 0.60, 0.68, 0.80, 0.80, 0.99]\ny1 = [0.23, 0.31, 0.49, 1.11, 1.03, 1.32, 1.58, 1.66, 1.65, 1.85, 1.69]\n\nx2 = [0.14, 0.25, 0.18, 0.27, 0.42, 0.50, 0.62, 0.70, 0.79, 0.85, 1.00]\ny2 = [2.00, 2.41, 2.69, 3.41, 3.43, 3.82, 4.18, 4.36, 4.45, 4.75, 4.69]\n\ndf = pd.DataFrame({'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2})\n\n\n# Create Plotly Express figure\nfig = px.scatter(df, x='x1', y='y1')\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='x1',\n    yaxis_title_text='y1',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Dataset 1&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n\n\n# Create Plotly Express figure\nfig2 = px.scatter(df, x='x2', y='y2')\n\n# Adjust the plot\nfig2.update_layout(\n    xaxis_title_text='x2',\n    yaxis_title_text='y2',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Dataset 2&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig2.show()\n</code></pre> <p>At first glance, a scatter plot supports this conclusion, but the impression changes when the axes are normalized equally. </p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul> Code <pre><code>import pandas as pd\nimport plotly.express as px\n\nx1 = [0.00, 0.12, 0.18, 0.26, 0.40, 0.51, 0.60, 0.68, 0.80, 0.80, 0.99]\ny1 = [0.23, 0.31, 0.49, 1.11, 1.03, 1.32, 1.58, 1.66, 1.65, 1.85, 1.69]\n\nx2 = [0.14, 0.25, 0.18, 0.27, 0.42, 0.50, 0.62, 0.70, 0.79, 0.85, 1.00]\ny2 = [2.00, 2.41, 2.69, 3.41, 3.43, 3.82, 4.18, 4.36, 4.45, 4.75, 4.69]\n\ndf = pd.DataFrame({'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2})\n\n\n# Create Plotly Express figure\nfig = px.scatter(df, x='x1', y='y1')\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='x1',\n    yaxis_title_text='y1',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Dataset 1&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig.update_layout(yaxis_range=[0,6])\n\n# Show the plot\nfig.show()\nfig.write_html(\"outputpic/regression_scatter_unscale1.html\", full_html=False, include_plotlyjs='cdn')\n\n\n# Create Plotly Express figure\nfig2 = px.scatter(df, x='x2', y='y2')\n\n# Adjust the plot\nfig2.update_layout(\n    xaxis_title_text='x2',\n    yaxis_title_text='y2',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Dataset 2&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig2.update_layout(yaxis_range=[0,6])\n\n# Show the plot\nfig2.show()\n</code></pre> <p>Proper scaling reveals:</p> <ol> <li>For every \\(X\\) value, the corresponding \\(Y\\) value in Sample 2 is consistently larger than in Sample 1.</li> <li>The change in \\(Y\\) is more significant in Sample 2 compared to Sample 1 when \\(X\\) changes.</li> </ol> <p>This phenomenon occurs because we intuitively focus on the overall picture and draw a mental line through the points. The question then arises: how do we determine this line? This leads us into the core of linear regression, where we aim to model the relationship between variables and make informed predictions.</p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul> Code <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\n\nx1 = [0.00, 0.12, 0.18, 0.26, 0.40, 0.51, 0.60, 0.68, 0.80, 0.80, 0.99]\ny1 = [0.23, 0.31, 0.49, 1.11, 1.03, 1.32, 1.58, 1.66, 1.65, 1.85, 1.69]\n\nx2 = [0.14, 0.25, 0.18, 0.27, 0.42, 0.50, 0.62, 0.70, 0.79, 0.85, 1.00]\ny2 = [2.00, 2.41, 2.69, 3.41, 3.43, 3.82, 4.18, 4.36, 4.45, 4.75, 4.69]\n\ndf = pd.DataFrame({'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2})\n\n# Linear Regression Sample 1\nmodel1 = LinearRegression()\nmodel1.fit(df[['x1']], df['y1'])\nintercept1 = model1.intercept_\nslope1 = model1.coef_[0]\ndf['y1_hat'] = intercept1 + slope1 * df['x1']\n\n# Linear Regression Sample 2\nmodel2 = LinearRegression()\nmodel2.fit(df[['x2']], df['y2'])\nintercept2 = model2.intercept_\nslope2 = model2.coef_[0]\ndf['y2_hat'] = intercept2 + slope2 * df['x2']\n\n# Create Plotly Express figure\nfig = px.scatter(df, x='x1', y='y1')\nfig['data'][0]['marker'] = {'color':'red', 'size':10}\n\n# Add regression line\nfig.add_traces(px.line(df, x='x1', y='y1_hat').data)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='x1',\n    yaxis_title_text='y1',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample 1&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig.update_layout(yaxis_range=[0,6])\n\n# Show the plot\nfig.show()\n\n\n# Create Plotly Express figure\nfig2 = px.scatter(df, x='x2', y='y2')\nfig2['data'][0]['marker'] = {'color':'red', 'size':10}\n\n# Add regression line\nfig2.add_traces(px.line(df, x='x2', y='y2_hat').data)\n\n# Adjust the plot\nfig2.update_layout(\n    xaxis_title_text='x2',\n    yaxis_title_text='y2',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Sample 2&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\nfig2.update_layout(yaxis_range=[0,6])\n\n# Show the plot\nfig2.show()\n</code></pre>"},{"location":"statistics/regression/LinearRegression/#linear-regression_1","title":"Linear Regression","text":"<p>In linear regression, the relationship between variables is not exactly described. To account for this, a random error \\( e_i \\) is added. </p> Definition <p>Approximation of the real values \\( y_i \\)</p> \\[ y_i = \\hat{y}_i + e_i \\] <p>With the linear regression</p> \\[ \\hat{y}_i = a + b \\cdot x_i \\] <p>The goal of linear regression is to find the best fit line by solving a minimization problem. This problem minimizes the sum of the squared residuals, expressed as</p> Definition <p>Minimization problem for fitting the linear regression</p> \\[ \\min_{a,b} \\sum_{i=1}^{n}e_i^2 = \\min_{a,b} \\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2= \\min_{a,b} \\sum_{i=1}^{n}(y_i-a-bx_i)^2 \\] <p>By solving this, the coefficients of the regression line can be determined, with</p> Definition <p>Intercept</p> \\[ a = \\bar{y} - b \\bar{x} \\] <p>Slope</p> \\[ b = \\frac{\\text{cov}(X, Y)}{\\sigma_x^2} \\] <p>The best fit line is the one that minimizes the sum of squared differences between observed and predicted values. These differences, known as residuals, represent the distance between the actual \\( Y \\)-values and the predicted \\( Y \\)-values from the model.</p>"},{"location":"statistics/regression/LinearRegression/#coefficient-of-determination","title":"Coefficient of Determination","text":"<p>To evaluate the goodness of fit of a model, the coefficient of determination \\( R^2 \\) can be used. It is calculated as:</p> Definition <p>Coefficient of Determination</p> \\[ R^2 = \\frac{\\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} = \\rho_{X,Y}^2 \\] <p>The coefficient of determination indicates the proportion of variance explained by the model. In essence, \\( R^2 \\) measures how well the regression model fits the observed data.</p> <p>\\( R^2 \\) can range from zero to one:</p> <ul> <li> <p>\\( R^2 = 0 \\): None of the variance is explained by the model, indicating a poor fit.</p> </li> <li> <p>\\( R^2 = 1 \\): All of the variance is explained by the regression, indicating a perfect fit, which can only occur when the original data points lie exactly on the regression line.</p> </li> </ul>"},{"location":"statistics/regression/LinearRegression/#recap","title":"Recap","text":"<ul> <li>Linear regression attempts to model the relationship between multiple variables.</li> <li>Using the model, further values can be predicted.</li> <li>In linear regression, the squared distance between the raw data points and the regression line is minimized.</li> <li>The coefficient of determination \\(R^2\\) is used to assess the model's goodness of fit.</li> <li>The closer the \\(R^2\\) value is to one, the better the model fits the data.</li> </ul>"},{"location":"statistics/regression/LinearRegression/#tasks","title":"Tasks","text":"Task <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ncars = fetch_ucirepo(id=9) \n# https://archive.ics.uci.edu/dataset/9/auto+mpg\n\n# data (as pandas dataframes) \ndata = cars.data.features\ndata = data.join(cars.data.ids)\n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following task: </p> <ol> <li>Perform a linear regression for the following attribute combinations: <ul> <li>Displacement vs. Weight</li> <li>Displacement vs. Acceleration</li> <li>Acceleration vs. Weight</li> </ul> </li> <li>For all performed regressions calculate the coefficient of determination</li> <li>Write down the formula for all performed regressions </li> </ol>"},{"location":"statistics/univariate/CentralTend/","title":"Measures of Central Tendency","text":"<p>Measures of central tendency characterize a distribution by using an average value. These metrics describe what can be considered a \"typical\" or \"representative\" observation within the data set.</p>"},{"location":"statistics/univariate/CentralTend/#mean","title":"Mean","text":"<p>(for numeric attributes) The arithmetic mean \\( \\bar{x} \\) is the most common and effective numerical measure for describing the average value of a distribution. </p> Definition: Arithmetic Mean \\[ \\bar{x}=\\frac{\\sum_{i=1}^{N}x_i}{N}=\\frac{x_1+x_2+\\dots+x_N}{N} \\] <p>with \\( x_1, x_2, \\dots, x_N \\) representing a set of \\( N \\) values of a metric variable \\( X \\).</p> <p>One disadvantage of the mean is its sensitivity to outliers. Even a small number of extreme values can distort the result.</p> <pre><code>import statistics \n\nstatistics.mean([1,2,1,2,3,4,1,100,1,2,1])\n</code></pre> &gt;&gt;&gt; Output<pre><code>10.73\n</code></pre> <p>To address this issue, the trimmed mean is used. This method involves removing a percentage of values from both the upper and lower ends of the distribution (e.g., 1%). </p> <pre><code>from scipy import stats\n\n#calculate 10% trimmed mean\nstats.trim_mean([1,2,1,2,3,4,1,100,1,2,1], 0.1)\n</code></pre> &gt;&gt;&gt; Output<pre><code>1.889\n</code></pre> <p>However, this approach can lead to a loss of information, especially when a large portion of the data is trimmed.</p> Example: Mean of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the mean and the trimmed mean (20% left and right) of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution </p> \\[ \\begin{eqnarray*} \\bar{x} &amp;=&amp; \\frac{28.3 + 27.2 + 27.4 + 22.7 + 14.3 + 11.9 + 13.8}{14}\\\\     &amp;&amp;+\\frac{19.8 + 9.6 + 21.1 + 20.8 + 19.8 + 25.3 + 22.8}{14} \\\\     &amp;=&amp; 20.34 \\end{eqnarray*} \\] \\[ \\begin{eqnarray*} \\bar{x}_{trim} &amp;=&amp; \\frac{13.8+ 14.3+ 19.8+ 19.8+20.8 }{14}\\\\     &amp;&amp;+\\frac{21.1+ 22.7+ 22.8+ 25.3+ 27.2}{14} \\\\     &amp;=&amp; 20.76 \\end{eqnarray*} \\] <p>The mean temperature is \\(20.34^\\circ C\\) and the trimmed mean is \\(20.76^\\circ C\\) .</p> Code <pre><code>import statistics \nTemp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\nprint('Mean: ' + str(statistics.mean(Temp)))\n\nfrom scipy import stats\nprint('Trimmed Mean: ' + str(stats.trim_mean(Temp, 0.2)))\n</code></pre>"},{"location":"statistics/univariate/CentralTend/#median","title":"Median","text":"<p>(for numeric and ordinal attributes) The median is more robust against outliers, as it splits the distribution into an upper and a lower half. It corresponds to the 50th percentile or the second quartile. </p> <pre><code>import numpy as np\nnp.sort([1,2,1,2,3,4,1,100,1,2,1])\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1,1,1,1,1,2,2,2,3,4,100]\n           \u2191\n</code></pre> <pre><code>statistics.median([1,2,1,2,3,4,1,100,1,2,1])\n</code></pre> &gt;&gt;&gt; Output<pre><code>2\n</code></pre> <p>However, one downside is that it can be more complex to calculate, especially when dealing with large datasets.</p> Definition: Median <p>A data set of \\( N \\) values of an attribute \\( X \\) is sorted in increasing order</p> <ul> <li>If \\( N \\) is odd, the median is the middle value of the ordered set</li> <li>\\( N \\) is even, the median is the two middlemost values and any value in between (average of those two for numeric attribute )</li> </ul> Example: Median of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the median of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution <pre><code>[9.6, 11.9, 13.8, 14.3, 19.8, 19.8, 20.8, 21.1, 22.7, 22.8, 25.3, 27.2, 27.4, 28.3]\n</code></pre></p> \\[ \\bar{x} =\\frac{20.8+21.1}{2} = 20.95\\\\ \\] <p>The median temperature is \\(20.95^\\circ C\\).</p> Code <pre><code>import statistics \nTemp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\nprint('Median: ' + str(statistics.median(Temp)))\n</code></pre>"},{"location":"statistics/univariate/CentralTend/#mode","title":"Mode","text":"<p>(for numeric, ordinal and nominal attributes) The mode is the most frequently occurring value in a distribution. It is also more robust against outliers, making it a useful measure in certain cases where extreme values might distort other central tendency metrics.</p> <pre><code># Calculate the first Mode\nstatistics.mode([1,2,1,2,3,4,1,100,1,2,2])\n</code></pre> &gt;&gt;&gt; Output<pre><code>1\n</code></pre> <pre><code># Calculate all Modes\nstatistics.multimode([1,2,1,2,3,4,1,100,1,2,2])\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1, 2]\n</code></pre> Definition: Mode <ul> <li>Mode is the value that occurs most frequently in a data set.</li> <li>There can be more than one mode (unimodal, bimodal, multimodal)</li> <li>If each data value occurs only once, then there is no mode</li> </ul> Example: Mode of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the mode of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution  The mode of the temperature is \\(19.8^\\circ C\\) because this value appears twice, making it the only value that occurs more than once. The distribution is unimodal.</p> Code <pre><code>import numpy as np\nTemp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n\nfrom scipy import stats\n#https://docs.scipy.org/doc/scipy/reference/stats.html\nprint('Mode: ' + str(stats.mode(Temp))) # returns the first mode (for unimodal)\n\nimport statistics \n#https://docs.python.org/3/library/statistics.html\nprint('Mode: ' + str(statistics.mode(Temp))) # returns the first mode (for unimodal)\nprint('Mode (Multi): ' + str(statistics.multimode(Temp))) # returns a list of all modes (for multimodal)\n</code></pre>"},{"location":"statistics/univariate/CentralTend/#quantile","title":"Quantile","text":"<p>(numeric and ordinal attributes) The \\( q \\)-quantile divides the data into \\( q \\) equal-sized parts. There are \\( q - 1 \\) quantiles (for example, 3 quantiles divide the data into 4 parts). The 2-quantile corresponds to the median. </p> <pre><code>import numpy as np\nnp.sort([1,2,1,2,3,4,1,100,1,2,2])\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1,1,1,1,2,2,2,2,3,4,100] \n</code></pre> <p>The quartiles can be determined graphically using</p> <pre><code>[1,1,1,1,2,2,2,2,3,4,100]\n |----|----|----|----|\n      Q1   Q2   Q3   \n</code></pre> <p>or by using pythons <code>scipy</code> package</p> <pre><code>from scipy import stats\n\nstats.mstats.mquantiles([1,2,1,2,3,4,1,100,1,2,2], prob=[0.25, 0.5, 0.75])\n</code></pre> &gt;&gt;&gt; Output<pre><code>[1. , 2. , 2.8]\n</code></pre> <p>There are different methods for determining quantiles, especially when \\( N \\) is even. As a result, it is possible for different libraries to produce a different result than a manual calculation of quantiles.</p> Definition: Quantile <p>A data set of \\(N\\) values of an attribute \\(X\\) is sorted in increasing order</p> <ul> <li>The \\(k\\)-th \\(q\\)-quantile is the value \\(x\\) where \\(k/q\\) of the data values are less and \\((q-k)/q\\) values are more than \\(x\\) (with \\(0 &lt; k &lt; q\\))</li> <li>If set of numbers are odd, you have to calculate the middle</li> </ul> <p>Most widely used forms</p> <ul> <li>2-quantile = median: Divides the data set in halves</li> <li>4-quantile = quartiles: Three data points split the data into four equal parts</li> <li>100-quantile = percentiles: Divide the data into 100 equal-sized sets</li> </ul> Example: Quantiles of Temperature Distribution <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the quartiles of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution <pre><code>[9.6, 11.9, 13.8, &lt;&lt;14.3&gt;&gt;, 19.8, 19.8, &lt;&lt;20.8&gt;&gt;, &lt;&lt;21.1&gt;&gt;, 22.7, 22.8, &lt;&lt;25.3&gt;&gt;, 27.2, 27.4, 28.3]\n</code></pre></p> \\[ Q_1 = 14.3^\\circ C \\qquad Q_2 = 20.95^\\circ C \\qquad Q_3 = 25.3^\\circ C \\] Code <pre><code>from scipy import stats\n#https://docs.scipy.org/doc/scipy/reference/stats.html\n\nprint('Quantile: ' + str(stats.mstats.mquantiles(Temp, prob=[0.25, 0.5, 0.75])))\n</code></pre>"},{"location":"statistics/univariate/CentralTend/#five-number-sumary-boxplot","title":"Five Number Sumary &amp; Boxplot","text":"<p>A single metric alone is not sufficient to fully characterize a distribution. The Five Number Summary is a useful approach to gain a deeper understanding of a distribution by combining multiple key metrics, offering a more comprehensive view of the data.</p> Definition: Five Number Summary <p>The Five Number Summary consists of the following metrics in this exact order:</p> <p>[Minimum, \\(Q_1\\), Median, \\(Q_3\\), Maximum]</p> <p>A boxplot is a visualization of a distribution and provides a graphical representation of the Five Number Summary. </p> <pre><code>import plotly.express as px\n\nfig = px.box([1,2,1,2,3,4,1,100,1,2,2])\nfig.show()\n</code></pre> <p>The first and third quartiles (\\(Q_1\\) and \\(Q_3\\)) mark the ends of the box, while the interquartile range (IQR) represents the length of the box. The median is depicted as a line within the box. Two lines, known as whiskers, extend from the box to the smallest and largest observations, provided these values are no more than \\(1.5\\times\\)IQR above \\(Q_3\\) or below \\(Q_1\\). Outliers are marked separately.</p> Example: Boxplot of the Temperature <ul> <li> <p>Without Outlier</p> <p></p> Code <pre><code>import plotly.express as px\nimport pandas as pd\n\nTemp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n\n# Generate Box Plot\nfig = px.box(pd.DataFrame(Temp))\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text=' ',\n    yaxis_title_text='Temperature',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Boxplot - no Outlier&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;variable: Temperature&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>With Outlier</p> <p></p> Code <pre><code>import plotly.express as px\nimport pandas as pd\n\nTemp2 = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8, 50, 60]\n\n# Generate Box Plot\nfig = px.box(pd.DataFrame(Temp2))\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text=' ',\n    yaxis_title_text='Temperature',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Boxplot - Outlier&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;variable: Temperature&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> </ul>"},{"location":"statistics/univariate/CentralTend/#recap","title":"Recap","text":"<ul> <li>Measures of central tendency characterize a distribution by using an average value.</li> <li>Common metrics include the mean, median, mode, and quantiles.</li> <li>Individual metrics alone are not sufficient to provide a full understanding.</li> <li>The Five Number Summary combines multiple values to offer a more comprehensive characterization of the distribution.</li> <li>The boxplot is the graphical representation of the Five Number Summary.</li> <li>However, the informative value of the Five Number Summary is limited.</li> <li>It can be misleading to describe a distribution solely through measures of central tendency. </li> </ul> <p>The following distributions have the same mean, median and mode</p> <ul> <li> <p>Without Outlier</p> <p></p> </li> <li> <p>With Outlier</p> <p></p> </li> </ul> Code <pre><code>a = [2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4]\nb = [1,1,1,1,1,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,4,5,5,5,5,5,5,5,5]\n\nimport plotly.express as px\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\n# Distribution a\n# Generate Histogram\nfig = px.histogram(pd.DataFrame(a))\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Value',\n    yaxis_title_text='Absolute Frequency',\n    showlegend=False,\n)\n\n# Show the plot\nfig.show()\n\n# Calculate Measures\nprint('Mean: ' + str(np.mean(a)))\nprint('Median: ' + str(np.median(a)))\nprint('Mode: ' + str(stats.mode(a)))\n\n# Distribution b\n# Generate Histogram\nfig = px.histogram(pd.DataFrame(b))\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Value',\n    yaxis_title_text='Absolute Frequency',\n    showlegend=False,\n)\n\n# Show the plot\nfig.show()\n\n# Calculate Measures\nprint('Mean: ' + str(np.mean(b)))\nprint('Median: ' + str(np.median(b)))\nprint('Mode: ' + str(stats.mode(b)))\n</code></pre>"},{"location":"statistics/univariate/CentralTend/#tasks","title":"Tasks","text":"Task: Measures of Central Tendency <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ncars = fetch_ucirepo(id=9) \n# https://archive.ics.uci.edu/dataset/9/auto+mpg\n\n# data (as pandas dataframes) \ndata = cars.data.features\ndata = data.join(cars.data.ids)\n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following task: </p> <ol> <li>Calculate all useful and meaningful measure of central tendency (mean, median, mode) for the following attributes (think about attribute types)<ul> <li><code>car_name</code></li> <li><code>origin</code></li> <li><code>displacement</code></li> </ul> </li> <li>Calculate the quartiles and plot a Boxplot fo the attribute <code>acceleration</code></li> </ol>"},{"location":"statistics/univariate/Dispersion/","title":"Measure of Dispersion","text":"<p>Measures of dispersion characterize a distribution by indicating how data is spread around an average value. These metrics describe the variability or heterogeneity of the data.</p> Info <p>Some formulas differ between samples and populations (e.g., variance), which may result in slight variations in the calculations. </p> <p> </p>"},{"location":"statistics/univariate/Dispersion/#range","title":"Range","text":"<p>The range, denoted as \\( R \\), is the difference between the largest and smallest value in a dataset. However, in the presence of extremely large or small outliers, the range can provide a distorted view of the data's variability.</p> <pre><code>import numpy as np\nnp.ptp([1,2,1,2,3,4,1,100,1,2,2])\n</code></pre> &gt;&gt;&gt; Output<pre><code>99\n</code></pre> Defintion: Range \\[ R = \\text{max}(X)-\\text{min}(X) \\] <p>with \\( x_1, x_2, \\dots, x_N \\) representing a set of \\( N \\) values of a metric variable \\( X \\). </p> Example: Range of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the range of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution </p> \\[ R = 28.3-9.6 = 18.7 \\] <p>The temperature range is \\(18.7^\\circ C\\).</p> Code <pre><code>Temp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n\nimport numpy as np\nprint('Range:', np.ptp(Temp))\n</code></pre>"},{"location":"statistics/univariate/Dispersion/#interquartile-range","title":"Interquartile Range","text":"<p>The interquartile range (IQR) is the difference between the third and first quartile. </p> <pre><code>from scipy import stats\nstats.iqr([1,2,1,2,3,4,1,100,1,2,2], interpolation = 'nearest')\n</code></pre> &gt;&gt;&gt; Output<pre><code>2\n</code></pre> <p>It describes the spread of the middle 50% of the data, providing a measure of variability that is less sensitive to outliers</p> Definition: Interquartile Range \\[ IQR = Q_3-Q_1 \\] <p>Q1 and Q3 are the first and third quartiles of a dataset with \\( N \\) values of a variable \\( X \\).</p> Example: IQR of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the IQR of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution <pre><code>[9.6, 11.9, 13.8, &lt;&lt;14.3&gt;&gt;, 19.8, 19.8, &lt;&lt;20.8&gt;&gt;, &lt;&lt;21.1&gt;&gt;, 22.7, 22.8, &lt;&lt;25.3&gt;&gt;, 27.2, 27.4, 28.3]\n</code></pre></p> \\[ $Q_1 = 14.3^\\circ C \\qquad Q_2 = 20.95^\\circ C \\qquad Q_3 = 25.3^\\circ C $ \\] \\[ IQR = 25.3 - 14.3 = 11^\\circ C \\] <p>The IQR of the temperature is \\(11^\\circ C\\).</p> Code <pre><code>Temp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n\nfrom scipy import stats\nprint('IQR: ', stats.iqr(Temp, interpolation = 'nearest'))\n</code></pre>"},{"location":"statistics/univariate/Dispersion/#variance","title":"Variance","text":"<p>Variance \\( \\sigma^2 \\) is the mean of the squared deviations from the average. It indicates how spread out a distribution is.</p> <pre><code>import statistics \nprint('Variance: ', statistics.variance([1,2,1,2,3,4,1,100,1,2,2]))\nprint('Population Variance: ', statistics.pvariance([1,2,1,2,3,4,1,100,1,2,2]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>Variance:  875.76\nPopulation Variance:  796.15\n</code></pre> Definition: Variance \\[ \\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N}(x_i-\\bar{x})^2 \\] <p>with \\( x_1, x_2, \\dots, x_N \\) representing a set of \\( N \\) values of a metric variable \\( X \\). This formula applies to the entire population. For samples, it differs slightly, as the division is by \\( N - 1 \\) instead of \\( N \\).</p> Example: Variance of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the variance of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution </p> \\[ \\sigma^2 = \\frac{463.7}{14} = 33.12 \\] <p>The variance of the temperature is \\(33.12^\\circ C^2\\).</p> Code <pre><code>Temp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n\nimport statistics \nprint('Variance: ', statistics.variance(Temp))\nprint('Population Variance: ', statistics.pvariance(Temp))\n</code></pre>"},{"location":"statistics/univariate/Dispersion/#standard-deviation","title":"Standard Deviation","text":"<p>The standard deviation \\( \\sigma \\) describes a \"typical\" deviation from the mean. It indicates how spread out a distribution is.</p> <pre><code>import statistics \nprint('Standard Deviation: ', statistics.stdev([1,2,1,2,3,4,1,100,1,2,2]))\nprint('Population Standard Deviation: ', statistics.pstdev([1,2,1,2,3,4,1,100,1,2,2]))\n</code></pre> &gt;&gt;&gt; Output<pre><code>Standard Deviation:  29.59\nPopulation Standard Deviation:  28.22\n</code></pre> <p>A small \\( \\sigma \\) suggests that the data tends to be close to the mean, while a large \\( \\sigma \\) indicates that the data is spread over a wide range of values.</p> Definition: Standard Deviation \\[ \\sigma=\\sqrt{\\sigma^2} \\] <p>With \\( x_1, x_2, \\dots, x_N \\) representing a set of \\( N \\) values of a metric variable \\( X \\), and \\( \\sigma^2 \\) being the corresponding variance.</p> Example: StD of the Temperature <p>Given a table with 14 temperature values in \u00b0C, the goal is to calculate the standard deviation of the distribution. <pre><code>[28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n</code></pre> Solution </p> \\[ \\sigma^2 = \\frac{463.7}{14} = 33.12 \\] \\[ \\sigma = \\sqrt{\\sigma^2} = \\sqrt{33.12} = 5.76 \\] <p>The temperature values deviate, on average, by \\(5.76^\\circ C\\) from the mean.</p> Code <pre><code>Temp = [28.3, 27.2, 27.4, 22.7, 14.3, 11.9, 13.8, 19.8, 9.6, 21.1, 20.8, 19.8, 25.3, 22.8]\n\nimport statistics \nprint('Standard Deviation: ', statistics.stdev(Temp))\nprint('Population Standard Deviation: ', statistics.pstdev(Temp))\n</code></pre>"},{"location":"statistics/univariate/Dispersion/#coefficient-of-variation","title":"Coefficient of Variation","text":"<p>We previously encountered the issue that variance and standard deviations of different data series were difficult to compare. The coefficient of variation (\\(c_v\\)) can be used to solve this problem. It is often referred to as the relative standard deviation.</p> <pre><code>stats.variation([1,2,1,2,3,4,1,100,1,2,2])\n</code></pre> &gt;&gt;&gt; Output<pre><code>2.61\n</code></pre> Definition: Coefficient of Variation \\[ c_v = \\frac{\\sigma}{\\bar{x}} \\] <p>With \\( x_1, x_2, \\dots, x_N \\) representing a set of \\( N \\) values of a metric variable \\( X \\), \\( \\sigma \\) being the corresponding standard deviation, and \\( \\bar{x} \\) the mean.</p> Example: Pizza Prices <p>You are given a table of pizza prices in New York listed in various currencies. <pre><code>dollar = [1, 2, 3, 3, 5, 6, 7, 8, 9, 11]\nPesos = [18.81, 37.62, 56.43, 56.43, 94.05, 112.86, 131.67, 150.48, 169.29]\n</code></pre></p> \\[ \\begin{eqnarray*}     \\sigma_{Dollar} = 3.1 &amp;\\qquad &amp; \\sigma_{Pesos} = 58.43\\\\     \\bar{x}_{Dollar} = 5.5 &amp;\\qquad &amp; \\bar{x}_{Pesos} = 103.46\\\\ \\end{eqnarray*} \\] <p>The goal is to calculate the coefficient of variation for both data series.</p> <p>Solution </p> \\[ \\begin{eqnarray*}     c_{v,dollar} = \\frac{3.1}{5.5} = 0.56 &amp;\\qquad&amp; c_{v,pesos} = \\frac{58.43}{103.46} = 0.56 \\end{eqnarray*} \\] Code <pre><code>Dollar = [1,2,3,3,5,6,7,8,9,11]\nPesos = [x * 18.81 for x in Dollar]\n\nfrom scipy import stats\nprint('CoV Dollar: ' + str(stats.variation(Dollar)))\nprint('CoV Pesos: ' + str(stats.variation(Pesos)))\n</code></pre>"},{"location":"statistics/univariate/Dispersion/#recap","title":"Recap","text":"<ul> <li>Measures of dispersion characterize a distribution by describing how data is spread around a central value.</li> <li>The IQR represents the middle 50% of the data.</li> <li>Variance indicates how wide a distribution is.</li> <li>Interpreting variance can be challenging because its units are squared.</li> <li>For this reason, standard deviation is a more suitable measure for interpretation.</li> <li>To better compare standard deviations across datasets, the coefficient of variation is used.</li> <li>There are different formulas for variance depending on whether the entire population or a sample is being analyzed</li> </ul>"},{"location":"statistics/univariate/Dispersion/#tasks","title":"Tasks","text":"Task: Measures of Dispersion <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ncars = fetch_ucirepo(id=9) \n# https://archive.ics.uci.edu/dataset/9/auto+mpg\n\n# data (as pandas dataframes) \ndata = cars.data.features\ndata = data.join(cars.data.ids)\n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following task: </p> <ol> <li>For the attribute <code>acceleration</code> calculate the following measures (use the sample formula - not population):<ul> <li>Range</li> <li>IQR --&gt; compare to the boxplot from the section Measures of Central Tendency</li> <li>Variance</li> <li>Standard Deviation</li> <li>CV</li> </ul> </li> </ol> Task: Weight of Euro Coins <p>  Download the following dataset from this page and load it into your notebook.</p> <pre><code># Website: https://jse.amstat.org/v14n2/datasets.aerts.html\n# Dataset: https://jse.amstat.org/datasets/euroweight.dat.txt\n# Description: https://jse.amstat.org/datasets/euroweight.txt\n\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\ndata = pd.read_csv('Daten/euroweight.dat.txt', sep='\\t', header=None, index_col=0, names=['Weight', 'Batch'])\n\n# Display the first few rows\ndata.head()\n</code></pre> <p>As the Head of Quality Control at the European Central Bank (ECB), you are responsible, among other duties, for the quality management of 1-Euro coins. Consequently, you have tasked an employee with selecting a random sample of 2,000 coins. (Dataset: 'euro.csv', Unit: grams)</p> <ol> <li>Calculate the average weight of the coins.</li> <li>Determine the corresponding standard deviation and interpret its significance.</li> <li>Create a histogram. Ensure that all axes are labeled and the chart is properly titled.</li> </ol>"},{"location":"statistics/univariate/Frequency/","title":"Frequency Distribution","text":"<p>A list \\( X \\) consists of \\( n \\) elements \\( x_1, \\dots, x_n \\). Within this list, \\( X \\) contains \\( k \\) distinct values (\\( a_1, \\dots, a_k \\)). The frequency refers to how often a specific value \\( a_k \\) appears in \\( X \\). </p> <pre><code>drinks = ['small', 'small', 'small', 'medium', 'medium', 'medium', 'large']\n</code></pre> <p>In this example, </p> <ul> <li>\\( X \\): <code>drinks</code></li> <li>\\( n \\): <code>7</code></li> <li>\\( x_1, \\dots, x_n \\): <code>['small', 'small', 'small', 'medium', 'medium', 'medium', 'large']</code></li> <li>\\( k \\): 3</li> <li>\\( a_1, \\dots, a_k \\): <code>['small', 'medium', 'large']</code></li> </ul> <p>In the case of a nominal scale, \\( k \\) is equal to the number of categories, with \\( k \\) typically much smaller than \\( n \\). For a metric scale, there are often only a few identical values, meaning \\( k \\) is approximately equal to \\( n \\).</p> <p>The representation of frequencies can be done in the form of a table or a graphical format. When a frequency distribution is depicted as a bar chart, it is referred to as a histogram. </p> <pre><code>import plotly.express as px\n\ndf = px.data.tips()\nfig = px.histogram(df, x=\"total_bill\")\nfig.show()\n</code></pre> <p>It is important that the data remains the focal point and is presented as accurately and objectively as possible, avoiding distortions such as 3D effects or shadows. Titles, axis labels, legends, the data source, and the time of data collection should always be clearly indicated.</p> Definition: Frequency <p>Absolute Frequency of the value \\( a_j \\)</p> \\[ h(a_j) = h_j \\] <p>Relative Frequency of the value \\( a_j \\)</p> \\[ f(a_j) = f_j = \\frac{h_j}{n} \\] <p>Absolute Frequency Distribution: \\( h_1, \\dots, h_k \\)</p> <p>Relative Frequency Distribution: \\( f_1, \\dots, f_k \\)</p> Code <p>For the upcoming analysis, the following data will be used:      <pre><code># Import Libraries\nimport pandas as pd\n\n# Import Data\ndata = pd.read_csv('https://raw.githubusercontent.com/JeffSackmann/tennis_atp/master/atp_matches_2023.csv')\n</code></pre></p>"},{"location":"statistics/univariate/Frequency/#nominal-scale","title":"Nominal Scale","text":"<p>For nominally scaled variables, the values correspond to the possible categories. The internal order of these categories is not relevant in the substantive analysis.</p> Example: Graphical Representation of Nominal Variables <ul> <li> <p></p> Code <pre><code>import plotly.express as px\n\n# Generate Histogram\nfig = px.histogram(\n    data, \n    x=\"surface\",\n)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Surface',\n    yaxis_title_text='Absolute Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Nominal Variable: Histogram&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: surface&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p></p> Code <pre><code>import plotly.express as px\n\n# Generate Pie Chart\nfig = px.pie(\n    data,\n    names=\"surface\",\n)\n\n# Adjust the plot\nfig.update_layout(\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Nominal Variable: Pie Chart&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: surface&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\nfig.update_traces(textposition='outside', textinfo='percent+label')\n\n# Show the plot\nfig.show()\n</code></pre> </li> </ul>"},{"location":"statistics/univariate/Frequency/#ordinal-scale","title":"Ordinal Scale","text":"<p>For ordinally scaled variables, the values also correspond to the possible categories. However, the internal order of these categories is relevant in the substantive analysis. The values should always be presented in either ascending or descending order. In order to tell <code>Python</code> the correct order, we need to define it first</p> <pre><code>round_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']\n</code></pre> <p>Afterwards we can use this order in the histogram <pre><code>fig = px.histogram(\n                data, \n                x=\"round\",\n                category_orders={\"round\": round_order}\n                )\n</code></pre></p> <p>and for the calculation of the corsstable <pre><code>data['round'] = pd.Categorical(data['round'], categories=round_order, ordered=True)\n</code></pre></p> Example: Graphical Representation of Ordinal Variables <ul> <li> <p>Histogram WITHOUT Order</p> <p></p> Code <pre><code>import plotly.express as px\n\n# Generate Histogram\nfig = px.histogram(\n    data, \n    x=\"round\",\n)\n# Adjust the plot\nfig.update_layout(\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Ordinal Variable: NO Order&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: round&lt;/span&gt;&lt;/b&gt;',\n        ),\n    xaxis_title_text='Round',\n    yaxis_title_text='Absolute Frequency',\n)\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Histogram WITH Order</p> <p></p> Code <pre><code>import plotly.express as px\n\n# Define the order of the ordinal variable\ndata_ord = data.copy()\nround_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']  # Define order of the rounds\n\n# HISTOGRAM sorted\n# Generate Histogram\nfig = px.histogram(\n    data_ord, \n    x=\"round\",\n    category_orders={\"round\": round_order[::-1]},\n)\n\n# Adjust the plot\nfig.update_layout(\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Ordinal Variable: WITH Order&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: round&lt;/span&gt;&lt;/b&gt;',\n        ),\n    xaxis_title_text='Round',\n    yaxis_title_text='Absolute Frequency',\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Table WITHOUT Order</p> round Absoulte Frequency Relative Frequency [%] F 68 2.278 QF 256 8.57 R128 416 13.93 R16 512 17.15 R32 880 29.47 R64 432 14.47 RR 286 9.58 SF 136 4.55 Code <pre><code># FREQUENCY TABLE unsorted\n# Generate table with absolute and relative frequencies\nabsolutefrequency_ord_unsort = pd.crosstab(index=data['round'], columns='Absoulte Frequency')\nrelativefrequency_ord_unsort = pd.crosstab(index=data['round'], columns='Relative Frequency [%]',normalize=True)*100\n\n# Combine the tables\nfrequencytable_ord_unsort = pd.concat([absolutefrequency_ord_unsort, relativefrequency_ord_unsort], axis=1).reset_index()\nfrequencytable_ord_unsort.columns.name = None\n\n# Show table\nprint(frequencytable_ord_unsort)\n</code></pre> </li> <li> <p>Table WITH Order</p> Round Absoulte Frequency Relative Frequency [%] F 68 2.28 SF 136 4.55 QF 256 8.57 R16 512 17.15 R32 880 29.47 R64 432 14.47 R128 416 13.93 RR 286 9.58 Code <pre><code># FREQUENCY TABLE sorte\nround_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']  # Define order of the roundsd\ndata['round'] = pd.Categorical(data['round'], categories=round_order, ordered=True)  # Define order of the rounds\n\n# Generate table with absolute and relative frequencies\nabsolutefrequency_ord_sort = pd.crosstab(index=data['round'], columns='Absoulte Frequency')\nrelativefrequency_ord_sort = pd.crosstab(index=data['round'], columns='Relative Frequency [%]',normalize=True)*100\n\n# Combine the tables\nfrequencytable_ord_sort = pd.concat([absolutefrequency_ord_sort, relativefrequency_ord_sort], axis=1).reset_index()\nfrequencytable_ord_sort.columns.name = None\n\n# Show table\nprint(frequencytable_ord_sort)\n</code></pre> </li> </ul> <p>In the case of ordinally scaled variables, a cumulative absolute or relative frequency can also be calculated. The cumulative absolute frequency indicates how often a reference value (or category) has not been exceeded. The cumulative relative frequency is this number divided by the total number of observations.</p> <p>To calculate the cumulative relative frequency in the histogram, we need to add the lines:  <pre><code>fig = px.histogram(\n                data, \n                x=\"round\",\n                category_orders={\"round\": round_order[::-1]},\n                cumulative=True,\n                histnorm=\"percent\"\n                )\n</code></pre> To do the same for the crosstab we need to add:</p> <pre><code>freq_rel_cum = pd.crosstab(\n                index=data['round'], \n                columns='Relative Frequency',\n                normalize=True\n                ).cumsum()\n</code></pre> Example: Cumulative Frequency of Ordinal Variables <ul> <li> <p>Histogram (Abolute, Cumulative)</p> <p></p> Code <pre><code>import plotly.express as px\n\n# HISTOGRAM sorted Cumulative Absolute\nround_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']  # Define order of the roundsd\ndata['round'] = pd.Categorical(data['round'], categories=round_order, ordered=True)  # Define order of the rounds\n\n# Generate Histogram\nfig = px.histogram(\n    data, \n    x=\"round\",\n    category_orders={\"round\": round_order[::-1]},\n    cumulative=True,\n)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Round',\n    yaxis_title_text='Absolute Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Ordinal Variable: Cumulated&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: round&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Histogram (Relative, Cumulative)</p> <p></p> Code <pre><code>import plotly.express as px\n# HISTOGRAM sorted cumulated Relative\nround_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']  # Define order of the roundsd\ndata['round'] = pd.Categorical(data['round'], categories=round_order, ordered=True)  # Define order of the rounds\n\n# Generate Histogram\nfig = px.histogram(\n    data, \n    x=\"round\",\n    category_orders={\"round\": round_order[::-1]},\n    cumulative=True,\n    histnorm=\"percent\"\n)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Round',\n    yaxis_title_text='Relative Frequency [%]',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Ordinal Variable: Cumulated&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: round&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Table (Abolute, Cumulative)</p> Round Absoulte Frequency Absoulte Frequency Cumulated F 68 68 SF 136 204 QF 256 460 R16 512 972 R32 880 1852 R64 432 2284 R128 416 2700 RR 286 2986 Code <pre><code># FREQUENCY TABLE sorted cumulated absolute\nround_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']  # Define order of the roundsd\ndata['round'] = pd.Categorical(data['round'], categories=round_order, ordered=True)  # Define order of the rounds\n\n# Generate table with absolute and relative frequencies\nabsolutefrequency_ord_sort = pd.crosstab(index=data['round'], columns='Absoulte Frequency')\nrelativefrequency_ord_sort = pd.crosstab(index=data['round'], columns='Relative Frequency [%]',normalize=True)*100\n\n# Combine the tables\nfrequencytable_ord_sort = pd.concat([absolutefrequency_ord_sort, relativefrequency_ord_sort], axis=1).reset_index()\nfrequencytable_ord_sort.columns.name = None\n\nfrequencytable_ord_sort['Absoulte Frequency Cumulated'] = frequencytable_ord_sort['Absoulte Frequency'].cumsum()\nfrequencytable_ord_sort.drop(columns='Relative Frequency [%]', inplace=True)\nprint(frequencytable_ord_sort)\n</code></pre> </li> <li> <p>Table (Relative, Cumulative)</p> Round Relative Frequency [%] Relative Frequency Cumulated F 2.28 2.28 SF 4.55 6.83 QF 8.57 15.41 R16 17.15 32.55 R32 29.47 62.02 R64 14.47 76.49 R128 13.93 90.42 RR 9.58 100 Code <pre><code># FREQUENCY TABLE sorted cumulated relative\nround_order = ['F', 'SF', 'QF', 'R16', 'R32', 'R64', 'R128', 'RR']  # Define order of the roundsd\ndata['round'] = pd.Categorical(data['round'], categories=round_order, ordered=True)  # Define order of the rounds\n\n# Generate table with absolute and relative frequencies\nabsolutefrequency_ord_sort = pd.crosstab(index=data['round'], columns='Absoulte Frequency')\nrelativefrequency_ord_sort = pd.crosstab(index=data['round'], columns='Relative Frequency [%]',normalize=True)*100\n\n# Combine the tables\nfrequencytable_ord_sort = pd.concat([absolutefrequency_ord_sort, relativefrequency_ord_sort], axis=1).reset_index()\nfrequencytable_ord_sort.columns.name = None\n\nfrequencytable_ord_sort['Relative Frequency Cumulated '] = frequencytable_ord_sort['Relative Frequency [%]'].cumsum()\nfrequencytable_ord_sort.drop(columns='Absoulte Frequency', inplace=True)\nprint(frequencytable_ord_sort)\n</code></pre> </li> </ul>"},{"location":"statistics/univariate/Frequency/#numeric-scale","title":"Numeric Scale","text":"<p>When the number of values \\( k \\) for a metrically scaled variable is small, it can be presented in the same way as an ordinal scale. However, when \\( k \\) is large, the representation can become cluttered and lose clarity.</p> Example: Few and Many Numeric Values <ul> <li> <p>Numeric Variable with Few of Values</p> <p></p> Code <pre><code>import plotly.express as px\n# HISTOGRAM Small Number of Values\n# Generate Histogram\nfig = px.histogram(\n    data, \n    x=\"draw_size\",\n)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Draw Size',\n    yaxis_title_text='Absolute Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Small Number of Values&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: draw_size&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Numeric Variable with Many Values</p> <p></p> Code <pre><code>import plotly.express as px\n# HISTOGRAM Large Number of Values\n# Generate Histogram\nfig = px.bar(\n    data['winner_rank_points'].value_counts().reset_index(), \n    x='winner_rank_points', \n    y='count'\n    )\n\n# Adjust the width of the bars\nfig.update_traces(width=50)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Winner Rank Points',\n    yaxis_title_text='Absolute Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Large Number of Values&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: winner_rank_points&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> </ul> <p>In such cases, categories (intervals or bins) should be created to reduce the number of displayed values, making the data easier to interpret. This can be done automatically e.g. by <code>px.histogram()</code> or manually:</p> <pre><code>data['points_cat'] = pd.cut(\n                        data['winner_rank_points'], \n                        bins=range(0,int(data['winner_rank_points'].max()),100), \n                        right=False)\n</code></pre> Example: Numeric Attribute Binning <ul> <li> <p>Automatic Binning</p> <p></p> Code <pre><code>import plotly.express as px\n\n# HISTOGRAM Large Number of Values\n# Generate Histogram\nfig = px.histogram(\n    data, \n    x=\"winner_rank_points\",\n)\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Winner Rank Points',\n    yaxis_title_text='Absolute Frequency',\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Automatic Binning&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: winner_rank_points&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> <li> <p>Manual Binning</p> <p></p> Code <pre><code>import plotly.express as px\n# Binning of the Data\ndata['points_cat'] = pd.cut(data['winner_rank_points'], bins=range(0,int(data['winner_rank_points'].max()),100), right=False) # 100 Bins between 0 and the maximum value of winner_rank_points\n# data_num['points_cat'] = pd.cut(data_num['winner_rank_points'], bins=[0, 60, 120, 180, 240, 300, 360, 420, 480, 540, 600, 660, 720, 780, 840, 900, 960, 1020, 1080, 1140, 1200], right=False) # Custom Bins\n\n# Count the values in each bin\npoints_cat_count = data['points_cat'].value_counts().sort_index()\npoints_cat_count.index = points_cat_count.index.astype(str)\n\n# Generate Bar Chart\nfig = px.bar(\n    points_cat_count,\n    )\n\n\n# Adjust the plot\nfig.update_layout(\n    xaxis_title_text='Winner Rank Points',\n    yaxis_title_text='Absolute Frequency',\n    showlegend=False,\n    title=dict(\n            text='&lt;b&gt;&lt;span style=\"font-size: 10pt\"&gt;Manual Binning&lt;/span&gt; &lt;br&gt; &lt;span style=\"font-size:5\"&gt;Data: atp_matches_2023.csv; variable: winner_rank_points&lt;/span&gt;&lt;/b&gt;',\n        ),\n)\n\n# Show the plot\nfig.show()\n</code></pre> </li> </ul> <p>Tables and charts are well-suited for providing an overview of the data. However, in some cases, it is beneficial to further condense the information within the data to reduce complexity. Nevertheless, care must be taken not to oversimplify, as this could lead to misleading interpretations. There are several key metrics available for further reducing complexity. These are typically divided into measures of central tendency and measures of dispersion.</p>"},{"location":"statistics/univariate/Frequency/#recap","title":"Recap","text":"<ul> <li>Data should always be the focus, with an unbiased representation.</li> <li>Frequencies indicate how often a particular value occurs.</li> <li>Relative frequency (%) is the absolute frequency divided by the total number of observations.</li> <li>The form of representation depends on the scale level of the variable.</li> <li>In general, both tables and charts can be used.</li> <li>Cumulative frequencies show how often a reference value has not been exceeded.</li> </ul>"},{"location":"statistics/univariate/Frequency/#tasks","title":"Tasks","text":"Task: Frequency Distribution <p>Use the following dataset: <pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \ncars = fetch_ucirepo(id=9) \n# https://archive.ics.uci.edu/dataset/9/auto+mpg\n\n# data (as pandas dataframes) \ndata = cars.data.features\ndata = data.join(cars.data.ids)\n\n# Show the first 5 rows\ndata.head()\n</code></pre> Work on the following task: </p> <ol> <li>Analyse the Dataset<ul> <li>Look at the website of the dataset and get familiar </li> </ul> </li> <li>Generate the following plot (think about attribute types, title, labeling of the axes)<ul> <li>Histogram | Absolute Frequency | Variable: <code>origin</code></li> <li>Bar Chart | Absoulte Frequency | no binning | Variable: <code>weight</code></li> <li>Histogram | Absoulte Frequency | automatic binning | Variable: <code>weight</code> </li> <li>Histogram | Relative Frequency | cumulated | Variable: <code>hoursepower</code></li> <li>Pie Chart | Relative Frequency | Variable: <code>cylinders</code></li> </ul> </li> </ol>"},{"location":"yolo/","title":"Introduction","text":"Project Setup <p>This chapter serves as an introduction to the topic of computer vision. We'll explore various tasks, demonstrating their use with code snippets. Even though this is just an introductory chapter and you might not grasp all the details yet, we encourage you to run the code on your own computer.</p> <p>To follow along, we recommend setting up a new project folder with a Jupyter notebook. Additionally, create a new virtual environment  and activate it. Install the required packages:</p> <pre><code>pip install ultralytics opencv-python pytesseract deepface tf-keras\n</code></pre> <p>Your project structure should look like this:</p> <pre><code>\ud83d\udcc1 vision_intro/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc4 input.jpg\n\u2514\u2500\u2500 \ud83d\udcc4 cv_intro.ipynb\n</code></pre> <p>Computer Vision is a field of artificial intelligence that enables machines to interpret and understand the visual world. By using digital images from cameras and videos along with deep learning models, machines can accurately identify and classify objects - and then react to what they \"see.\"</p> <p>In this introduction, we'll delve into the basics of computer vision, its challenges, and how it's interconnected with other fields. Let's embark on this visual journey together!</p>"},{"location":"yolo/#what-is-computer-vision","title":"What Is Computer Vision?","text":"<p>Before diving into computer vision, let's briefly touch upon artificial intelligence (AI). AI is a broad field aiming to create systems capable of performing tasks that typically require human intelligence. As one of the pioneers of AI, John McCarthy, described it:</p> <p>\"An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves.\"</p> <p>-- John McCarthy</p> <p>Artificial Intelligence is a multidisciplinary field divided into several subfields, each contributing to simulating intelligent behavior in machines. These include:</p> <ul> <li>Machine Learning</li> <li>Natural Language Processing</li> <li>Robotics</li> <li>Computer Graphics</li> <li>Computer Vision</li> </ul> <p>These subfields are interconnected; advancements in one often benefit the others. For instance, computer vision is essential in robotics for environment perception and in natural language processing for image captioning.</p> <p>But now we still want to know: What is computer vision exactly?</p> <p>At its core, computer vision seeks to automate tasks that the human visual system can do. It involves techniques for acquiring, processing, analyzing, and understanding images to produce numerical or symbolic information.</p> Biological Vision (Source: Ai   Miquel Perello Nieto on Wikipedia)  Interesting Fact <p>Did you know, that over 50% of the processing in the human brain is devoted directly or indirectly to visual information (Source: MIT News)</p> <p>In other words, computer vision transforms visual data into meaningful information. Now, let's explore some typical computer vision  tasks and see how they come to life through examples you can try yourself!</p>"},{"location":"yolo/#typical-computer-vision-tasks","title":"Typical Computer Vision Tasks","text":""},{"location":"yolo/#classification","title":"Classification","text":"<p>Assigning objects within an image to predefined categories or classes.</p> Example: Classification <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> &gt;&gt;&gt; Output<pre><code>1 person, 1 dog\n</code></pre> </li> </ul> Code <pre><code>from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load an official model\n\n# Predict with the model\nresults = model(\"dog.jpg\")  # predict on an image\n</code></pre>"},{"location":"yolo/#localization","title":"Localization","text":"<p>Determining the exact location of an object within an image.</p>"},{"location":"yolo/#detection","title":"Detection","text":"<p>Identifying and locating multiple objects within an image, effectively combining classification and localization.</p> Example: Detection <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load an official model\n\n# Predict with the model\nresults = model(\"dog.jpg\", save=True)  # predict on an image\nresults[0].show()  # display the image\n</code></pre>"},{"location":"yolo/#segmentation","title":"Segmentation","text":"<p>Precisely delineating the pixels that belong to an object, separating it from the background.</p> Example: Segmentation <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-seg.pt\")  # load an official model\n\n# Predict with the model\nresults = model(\"dog.jpg\", save=True)  # predict on an image\nresults[0].show()  # display the image\n</code></pre>"},{"location":"yolo/#tracking","title":"Tracking","text":"<p>Monitoring the movement of objects over time in videos or live streams, analyzing factors like velocity and relative position.</p> Example: Tracking <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>from collections import defaultdict\nimport cv2\nimport numpy as np\n\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Open the video file\nvideo_path = \"street2.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Store the track history\ntrack_history = defaultdict(lambda: [])\n\nvideo = cv2.VideoWriter(\"output.mp4\", 0, 25, (960,540))\n\n# Loop through the video frames\nwhile cap.isOpened():\n    # Read a frame from the video\n    success, frame = cap.read()\n\n    if success:\n        # Run YOLO11 tracking on the frame, persisting tracks between frames\n        results = model.track(frame, persist=True, classes=[2])\n\n        # Get the boxes and track IDs\n        boxes = results[0].boxes.xywh.cpu()\n        track_ids = results[0].boxes.id.int().cpu().tolist()\n\n        # Visualize the results on the frame\n        annotated_frame = results[0].plot()\n\n        # Plot the tracks\n        for box, track_id in zip(boxes, track_ids):\n            x, y, w, h = box\n            track = track_history[track_id]\n            track.append((float(x), float(y)))  # x, y center point\n            if len(track) &gt; 30:  # retain 90 tracks for 90 frames\n                track.pop(0)\n\n            # Draw the tracking lines\n            points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n            cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n\n        # Display the annotated frame\n        cv2.imshow(\"YOLO11 Tracking\", annotated_frame)\n        video.write(annotated_frame)\n        # Break the loop if 'q' is pressed\n        if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n            break\n    else:\n        # Break the loop if the end of the video is reached\n        break\n\n# Release the video capture object and close the display window\ncap.release()\ncv2.destroyAllWindows()\nvideo.release()\n</code></pre>"},{"location":"yolo/#optical-character-recognition","title":"Optical Character Recognition","text":"<p>Recognizing and extracting printed or handwritten text from images, enabling machines to read and process written information.</p> Example: OCR <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> &gt;&gt;&gt; Output<pre><code>LAN DOR.\n\nCHAPTER IL\n\nBIRTH AND PARENTAGE\u2014SCHOOL \u2014 COLLEGE.\n(1775 \u20141794.)\n\nFew men have ever impressed their peers so much, or the\ngeneral public so little, as Watrer Savage Lanpor. Of\nall celebrated authors, he has hitherto been one of the\nleast popular. Nevertheless he is among the most strik-\ning figures in the history of English literature ; striking\nalike by his character and his powers. Personally, Landor\nexercised the spell of genius upon every one who came\nnear him. His gifts, attainments, impetuosities, his\noriginality, his force, his charm, were all of the same\nconspicuous and imposing kind. Not to know what is\nto be known of so remarkable a man is evidently to be a\nloser. Not to be familiar with the works of so noble\n</code></pre> </li> </ul> Code Warning <p>To run the code, you need to install tesseract on your PC. This can be a tricky process, especially on MacOS. Therefore it is okay to skip this example if you want.    </p> <pre><code># Need to install tesseract on your PC https://www.nutrient.io/blog/how-to-use-tesseract-ocr-in-python/\nfrom PIL import Image\nimport pytesseract\n\nprint(pytesseract.image_to_string(Image.open('scan.png')))\n</code></pre>"},{"location":"yolo/#facial-recognition-and-analysis","title":"Facial Recognition and Analysis","text":"<p>Identifying individuals based on their facial features and recognizing various facial expressions.</p> Example: Facial Recognition <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> <p> (Source: Wikipedia)</p> Code <pre><code># Load Packages\nimport cv2\nimport matplotlib.pyplot as plt\nfrom deepface.modules  import streaming  # Corrected import path\nfrom deepface import DeepFace\n\n# Load Image\nimg_path = \"pic_cv_approaches/obama.jpg\"\nimg = cv2.imread(img_path)\nraw_img = img.copy()\n\n# Analyze Image\ndemographies = DeepFace.analyze(img_path=img_path, actions=(\"age\", \"gender\", \"emotion\"))\ndemography = demographies[0]\n\n# Get Region of Interest\nx = demography[\"region\"][\"x\"]\ny = demography[\"region\"][\"y\"]\nw = demography[\"region\"][\"w\"]\nh = demography[\"region\"][\"h\"]\n\n# Overlay Emotion\nimg = streaming.overlay_emotion(img=img, emotion_probas=demography[\"emotion\"], x=x, y=y, w=w, h=h)\n\n# Overlay Age and Gender\nimg = streaming.overlay_age_gender(img=img, apparent_age=demography[\"age\"], gender=demography[\"dominant_gender\"][0:1], x=x, y=y, w=w, h=h)\n\n# Display Image\nplt.imshow(img[:, :, ::-1])\nplt.axis('off')\nplt.show()\n\n# Save the image with overlays\ncv2.imwrite(\"obama_out.jpg\", img)\n</code></pre>"},{"location":"yolo/#pose-estimation","title":"Pose Estimation","text":"<p>Determining the position and orientation of an object or person relative to a reference point or coordinate system.</p> Example: Pose Estimation <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load an official model\n\n# Predict with the model\nresults = model(\"dog.jpg\", save=True)  # predict on an image\nresults[0].show()  # display the image\n</code></pre> <p>These tasks represent the core of computer vision, each contributing to its wide-ranging real-world applications. From enabling machines to read and understand handwritten documents to enhancing interactive gaming experiences through accurate motion tracking, the advancements in computer vision are transforming industries and everyday life.</p> <p>And: as you can see, they're not just theoretical concepts - you can try them out yourself !</p>"},{"location":"yolo/#applications","title":"Applications","text":"<p>Computer vision has a wide range of applications across various industries.</p> Possible Applications for Computer Vision RoboticsAutonomous VehiclesMedicalQuality ControlRetailFacial Recognition <p>Robots use computer vision to navigate and interact with their environment.</p> <p> </p> <p>Self-driving cars rely heavily on computer vision to perceive the road and make driving decisions.</p> <p> </p> <p>Computer vision aids in medical imaging for diagnostics and treatment planning.</p> <p> (Source: Newe A, Ganslandt T on Wikipedia) </p> <p>Automated inspection systems detect defects in manufacturing processes.</p> <p> (Source: elunic) </p> <p>Augmented reality shopping experiences enhance customer engagement.</p> <p> (Source: SNAP INC via Forbes) </p> <p>Used for security and authentication purposes.</p> <p> (Source: Sylenius on Wikipedia) </p>"},{"location":"yolo/#how-can-machines-see","title":"How can Machines \"See\"?","text":"<p>When we look at the world, our eyes receive light reflected from objects. Similarly, cameras capture light to create images. </p>          Camera sensor prinicpal (Source: Neg)       <p>However, interpreting these images to understand the scene involves complex algorithms that can discern patterns, shapes, and colors. This process involves several steps:</p> <ol> <li>Image Acquisition: Capturing the visual data using cameras or sensors.</li> <li>Preprocessing: Enhancing image quality and correcting distortions.</li> <li>Feature Extraction: Identifying edges, textures, and other significant parts of the image.</li> <li>High-Level Processing: Recognizing objects, understanding scenes, and making decisions.</li> </ol>"},{"location":"yolo/#challenges-in-computer-vision","title":"Challenges in Computer Vision","text":"<p>Despite the advancements, computer vision faces several challenges. Let's explore them.</p> <ul> <li> <p>Inverse Problem </p> <p>One of the fundamental challenges in computer vision is the inverse problem: Reconstructing a 3D scene from a 2D image is challenging because multiple 3D scenes can produce the same 2D projection.</p> </li> <li> <p> (Source: mosso on Wikipedia)  </p> </li> <li> <p> (Source: Palazzi et al at Computer.org)  </p> </li> <li> <p>Variability Due to Viewpoint </p> <p>An object can look vastly different from various angles. For example, a car viewed from the front, side, or top presents different shapes and features, complicating recognition tasks.</p> </li> <li> <p>Deformation </p> <p>Non-rigid objects, like clothing or human bodies, can change shape, making it challenging to maintain consistent recognition.</p> </li> <li> <p> </p> </li> <li> <p> </p> </li> <li> <p>Occlusion </p> <p>Objects in images often block parts of other objects. Detecting partially visible objects requires algorithms to infer the hidden parts.</p> </li> <li> <p>Illumination </p> <p>Lighting conditions can alter the appearance of objects. An apple under bright sunlight looks different from one under indoor lighting.</p> </li> <li> <p> (Source: Flocutus)  </p> </li> <li> <p> (Source: Osi on Wikipedia)  </p> </li> <li> <p>Motion Blur </p> <p>Movement during image capture can blur images, obscuring details necessary for recognition.</p> </li> <li> <p>Optical Illusions </p> <p>Our perception can be deceived by optical illusions, where our brain interprets images differently from the actual measurements.</p> </li> <li> <p> </p> </li> <li> <p> (Source: Nizar Massouh on ResearchGate)  </p> </li> <li> <p>Intra Class Variation </p> <p>Objects within the same category can look very different.Chairs come in numerous designs\u2014armchairs, stools, recliners\u2014but they all serve the same function. Recognizing all variations as \"chairs\" is challenging for computer vision systems.</p> </li> <li> <p>Number of Categories </p> <p>There are thousands of object categories, each with its own variations. Building systems that can recognize all of them requires extensive data and sophisticated algorithms.</p> </li> <li> <p> (Source: Cees Snoek on ResearchGate)  </p> </li> </ul> <p>By understanding these challenges, you're better equipped to appreciate the complexities involved in teaching machines to see.</p> <p>Congratulations! You've taken your first steps into the world of computer vision. Feel free to experiment with the code examples provided and explore further. In the next chapters, we'll delve deeper into specific algorithms and techniques.</p> <p>See you in the next chapter! \ud83d\udc4b</p>"},{"location":"yolo/approaches/","title":"Approaches in CV","text":"Running the Code <p>In this chapter, we'll explore different approaches in computer vision. We'll delve into traditional methods and then move on to deep learning techniques. We'll provide code snippets along the way, so feel free to run them on your own machine!</p> <p>To get started, ensure you have the necessary packages installed. If you haven't already, set up a new project folder with a Jupyter notebook and activate a new virtual environment. Then, install the required packages:</p> <pre><code>pip install torch torchvision matplotlib ultralytics opencv-python scikit-image\n</code></pre> <p>Your project structure should look like this:</p> <pre><code>\ud83d\udcc1 vision_approaches/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc4 input.jpg\n\u2514\u2500\u2500 \ud83d\udcc4 cv_approaches.ipynb\n</code></pre> <p>To tackle the complex problems in computer vision, various approaches have been developed over the years. In this chapter, we'll journey from traditional methods to the cutting-edge deep learning techniques that are revolutionizing the field.</p>"},{"location":"yolo/approaches/#traditional-approaches-in-object-detection","title":"Traditional Approaches in Object Detection","text":"<p>Before the rise of deep learning, object detection relied heavily on handcrafted features and traditional machine learning techniques. Let's explore some of these foundational methods.</p>"},{"location":"yolo/approaches/#haar-cascades","title":"Haar Cascades","text":"<p>Introduced by Viola and Jones in 2001, Haar-like features were used for rapid face detection by scanning an image at multiple scales and positions. Haar Cascades work by training a model with positive and negative images, where the positive images contain the object to detect (e.g., faces), and the negative images do not. </p> Process <ul> <li>The model scans the entire image at different sizes (scales) and positions.</li> <li>It uses simple patterns (called Haar-like features) to look for areas that resemble the object.</li> <li>By combining these simple features, the model can decide whether the object is present in a particular area of the image.</li> </ul> <ul> <li> Advantage <ul> <li>Real-Time Detection: Haar Cascades offer fast detection speeds, making them suitable for real-time applications.</li> <li>Low Computational Resources: They require relatively low computational power compared to more complex algorithms.</li> </ul> </li> <li> Disadvantage <ul> <li>Limited Accuracy: They may produce a high rate of false positives and are less accurate in detecting objects under varying lighting and orientations.</li> <li>Rigid Training Process: Training Haar Cascades requires a large amount of positive and negative images, and they are not easily adaptable to new object classes without retraining.</li> </ul> </li> </ul> <p>There are already available pretrained models which we can try right away. Let's try them out!</p> Example: Haar Cascades <p> </p> Code <pre><code>#Source: https://medium.com/analytics-vidhya/haar-cascades-explained-38210e57970d\n\n#-- Load Packages\nimport cv2\nfrom skimage import data\n\n#-- Load Image and Convert to RGB\nimage = data.astronaut()\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert to RGB\n\n#-- Load Haar Cascades\nf_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\ne_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n\n#-- Detect Faces and Eyes\nfaces = f_cascade.detectMultiScale(image, 1.3, 5)\nfor (x,y,w,h) in faces:\n    img = cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n    roi_color = img[y:y+h, x:x+w]\n    eyes = e_cascade.detectMultiScale(roi_color)\n    for (ex,ey,ew,eh) in eyes:\n        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n\n#-- Display and Save Image\ncv2.imwrite('output_haar_cascade.jpg',image)\ncv2.imshow('Haar Cascade',image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"yolo/approaches/#hog","title":"HOG","text":"<p>Proposed for human detection, Histogram of Oriented Gradients (HOG) features capture edge orientations and are combined with Support Vector Machines (SVMs) for classification.</p> Process <ul> <li>Edge Direction Detection: HOG focuses on the edges within an image. It looks at the direction in which the edges or lines in the image are pointing. Think of it as analyzing the outline or shape of objects by observing where the edges go.</li> <li>Counting Edge Directions Locally: The image is divided into small regions or cells. In each of these small areas, HOG counts how many edges point in each direction. It's like keeping track of the directions of all the tiny lines within that small patch.</li> <li>Creating a Histogram: For each cell, HOG creates a histogram (a simple bar chart) that represents the number of edges pointing in various directions. This captures the local shape information.</li> <li>Combining Information: These histograms from all the cells are combined to form a detailed description of the entire image's shape and structure.</li> <li>Object Detection with SVM: This combined information is then fed into a machine learning algorithm called a Support Vector Machine (SVM). The SVM uses this data to classify the image - for example, determining whether a human is present in the image or not.</li> </ul> <p>In essence, HOG helps computers understand and recognize objects by analyzing the directions of edges in an image, much like how we might recognize a shape by its outline.</p> <ul> <li> Advantage <ul> <li>Effective Feature Representation: HOG descriptors are robust in capturing shape and appearance information, improving object detection performance.</li> <li>Invariance to Illumination and Geometric Transformations: They are relatively invariant to changes in illumination and small geometric transformations.</li> </ul> </li> <li> Disadvantage <ul> <li>Computationally Intensive: Calculating HOG features can be time-consuming, which may not be ideal for real-time applications.</li> <li>Not Deep Learning-Based: HOG relies on manual feature extraction and may not capture complex patterns as effectively as deep learning methods.</li> </ul> </li> </ul> <p>Let's see it in action.</p> Example: HOG <p> </p> Code <pre><code>#Source: https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_hog.html\n\n#-- Load Packages\nimport cv2\nfrom skimage.feature import hog\nfrom skimage import data, exposure\n\n#-- Load Image\nimage = data.astronaut()\n\n#-- Compute HOG\nfd, hog_image = hog(\n    image,\n    orientations=8,\n    pixels_per_cell=(16, 16),\n    cells_per_block=(1, 1),\n    visualize=True,\n    channel_axis=-1,\n)\n# Rescale histogram for better display\nhog_image= exposure.rescale_intensity(hog_image, in_range=(0, 10))\nhog_image = cv2.normalize(hog_image, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n\n#-- Display and Save Image\ncv2.imwrite('output_hog.jpg',hog_image)\ncv2.imshow('HOG',hog_image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre> Task: Haar Cascades &amp; HOG <p>Now it's your turn! Try running the code from above for </p> <ul> <li>Haar Cascade  </li> <li>HOG </li> </ul> <p>Can you detect faces and eyes in a different image? Replace <code>data.astronaut()</code> with your own image and see how it works!</p> <p>If you're interested in learning more about Haar Cascades and HOG, check out this comparative article</p>"},{"location":"yolo/approaches/#deep-learning-approaches","title":"Deep Learning Approaches","text":"<p>As datasets grew and computational power increased, deep learning methods began to outperform traditional techniques, especially in complex tasks like object detection.</p> Convolutional Neural Network Architecture (Source: Geeksforgeeks.org) <p>Deep learning models, particularly Convolutional Neural Networks (CNNs), automatically learn hierarchical feature representations from images, eliminating the need for manual feature extraction. In this section, we'll explore deep learning approaches to object detection, focusing on both multi-stage and one-stage detectors.</p> Info <p>Deep learning models can learn complex patterns directly from data. This ability has significantly improved the performance of object detection systems, making them more accurate and robust.</p>"},{"location":"yolo/approaches/#multi-stage-object-detection","title":"Multi-Stage Object Detection","text":"<p>Multi-stage object detection approaches the detection problem in sequential steps, typically starting with region proposals followed by classification. This methodology emerged from the need to combine localization and classification effectively. Let's explore some key models in this category.</p>"},{"location":"yolo/approaches/#r-cnn","title":"R-CNN","text":"<p>R-CNN, introduced in 2014, was a breakthrough in applying deep learning to object detection. It follows a three-step process:</p> Process <ol> <li>Region Proposal: The algorithm starts by scanning the image to find areas that might contain an object. It creates a bunch of boxes (called region proposals) that potentially surround objects in the image. Think of it as highlighting all the spots where something interesting might be.</li> <li>Feature Extraction: Each proposed region is warped to a fixed size and passed through a pre-trained CNN (often AlexNet at the time) to extract a feature vector.</li> <li>Classification: The extracted features are fed into Support Vector Machines (SVMs) to classify the presence of objects in each region. A separate regression model refines the bounding box coordinates.</li> </ol> R-CNN Architecture (Source: Geeksforgeeks.org) <ul> <li> Advantage <ul> <li>Improved Accuracy: Leveraging CNNs for feature extraction significantly enhances detection accuracy over traditional methods.</li> <li>Modular Design: Allows the use of different region proposal methods and classifiers, providing flexibility in the pipeline.</li> </ul> </li> <li> Disadvantage <ul> <li>Slow Processing Time: Each region proposal is processed individually through the CNN, leading to long training and inference times.</li> <li>High Storage Requirements: Requires a large amount of storage for caching features extracted from region proposals.</li> </ul> </li> </ul>"},{"location":"yolo/approaches/#fast-r-cnn","title":"Fast R-CNN","text":"<p>Fast R-CNN, published in 2015, addressed several inefficiencies of R-CNN while maintaining its accuracy. Key improvements include:</p> Process <ol> <li>Single CNN Pass: Instead of running the CNN thousands of times on each region proposal, Fast R-CNN processes the entire image once to create a feature map, then projects the region proposals onto this map.</li> <li>RoI Pooling: This layer transforms regions of different sizes into fixed-size feature vectors efficiently, enabling end-to-end training.</li> <li>Multi-task Learning: The network simultaneously predicts object class probabilities and bounding box coordinates, eliminating the need for separate SVM classifiers</li> </ol> Fast R-CNN Architecture (Source: Geeksforgeeks.org) <ul> <li> Advantage <ul> <li>Faster Training and Inference: Reduces computation by sharing convolutions across proposals, making it significantly faster than R-CNN.</li> <li>End-to-End Training: Simplifies the training process by allowing the entire network to be trained jointly.</li> </ul> </li> <li> Disadvantage <ul> <li>Dependency on External Proposals: Still relies on external region proposal algorithms like Selective Search, which can be slow.</li> <li>Memory Intensive: Processing large images with many proposals can consume substantial memory resources.</li> </ul> </li> </ul>"},{"location":"yolo/approaches/#faster-r-cnn","title":"Faster R-CNN","text":"<p>Faster R-CNN, also from 2015, introduced the Region Proposal Network (RPN), making the entire object detection pipeline trainable end-to-end. This architecture consists of two main networks:</p> Process <ol> <li>Region Proposal Network (RPN):<ul> <li>Slides a small network over the CNN feature map</li> <li>At each location, predicts multiple potential object regions using anchor boxes</li> <li>Outputs \"objectness\" scores and box coordinates for each proposal</li> </ul> </li> <li>Detection Network:<ul> <li>Similar to Fast R-CNN</li> <li>Uses RoI Pooling on proposals from RPN</li> <li>Outputs final classifications and refined box coordinates</li> </ul> </li> </ol> <p>Both the RPN (which proposes potential object regions) and the part of the network that classifies these regions use the same underlying data from the image. In other words, they share the same convolutional features extracted from the image. This means the heavy processing of the image is done only once, and both tasks use this shared information.</p> Faster R-CNN Architecture (Source: Geeksforgeeks.org) <ul> <li> Advantage <ul> <li>Significant Speed Improvement: Eliminates the need for external proposal algorithms, making detection faster.</li> <li>High Accuracy: Maintains high detection accuracy while improving computational efficiency.</li> </ul> </li> <li> Disadvantage <ul> <li>Complex Architecture: The addition of the RPN adds complexity, making the model harder to implement and tune.</li> <li>Hardware Requirements: May require powerful GPUs to achieve real-time performance due to computational demands.</li> </ul> </li> </ul> Example: Object Detection with Faster R-CNN <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>import torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load a pre-trained Faster R-CNN model\nmodel = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nmodel.eval()\n\n# Transform the input image\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n# Load and transform the image\nimage = Image.open('dog.jpg')\nimage_tensor = transform(image)\n\n# Perform object detection\nwith torch.no_grad():\n    outputs = model([image_tensor])\n\n# Visualize the results\nlabels = outputs[0]['labels'].numpy()\nscores = outputs[0]['scores'].detach().numpy()\nboxes = outputs[0]['boxes'].detach().numpy()\n\n# COCO dataset label names (for mapping label IDs to names)\nCOCO_INSTANCE_CATEGORY_NAMES = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A',\n    'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n    'N/A', 'N/A', 'toilet', 'N/A', 'TV', 'laptop', 'mouse', 'remote', 'keyboard',\n    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\n# Plot the image with bounding boxes and labels\nfig, ax = plt.subplots(1)\nax.imshow(image)\nax.axis('off')  # Turn off the axes\nfor idx, box in enumerate(boxes):\n    if scores[idx] &gt; 0.8:\n        xmin, ymin, xmax, ymax = box\n        width, height = xmax - xmin, ymax - ymin\n        rect = plt.Rectangle((xmin, ymin), width, height, fill=False, color='red', linewidth=2)\n        ax.add_patch(rect)\n        # Add label with score\n        label_text = f\"{COCO_INSTANCE_CATEGORY_NAMES[labels[idx]]}: {scores[idx]:.2f}\"\n        ax.text(xmin, ymin - 5, label_text, color='red', fontsize=10, backgroundcolor='white')\nplt.tight_layout()\nplt.show()\n</code></pre> Task: Faster R-CNN <p>Again it's your turn! Try running the Faster R-CNN code from above. Perform following tasks: </p> <ul> <li>Can you detect objects in a different image? Try your own image and see how it works!</li> <li>What is the <code>COCO_INSTANCE_CATEGORY_NAMES</code> variable? Do some research.</li> </ul>"},{"location":"yolo/approaches/#one-stage-object-detection","title":"One-Stage Object Detection","text":"<p>One-stage detectors aim to predict object classes and bounding boxes directly from image pixels in a single network forward pass, without the region proposal step. This approach trades some accuracy for significant speed improvements.</p>"},{"location":"yolo/approaches/#ssd","title":"SSD","text":"<p>SSD, introduced in 2016, is a one-stage object detection model that performs object localization and classification in a single forward pass of the network, using default boxes of different scales and aspect ratios.</p> Process <ul> <li>One-Step Detection: SSD performs object detection in a single pass through the neural network. This means it looks at the image once and simultaneously figures out where objects are (localization) and what they are (classification). There's no separate step for proposing regions where objects might be.</li> <li>Default Boxes of Various Sizes: SSD uses a set of predefined boxes called default boxes or anchor boxes. These boxes come in different sizes and shapes (scales and aspect ratios) and are spread out across the image at various locations. The network checks each of these boxes to see if there's an object inside and determines the object's class (like a car, person, or dog).</li> </ul> SSD Architecture (Source: Medium) <ul> <li> Advantage <ul> <li>Real-Time Detection: Capable of high-speed detection suitable for real-time applications.</li> <li>Simplified Architecture: Combines localization and classification tasks into one network, simplifying deployment.</li> </ul> </li> <li> Disadvantage <ul> <li>Difficulty with Small Objects: Tends to have lower accuracy in detecting small objects compared to two-stage detectors.</li> <li>Trade-Off Between Speed and Accuracy: While faster, it may not achieve the same accuracy levels as more complex models like Faster R-CNN.</li> </ul> </li> </ul> Example: Object Detection with SSD <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>import torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load a pre-trained SSD model\nmodel = models.detection.ssd300_vgg16(pretrained=True)\nmodel.eval()\n\n# Transform the input image\ntransform = transforms.Compose([\n    transforms.Resize((300, 300)),\n    transforms.ToTensor(),\n])\n\n# Load and transform the image\nimage = Image.open('dog.jpg')\noriginal_width, original_height = image.size\nimage_tensor = transform(image)\n\n# Perform object detection\nwith torch.no_grad():\n    outputs = model([image_tensor])\n\n# Extract results\nlabels = outputs[0]['labels'].numpy()\nscores = outputs[0]['scores'].detach().numpy()\nboxes = outputs[0]['boxes'].detach().numpy()\n\n# Scale boxes back to original image size\nboxes[:, [0, 2]] *= original_width / 300  # Scale x-coordinates\nboxes[:, [1, 3]] *= original_height / 300  # Scale y-coordinates\n\n# Plot the image with bounding boxes and labels\nfig, ax = plt.subplots(1)\nax.imshow(image)\nax.axis('off')  # Turn off the axes\nfig.subplots_adjust(left=0, right=1, top=1, bottom=0)  # Remove padding/margins\nfor idx, box in enumerate(boxes):\n    if scores[idx] &gt; 0.5:\n        xmin, ymin, xmax, ymax = box\n        width, height = xmax - xmin, ymax - ymin\n        rect = plt.Rectangle((xmin, ymin), width, height, fill=False, color='red', linewidth=2)\n        ax.add_patch(rect)\n        # Add label with score\n        label_text = f\"{COCO_INSTANCE_CATEGORY_NAMES[labels[idx]]}: {scores[idx]:.2f}\"\n        ax.text(xmin, ymin - 5, label_text, color='red', fontsize=10, backgroundcolor='white')\n\n# Save the plot to remove additional padding (optional)\nplt.savefig(\"output_ssd.png\", bbox_inches='tight', pad_inches=0, dpi=300)\nplt.show()\n</code></pre> Task: SSD <p>Now try to perform a SSD. Perform following tasks: </p> <ul> <li>Can you detect obects in a different image? Replace with your own image and see how it works! </li> <li>Also try adjusting the confidence threshold in the code to see how it affects the detection results. (Hint: take a look at <code>scores</code>)</li> </ul>"},{"location":"yolo/approaches/#yolo","title":"YOLO","text":"<p>YOLO (You Only Look Once), introduced by Redmon et al., is an object detection algorithm that treats detection as a regression problem, directly predicting bounding boxes and class probabilities from full images in one evaluation, enabling end-to-end real-time detection. Since we will focus on YOLO in the next couple sections, we will not further discuss the process here. </p> <ul> <li> Advantage <ul> <li>Extremely Fast Performance: Designed for real-time detection with high frame rates.</li> <li>Unified Model Architecture: Simplifies the detection pipeline by using a single neural network.</li> </ul> </li> <li> Disadvantage <ul> <li>Struggles with Small Objects: May miss small objects or closely packed objects due to spatial constraints in its grid system.</li> <li>Localization Errors: Can be less accurate in localizing objects precisely compared to two-stage detectors.</li> </ul> </li> </ul> Example: Object Detection with YOLOv8 <ul> <li> <p>Input</p> <p> </p> </li> <li> <p>Output</p> <p> </p> </li> </ul> Code <pre><code>from ultralytics import YOLO\n\n# Load a pre-trained YOLOv8 model\nmodel = YOLO(\"yolov8n.pt\")\n\n# Perform object detection on an image\nresults = model(\"input.jpg\", save=True, conf=0.5)\n\n# Display the results\nresults[0].show()\n</code></pre>"},{"location":"yolo/approaches/#comparison","title":"Comparison","text":"Method Speed Accuracy Complexity Suitable For R-CNN Slow High High Research, small datasets Fast R-CNN Moderate High Moderate Applications requiring accuracy Faster R-CNN Moderate Very High Moderate Balanced speed and accuracy SSD Fast Moderate-High Low Real-time applications YOLO Very Fast High Low Real-time applications <p> Key Takeaways:</p> <ul> <li>Multi-Stage Detectors: Offer higher accuracy but at the cost of speed and computational complexity.</li> <li>One-Stage Detectors: Provide faster inference suitable for real-time applications, with a trade-off in accuracy.</li> </ul>"},{"location":"yolo/approaches/#conclusion","title":"Conclusion","text":"<p>You've journeyed through the evolution of object detection methods, from traditional techniques like Haar Cascades and HOG to advanced deep learning models like Faster R-CNN, SSD, and YOLO.</p> <p>Object detection has evolved significantly, with deep learning methods pushing the boundaries of what's possible. Multi-stage detectors like Faster R-CNN provide high accuracy but can be computationally intensive. One-stage detectors like SSD and YOLO offer faster detection suitable for real-time applications.</p> \ud83c\udf89 Congratulations <p>You've gained an understanding of deep learning approaches in object detection!</p> <p>Data without context is noise! (With Zoom) byu/Anxious_City_7864 indatascience</p>"},{"location":"yolo/approaches/#whats-next","title":"What's Next?","text":"<p>In the upcoming sections, we'll delve deeper into YOLO, explore training custom models, and discuss real-world applications.</p> <p>References:</p> <ul> <li>Liu, W. et al. (2016). \"SSD: Single Shot MultiBox Detector\". European Conference on Computer Vision (ECCV).</li> <li>Girshick, R. (2015). \"Fast R-CNN\". IEEE International Conference on Computer Vision (ICCV).</li> <li>Ren, S. et al. (2016). \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\". IEEE Transactions on Pattern Analysis and Machine Intelligence.</li> <li>Redmon, J. et al. (2016). \"You Only Look Once: Unified, Real-Time Object Detection\". IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</li> </ul> <p>Further Reading:</p> <ul> <li>Understanding SSD MultiBox\u2014Real-Time Object Detection In Deep Learning</li> <li>R-CNN, Fast R-CNN, Faster R-CNN, YOLO \u2014 Object Detection Algorithms</li> <li>YOLOv8 vs SSD: Choosing the Right Object Detection Model</li> </ul>"},{"location":"yolo/yolo/","title":"Getting Started with YOLO","text":""},{"location":"yolo/yolo/#introduction","title":"Introduction","text":"<p>Remember the object detection approaches we covered earlier? YOLO revolutionized this field when it was introduced in 2015. Instead of using complex pipelines or scanning an image multiple times, YOLO takes a refreshingly simple approach: it looks at the image just once (hence the name \"You Only Look Once\") to detect all objects.</p> Historical Context <p>When YOLO was first released, object detection systems were complex multi-stage pipelines. The original paper titled \"You Only Look Once: Unified, Real-Time Object Detection\" by Joseph Redmon et al. introduced a radically different approach that would change the field forever.</p>"},{"location":"yolo/yolo/#the-yolo-approach","title":"The YOLO Approach","text":"<p>Let's break down how YOLO works in simple terms:</p> <ol> <li>Grid Division: YOLO first divides your image into a grid (say 13x13).</li> <li>Grid Cells Predictions: Each cell in the grid is responsible for predicting objects centered in that cell. Each cell predicts a certain number of bounding boxes and confidence scores for those boxes. A confidence score reflects how confident the model is that the box contains an object and also how accurate it thinks the box is.</li> <li>Bounding Box Parameters: Each bounding box has five predictions: <code>x</code>, <code>y</code>, <code>w</code>, <code>h</code>, and a confidence score. (<code>x</code>, <code>y</code>) coordinates represent the center of the box relative to the bounds of the grid cell. Width (<code>w</code>) and height (<code>h</code>) are predicted relative to the whole image. Finally, the confidence score represents the likelihood that the box contains an object and how accurate the box is.</li> <li>Class Predictions: In addition to predicting bounding boxes, each cell also predicts class probabilities. These probabilities are conditioned on the grid cell containing an object. </li> <li>Combining Predictions: The bounding box predictions and class predictions are combined to create a complete detection. If a grid cell is confident that it contains an object, and if the predicted class score is high, then it\u2019s a strong detection.</li> <li>Non-Max Suppression: Since YOLO predicts multiple boxes for each grid cell, it uses a technique called non-max suppression</li> </ol> <p>Here's a visualization of how YOLO divides an image and makes predictions:</p> YOLO Grid System (Source: Jonathan Hui on Medium) Task: YOLO Approach <p>Watch the following video about YOLO and answer the questions below:</p> <p></p> <p>Basics: </p> <ol> <li>In basic object detection, what two main things does YOLO need to determine about an object?</li> <li>What happens when no object is detected in a grid cell?</li> <li>True or False: YOLO must always use a 4x4 grid to divide images.</li> <li>Which is faster at detecting objects: YOLO or older methods like R-CNN?</li> <li>What are the components of YOLO's 7-dimensional output vector for a single grid cell prediction?</li> </ol>"},{"location":"yolo/yolo/#advantages-and-disadvantages","title":"Advantages and Disadvantages","text":"<p>Let's examine what makes YOLO special and where it might not be the best choice:</p> <ul> <li> Advantage <ul> <li>Speed: YOLO is blazingly fast, capable of processing images in real-time</li> <li>Accuracy: Despite its speed, it maintains good detection accuracy</li> <li>Global Context: By looking at the entire image at once, YOLO understands context better than sliding window approaches</li> <li>Generalization: YOLO learns generalizable representations of objects</li> </ul> </li> <li> Disadvantage <ul> <li>Small Objects: YOLO can struggle with detecting small objects, especially in groups</li> <li>Unusual Aspects: Objects in unusual aspects ratios or configurations might be missed</li> <li>Precision: While fast, it might not be as precise as two-stage detectors for some applications</li> </ul> </li> </ul>"},{"location":"yolo/yolo/#yolo-versions","title":"YOLO Versions","text":"<p>The YOLO family has evolved significantly since its introduction in 2016. Each version brought important improvements:</p>"},{"location":"yolo/yolo/#yolov1-v3-2016-2018","title":"YOLOv1-v3 (2016-2018)","text":"<ul> <li>YOLOv1: First version, introduced the grid-based approach</li> <li>YOLOv2/YOLO9000: Added anchor boxes, batch normalization</li> <li>YOLOv3: Implemented feature pyramid networks, better backbone (Darknet-53)</li> </ul>"},{"location":"yolo/yolo/#yolov4-v5-2020-2021","title":"YOLOv4-v5 (2020-2021)","text":"<ul> <li>YOLOv4: Introduced Mosaic augmentation, CSPNet backbone</li> <li>YOLOv5: Brought PyTorch implementation, improved training methods</li> </ul>"},{"location":"yolo/yolo/#latest-versions-2022-2024","title":"Latest Versions (2022-2024)","text":"<ul> <li>YOLOv6: Released by Meituan, optimized for industrial applications</li> <li>YOLOv7: Improved architecture design and training strategies</li> <li>YOLOv8: Ultralytics' flagship model with multi-task capabilities</li> <li>YOLOv9: Introduced revolutionary new features</li> <li>YOLOv10: Enhanced previous versions' capabilities</li> <li>YOLOv11: Latest iteration with significant improvements</li> </ul> YOLO Version Comparison (Source: Ultralytics)"},{"location":"yolo/yolo/#installation-and-setup","title":"Installation and Setup","text":"<p>Getting started with YOLO is straightforward. You can use the Ultralytics implementation of YOLOv11, which offers a user-friendly API and excellent documentation.</p> <ol> <li> <p>First, create a new project folder and virtual environment and activate it:</p> <pre><code>\ud83d\udcc1 computer_vision/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 pics/\n\u2514\u2500\u2500 \ud83d\udcc4 your_files.ipynb\n</code></pre> </li> <li> <p>Install the required packages: <pre><code>pip install ultralytics\n</code></pre></p> </li> <li> <p>Verify your installation: <pre><code>from ultralytics import YOLO\nmodel = YOLO('yolov8n.pt')  # load a pretrained model\n</code></pre></p> </li> </ol> Installation Tips for Pro's <p>If you're using a GPU, make sure you have the correct CUDA version installed</p>"},{"location":"yolo/yolo/#reading-the-docs","title":"Reading the Docs","text":"(Source: Imgflip Meme Generator)  <p>The Ultralytics documentation is your best friend when working with YOLO. Here are the key resources you should bookmark:</p> <ol> <li>Official Ultralytics Docs</li> <li>YOLO Quickstart Guide</li> <li>YOLO11 Tasks</li> </ol> Documentation Best Practices <ul> <li>Keep the API reference handy for specific function documentation</li> <li>Check the examples section for common use cases</li> <li>Join the Ultralytics Discord community for help and updates</li> </ul>"},{"location":"yolo/yolo/#whats-next","title":"What's Next?","text":"<p>In the upcoming sections, we'll dive deeper into:</p> <ul> <li>Working with pretrained models</li> <li>Analyzing images and videos</li> <li>Preparing custom datasets</li> <li>Training YOLO on your own data</li> </ul>"},{"location":"yolo/image/detection/","title":"Detection","text":"<p>After learning about computer vision in general and how YOLO works, we can start using YOLO for our purposes. We will see, how much we can do with little code. </p>"},{"location":"yolo/image/detection/#project-setup","title":"Project Setup","text":"<p>We start with the project structure from before and create a new jupyter notebook <code>yolo_detect.ipynb</code> <pre><code>\ud83d\udcc1 computer_vision/\n    \u251c\u2500\u2500 \ud83d\udcc1 .venv/\n    \u251c\u2500\u2500 \ud83d\udcc1 pics/\n    \u2514\u2500\u2500 \ud83d\udcc4 yolo_detect.ipynb\n</code></pre> Make sure the virtual environment (here <code>.venv</code>) is selected and all needed packages (<code>ultralytics</code>) are installed. </p> <p>Now we can download our test pictures, extract the ZIP file and save it in the <code>\ud83d\udcc1 pics/</code> folder.</p> <p>Test Pictures </p> <p>Now we are all set for our first detection \ud83c\udf89</p> <p>We will start with the picture <code>pic2.jpg</code> which is quite challenging due to motion blur. </p>"},{"location":"yolo/image/detection/#inference","title":"Inference","text":"Inference <p>In machine learning, inference refers to the process of using a trained model to make predictions or decisions on new, unseen data. It's the production phase where the model applies what it learned during training to analyze new inputs and generate outputs, like when a trained image recognition model identifies objects in a new photo.</p>"},{"location":"yolo/image/detection/#pretrained-models","title":"Pretrained Models","text":"<p>Fortunately, so that we don't have to start from scratch, there are already pre-trained models from YOLO that we can use. These pre-trained models have been trained with the help of a lot of data (for detection e.g. on the COCO dataset) and are now available to us without any further effort. For example for detection: </p> Model size<sup>(pixels) mAP<sup>val50-95 Speed<sup>CPU ONNX(ms) Speed<sup>T4 TensorRT10(ms) params<sup>(M) FLOPs<sup>(B) YOLO11n 640 39.5 56.1 \u00b1 0.8 1.5 \u00b1 0.0 2.6 6.5 YOLO11s 640 47.0 90.0 \u00b1 1.2 2.5 \u00b1 0.0 9.4 21.5 YOLO11m 640 51.5 183.2 \u00b1 2.0 4.7 \u00b1 0.1 20.1 68.0 YOLO11l 640 53.4 238.6 \u00b1 1.4 6.2 \u00b1 0.1 25.3 86.9 YOLO11x 640 54.7 462.8 \u00b1 6.7 11.3 \u00b1 0.2 56.9 194.9 Available pretrained YOLO models for detection (Source: Ultralytics)"},{"location":"yolo/image/detection/#running-the-detection","title":"Running the Detection","text":"<p>Based on these pretrained models, to detect objects in an image with YOLO, only a few lines of code are required:</p> <p><pre><code># Import required librarys\nfrom ultralytics import YOLO\n\n# Define the path to the source picture\npicpath = \"pics/pic2.jpg\"\n\n# Load a pretrained YOLO11 Model (Size: Nano)\nmodel_det = YOLO(\"yolo11n.pt\")\n\n# Apply the model to our source picture\nresults = model_det(picpath)\n</code></pre> &gt;&gt;&gt; Output<pre><code>image 1/1 c:\\path\\to\\pics\\pic2.jpg: 448x640 6 persons, 1 car, 5 motorcycles, 1 traffic light, 1 stop sign, 40.3ms\nSpeed: 2.0ms preprocess, 40.3ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n</code></pre></p> <p>And that's it! Your performed your first detection. </p> (Source: Imgflip Meme Generator)"},{"location":"yolo/image/detection/#analyzing-the-output","title":"Analyzing the Output","text":"<p>Now we can take a closer look at the different parts of the output:</p> <ul> <li> <p>Information about the image:</p> <ul> <li> <p><code>image 1/1 c:\\path\\to\\pics\\pic2.jpg: 448x640</code></p> <p>This includes the path to the image and the image size YOLO uses for the detection (this is not the original image size)</p> </li> <li> <p><code>shape (1, 3, 448, 640)</code></p> <p>batch size (number of pictures): <code>1</code></p> <p>picture channels: <code>3</code> (RGB)</p> <p>picture hight: <code>448</code></p> <p>picture width: <code>640</code></p> </li> </ul> </li> <li> <p>Detected objects: </p> <ul> <li><code>6 persons, 1 car, 5 motorcycles, 1 traffic light, 1 stop sign</code></li> </ul> </li> <li> <p>Speed Metrics:</p> <ul> <li><code>Speed: 2.0ms preprocess, 40.3ms inference, 0.0ms postprocess per image</code></li> </ul> </li> </ul> <p>But are those all of the results? Where are those objects in the image? How sure are we, that there are six persons? We need to go deeper into the results. </p>"},{"location":"yolo/image/detection/#understanding-the-results","title":"Understanding the results","text":"<p>The before seen output of the detection is just a brief overview. All the information is stored in <code>results</code></p> <pre><code># Get the first (and only) image's results\nresult = results[0]\n\nprint(result)\n</code></pre> Task: Analyze the Results <p>Take a look at the <code>result</code> and answer the following questions (don't forget to use google and the docs)</p> <ul> <li>What is the original shape of the image?</li> <li>How many different classes are available and what are they?</li> <li>Why is <code>keypoints</code>, <code>masks</code> and <code>obb</code> <code>None</code>? What do you think?</li> </ul> <p>Now go deeper and analyze <code>result.boxes</code></p> <ul> <li>What are the detected classes? How do they respond to the labels (person, car,...)</li> <li>How sure is YOLO about the detected objects? (hint: the right wording is confidence)</li> <li>What are the differences between <code>xywh</code>, <code>xywhn</code>, <code>xyxy</code> and <code>xyxyn</code></li> <li><code>data</code> is the collection of what? </li> <li>How are <code>data</code> and <code>shape</code> are connected? </li> </ul> Multiple Images <p>You can also pass multiple images at once. To access the results for a specific image, you need to choose one from the results list.</p> <pre><code>results = model_det(['pics/pic1.jpg','pics/pic1.jpg'])\nfirst_pic_result = results[0]\nsecond_pic_result = results[1]\n</code></pre> <p>Now, that we are familiar with the results, we can write a little function to print a more detailed overview:</p> <pre><code>def print_overview(result): \n    # Print object count\n    print(f\"Detected {len(result.boxes)} objects\")\n\n    # Examine each detection\n    for box in result.boxes:\n        # Get class name\n        class_id = int(box.cls)\n        class_name = model_det.names[class_id]\n\n        # Get confidence\n        confidence = float(box.conf)\n\n        # Get coordinates (x1, y1, x2, y2 format)\n        x1, y1, x2, y2 = box.xyxy[0].tolist()\n\n        print(f\"\\nDetection:\")\n        print(f\"- Class: {class_name}\")\n        print(f\"- Confidence: {confidence:.2f}\")\n        print(f\"- Coordinates: ({x1:.1f}, {y1:.1f}) to ({x2:.1f}, {y2:.1f})\")\n\nprint_overview(result)\n</code></pre> &gt;&gt;&gt; Output<pre><code>Detected 14 objects\n\nDetection:\n- Class: motorcycle\n- Confidence: 0.81\n- Coordinates: (2177.1, 3142.4) to (3490.8, 4281.8)\n\nDetection:\n- Class: motorcycle\n- Confidence: 0.75\n- Coordinates: (0.0, 3574.5) to (755.0, 4952.4)\n\n...\n</code></pre> <p>So far we have everything we need to detect objects in images and work with the results. </p> <p>But sometimes a picture is worth a thousand words!</p>"},{"location":"yolo/image/detection/#visualize-the-results","title":"Visualize the Results","text":"<p>In some cases, the 'simple' extraction of the results is not sufficient and a visualization is needed.</p> <p>For visualizing the results there are two options available: showing and saving the picture. </p>"},{"location":"yolo/image/detection/#showing-the-resulting-image","title":"Showing the Resulting Image","text":"<p>YOLO makes it really easy to show the resulting image. With the method <code>.show()</code> the result can be visualized: </p> <pre><code>result.show()\n</code></pre> <p>This should result in the above shown image. </p>"},{"location":"yolo/image/detection/#saving-the-resulting-image","title":"Saving the Resulting Image","text":"<p>There are multiple ways to save the resulting image. </p> Saving Images Option 1: YOLOOption 2: openCV <p>YOLO offers various visualization arguments, which can be used directly in the inference command. One of them is <code>save=True</code> <pre><code>results = model_det(picpath, save=True)\n</code></pre></p> <p>Unless other specified, the file will be saved in a new folder <code>\ud83d\udcc1 runs/detect/predict/</code>. Use the argument <code>save_dir='your/custom/path'</code> to specify a different folder.</p> <p>When you install <code>ultralytics</code> the package <code>opencv-python</code> will be installed automatically as dependency. <code>openCV</code> is a powerful tool for image and video processing and can also be used for saving the image</p> <p><pre><code>annotated_image = result.plot()\n\n# Display using OpenCV\nimport cv2\n\n# Save the image\ncv2.imwrite(\"output_detection.jpg\", annotated_image)\n</code></pre> Unless other specified, the file will be saved directly into the working directory <code>\ud83d\udcc1 computer_vision</code>. </p>"},{"location":"yolo/image/detection/#see-the-magic-happen","title":"See the Magic Happen","text":"<p>If you are interessted in seeing, what the model is seeing and doing, you can use another visualization argument  <pre><code>results = model_det(picpath, visualize=True)\n</code></pre></p> <p>The resulting pictures in the folder <code>\ud83d\udcc1 runs/detect/predict/pic2</code> are a 'Intermediate Features Visualization'</p> <ul> <li>This parameter saves visualizations of the intermediate feature maps or activations from the YOLO model.</li> <li>These feature maps show what parts of the image the model focuses on during different stages of the neural network.</li> <li>These outputs help in debugging and understanding the model's inner workings.</li> </ul>"},{"location":"yolo/image/detection/#inference-arguments","title":"Inference Arguments","text":"<p>After we worked on the output side of the Yolo, it's now time to focus more on the input side. Besides those visualization arguments we have used before, there are numerous inference arguments, which can be handed over to the model. An overview can be found in the documentation</p> Task: Inference Arguments <ol> <li>Confidence<ul> <li>Run a detection with confidence threshold 0.5.</li> <li>Save this image as <code>high_conf.jpg</code></li> <li>Now try confidence 0.25, save as <code>low_conf.jpg</code></li> <li>Compare both images - what differences do you notice?</li> <li>What is the default value? </li> </ul> </li> <li>Classes<ul> <li>Limit your detection to just detect <code>motorcycle</code> and <code>car</code></li> <li>Save the results as <code>class_limit.jpg</code></li> </ul> </li> <li>Adjusting the output path<ul> <li>Use the two inference arguments <code>project</code> and <code>name</code> to adjust the output folder to <code>\ud83d\udcc1 output_pics/detection_pics/</code>. </li> </ul> </li> <li>Adjust the visual outcome<ul> <li>Run a detection for our <code>pic2.jpg</code> image and adjust:<ul> <li>no label should be shown</li> <li>no confidence should be shown</li> <li>line width of the boxes should be 3  </li> </ul> </li> </ul> </li> </ol>"},{"location":"yolo/image/detection/#further-adjustments","title":"Further Adjustments","text":"<p>As we have discussed before, YOLO comes in different model sizes, trading speed for accuracy.</p> Task: Model Comparison <p>For each model size determine the following characteristics by running a detection on the <code>pic2.jpg</code>:</p> <ol> <li>Time the detection speed</li> <li>Count detected objects</li> <li>Compare confidence scores</li> <li>Create a table with your findings:</li> </ol> Model Detection Time Objects Found Avg Confidence nano small medium large extra large \ud83c\udf89 Congratulations <p>You are now able to perform object detection on images and work with the results!    Detection Fail (Source: Visualizing Object Detection Features on Springer Nature Link)  </p>"},{"location":"yolo/image/kp-extraction/","title":"Keypoint Extraction","text":"<p>The next step in computer vision is the extraction of keypoints in images. YOLO also has a solution for this and its own models.  YOLO can detect and track specific points of interest in images, which is particularly useful for pose estimation, facial landmark detection, and custom keypoint tracking.</p>"},{"location":"yolo/image/kp-extraction/#project-setup","title":"Project Setup","text":"<p>We'll continue with the project structure from before and create a new jupyter notebook <code>yolo_keypoint.ipynb</code>:</p> <pre><code>\ud83d\udcc1 computer_vision/\n    \u251c\u2500\u2500 \ud83d\udcc1 .venv/\n    \u251c\u2500\u2500 \ud83d\udcc1 pics/\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_detect.ipynb\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_segment.ipynb\n    \u2514\u2500\u2500 \ud83d\udcc4 yolo_keypoints.ipynb\n</code></pre> <p>Ensure your virtual environment is active, and the required packages (<code>ultralytics</code>) are installed.</p> <p>We will start with the picture <code>pic1.jpg</code> which we already downloaded before. </p>"},{"location":"yolo/image/kp-extraction/#yolo-keypoint-detection","title":"YOLO Keypoint Detection","text":"<p>YOLO's keypoint detection is primarily used for human pose estimation, where it can identify specific points on objects or bodies. This is particularly useful for:</p> <ul> <li>Human pose estimation</li> <li>Facial landmark detection</li> <li>Custom keypoint tracking</li> <li>Sports motion analysis</li> <li>Animal pose tracking</li> </ul> <p>Therefore, the pre-trained models provided by YOLO are only trained for humans and do not recognize anything else. We can also train models ourselves to recognize various types of key points besides humans. We will look at this topic a little later. </p>"},{"location":"yolo/image/kp-extraction/#inference","title":"Inference","text":"Pretrained Models <p>Again, just like with detection and segmentation, YOLO provides pre-trained models specifically for keypoint detection. These models have been trained on various datasets to recognize different types of keypoints.</p>"},{"location":"yolo/image/kp-extraction/#running-keypoint-detection","title":"Running Keypoint Detection","text":"<p>The code for keypoint detection and the results are similar to other YOLO tasks - we just need to use a keypoint-specific model:</p> <pre><code># Import required libraries\nfrom ultralytics import YOLO\n\n# Define the path to the source picture\npicpath = \"pics/pic1.jpg\"\n\n# Load a pretrained YOLO11 Keypoint Model (Size: Nano)\nmodel_kp = YOLO(\"yolo11n-pose.pt\") # (1)!\n\n# Apply the model to our source picture\nresults = model_kp(picpath)\n</code></pre> <ol> <li>As with detection and segmentation, there are also different models for keypoint extraction: <code>YOLO11n-pose</code>, <code>YOLO11s-pose</code>, <code>YOLO11m-pose</code>, <code>YOLO11l-pose</code>, <code>YOLO11x-pose</code></li> </ol> &gt;&gt;&gt; Output<pre><code>image 1/1 c:\\path\\to\\pics\\pic_keypoints.jpg: 384x640 10 persons, 63.4ms\nSpeed: 3.0ms preprocess, 63.4ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n</code></pre>"},{"location":"yolo/image/kp-extraction/#visualizing-keypoint-results","title":"Visualizing Keypoint Results","text":"<p>Like inference, the visualization of keypoints is equivalent to segmentation and detection. You can customize how the keypoint results are displayed. For more control over the visualization:</p> <pre><code>fname = \"output_kp.jpg\"\n\nresult.plot(\n    show = True,        # Display the plot immediately\n\n    save = True,        # Save the plotted image to a file\n    filename = fname,    # Specify the filename for the saved image\n\n    conf=True,           # Show confidence scores\n    line_width=2,        # Line thickness for connections\n    font_size=14,        # Font size for labels\n    boxes=True,          # Show bounding boxes\n    labels=True          # Show keypoint labels\n)\n</code></pre>"},{"location":"yolo/image/kp-extraction/#inference-arguments","title":"Inference Arguments","text":"<p>Many of the same inference arguments from detection and segmentation also work with keypoint extraction, plus some additional ones specific to masks. Therefore check the documentation.</p> Task: Perform a Keypoint Extraction <p>Perform a keypoint extraction on the above shown image <code>pic1.jpg</code>. Take a closer look at the results and answer these questions:</p> <ol> <li>How do the results differ from those of the detection/segmentation?</li> <li>Take a look at the names attribute. How many different classes can be detected?</li> <li>How many poses are detected?</li> <li>What parts does the keypoints object consist of?</li> <li>Whats the difference between conf, data and xy?</li> <li>Do some research about which keypoint represents what body part. </li> <li>Now visualize the results by saving the image. </li> </ol> \ud83c\udf89 Congratulations <p>You've learned how to use YOLO for image processing! Try applying these concepts to your own projects and explore more advanced applications.</p> <p> (Source: Reddit)  </p> <p>Stay tuned for more advanced techniques and use cases in the next chapters!</p>"},{"location":"yolo/image/segmentation/","title":"Segmentation","text":"<p>After learning how to detect objects in images, we can now go one step further: Instead of just detecting where objects are located using bounding boxes, we can identify exactly which pixels belong to each object. This is called segmentation.</p>"},{"location":"yolo/image/segmentation/#project-setup","title":"Project Setup","text":"<p>We'll continue with the project structure from before and create a new jupyter notebook <code>yolo_segment.ipynb</code>: <pre><code>\ud83d\udcc1 computer_vision/\n    \u251c\u2500\u2500 \ud83d\udcc1 .venv/\n    \u251c\u2500\u2500 \ud83d\udcc1 pics/\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_detect.ipynb\n    \u2514\u2500\u2500 \ud83d\udcc4 yolo_segment.ipynb\n</code></pre> Make sure your virtual environment is still active. We'll use the same test pictures as before.</p> <p>We'll start with the picture <code>pic2.jpg</code> that we used for detection:</p>"},{"location":"yolo/image/segmentation/#inference","title":"Inference","text":"Pretrained Models <p>Just like with detection, YOLO provides pre-trained models specifically for segmentation. These models have been trained on the COCO dataset but with segmentation masks instead of just bounding boxes.</p>"},{"location":"yolo/image/segmentation/#running-the-segmentation","title":"Running the Segmentation","text":"<p>The code for segmentation is very similar to detection - we just need to use a segmentation model instead:</p> <pre><code># Import required libraries\nfrom ultralytics import YOLO\n\n# Define the path to the source picture\npicpath = \"pics/pic2.jpg\"\n\n# Load a pretrained YOLO11 Segmentation Model (Size: Nano)\nmodel_seg = YOLO(\"yolo11n-seg.pt\") # (1)!\n\n# Apply the model to our source picture\nresults = model_seg(picpath)\n</code></pre> <ol> <li>As with detection, there are also different models for segmentation: <code>YOLO11n-seg</code>, <code>YOLO11s-seg</code>, <code>YOLO11m-seg</code>, <code>YOLO11l-seg</code>, <code>YOLO11x-seg</code></li> </ol> &gt;&gt;&gt; Output<pre><code>image 1/1 c:\\path\\to\\pics\\pic2.jpg: 448x640 4 persons, 1 car, 5 motorcycles, 3 traffic lights, 1 stop sign, 52.7ms\nSpeed: 2.0ms preprocess, 52.7ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n</code></pre> <p>The output looks similar to detection, but behind the scenes YOLO has created detailed segmentation masks for each object!</p> Task: Analyze the Segmentation Results <p>Now it's time to analyze the segmentation results and compare them to the results from our detection. Take a closer look at the <code>results</code> and answer the following questions:</p> <ol> <li>What's the difference between the detection results and the segmentation results now? </li> <li>What's the difference between <code>boxes</code> and <code>masks</code>? What information is stored in these variables?</li> <li>What's the shape of a mask and what does each dimension represent?</li> <li>How are the coordinates in masks different from bounding boxes?</li> <li>Visualize the results by saving the resulting image. </li> </ol>"},{"location":"yolo/image/segmentation/#visualizing-segmentation-results","title":"Visualizing Segmentation Results","text":"<p>A graphical representation of the results can also be useful for segmentation. For this, the same commands are available as for detection. </p> <p>Another visualization option is the <code>result.plot</code> command. With this, you can customize how the segmentation (or detection) results are displayed to better suit your analysis or presentation needs, allowing you to highlight specific features like bounding boxes, segmentation masks, confidence scores, or class labels.</p> <pre><code>fname = \"output_segmentation.jpg\"\n\nresult.plot(\n    show = True,        # Display the plot immediately\n\n    save = True,        # Save the plotted image to a file\n    filename = fname,   # Specify the filename for the saved image\n\n    boxes = True,       # Include bounding boxes around detected objects\n    masks = True,       # Overlay segmentation masks on the image\n    conf = False,       # Do not display confidence scores for the predictions\n    labels = True,      # Display class labels for each detected object\n)\n</code></pre>"},{"location":"yolo/image/segmentation/#inference-arguments","title":"Inference Arguments","text":"<p>Many of the same inference arguments from detection also work with segmentation, plus some additional ones specific to masks. Therefore check the documentation.</p> Task: Segmentation Practice <p>Try these exercises to better understand image segmentation:</p> <ol> <li>Mask Quality (Inference Argument)<ul> <li>Run segmentation with <code>retina_masks=True</code></li> <li>Compare the output with default masks</li> <li>What differences do you notice in quality and speed?</li> </ul> </li> <li>Compare different model sizes (nano vs. small vs. medium) for segmentation</li> <li>Experiment with different confidence thresholds</li> <li>Try segmenting different types of images</li> </ol> \ud83c\udf89 Congratulations <p>You've learned the basics of image segmentation! Try applying these concepts to your own projects and explore more advanced techniques.</p>"},{"location":"yolo/train/","title":"Introduction","text":"<p>In this course block and its subsequent chapters we will demonstrate how to build your own YOLO model in practice.  In the end, you will be able to build a \"ready-to-go\" model that can detect objects in images, videos, and live streams.</p> <p>Along the way we will explore the four major steps of building a YOLO model:</p> <pre><code>graph LR\n  A[Data Acquisition] --&gt; B[Annotation];\n  B --&gt; C[Training];\n  C --&gt; D[Inference];\n  click A \"../acquisition\" _self\n  click B \"../annotation\" _self\n  click C \"../training\" _self\n  click D \"../inference\" _self\n  classDef active fill:#950f42</code></pre> <p>Let's get started! \ud83d\ude80</p> <p>In addition to the theoretical foundation, we will look at the following chapters using a practical example.  This will enable us to better understand and apply the theoretical concepts.</p>"},{"location":"yolo/train/#prerequisites","title":"Prerequisites","text":""},{"location":"yolo/train/#0-whats-our-goal","title":"0.  What's our goal?","text":"<p>We will start with defining a goal for our practical example. </p> <p> Build a YOLO model that can detect and classify different types of banknotes (5\u20ac and 10\u20ac). </p> <p>In order to build this kind of model, we will first need to find and generate suitable dataset. </p>"},{"location":"yolo/train/#1-project-structure","title":"1.  Project structure","text":"<p>Start with creating a new folder for our project:</p> <pre><code>\ud83d\udcc1 yolo_training/\n</code></pre>"},{"location":"yolo/train/#2-virtual-environment","title":"2.  Virtual environment","text":"<p>Create a virtual environment. Now, you should have the following structure:</p> <pre><code>\ud83d\udcc1 yolo_training/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n</code></pre> <p>Be sure to activate the environment!</p>"},{"location":"yolo/train/#3-install-packages","title":"3.  Install packages","text":"<p>Install the necessary packages in this specific order: <code>label-studio</code>, <code>ImageEngine</code>, <code>opencv-python</code>, <code>ultralytics</code>. This time, the sequence of installation is important to due some dependencies.</p>"},{"location":"yolo/train/acquisition/","title":"Data Acquisition","text":"<pre><code>graph LR\n  A[Data Acquisition]:::active --&gt; B[Annotation];\n  B --&gt; C[Training];\n  C --&gt; D[Inference];\n  click A \"../acquisition\" _self\n  click B \"../annotation\" _self\n  click C \"../training\" _self\n  click D \"../inference\" _self\n  classDef active fill:#950f42</code></pre> Used Task <p>In the following sections we will train a YOLO model for the task of object detection. If you want to train a YOLO model for a different task (Segmentation, Keypoint Extraction, etc.), the process is quite similar. At some point you will need to make some changes to the annotation and the configuration file.</p> <p>After learning about different computer vision tasks with YOLO, you might want to train your own model for specific use cases. The first step in training a custom YOLO model is acquiring a suitable dataset. A well-curated and diverse dataset is key to achieving high performance and generalization in computer vision tasks. This chapter will guide you through various methods of collecting training data.</p> <p>We'll start by adding a new folder for our dataset and a new Jupyter notebook for data acquisition:</p> <pre><code>\ud83d\udcc1 yolo_training/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 rawdata/\n\u2514\u2500\u2500 \ud83d\udcc4 data_acquisition.ipynb\n</code></pre>"},{"location":"yolo/train/acquisition/#the-need-for-data","title":"The Need for Data","text":"<p>Training an effective YOLO model requires a substantial amount of data. Here's why:</p> <ul> <li>Better Generalization: A diverse dataset helps the model learn features that generalize to new, unseen data.</li> <li>Handling Variability: Capturing different lighting conditions, perspectives, and object appearances ensures robust performance.</li> <li>Avoiding Overfitting: A small dataset can cause the model to memorize specific examples rather than learning general patterns, leading to overfitting.</li> </ul> <p>The amount of data needed depends on several factors like Task Complexity, Required Accuracy, Object Variation and Background Variation</p> Task Complexity Recommended Images Simple (e.g., logo detection) 100 - 2,000 Moderate (e.g., car types) 2,000 - 10,000 Complex (e.g., defect detection) 5,000 - 20,000+ <p>Building a large dataset can be a challenging task, but there are several strategies to gather the required data efficiently.  When collecting your dataset think about the following best practices:</p> Best Practices for Data Collection <ol> <li> <p>Diversity</p> <ul> <li>Include negative samples (no object)</li> <li>Vary lighting conditions</li> <li>Include different backgrounds</li> <li>Capture different angles</li> </ul> </li> <li> <p>Quality Control</p> <ul> <li>Check image resolution</li> <li>Remove blurry images</li> <li>Ensure correct labeling</li> <li>Verify class balance</li> </ul> </li> <li> <p>Organization</p> <ul> <li>Use clear folder structure</li> <li>Maintain consistent naming</li> </ul> </li> </ol>"},{"location":"yolo/train/acquisition/#automatic-image-collection","title":"Automatic Image Collection","text":"<p>With this knowledge in mind, we can start to collect images for our training dataset. Web scraping can be used to download large amounts of images for training datasets. Python libraries like <code>requests</code> and <code>BeautifulSoup</code> are common tools for this purpose. An even more comfortable way is to use an API of a search engine like Bing , Google  or DuckDuckGo . </p> <p>The <code>ImageEngine</code> package can be used to search all three search engines at ones.  You can run a search  with just a couple of lines: </p> <pre><code>from ImageEngine import searchDDG       # Only search DDG\nfrom ImageEngine import searchBing      # Only search Bing\nfrom ImageEngine import searchGoogle    # Only search Google\nfrom ImageEngine import searchWeb       # Search all three engines\n\n# Search images from DuckDuckGo\n# \"Homer Simpson\" is the search string and \"homer\" is the directory where images will be stored\nsearchDDG(term=\"Homer Simpson\", path=\"rawdata/homer\", max_images=5)\n# Search images from Bing\nsearchBing(term=\"Marge Simpson \", path=\"rawdata/marge\", max_images=5)\n# Search images from Google\nsearchGoogle(term=\"Bart Simpson\", path=\"rawdata/bart\", max_images=5)\n# Search images from all three engines\nsearchWeb(term=\"Lisa Simpson\", path=\"rawdata/lisa\", max_images=5)\n</code></pre>"},{"location":"yolo/train/acquisition/#data-cleaning-tips","title":"Data Cleaning Tips","text":"<p>After downloading, it's important to clean your dataset by going through the following steps:</p> Dataset Cleaning Checklist <ul> <li> Check copyright restrictions (if you plan to use the model for commercial purposes)</li> <li> Remove corrupted images</li> <li> Remove duplicates</li> <li> Verify image quality</li> <li> Check image relevance</li> <li> Ensure consistent format (not allways needed)</li> </ul> <p>Those steps can be done manually by looking through the pictures. Finding duplicates and corrupted images can also be automated by using this custom function</p> Remove Corrupted Images and Duplicates <pre><code>import os\nfrom PIL import Image\nimport hashlib\n\ndef clean_dataset_by_subfolder(directory):\n    \"\"\"\n    Cleans all subfolders in 'directory' by removing corrupted files and\n    duplicates *only* within each subfolder.\n\n    Args:\n        directory (str): Path to a top-level directory that contains subfolders.\n    \"\"\"\n    # 1) Loop through all entries in the top-level directory\n    for entry in os.listdir(directory):\n        subpath = os.path.join(directory, entry)\n\n        # Check if the entry is a directory (i.e., a subfolder)\n        if os.path.isdir(subpath):\n            print(f\"\\n--- Checking subfolder: {subpath} ---\")\n\n            # Create a new hash dictionary for each subfolder\n            hash_dict = {}\n\n            # 2) Recursively walk through all files in the subfolder\n            for root, dirs, files in os.walk(subpath):\n                for filename in files:\n                    filepath = os.path.join(root, filename)\n\n                    try:\n                        # Attempt to open the image file\n                        with Image.open(filepath) as img:\n                            # Compute the MD5 hash of the image bytes\n                            img_hash = hashlib.md5(img.tobytes()).hexdigest()\n\n                            # Check if this hash already exists (i.e., a duplicate)\n                            if img_hash in hash_dict:\n                                print(f\"Removing duplicate: {filepath}\")\n                                os.remove(filepath)\n                            else:\n                                # If not a duplicate, store it in the dictionary\n                                hash_dict[img_hash] = filepath\n\n                    except Exception as e:\n                        # If there's any error (e.g., corrupted file), remove it\n                        print(f\"Removing corrupted file {filepath}: {e}\")\n                        os.remove(filepath)\n\n# Example call, if 'dataset/homer' contains multiple subfolders\nclean_dataset_by_subfolder(\"data\")\n</code></pre> Task: Download Images <p>Now it's your turn. We want to collect images of different Euro notes. </p> <ul> <li>Try to use the <code>ImageEngine</code> package to download 100 suitable images of a <code>5\u20ac</code> and a <code>10\u20ac</code> note and save them into the folder <code>rawdata/five</code> and <code>rawdata/ten</code> (in total 200 images).</li> <li>Go through the data cleaning checklist (you can use the function from above)</li> </ul> <p> </p>"},{"location":"yolo/train/acquisition/#video-frame-extraction","title":"Video Frame Extraction","text":"<p>Another effective way to collect a vast amount of images is by extracting and saving each single frame from a video . This video can be a live stream from the webcam or a saved video from your hard drive. There are some benefits of using video data:</p> <ul> <li>Efficiency: Videos can capture many frames in one recording session, saving time compared to capturing individual photos.</li> <li>Diverse Scenarios: Recording videos in various environments ensures that frames capture different conditions and perspectives.</li> </ul>"},{"location":"yolo/train/acquisition/#frame-extraction","title":"Frame Extraction","text":"<p>We already introduced OpenCV in the previous chapter. We can use this package to access the video (saved or webcam) and instead of showing the image, we can save it as an image in a folder</p> <pre><code>cv2.imwrite(f'rawdata/video/frame_{frameNr}.jpg', frame)\n</code></pre> Unique File Name <p><code>frameNr</code> is simply a frame counter. It starts at 0 before entering the loop and increments by 1 every time a frame is successfully read from the video. This counter is used to give each extracted frame a unique filename (e.g., <code>frame_0.jpg</code>, <code>frame_1.jpg</code>, etc.).</p> Task: Frame Extraction <p>Now we continue from before and try to collect data by recording a video of Euro notes.</p> <ul> <li>Record a video of a <code>5\u20ac</code> and a <code>10\u20ac</code> note. The video should be at least 25 seconds long and including the following parts:<ul> <li><code>5\u20ac</code> note in different angles</li> <li><code>10\u20ac</code> note in different angles</li> <li><code>5\u20ac</code> and <code>10\u20ac</code> note together in different angles</li> <li>negative samples (no note in the frame)</li> </ul> </li> <li>You can use the <code>VideoCapture</code> function of OpenCV from the previous chapter and add <code>imwrite</code> to save (and not only show) the frames. Save each 4th frame seperately in the folder <code>rawdata/video</code>.</li> <li>Keep the recording guidelines in your mind.</li> </ul> <p> </p>"},{"location":"yolo/train/acquisition/#whats-next","title":"What's Next?","text":"<p>After collecting the data, the next step is to annotate it with labels or bounding boxes for training the model. Continue to the next section, Image Annotation, to learn how to prepare your dataset for training!</p>"},{"location":"yolo/train/annotation/","title":"Image Annotation","text":"<pre><code>graph LR\n  A[Data Acquisition]:::active --&gt; B[Annotation];\n  B:::active --&gt; C[Training];\n  C --&gt; D[Inference];\n  click A \"../acquisition\" _self\n  click B \"../annotation\" _self\n  click C \"../training\" _self\n  click D \"../inference\" _self\n  classDef active fill:#950f42</code></pre> <p>Now that we've collected a dataset, the next step is image annotation - marking objects in images so that our model knows what to learn. Without properly labeled data, even the most advanced models won't be able to correctly detect objects.</p> <p>There are many tools for image annotation like Label Studio, Roboflow, CVAT, and more. Each tool has its own features and advantages. In this chapter, we'll explore Label Studio, which is open-source and can be easily installed on your computer. Therefore your data stays on your own computer and you don't need to open your dataset to the public.</p>"},{"location":"yolo/train/annotation/#what-is-annotation","title":"What is Annotation? \ud83c\udff7\ufe0f","text":"<p>Annotation in computer vision is the process of labeling data so that a machine learning model can understand what to learn. In the case of object detection, annotation involves marking objects in images by drawing bounding boxes around them and assigning labels to specify what they represent.</p>"},{"location":"yolo/train/annotation/#why-is-annotation-important","title":"Why is Annotation Important?","text":"<p>For a model to recognize objects, it needs examples to learn from. Annotation provides these examples by telling the model:  </p> <ul> <li> What objects exist in the image? </li> <li> Where are they located? </li> <li> Which category do they belong to? </li> </ul> <p>For example, if we're training a model to recognize euro notes, we need to manually label images by drawing boxes around <code>5\u20ac</code> and <code>10\u20ac</code> notes, so the model can later detect them automatically. </p> Commercial Tools <p>Annotation is a time-consuming  process, which is why commercial tools like Roboflow with AI assistance are becoming more popular. Furthermore, there are companies that offer annotation services, which can be a good option if you don't have the time or resources to annotate the data yourself.</p>"},{"location":"yolo/train/annotation/#types-of-annotations-in-computer-vision","title":"Types of Annotations in Computer Vision","text":"Annotation Type Description Example Bounding Boxes Draw rectangular boxes around objects Detecting a euro note Polygons Outline objects with irregular shapes Annotating a curved object Keypoints Mark specific points Facial feature detection Segmentation Masks Assign pixel-level labels Separating an object from the background <p>\ud83d\udca1 Think of annotation as teaching a model how to \"see\" by giving it labeled examples!</p>"},{"location":"yolo/train/annotation/#annotation-with-label-studio","title":"Annotation with Label Studio","text":""},{"location":"yolo/train/annotation/#step-1-install-label-studio","title":"Step 1: Install Label Studio","text":"<p>Label Studio runs as a web application on your computer. We already installed it in the introduction chapter. To start Label Studio, simply run the following command in your terminal:</p> <pre><code>label-studio\n</code></pre> <p>This will open Label Studio in your browser at <code>http://localhost:8080/</code> </p> Label Studio Port <p>Sometimes the default port is already taken, then a new one is chosen automatically.</p> <pre><code>*** WARNING! ***\n* Port 8080 is in use.\n* Trying to start at 8081\n****************\n</code></pre>"},{"location":"yolo/train/annotation/#step-2-sign-up-log-in","title":"Step 2: Sign Up &amp; Log In","text":"<p>To use Label Studio, you need to sign up first. You can do this by clicking on Sign Up under the login button. After signing up, you can login with your new credentials.</p>"},{"location":"yolo/train/annotation/#step-3-setting-up-a-new-annotation-project","title":"Step 3: Setting Up a New Annotation Project","text":""},{"location":"yolo/train/annotation/#step-31-create-a-new-project","title":"Step 3.1: Create a New Project","text":"<p>Once Label Studio is running:</p> <ol> <li>Click Create (upper right corner).</li> <li>Enter a Project Name (e.g., \"Euro Note Detection\") and Description if you want.</li> </ol>"},{"location":"yolo/train/annotation/#step-32-uploading-your-images","title":"Step 3.2: Uploading Your Images","text":"<p>Now we need to upload the images we've collected before. Label Studio supports multiple ways to do this:</p> <ul> <li>Upload from your computer: Drag and drop images.</li> <li>Load from a folder: Connect a dataset directory.</li> <li>Use a cloud storage provider: Connect AWS S3, Google Cloud, or other services.</li> </ul> <p>To upload the data by drag and drop:</p> <ol> <li>Click Data Import in your 'Create Project' module.</li> <li>Select your collected euro note images (from <code>rawdata/five</code>, <code>rawdata/ten</code>, and <code>rawdata/video</code>) and drag and drop them into the upload area.</li> </ol> Uploading Data <p>Label Studio only allows to drag and drop 100 images at a time. If you have more than 100 images, you need to drag and drop them in batches. </p> <p>Furthermore, you can also upload images after the project is created. Therefore open your project and click on the Import button in the upper right corner.</p>"},{"location":"yolo/train/annotation/#step-33-labeling-setup","title":"Step 3.3: Labeling Setup","text":"<p>The last step in the project setup ist to configure the labeling interface (define, what we want to label). There are a bunch of different templates available. In our example we want to label euro notes with bounding boxes for detection.</p> <ol> <li>Click Labeling Setup in your 'Create Project' module.</li> <li>Select Object Detection with Bounding Box in the Computer Vision section.</li> <li>Define your object labels in the Labeling Interface by adding the labels you want to label:<ul> <li><code>5 euro</code></li> <li><code>10 euro</code></li> </ul> </li> <li>You should also change the colors of the labels to make them more visible. Therefore click on the label once you added it and click on the color picker.</li> <li>Save the settings.</li> </ol> Project Setup <p>You can always change the labeling interface later, if you want to label different objects or use a different annotation type. Therefore open your project and click on the Setting button in the upper right corner.</p>"},{"location":"yolo/train/annotation/#step-4-annotating-your-images","title":"Step 4: Annotating Your Images","text":"<p>Now we are all set and we can start labeling:</p> <ol> <li>In your project landing page click on the Label All Tasks button.</li> <li>Select the bounding box tool (<code>5 euro</code> or <code>10 euro</code>) from below the image.</li> <li>Draw a box around each note in the image.</li> <li>Click Submit and move to the next image (even if no note is in the image, click on submit and NOT skip).</li> </ol> Labeling Tips <ul> <li>Use the shortcuts for the bounding box tool. E.g. <code>1</code> for <code>5 euro</code> and <code>2</code> for <code>10 euro</code>. The corresponding shortcut is shown besids the label name.</li> <li>Be consistent in your labeling to ensure high-quality training data.</li> <li>You can always undo your last action by clicking on the Undo button.</li> </ul>"},{"location":"yolo/train/annotation/#step-5-exporting-annotations-for-yolo-training","title":"Step 5: Exporting Annotations for YOLO Training","text":"<p>Once all images are labeled, export your annotations in a format compatible with YOLO:</p> <ol> <li>Click Export in your project landing page.</li> <li>Select YOLO with Images (<code>.txt</code> files).</li> <li>Download the exported dataset.</li> </ol> <p>Congratulation \ud83c\udf89, you have just annotated your first dataset!</p> (Source: Imgflip Meme Generator)"},{"location":"yolo/train/annotation/#annotation-format","title":"Annotation Format","text":"<p>Once you have downloaded the exported dataset, you can use it to train your YOLO model. But first, let's take a look at the annotation format.</p> <p>After the export, Label Studio will hand over a zip file with two folders: <code>images</code> and <code>labels</code> as well as a <code>notes.json</code> and a <code>classes.txt</code> file. The folders contain the annotated images (<code>images/</code>) and the corresponding annotations in YOLO format (<code>labels/</code>).  We will copy those folders into our project folder. </p> <pre><code>\ud83d\udcc1 yolo_training/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 rawdata/\n\u251c\u2500\u2500 \ud83d\udcc1 annotations/\n|   \u251c\u2500\u2500 \ud83d\udcc1 images/\n|   \u2514\u2500\u2500 \ud83d\udcc1 labels/\n\u2514\u2500\u2500 \ud83d\udcc4 data_acquisition.ipynb\n</code></pre> <p>We will then navigate to <code>annotations</code> and open the <code>labels</code> folder. Inside this folder, you will find a text file for each image. Each text file contains the annotation in the YOLO format like: <pre><code>0 0.5029612756264237 0.5018223234624145 0.9858769931662869 0.980410022779043\n</code></pre></p> <p>The structure of the annotation can be described as follows: <pre><code>&lt;CLASS_ID&gt; &lt;X_CENTER&gt; &lt;Y_CENTER&gt; &lt;WIDTH&gt; &lt;HEIGHT&gt;\n</code></pre></p> <p>For our example the class ID is <code>0</code> for <code>five_euro</code> and <code>1</code> for <code>ten_euro</code>. This information can be looked up in <code>notes.json</code> in the zip file. The coordinates are the normalized center of the bounding box and the width and height of the bounding box relative to the image size (xywh format).</p> Labels / Images <p>It is important to note, that the for each image there needs to be a corresponding label file with the same name. For example, if you have an image <code>images/image_1.jpg</code>, there needs to be a label file <code>labels/image_1.txt</code>.</p> Task: YOLO Format <p>Take a closer look at the label file for an image where both - a <code>5\u20ac</code> and a <code>10\u20ac</code> note - are present. What do you observe? How is the label file structured?</p>"},{"location":"yolo/train/annotation/#next-steps","title":"Next Steps","text":"<p>Now that you have annotated data, you're ready to train your YOLO model! In the next chapter, we'll explore how to train YOLO with your labeled dataset.</p>"},{"location":"yolo/train/colab/","title":"Bonus: Training on Google Colab","text":"<p>Training a YOLO model can be computationally demanding, especially when working with large datasets or running multiple training iterations. This bonus chapter explains how to use Google Colab as an alternative to local training and why cloud-based solutions can be advantageous.</p>"},{"location":"yolo/train/colab/#why-use-cloud-based-training","title":"Why Use Cloud-Based Training?","text":"<p>Training machine learning models locally is not always practical. Most laptops and desktop computers do not have a dedicated GPU suitable for deep learning. Training on a CPU can take 10-50 times longer than on a GPU. For our Euro note detection model, what takes 5 minutes on a GPU could take over an hour on a CPU.</p> <p>While local training on GPU is possible, it requires a lot of setup and maintenance. You need to install the CUDA driver, cuDNN libraries, PyTorch with CUDA support, and manage dependency conflicts. Google Colab comes pre-configured with all necessary deep learning libraries and GPU drivers. You can start training within minutes without any installation hassle.</p> <p>Local development still has advantages in some scenarios:</p> <ul> <li>Data privacy: Sensitive data should not be uploaded to cloud services</li> <li>Large datasets: Uploading/downloading gigabytes of data can be slow</li> <li>Long training runs: Free Colab sessions have time limits (~12 hours)</li> <li>Production deployment: Final models often need local testing</li> </ul> <p>After we have already learned how to train a YOLO model locally, we will now use Google Colab to train our model.</p> Video Tutorial <p>If you prefer a visual guide, here is a video tutorial on how to train a YOLO model on Google Colab:</p> <p> </p>"},{"location":"yolo/train/colab/#google-colab","title":"Google Colab","text":""},{"location":"yolo/train/colab/#setup","title":"Setup","text":"<p>Navigate to Google Colab and sign in (right upper corner) with your Google account. Create a new notebook by clicking New notebook.</p> <p>On the top left you see the name of the current notebook. You can also change the name by clicking on it and typing a new name.</p> <p>By default, Colab uses a CPU to run your code. To enable GPU acceleration - which is the reason we are using Colab in the first place - you need to change the runtime type to a GPU runtime. Therefore:</p> <ol> <li>Go to Runtime &gt; Change runtime type</li> <li>Select T4 GPU (or any available GPU option)</li> <li>Click Save</li> </ol> <p>You can verify the GPU availability by running the following code in a new code cell in colab:</p> <pre><code>import torch\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nprint(f\"GPU name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")\n</code></pre> <ul> <li> <p> Correct Setup</p> &gt;&gt;&gt; Output<pre><code>GPU available: True\nGPU name: Tesla T4\n</code></pre> </li> <li> <p> Faulty Setup</p> &gt;&gt;&gt; Output<pre><code>GPU available: False\nGPU name: N/A\n</code></pre> </li> </ul> <p>The last setup step is to install the YOLO library. There are two ways to do this:</p> TerminalInline Code <p>To install the YOLO library, you can use the terminal (left bottom of the page) by running the following command:</p> <pre><code>pip install ultralytics\n</code></pre> <p> </p> <p>You can also install the YOLO library by running the following code in a new code cell in colab:</p> <pre><code>!pip install ultralytics\n</code></pre> <p>Note that the <code>!</code> is used to run shell commands in the terminal.</p>"},{"location":"yolo/train/colab/#preparing-the-files","title":"Preparing the files","text":"<p>Before training, you need to upload your dataset and the configuration file to Colab. There are multiple options available to do this, like mounting Google Drive (especially for larger datasets), or downloading from an URL link. We take a closer look on the easiest option: to directly upload the files to the Colab session.</p> <p>On the left side of colab you see the files icon . By clicking on it, something like a file explorer will open. We can now upload the files by simply dragging and dropping the files into this area. In our case, we want to upload the <code>annotations</code> folder and the <code>config.yaml</code> file. For both we need to make small changes compared to the local training.</p> <p>Since the upload only allows files, the <code>annotations</code> folder can not be uploaded directly. Therefore we need to create a zip file (here called <code>annotated.zip</code>) of the folder and upload this file instead. The upload can take a few minutes. The progress is shown in the bottom of the file explorer.</p> <p>In order to unzip the file, we need to run the following code in a new code cell in colab:</p> <pre><code># Unzip images to a custom data folder\n!unzip -q /content/annotated.zip -d /content/dataset\n</code></pre> <p>After the unzipping, the <code>annotations</code> folder should be available in the file explorer.</p> Session Storage <p>Files uploaded directly to Colab are temporary and will be deleted when the session ends. Use Google Drive for persistent storage.</p> <p>The second file we need to upload is the <code>config.yaml</code> file. We already created this file in the training chapter. We need to make small changes to the dataset path.</p> <pre><code># Data\npath: '/content/dataset' # path to your project folder\ntrain: images/train # train images (relative to 'path')\nval: images/val # val images (relative to 'path')\n#test: # test images (optional) (relative to 'path')\n\nnc: 2 # number of classes\n\n# Classes\nnames:\n  0: 10euro # Name of the Object\n  1: 5euro\n</code></pre> <p>After uploading both things - the annotations and the config file - we are ready to start training.</p>"},{"location":"yolo/train/colab/#training-in-colab","title":"Training in Colab","text":"<p>Now you can train your model exactly as you would locally:</p> <pre><code>from ultralytics import YOLO\n\n# Load a pre-trained model\nmodel = YOLO('yolov8n.pt')\n\n# Train the model\nresults = model.train(\n    data='/content/config.yaml',\n    epochs=10,\n    device=0  # Explicitly tells YOLO to use the GPU\n)\n</code></pre> Time Consumption <p>Some self-performed tests to train the model for 10 epochs on ~300 images showed the following time consumption: </p> <ul> <li>locally on a CPU (in this case an Intel Core i9-12900): ~8.5 minutes.</li> <li>locally on a GPU (in this case a NVIDIA GeForce RTX 3060): ~1.5 minutes.</li> <li>in Colab on a Tesla T4 GPU: ~1.5 minutes.</li> </ul> <p>What we can see here is that the performance of the training depends massively on the hardware. If you are not in possession of a GPU, it is a good idea to use Colab to train your model.</p>"},{"location":"yolo/train/colab/#working-with-the-results","title":"Working with the results","text":"<p>After the training is finished, we can work with the results just as we did locally. In the Colab file explorer you can see the <code>runs</code> folder with the same results as explained in the training chapter. Everything - including the model weights - can be downloaded by right clicking on the file and selecting \"Download\".</p> <p>How to download all files? You can use the following code in a new code cell in Colab:</p> <pre><code>!zip -r runs.zip runs/detect/trainX\n</code></pre> <p>This will create a zip file of the <code>runs/detect/trainX</code> folder which you can then download by right clicking on the file and selecting \"Download\".</p> <p>Now you are all set! You can download your model and try to start the inference process.</p> \ud83c\udf89 Congratulations <p>You have now trained your own YOLO model on Google Colab. If you do not have a GPU, this is a really good way to speed up the training process.</p> <p> Going Fast Riding Fast GIFfrom Going Fast GIFs \"Hold on tight!\" </p>"},{"location":"yolo/train/inference/","title":"Inference","text":"<pre><code>graph LR\n  A[Data Acquisition]:::active --&gt; B[Annotation];\n  B:::active --&gt; C[Training];\n  C:::active --&gt; D[Inference]:::active;\n  click A \"../acquisition\" _self\n  click B \"../annotation\" _self\n  click C \"../training\" _self\n  click D \"../inference\" _self\n  classDef active fill:#950f42</code></pre> <p>Now that our model is trained and optimized, it\u2019s time to use it for inference, meaning we will test the model by running it on new images and evaluating its real-world performance.</p> <p>Therefore we start by adding a last notebook to our project folder:</p> <pre><code>\ud83d\udcc1 yolo_training/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 rawdata/\n\u251c\u2500\u2500 \ud83d\udcc1 annotations/\n|   \u251c\u2500\u2500 \ud83d\udcc1 images/\n|   |   \u251c\u2500\u2500 \ud83d\udcc1 train/\n|   |   \u2514\u2500\u2500 \ud83d\udcc1 val/\n|   \u2514\u2500\u2500 \ud83d\udcc1 labels/\n|       \u251c\u2500\u2500 \ud83d\udcc1 train/\n|       \u2514\u2500\u2500 \ud83d\udcc1 val/\n\u251c\u2500\u2500 \ud83d\udcc4 config.yaml\n\u251c\u2500\u2500 \ud83d\udcc4 data_acquisition.ipynb\n\u251c\u2500\u2500 \ud83d\udcc4 inference.ipynb\n\u2514\u2500\u2500 \ud83d\udcc4 training.ipynb\n</code></pre>"},{"location":"yolo/train/inference/#loading-the-trained-model","title":"Loading the Trained Model","text":"<p>Before making predictions, we need to load the trained YOLO model. The weights for the best-performing model are stored in the <code>runs/detect/trainX/weights/best.pt</code> file.</p> <p>To load the model in Python we simply run:  </p> <pre><code>model = YOLO(\"./runs/detect/trainX/weights/best.pt\")  # Update with your actual path\n</code></pre> Best vs. Last Model <p>If you want to use the last trained model instead of the best-performing one, replace <code>best.pt</code> with <code>last.pt</code>.</p>"},{"location":"yolo/train/inference/#running-inference-on-images","title":"Running Inference on Images","text":"<p>Now that the model is loaded, we can use this model like we did in the previous chapters.</p> <p></p> <p>As we can see in the gif above, the model is able to detect the 5\u20ac and 10\u20ac bills. Even though it is not perfect (especially when there is a overlap of bills), it is still able to detect the bills in most of the cases.</p> <p>With inference, we can now test our trained YOLO model on new images, videos, and live streams. By adjusting settings like confidence thresholds, image resolution, and hardware acceleration, we can further improve performance and accuracy for real-world applications. \ud83d\ude80</p>"},{"location":"yolo/train/inference/#conclusion","title":"Conclusion","text":"<p>Throughout this guide, we've taken a deep dive into the world of Computer Vision, exploring how YOLO (You Only Look Once) and its related techniques enable fast and accurate object detection. From understanding the fundamentals to training a custom model and deploying it in real-world applications, this journey has equipped you with the knowledge needed to leverage YOLO in various scenarios.</p> \ud83c\udf89 <p>Congratulations! You've completed the our computer vision journey. You should now be able to build your own custom YOLO models for your own projects and use them to detect objects in images, videos, and live streams.</p> <p> </p>"},{"location":"yolo/train/training/","title":"Training","text":"<pre><code>graph LR\n  A[Data Acquisition]:::active --&gt; B[Annotation];\n  B:::active --&gt; C[Training];\n  C:::active --&gt; D[Inference];\n  click A \"../acquisition\" _self\n  click B \"../annotation\" _self\n  click C \"../training\" _self\n  click D \"../inference\" _self\n  classDef active fill:#950f42</code></pre> <p>Now that we have collected and annotated our dataset, it's time to train our very own YOLO model. This chapter will guide you through the complete training process, from setting up the configuration file to interpreting results and fine-tuning the model.  </p>"},{"location":"yolo/train/training/#preparation","title":"Preparation","text":"<p>Before we can start the training of the model, we need to get a few things done. Add the following files and folders to your project folder:</p> <pre><code>\ud83d\udcc1 yolo_training/\n\u251c\u2500\u2500 \ud83d\udcc1 .venv/\n\u251c\u2500\u2500 \ud83d\udcc1 rawdata/\n\u251c\u2500\u2500 \ud83d\udcc1 annotations/\n|   \u251c\u2500\u2500 \ud83d\udcc1 images/\n|   |   \u251c\u2500\u2500 \ud83d\udcc1 train/\n|   |   \u2514\u2500\u2500 \ud83d\udcc1 val/\n|   \u2514\u2500\u2500 \ud83d\udcc1 labels/\n|       \u251c\u2500\u2500 \ud83d\udcc1 train/\n|       \u2514\u2500\u2500 \ud83d\udcc1 val/\n\u251c\u2500\u2500 \ud83d\udcc4 config.yaml\n\u251c\u2500\u2500 \ud83d\udcc4 data_acquisition.ipynb\n\u2514\u2500\u2500 \ud83d\udcc4 training.ipynb\n</code></pre>"},{"location":"yolo/train/training/#dataset-structure","title":"Dataset Structure","text":"<p>In machine learning, datasets \ud83d\udcc2 are divided into:</p> <ul> <li>Training Set (Train) \u2192 Used to teach the model by adjusting its parameters.</li> <li>Validation Set (Val) \u2192 Used to evaluate how well the model is learning.</li> <li>Test Set (Test) \u2192 Used to test the model after training.</li> </ul> <p>Both the training and the validation set are passed to the training algorithm in advance. The model is then trained using the training set and then validated using the validation set.  The results of the validation are then used to optimize the hyperparameters of the model. Furthermore, the results are saved and can be used for further analysis. We will look at this topic in more detail a little later. The test dataset is then used to test the performance of the model in real-world use. In our case, we will not use this data set and instead record new images via webcam to test the performance of our model in the inference chapter.</p> (Source: AlgoTrading101)  <p>A typical split ratio which is used in the machine learning community is 80% for training and 20% for validation. We split our dataset by moving the images and the corresponding labels into the newly created <code>train</code> and <code>val</code> folders.</p> Splitting Images and Labels <p>It is important to note, that the images and labels need to be split in the same way. For example, if you want to use the image <code>image_1.jpg</code> for training, you need to copy the label <code>image_1.txt</code> in the trainging folder as well.</p>"},{"location":"yolo/train/training/#configuration-file","title":"Configuration File","text":"<p>Now that we have the data in the correct structure, we can create a configuration file \ud83d\udcdd that tells YOLO where to find the dataset and how to train the model. This file contains the following information:  </p> <ul> <li>Dataset paths \u2013 Where the images and annotations are stored.  </li> <li>Class labels \u2013 The names of the object categories.  </li> </ul> <p>The easiest way to create the configuration file is to use the before created <code>config.yaml</code> file.</p> <pre><code># Data\npath: C:/path/to/your/yolo_training/annotations # path to your project folder\ntrain: images/train # training images (relative to 'path')\nval: images/val # validation images (relative to 'path')\n#test: # test images (optional) (relative to 'path')\n\nnc: 2 # number of classes\n\n# Classes\nnames:\n  0: Class1 # Name of the Object # (1)!\n  1: Class2\n</code></pre> <ol> <li>The class numbers are defined in the <code>notes.json</code> file from the annotation chapter. The Names (here <code>Class1</code> and <code>Class2</code>) are arbitrary and can be chosen freely.</li> </ol> Task: Preperation <p>Adjust the configuration file for our specific project (Euro Note Detection) and your specific path structure</p>"},{"location":"yolo/train/training/#training-process","title":"Training Process","text":"<p>Once we have our dataset and configuration ready, we can start training our own YOLO model. Therefore we will use the <code>training.ipynb</code> notebook.</p>"},{"location":"yolo/train/training/#running-yolo-training","title":"Running YOLO Training","text":"<p>As with the use of pre-trained models, the training process is very simple and can be carried out with just a few lines of code.</p> <pre><code>from ultralytics import YOLO\n\n# Load the YOLO model as a starting point\nmodel = YOLO(\"yolov8n.pt\")  # Start with a pre-trained YOLOv8 model\n\n# Train the model on the dataset\nresults = model.train(data=\"config.yaml\", epochs=3)\n</code></pre> <p>Training will take some time, depending on the dataset size, the settings and the hardware. So it's time to go for a coffee or tea and come back later.</p> (Source: YourTango)  Training Hardware <p>Training a YOLO model requires a lot of computational resources. The best way to train a computer vision model is to use a GPU. Since a lot of you might work on a laptop without a GPU you can try to train the model on a CPU, but it will take much longer to train the model. If your hardware is limited, it is a good idea to use a free online service like Google Colab or Kaggle to train your model. In the bonus chapter you will find a guide on how to use Colab to train your model.</p>"},{"location":"yolo/train/training/#interpreting-the-output","title":"Interpreting the Output","text":"<p>While the training is running, you can see the progress in the terminal or right under your jupyter notebook code cell. The length of the output is quite long, so we will split it into parts. </p> Example: Training Output 1. YOLO Model and Configuration <p>Software &amp; Hardware Information</p> <pre><code>New https://pypi.org/project/ultralytics/8.3.75 available  Update with 'pip install -U ultralytics'\nUltralytics 8.3.74  Python-3.12.6 torch-2.3.0+cpu CPU (12th Gen Intel Core(TM) i9-12900)\n</code></pre> <ul> <li>A newer YOLO version (8.3.75) is available, but you are using version 8.3.74.</li> <li>Python 3.12.6 and Torch 2.3.0 are being used.</li> <li>The model is running on CPU (Intel Core i9-12900). \ud83d\ude80  </li> </ul> <p>\ud83d\udca1 Tip: If training speed is slow, using a GPU (e.g., NVIDIA CUDA) would significantly improve performance. Therefore you need to install the CUDA driver, update PyTorch (CUDA Version) and change the YOLO settings to use the GPU (<code>device=0</code>).</p> <p>Training Configuration</p> <pre><code>task=detect, mode=train, model=yolo11s.pt, data=config.yaml, epochs=3, [...], batch=16, imgsz=640, save=True\n</code></pre> <ul> <li>task=detect \u2192 This is an object detection task.  </li> <li>model=yolo11s.pt \u2192 Using the YOLO11s model (a lightweight version).  </li> <li>data=config.yaml \u2192 Loading dataset configuration from <code>config.yaml</code>.  </li> <li>epochs=3 \u2192 Training for 3 epochs (a very short training session; just for getting started).  </li> <li>batch=16 \u2192 Processing 16 images per batch (each training step).  </li> <li>imgsz=640 \u2192 Resizing images to 640x640 pixels.  </li> </ul> <p>\u2705 This is a short test run with just 3 epochs. Typically, models need 50+ epochs for better accuracy.</p> <p>Model Architecture and Parameters</p> <pre><code>                  from  n    params  module                                       arguments                     \n  0                 -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                 -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]       \n[...]         \nYOLO11s summary: 319 layers, 9,428,566 parameters, 9,428,550 gradients, 21.6 GFLOPs\n</code></pre> <ul> <li>Detailed information about the model architecture and each layer.</li> <li>319 layers \u2192 The total number of layers in the neural network.</li> <li>9.4 million parameters \u2192 These are the trainable weights of the model.</li> <li>21.6 GFLOPs \u2192 Number of operations the model performs per second.  </li> </ul> <pre><code>Transferred 493/499 items from pretrained weights\nFreezing layer 'model.23.dfl.conv.weight'\n</code></pre> <ul> <li>Pretrained Weights: The model is using a pretrained YOLO11s model and transferring 493 out of 499 weights (If the number of classes in your dataset is different from the number of classes in the pretrained model, not all parameter groups may be transferred).  </li> <li>Layer Freezing: Some layers are frozen (not updated during training), possibly to retain knowledge from the pre-trained model.</li> </ul> 2. Dataset Information <pre><code>train: Scanning [...] 301 images, 25 backgrounds, 0 corrupt\nval: Scanning [...] 74 images, 3 backgrounds, 0 corrupt\n</code></pre> <ul> <li>Training Set: 301 images, with 25 background images (images without objects).  </li> <li>Validation Set: 74 images, with 3 background images.  </li> <li>No corrupt images were found (which is good!). \u2705  </li> </ul> <pre><code>Plotting labels to runs\\detect\\trainX\\labels.jpg...\n</code></pre> <ul> <li>The label distribution is being visualized in a plot (<code>labels.jpg</code>).</li> </ul> <p> </p> 3. Training Process <p>Training Overview</p> <p>For each epoch, the model learns and improves.</p> <pre><code>Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n  1/3         0G       1.03      2.892       1.36         39        640\n  2/3         0G     0.8623      1.327      1.188         47        640\n  3/3         0G     0.7538     0.9084      1.141         42        640\n</code></pre> <ul> <li>Epoch 1/3 \u2192 Epoch 3/3: Training for 3 cycles over the dataset.</li> <li>box_loss (Bounding Box Loss): Started at 1.03 \u2192 Decreased to 0.75 (showing improvement).</li> <li>cls_loss (Classification Loss): Decreased from 2.89 \u2192 0.90 (model is learning to classify objects).</li> <li>dfl_loss (Distribution Focal Loss): Decreased from 1.36 \u2192 1.14 (better bounding box predictions).</li> <li>Instances per batch: Around 39-47 objects detected in each batch of training.</li> </ul> <p>\u2705 Lower loss values = model is learning well.</p> <p>Evaluation on Validation Set (After Each Epoch) </p> Metrics <p>We will look at the metrics in more detail a little later in the corresponding section. So don't worry if you don't understand the values in detail. </p> <pre><code>Class     Images  Instances      Box(P          R      mAP50  mAP50-95)\n  all         74         98      0.707      0.642      0.672      0.468  (Epoch 1)\n  all         74         98      0.463       0.62      0.513      0.266  (Epoch 2)\n  all         74         98      0.914      0.852      0.941      0.717  (Epoch 3)\n</code></pre> <ul> <li>P (Precision): Increased from 0.707 \u2192 0.914 (better at correctly identifying objects).  </li> <li>R (Recall): Increased from 0.642 \u2192 0.852 (better at detecting all objects).  </li> <li>mAP50 (Mean Average Precision @ IoU 0.5): Increased from 0.672 \u2192 0.941 (high accuracy).  </li> <li>mAP50-95 (More strict accuracy metric): Improved to 0.717 (good performance).</li> </ul> <p>\u2705 This shows the model improved significantly by the 3rd epoch.</p> 4. Final Results &amp; Model Saving <p>Training Summary</p> <pre><code>3 epochs completed in 0.088 hours.\nOptimizer stripped from runs\\detect\\trainX\\weights\\last.pt, 19.2MB\nOptimizer stripped from runs\\detect\\trainX\\weights\\best.pt, 19.2MB\n</code></pre> <ul> <li>Training took 0.088 hours (~5 minutes).</li> <li>Saved model weights:<ul> <li><code>last.pt</code> \u2192 Model at the end of training.</li> <li><code>best.pt</code> \u2192 Model with the best performance.</li> </ul> </li> </ul> <p>Running evaluation on the best model</p> Metrics <p>Again, we will talk about the metrics in more detail in the corresponding section.</p> <pre><code>Validating runs\\detect\\trainX\\weights\\best.pt...\n</code></pre> <pre><code> Class     Images  Instances      Box(P          R      mAP50  mAP50-95)\n   all         74         98      0.915      0.856      0.943      0.718\n10euro         48         54      0.977      0.792      0.923      0.706\n 5euro         39         44      0.853       0.92      0.962       0.73\n</code></pre> <p>So, how can we read this? In the following, we will focus on the results of <code>all</code> classes and look at the metrics one by one and interpret them.</p> <ul> <li>Precision (P = 0.915) \u2192 91.5% of detected objects were correct.</li> <li>Recall (R = 0.856) \u2192 85.6% of actual objects were detected.</li> <li>mAP@50 (0.943) \u2192 94.3% accuracy at a basic IoU level (good performance).</li> <li>mAP@50-95 (0.718) \u2192 71.8% accuracy with stricter evaluation.</li> </ul> <p>Class-Specific Performance:</p> <ul> <li>10\u20ac Note: Precision 0.977, Recall 0.792, mAP50 0.923.</li> <li>5\u20ac Note: Precision 0.853, Recall 0.920, mAP50 0.962.</li> </ul> <p>Conclusion: - The model is performing very well! - The 5\u20ac note is slightly easier to detect than the 10\u20ac note (higher recall). - Further fine-tuning could improve results.  </p> 5. Speed Analysis <pre><code>Speed: 0.4ms preprocess, 70.7ms inference, 0.0ms loss, 32.6ms postprocess per image\n</code></pre> <ul> <li>Preprocessing Time: 0.4ms per image.</li> <li>Inference Time: 70.7ms per image (object detection time).</li> <li>Post-processing Time: 32.6ms (final adjustments).</li> </ul> <p>\ud83d\udca1 This means the model takes about ~100ms per image \u2192 ~10 FPS on CPU (slower than GPU).</p>"},{"location":"yolo/train/training/#training-settings","title":"Training Settings","text":"<p>There are many different training settings available to tune the model's learning process. You can explore all options in the YOLO documentation.  </p> <p>Below are some key settings that significantly impact training:  </p> Setting Default Description epochs <code>100</code> Number of times the model will pass through the entire training dataset. More epochs allow the model to learn better but take longer to train. Example: <code>epochs=50</code>. batch <code>16</code> Number of images processed in each training step. A higher batch size can speed up training but requires more memory. Example: <code>batch=16</code>. imgsz <code>640</code> Input image size for training. Larger sizes improve accuracy but increase computation time. Example: <code>imgsz=640</code>. device <code>\"cpu\"</code> Specifies where to run the training. <code>\"cpu\"</code> for a regular processor or <code>0</code> to use a GPU (which is much faster). Example: <code>device=0</code>. <p>\ud83d\udca1 Tip: If training is slow, try reducing <code>imgsz</code> or <code>batch</code>.</p>"},{"location":"yolo/train/training/#detour-metrics","title":"Detour: Metrics","text":"<p>Before fine-tuning our model, we need to understand metrics - the key numbers that tell us how well our model is performing. These metrics help us analyze errors, optimize training, and improve detection accuracy. When training a model, we need to constantly evaluate the performance. This is done by using different metrics. YOLO typically uses the following metrics:</p> Metric Meaning Goal Loss Measures how much the model's predictions differ from the actual labels. Minimize Precision Of all the detections, how many were correct? Maximize Recall Of all the objects that should have been detected, how many were found? Maximize mAP (mean Average Precision) A combined measure of precision &amp; recall over all object classes. YOLO will calculate different mAP (@50, @50-95, @75, @95) for different IoU thresholds. Maximize <p>The metrics and the combination of them are crucial to understand the performance of the model. Especially the mAP is a good indicator for the performance of an object detection model.</p> Combined Interpretation of Metrics <ul> <li>High Precision, Low Recall: Model is careful but misses some objects.  </li> <li>High Recall, Low Precision: Model detects many objects but makes mistakes.  </li> </ul> (Source: Wikipedia by Walber)  <p>\ud83d\udca1 A good YOLO model should have high precision, recall, and mAP.</p>"},{"location":"yolo/train/training/#training-results","title":"Training Results","text":"<p>Now that we have a good understanding of YOLO metrics, let\u2019s dive deeper into how to interpret the training results. After each training run, YOLO creates a folder inside <code>runs\\detect</code> (e.g., <code>train</code>). This folder contains multiple files and visualizations that help us analyze the performance of our model.</p>"},{"location":"yolo/train/training/#model-weights","title":"Model Weights","text":"<p>The <code>weights</code> folder contains the saved model weights from training. </p> <ul> <li><code>best.pt</code>: The best model based on validation performance. (Model with highest accuracy)</li> <li><code>last.pt</code>: The final model at the end of training. (most recently trained model = last epoch)</li> </ul>"},{"location":"yolo/train/training/#confusion-matrix","title":"Confusion Matrix","text":"<p>\ud83d\udd0e What does it show? </p> <ul> <li>The confusion matrix (<code>confusion_matrix_normalized.png</code> or <code>confusion_matrix.png</code>) helps us understand misclassifications by showing the relationship between actual and predicted classes.  </li> <li>Each column represents the true class, and each row represents the predicted class.  </li> <li>A perfect model would have all values on the diagonal (everything correctly classified).  </li> <li>If there are many off-diagonal values, the model is making classification errors (e.g., mistaking <code>5\u20ac</code> for <code>10\u20ac</code>).</li> </ul> <ul> <li> We want: <ul> <li>High values on the diagonal = Good model performance.</li> </ul> </li> <li> We don't want: <ul> <li>Many off-diagonal values = The model is confusing some classes. Consider improving data quality or fine-tuning.</li> </ul> </li> </ul>"},{"location":"yolo/train/training/#label-distribution","title":"Label Distribution","text":"<p>\ud83d\udd0e What does it show?</p> <p>The image <code>labels.jpg</code> shows:</p> <ul> <li>Upper left: The distribution of object labels in the dataset. How often each class appears.</li> <li>Upper right: Overlay of all bounding boxes on the image.</li> <li>Lower left: 3D Histogram of bounding box center coordinates.</li> <li>Lower right: 3D Histogram of bounding box width and height.</li> </ul> <ul> <li> We want: <ul> <li>Balanced dataset = The model can learn all classes equally well.  </li> </ul> </li> <li> We don't want: <ul> <li>Some classes appear much less frequently = Model may struggle with those objects.</li> </ul> </li> </ul>"},{"location":"yolo/train/training/#training-batch","title":"Training Batch","text":"<p>\ud83d\udd0e What does it show? </p> <p>The image <code>train_batch0.jpg</code> represents an input example and shows:</p> <ul> <li>Sample images from the training dataset, including applied augmentations like rotation, scaling, and flipping.  </li> <li>This helps the model generalize better and avoid overfitting.  </li> </ul> <ul> <li> We want: <ul> <li>If bounding boxes are correct, the annotations are likely fine.    </li> </ul> </li> <li> We don't want: <ul> <li>If bounding boxes look incorrect or missing, check your annotations.  </li> </ul> </li> </ul>"},{"location":"yolo/train/training/#validation-predictions","title":"Validation Predictions","text":"Ground Truth (Labels) Model Predictions <p>\ud83d\udd0e What do they show? </p> <ul> <li>Left (<code>val_batch0_labels.jpg</code>): The correct bounding boxes (ground truth labels).  </li> <li>Right (<code>val_batch0_pred.jpg</code>): The model\u2019s predictions after training.  </li> </ul> <ul> <li> We want: <ul> <li>Bounding boxes match well = Model is learning correctly.  </li> </ul> </li> <li> We don't want: <ul> <li>Bounding boxes are missing or wrong = Model might need more training or data improvements.  </li> </ul> </li> </ul>"},{"location":"yolo/train/training/#resulting-metrics","title":"Resulting Metrics","text":"<p>The <code>results.png</code> file tracks key metrics across all epochs (training cycles). This image is very useful to understand the performance of the model over time and can be used to define the further training process.   </p> <p>\ud83d\udd0e What does it show? </p> Training &amp; Validation Losses <ul> <li><code>train/box_loss</code> &amp; <code>val/box_loss</code>: Measures how accurately the model predicts object bounding boxes. A decreasing trend indicates improved localization.  </li> <li><code>train/cls_loss</code> &amp; <code>val/cls_loss</code>: Represents the classification loss, showing how well the model differentiates between object classes. A lower value suggests better class predictions.  </li> <li><code>train/dfl_loss</code> &amp; <code>val/dfl_loss</code>: The distribution focal loss helps refine bounding box predictions. A decreasing loss means improved precision in object localization. </li> </ul> Metrics <ul> <li><code>metrics/precision(B)</code>: Precision measures the proportion of correctly predicted objects among all detections. A higher value means fewer false positives.  </li> <li><code>metrics/recall(B)</code>: Recall indicates how many actual objects were detected. A higher recall means fewer false negatives.  </li> <li><code>metrics/mAP50(B)</code>: The mean Average Precision at IoU 0.5 evaluates overall detection performance. A higher value reflects better accuracy.  </li> <li><code>metrics/mAP50-95(B)</code>: The mAP averaged over multiple IoU thresholds (0.50 to 0.95) provides a more comprehensive evaluation of the model\u2019s performance.</li> </ul> <ul> <li> We want: <ul> <li>Loss decreases steadily = The model is learning well.</li> <li>Metrics increase = The model is detecting more objects correctly.</li> </ul> </li> <li> We don't want: <ul> <li>Loss stays high or fluctuates a lot = Model might be struggling (check learning rate or dataset quality).</li> <li>Metrics stay low = The model is not learning well.</li> </ul> </li> </ul>"},{"location":"yolo/train/training/#interpreting-the-results","title":"Interpreting the Results","text":"<p>So let's get back to our Euro note  example and interpret the results. In the <code>results.png</code> file, we can analyze the before described metrics and derive the following observations:</p> Observations <ol> <li> <p>Training Losses (Box, Class, DFL) are Decreasing </p> <ul> <li>The box loss, classification loss, and DFL loss are consistently decreasing, indicating that the model is learning effectively and improving its predictions.  </li> <li>\u2705 This is a good sign - it means the model is adjusting weights correctly and optimizing performance.  </li> </ul> </li> <li> <p>Validation Losses (Box, Class, DFL) are Fluctuating </p> <ul> <li>The validation losses increase in epoch 2 but then decrease in epoch 3.  </li> <li>\u26a0\ufe0f This could indicate some instability in training, possibly due to a small dataset or high variance in validation samples.  </li> <li>Further monitoring is required to ensure stability in later epochs.  </li> </ul> </li> <li> <p>Precision and Recall are Inconsistent </p> <ul> <li>The precision and recall values drop in epoch 2 but recover in epoch 3.  </li> <li>This fluctuation suggests that the model might still be adjusting to the data distribution.  </li> <li>\u26a0\ufe0f If these values remain unstable in later epochs, you may need to fine-tune hyperparameters or use more training data.  </li> </ul> </li> <li> <p>mAP50 and mAP50-95 Initially Drop but Recover </p> <ul> <li>The mAP metrics (mean Average Precision) drop in epoch 2 and then increase significantly in epoch 3.  </li> <li>\u2705 This suggests that while there were performance fluctuations, the model eventually improved its object detection accuracy.  </li> </ul> </li> </ol> <p>So what should we do now? </p> <p>Since the model is still fluctuating, we should continue training for more epochs (10+), monitor validation loss closely, and adjust the learning rate if needed. Keep an eye on precision/recall stability!</p>"},{"location":"yolo/train/training/#fine-tuning-the-model","title":"Fine-Tuning the Model","text":"<p>To do so, we do not need to train the model from scratch again. We can continue training the model from the last checkpoint.</p> <pre><code># Load the already trained model\nmodel = YOLO('./runs/detect/trainX/weights/last.pt')  # (2)!\n\n# Resume training\nmodel.train(data = 'config.yaml', epochs = 50) #(1)!\n</code></pre> <ol> <li>We now continue training the model for 50 more epochs.</li> <li>In this case, we load the last model from the <code>trainX</code> folder.</li> </ol> <p>After some time (or a lot of time depending on your hardware \ud83d\ude48) our results should look like this: </p> <p>This training run is significantly better than the previous one because:  </p> <ul> <li>Losses are decreasing smoothly with no major fluctuations.</li> <li>Validation loss is following training loss, meaning the model is generalizing well.</li> <li>Precision, recall, and mAP are increasing steadily and stabilizing near 1.0, showing strong detection capabilities.</li> </ul> Monitor for overfitting <p>If validation loss starts increasing while training loss keeps decreasing, it might be overfitting. Consider adding data augmentation or early stopping if needed. Additionally, YOLO offers the training paramter <code>patience</code> to stop the training if the validation loss does not improve after a certain number of epochs (default is 100).</p> Things to Try <ul> <li> <p>Increase the number of epochs:</p> <ul> <li>Try training for <code>100+</code> epochs instead of <code>50</code>.    <pre><code>model.train(data=\"config.yaml\", epochs=100)\n</code></pre></li> </ul> </li> <li> <p>Use a larger YOLO model:</p> <ul> <li>Instead of <code>yolo11s.pt</code> (small model), try <code>yolo11m.pt</code> or <code>yolo11l.pt</code> (medium/large models).   <pre><code>model = YOLO(\"yolo11m.pt\")  # Medium model\n</code></pre></li> </ul> </li> <li> <p>Train on a larger dataset:</p> <ul> <li>More labeled images can significantly boost performance.</li> <li>Try adding more angles, backgrounds, and lighting conditions.</li> </ul> </li> </ul>"},{"location":"yolo/train/training/#next-steps","title":"Next Steps","text":"<p>As already mentioned in the training results section, the best and the final model are saved in the <code>best.pt</code> and <code>last.pt</code> files. We will use those models now for the inference and a real-world evaluation.</p>"},{"location":"yolo/video/","title":"Analysing Videos","text":"(Source: Medium)  <p>Moving from static images to videos, YOLO demonstrates its real power. With its high-speed analysis, YOLO can process each video frame individually and provide real-time insights. Whether detecting objects, segmenting images, or estimating poses, the procedure mirrors that used for static images but adds the dimension of handling sequential frames.</p>"},{"location":"yolo/video/#project-setup","title":"Project Setup","text":"<p>To start working with video analysis, we'll extend our previous project structure and create a new Jupyter notebook <code>yolo_video.ipynb</code>: <pre><code>\ud83d\udcc1 computer_vision/\n    \u251c\u2500\u2500 \ud83d\udcc1 .venv/\n    \u251c\u2500\u2500 \ud83d\udcc1 pics/\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_detect.ipynb\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_segment.ipynb\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_keypoints.ipynb\n    \u2514\u2500\u2500 \ud83d\udcc4 yolo_video.ipynb\n</code></pre> Ensure your virtual environment (<code>.venv</code>) is active and that all necessary packages, including <code>ultralytics</code> and <code>opencv-python</code>, are installed.</p> OpenCV Installation <p>OpenCV is one of the few Python packages that have different names when installed and integrated into our notebook. The installation is carried out using:</p> <pre><code>pip install opencv-python\n</code></pre>"},{"location":"yolo/video/#capture-video-stream","title":"Capture Video Stream","text":"<p>To analyze video data, whether from a webcam or a saved file, we leverage the Python package OpenCV (<code>cv2</code>). Let's begin with a program to access your web camera and display its live feed.</p>"},{"location":"yolo/video/#step-1-import-the-library","title":"Step 1: Import the Library","text":"<p>To use OpenCV, start by importing the library:</p> <p><pre><code>import cv2\n</code></pre> This statement includes the OpenCV library in our program, giving us access to its methods and properties.</p>"},{"location":"yolo/video/#step-2-create-a-videocapture-object","title":"Step 2: Create a VideoCapture Object","text":"<p>In OpenCV, the <code>VideoCapture()</code> method allows us to capture the video stream from our webcam:</p> <p><pre><code>cap = cv2.VideoCapture(0)\n</code></pre> The argument <code>0</code> refers to the first camera connected to the device. If additional cameras are connected, you can use <code>1</code>, <code>2</code>, etc.</p> Saved Video <p>You can also open videos from your hard drive or from a website like Youtube . Simply enter the path to your video instead of <code>0</code>. Don't forget to read the docs.</p>"},{"location":"yolo/video/#step-3-read-frames","title":"Step 3: Read Frames","text":"<p>The <code>read()</code> method of the <code>VideoCapture</code> object retrieves each frame from the video stream:</p> <pre><code>ret, frame = cap.read()\n</code></pre> <ul> <li><code>ret</code>: Boolean indicating if the frame was captured successfully.</li> <li><code>frame</code>: The captured frame as a NumPy array.</li> </ul>"},{"location":"yolo/video/#step-4-display-frames","title":"Step 4: Display Frames","text":"<p>To display the captured frames in a window, use the <code>imshow()</code> method:</p> <p><pre><code>cv2.imshow('Capturing Video', frame)\n</code></pre> The first argument is the window title, and the second argument is the frame to display.</p>"},{"location":"yolo/video/#step-5-loop-and-exit","title":"Step 5: Loop and Exit","text":"<p>To continuously capture frames, use a <code>while</code> loop and break it based on user input. Use <code>cv2.waitKey()</code> to listen for key presses:</p> <p><pre><code>if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n    break\n</code></pre> This stops the loop when the <code>q</code> key is pressed.</p>"},{"location":"yolo/video/#step-6-release-resources","title":"Step 6: Release Resources","text":"<p>Release the video stream and close any OpenCV windows:</p> <pre><code>cap.release() # release the resource\ncv2.destroyAllWindows()     # closes all OpenCV windows\n</code></pre> Release Resources <p>Releasing resources at the end is crucial to avoid issues. If the resource (e.g., webcam) is not released, it may remain locked, preventing further connections. This issue can occur if an error interrupts your code, skipping the release command. In such cases, manually execute the release method before attempting to use the webcam again. Alternatively, restarting the kernel can also resolve the issue:</p>"},{"location":"yolo/video/#complete-program","title":"Complete Program","text":"<p>Here\u2019s the complete program to access your webcam and show the live feed. </p> <pre><code>import cv2\n\n# Define the video source (0 for webcam or path to a video file)\nvideo_source = 0  # Use \"video.mp4\" for a saved video\ncap = cv2.VideoCapture(video_source)\n\n# Process the video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Display the  frame\n    cv2.imshow(\"Video Analysis\", frame)\n\n    # Exit when 'q' is pressed\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\n# Release resources\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"yolo/video/#inference","title":"Inference","text":"<p>YOLO processes each video frame independently, making it suitable for real-time applications like detection, segmentation, or keypoint extraction. To use any of these vision algorithms, simply analyze each frame in sequence and display the annotated frame.</p> <pre><code>results = model(frame)\nannotated_frame = results[0].plot()\n</code></pre> Task: Analyze a Video <ol> <li>Try to access the webcam and display the live video</li> <li>Use the YOLOv8 Nano model to perform a:<ul> <li>Detection</li> <li>Segmentation</li> <li>Pose Estimation</li> <li>Tracking </li> </ul> </li> <li>Visualize and save results for further analysis.</li> </ol>"},{"location":"yolo/video/solutions/","title":"YOLO Solutions","text":"Level up your computer vision skills with YOLO solutions! \ud83d\udd0d (Source: Ultralytics)  Ultralytics Solutions <p>This chapter introduces Ultralytics Solutions, a collection of  ready-to-use applications built on top of YOLO models. These solutions make it easier to implement  common computer vision tasks without extensive customization.</p> <p>We'll explore practical applications and create our first object counting system.</p>"},{"location":"yolo/video/solutions/#project-setup","title":"Project Setup","text":"<p>If you've followed the previous chapters, your project structure should look like this:</p> <pre><code>\ud83d\udcc1 computer_vision/\n    \u251c\u2500\u2500 \ud83d\udcc1 .venv/\n    \u251c\u2500\u2500 \ud83d\udcc1 pics/\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_detect.ipynb\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_segment.ipynb\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_keypoints.ipynb\n    \u251c\u2500\u2500 \ud83d\udcc4 yolo_video.ipynb\n    \u2514\u2500\u2500 \ud83d\udcc4 yolo_object_counting.ipynb\n</code></pre> <p>For this section, we added a new Jupyter notebook named <code>yolo_object_counting.ipynb</code> within the project to follow along.</p>"},{"location":"yolo/video/solutions/#object-counting","title":"Object Counting","text":"<p>One of the most practical applications of computer vision is counting objects in specific regions. This could be:</p> <ul> <li>Counting vehicles in traffic lanes</li> <li>Monitoring people in store sections</li> <li>Tracking inventory movement</li> <li>Analyzing crowd density in areas</li> </ul> <p>Let's implement a basic object counting system using YOLO and explore how to customize it for different scenarios.</p>"},{"location":"yolo/video/solutions/#basic-setup","title":"Basic Setup","text":"<p>Before we start counting, we need to define where we want to count objects. Therefore we need to know the frame size of our webcam or video. </p> <pre><code>import cv2\nfrom ultralytics import solutions\n\n# Define video source (0 for webcam)\nvideo_source = 0  # Change to video path for file\ncap = cv2.VideoCapture(video_source)\n\n# Get video properties\nw, h, fps = (int(cap.get(x)) for x in (\n    cv2.CAP_PROP_FRAME_WIDTH, \n    cv2.CAP_PROP_FRAME_HEIGHT, \n    cv2.CAP_PROP_FPS\n))\n\nprint(f\"Video properties: {w}x{h} @ {fps}fps\")\n</code></pre> &gt;&gt;&gt; Output<pre><code>Video properties: 640x480 @ 30fps\n</code></pre>"},{"location":"yolo/video/solutions/#define-region-of-interest","title":"Define Region of Interest","text":"<p>Now that we know the frame size, we can define a 'region of interest'. We start with a generic rectangle which has a distance of 20 pixels to all sides.</p> <pre><code># Define counting region (rectangle)\nregion_points = [\n    (20, h-20),     # Bottom left\n    (w-20, h-20),   # Bottom right\n    (w-20, 20),     # Top right\n    (20, 20)        # Top left\n]\n</code></pre>"},{"location":"yolo/video/solutions/#initialize-counter","title":"Initialize Counter","text":"<p>Now we'll set up the YOLO-based counter:</p> <pre><code># Initialize RegionCounter\ncounter = solutions.RegionCounter(\n    show=False,            # Show visualization\n    region=region_points,  # Our defined region\n    model=\"yolo11n.pt\",    # Use nano model for speed\n    classes=[0]            # Only count persons (class 0)\n)\n</code></pre>"},{"location":"yolo/video/solutions/#process-video","title":"Process Video","text":"<p>Let's create the main processing loop:</p> <pre><code>while cap.isOpened():\n    success, frame = cap.read()\n    if not success:\n        break\n\n    # Process frame and count objects\n    countresult = counter.process(frame)\n    annotated_frame = countresult.plot_im\n    # Display results\n    cv2.imshow(\"Object Counting\", annotated_frame)\n\n    # Press 'q' to quit\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> What Counts to the Region? <p>If you run the code, you will see that YOLO counts the people in the region of interest. Only objects whose center point of the bounding box lies within the region are counted.</p>"},{"location":"yolo/video/solutions/#save-results","title":"Save Results","text":"<p>To save your counting results for later analysis you need to add the following lines</p> <ul> <li>Before the Main Loop:      <pre><code># Create video writer\noutput_path = 'counting_results.mp4'\nwriter = cv2.VideoWriter(\n    output_path,\n    cv2.VideoWriter_fourcc(*'mp4v'),\n    fps,\n    (w, h)\n)\n</code></pre></li> <li>In the Main Loop:     <pre><code>writer.write(annotated_frame)\n</code></pre></li> <li>After the Main Loop     <pre><code>writer.release()\n</code></pre></li> </ul> Experiment with Different Settings <p>Perform an object counting in region with the above code by accessing your webcam.  Try these modifications to enhance your counter:</p> <ol> <li> <p>Change the size of the region.</p> </li> <li> <p>Change the counting region shape: <pre><code># Triangle region\nregion_points = [\n    (w//2, 100),    # Top\n    (50, h-100),    # Bottom left\n    (w-50, h-100)   # Bottom right\n]\n</code></pre></p> </li> <li> <p>Count different objects: <pre><code># Count multiple classes\ncounter = solutions.RegionCounter(\n    classes=[0, 2, 3]  # Person, car, motorcycle\n)\n</code></pre></p> </li> <li>Save the annotated video with object counts for later analysis.</li> </ol> <p>What other modifications could make this more useful for your needs?</p>"},{"location":"yolo/video/solutions/#recap","title":"Recap","text":"<p>In this chapter, we've learned how to:</p> <ul> <li>Set up a basic object counting system</li> <li>Define custom regions of interest</li> <li>Process video streams in real-time</li> <li>Save and analyze counting results</li> </ul> <p>Next, we'll explore how we can train our own models. </p> (Source: Medium)"},{"location":"blog/archive/2025/","title":"2025","text":""}]}